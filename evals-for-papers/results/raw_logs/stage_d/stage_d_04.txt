Below is a concrete ablation plan focused on FC3 (Task Verification) in multi-agent LLM systems, with a suitable system/benchmark, minimal modifications to isolate the verifier’s contribution, the FC3-aligned failure modes to tag, and the metrics to track.

Recommended system and benchmark
- System: ChatDev (lightweight, open-source “software company” multi-agent pipeline; easy to add/remove a Verifier role) or MetaGPT (more modular, role-based pipelines). Use whichever you already have integrated; both allow inserting a Verifier between Implementer and Merger/Reviewer with minimal code.
- Benchmark:
  - Primary: SWE-bench Lite or SWE-bench (if infra allows) to exploit ground-truth unit tests for an objective oracle of “task completed.” This directly supports measuring verification false accepts/rejects. [SWE-bench: Can Language Models Resolve Real-World GitHub Issues?, 2023]
  - Secondary: HumanEval/MBPP variants for smaller-scale checks; use embedded unit tests as the oracle.

Why these choices: They let the Verifier agent check concrete acceptance criteria (unit tests/issue reproduction), making FC3 failures observable and quantifiable. The MST paper emphasizes that multi-agent systems often underperform expectations and require principled failure analysis and evaluation beyond headline success rates [P1].

Minimal modifications for verifier ablation
Implement these as toggles in the same pipeline to keep all other factors fixed (same prompts, same tools, same seeds/temperatures, identical Implementer/Reviewer roles):

A. Baseline (Explicit Verifier)
- Verifier has access to: task spec, diffs, logs, and test runner; it can request revisions and decide acceptance/termination.

B. No-Verifier (Removal)
- Remove the Verifier role; Implementer hands off directly to Reviewer/Merger who must accept/terminate. All prompts, tools, and temperatures unchanged.

C. Tool-blinded Verifier (Access ablation)
- Keep Verifier but revoke tool access (no tests/commands). It must verify from text/spec only. This isolates the value of external tools in verification.

D. Silent Verifier (Decision ablation)
- Verifier can comment but cannot gate acceptance; Reviewer/Merger must ignore its accept/reject signal. Tests whether mere critique (without authority) changes outcomes.

E. Self-check Instead of Verifier (Role conflation ablation)
- Replace Verifier with Implementer self-critique (one extra self-review turn) at fixed budget. Tests separation-of-duties benefit versus single-agent self-check.

Keep: fixed number of iterations, identical routing, identical tool set for non-ablated roles, same temperature and max tokens, same seeds, same test-time budget.

FC3 failure modes to tag during runs
Define operational tags aligned to FC3; if you are mapping to MST labels, use your paper’s exact FM-3.x names/IDs in your logging schema:

- FM-3.1 Mis-specified acceptance criteria: Verifier applies criteria misaligned with the task or benchmark oracle (e.g., treating style warnings as blocking).
- FM-3.2 False acceptance (false positive verification): Verifier accepts when the oracle (unit tests/issue status) is failing.
- FM-3.3 False rejection (false negative verification): Verifier rejects when the oracle is passing.
- FM-3.4 Non-actionable verification feedback: Verifier requests changes without concrete, test-linked guidance; leads to churn.
- FM-3.5 Inconsistent criteria across turns: Verifier applies inconsistent standards between iterations.
- FM-3.6 Tool-misuse in verification: Verifier fails to run tests, misreads logs, or misinterprets pass/fail signals.

Note: If the MST paper provides precise names for FM-3.2/FM-3.3, align the above “false acceptance/rejection” to those exact definitions; confirm the mapping in the paper’s failure taxonomy table [P1].

Primary performance metrics
Task-level outcomes
- Success rate (pass@1) on oracle tests.
- Time-to-success (iterations to first passing solution).
- Token/compute cost to success.

Verifier-centric metrics (use the oracle as ground truth)
- Verification precision = accepted solutions that truly pass / all accepted.
- Verification recall = truly passing solutions that get accepted / all truly passing.
- False acceptance rate (FAR) = FM-3.2 frequency.
- False rejection rate (FRR) = FM-3.3 frequency.
- Calibration: Expected Calibration Error (ECE) between verifier confidence and correctness; reliability curve.

Collaboration/process metrics
- Number of rework cycles triggered by Verifier.
- Share of non-actionable feedback (FM-3.4).
- Tool-use success for verification (rate of correctly executed tests vs tool errors; FM-3.6).
- Stopping-condition accuracy: fraction of episodes where “termination” coincides with oracle success.

Ablation experiments
Run all experiments on the same task set with paired evaluation (per-task comparisons) to enable stronger statistics.

1) With vs Without Verifier (effect of explicit verification)
- Conditions: Baseline vs No-Verifier.
- Hypothesis: The explicit Verifier reduces FAR (FM-3.2) and increases success rate on SWE-bench(-Lite); effect size depends on test access and critique quality. Conjecture, to be tested [P1].
- Metrics: Success rate, FAR/FRR, verification precision/recall, time-to-success, token cost.
- Analysis: Paired McNemar test for success differences; bootstrap CIs for FAR/FRR deltas.

2) Tool Access Ablation (value of external tools in verification)
- Conditions: Baseline vs Tool-blinded Verifier.
- Hypothesis: Removing test access increases FAR/FRR and reduces verification precision; more non-actionable feedback (FM-3.4) as the Verifier guesses from text. Conjecture.
- Metrics: Verification precision/recall, FAR/FRR, tool-misuse (should drop to zero by design), success rate.
- Analysis: Compare precision/recall; report delta success rate and churn (rework cycles).

3) Authority vs Advice (does gating power matter?)
- Conditions: Baseline vs Silent Verifier vs Self-check Instead of Verifier.
- Hypothesis: A Verifier with accept/reject authority outperforms an advisory-only Verifier, and both outperform a self-check Implementer on false acceptance control (FM-3.2). Conjecture consistent with the need for principled verification in MAS [P1].
- Metrics: FAR/FRR, success rate, rework cycles, stopping-condition accuracy.
- Analysis: Trade-off plots of FAR vs success; measure whether authority reduces premature termination on failing outputs.

Optional extensions (if budget allows)
- Strictness sweep: vary Verifier temperature and “strictness” prompt; trace ROC-like curve between FRR and FAR to identify an operating point.
- Replace Verifier model: same prompt/tools with a stronger/weaker LLM to isolate model quality effects on verification.
- Reviewer-Only vs Verifier-Only: to separate code-quality review from acceptance verification.

Methodological controls
- Use the same seeds, temperatures, and tool versions; log every tool call and test result.
- Pre-register stopping criteria and iteration budget.
- Use paired evaluation and report CIs; test for significance (e.g., McNemar for paired binary success; bootstrap for rate differences).
- Release a failure-mode–tagged trace log to allow external auditing of FC3 labels [P1].

Practical notes
- Prefer SWE-bench Lite initially to keep infra manageable; scale up once instrumentation is stable.
- Add one thin “Verification API” boundary in the codebase so you can flip between Baseline/ablations without touching other roles.
- Implement automatic FM tagging where possible:
  - FM-3.2/3.3 are automatable via oracle vs Verifier decision.
  - FM-3.6 from tool stderr/exit codes.
  - FM-3.4 via a simple rubric classifier; validate with spot human checks.

Anchors and context
- MST context: Multi-agent systems often deliver modest gains over single-agent baselines and benefit from failure-mode–aware evaluation; FC3-style verification is a key locus of error to diagnose [P1].
- Domain-specific auditing work similarly argues for moving beyond final-answer accuracy to diagnose collaborative failure modes, including verification steps [P2].

Citations
- [P1] Why Do Multi-Agent LLM Systems Fail? — http://arxiv.org/abs/2503.13657v2
- [P2] MedAgentAudit: Diagnosing and Quantifying Collaborative Failure Modes in Medical Multi-Agent Systems — http://arxiv.org/abs/2510.10185v1
- MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework — http://arxiv.org/abs/2308.00352v7
- SWE-bench: Can Language Models Resolve Real-World GitHub Issues? — http://arxiv.org/abs/2310.06770v3
- ChatDev (repository) — https://github.com/OpenBMB/ChatDev

Key takeaways
- Use a role-based system (ChatDev/MetaGPT) plus SWE-bench(-Lite) to make verification observable via unit-test oracles.
- Ablate the Verifier’s presence, tool access, and authority to isolate its function.
- Track FAR/FRR (FM-3.2/FM-3.3), verification precision/recall, stopping-condition accuracy, and process costs.
- Use paired designs and CIs; publish labeled traces so others can inspect FC3 failures.