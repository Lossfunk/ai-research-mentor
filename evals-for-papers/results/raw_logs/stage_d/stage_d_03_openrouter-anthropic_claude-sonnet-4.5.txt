Based on the attached paper, I'll design a comprehensive efficiency evaluation extension that maintains comparability with the original accuracy-focused results while enabling rigorous analysis of accuracy-efficiency trade-offs.

## Extended Efficiency Metrics

### 1. Latency Metrics

**Rollout Latency** [heirarchical-transformer.pdf:3]
- **Single-step latency (μs)**: Time to predict one future state given current state
  - Measured from token input to token output
  - Excludes VQ-VAE encoding/decoding (isolates world model)
  - Report: mean, p50, p95, p99 across 10,000 rollout steps
  
- **K-step rollout latency (ms)**: Time for full imagination trajectory
  - K ∈ {5, 10, 15} steps (matching reconstruction checkpoints [heirarchical-transformer.pdf:3])
  - Includes all masking decisions and memory token updates
  - Critical for policy planning latency

- **End-to-end inference latency (ms)**: Full pipeline from raw observation to action
  - VQ-VAE encoding + world model rollout + policy network
  - Real-world deployment metric
  - Measured at 60 FPS target (16.67ms budget)

**Training Latency**
- **Steps per second**: Environment steps processed during training
- **Time to threshold**: Wall-clock hours to reach 50% of final performance
- **Gradient computation time**: Backward pass latency per batch

### 2. Memory Metrics

**Peak Memory Usage (GB)**
- **Model parameters**: Static memory for weights
  - Transformer: embedding + attention + FFN layers
  - VQ-VAE: encoder + decoder + codebook
  - SPARTAN: causal graph network [heirarchical-transformer.pdf:2]
  
- **Activation memory**: Dynamic memory during forward pass
  - Attention maps: O(L² × H × B) for sequence length L, heads H, batch B
  - KV cache for autoregressive rollout
  - Intermediate activations

- **Rollout buffer**: Memory for K-step imagination
  - Token sequences: K × L × B × embedding_dim
  - Hidden states: K × B × hidden_dim
  - Masked token storage

**Memory Efficiency Ratios**
- **Memory per token**: Peak memory / active tokens
- **Sparsity benefit**: (Dense memory - Sparse memory) / Dense memory
- **Batch scaling**: Memory growth rate as batch size increases

### 3. Throughput Metrics

**Training Throughput**
- **Samples per second**: Environment transitions processed
- **Tokens per second**: Discrete latent tokens processed
- **GPU utilization (%)**: Actual compute usage
- **Batch efficiency**: Throughput / theoretical peak

**Inference Throughput**
- **Rollouts per second**: Parallel imagination trajectories
- **Actions per second**: Policy decisions in deployment
- **Multi-environment scaling**: Throughput with N parallel environments

### 4. Computational Cost Metrics [heirarchical-transformer.pdf:3]

**FLOPs Analysis**
- **Rollout FLOPs**: Total floating-point operations per K-step rollout
  - Attention: 2 × L² × d_model per layer
  - FFN: 4 × L × d_model × d_ff per layer
  - Masking overhead: SPARTAN + uncertainty computation
  
- **Training FLOPs**: Total FLOPs to convergence
- **FLOPs reduction**: (Baseline FLOPs - SHI FLOPs) / Baseline FLOPs

**Token Efficiency**
- **Active tokens per step**: Mean number of unmasked tokens
- **Token retention rate**: Active tokens / total tokens
- **Hierarchical sparsity**: Retention rate per hierarchy level

### 5. Energy Metrics

**Power Consumption**
- **GPU power draw (W)**: Measured via nvidia-smi during rollout
- **Energy per rollout (J)**: Power × latency
- **Training energy (kWh)**: Total energy to 100k steps

**Carbon Footprint** (optional but recommended)
- **CO₂ equivalent (kg)**: Using regional grid carbon intensity
- **Efficiency score**: Performance / energy

## Measurement Protocol

### Hardware Standardization

**Primary Platform:**
- GPU: NVIDIA A100 40GB (PCIe)
- CPU: AMD EPYC 7742 (16 cores allocated)
- RAM: 128GB DDR4
- CUDA: 11.8, PyTorch 2.0
- Batch size: 64 (fixed across all methods)

**Secondary Platform (for generalization):**
- GPU: NVIDIA RTX 3090 24GB
- Same software stack
- Tests deployment on consumer hardware

**Measurement Tools:**
- Latency: PyTorch CUDA events (microsecond precision)
- Memory: torch.cuda.max_memory_allocated()
- FLOPs: fvcore.nn.FlopCountAnalysis
- Power: nvidia-smi --query-gpu=power.draw (100ms polling)
- Profiling: PyTorch Profiler with CUPTI

### Measurement Procedure

**Warmup Protocol:**
1. Run 100 dummy rollouts to warm up GPU
2. Clear CUDA cache
3. Reset peak memory counters
4. Synchronize CUDA streams

**Latency Measurement:**
```python
# Pseudocode
torch.cuda.synchronize()
start_event = torch.cuda.Event(enable_timing=True)
end_event = torch.cuda.Event(enable_timing=True)

start_event.record()
output = model.rollout(input, k_steps=15)
end_event.record()

torch.cuda.synchronize()
latency_ms = start_event.elapsed_time(end_event)
```

**Repeated Measurements:**
- 1,000 iterations per configuration
- Report median (robust to outliers) and 95% CI via bootstrap
- Discard first 10% (warmup effects)

**Controlled Conditions:**
- Disable GPU boost clocks (fixed frequency)
- Exclusive GPU access (no concurrent jobs)
- Fixed random seeds for reproducibility
- Same input sequences across methods

## Preserving Comparability to Original Results

### Accuracy Metrics (Unchanged) [heirarchical-transformer.pdf:3]

**Primary:**
- Final median human-normalized score (Atari 100k, Crafter)
- Rollout error at checkpoints {t₅, t₁₀, t₁₅}

**Secondary:**
- Sample efficiency curves
- Reward prediction error

### Comparability Requirements

**1. Identical Training Setup**
- Same hyperparameters as accuracy-only evaluation
- Same random seeds (enables paired analysis)
- Same number of environment steps
- Same evaluation frequency

**2. Non-Invasive Measurement**
- Efficiency metrics collected **without** modifying training
- Separate profiling runs for detailed metrics
- Validation that measurement overhead < 1%

**3. Baseline Parity**
- Measure efficiency for all baselines (IRIS, DART, STORM) [heirarchical-transformer.pdf:3]
- Use official implementations (no re-implementation bias)
- Same hardware and software stack
- Document any baseline-specific optimizations

**4. Reporting Standards**
- Report both accuracy and efficiency for same runs
- Provide correlation analysis between metrics
- Include raw scores (not just normalized)
- Share full data for independent analysis

### Validation Checks

**Accuracy Preservation:**
- Verify final scores match original evaluation (±2% tolerance)
- Compare learning curves visually
- Statistical test: paired t-test on final scores (original vs. extended)
- If p > 0.05, measurement is non-invasive

**Measurement Reliability:**
- Coefficient of variation < 5% for latency
- Repeat measurements on different days (test temporal stability)
- Cross-validate FLOPs with manual calculation
- Compare memory measurements with profiler tools

## Statistical Analysis Plan: Accuracy-Efficiency Trade-offs

### Research Questions

**RQ1:** Does SHI achieve Pareto improvement (better accuracy AND efficiency)?

**RQ2:** What is the accuracy cost per unit efficiency gain?

**RQ3:** Are accuracy and efficiency gains independent or correlated?

**RQ4:** Which efficiency dimension (latency/memory/throughput) best predicts accuracy?

### Analysis 1: Pareto Frontier Analysis

**Objective:** Visualize trade-off space and identify Pareto-optimal methods

**Procedure:**
1. For each method, compute mean accuracy (human-normalized score) and mean efficiency (1/latency or 1/FLOPs)
2. Plot 2D scatter: x-axis = efficiency, y-axis = accuracy
3. Identify Pareto frontier: methods not dominated on both dimensions
4. Compute hypervolume indicator (area under Pareto curve)

**Statistical Test:**
- **Null:** SHI is not on Pareto frontier
- **Test:** Bootstrap confidence regions (1,000 samples)
  - Resample games/seeds
  - Recompute Pareto frontier
  - Count % of bootstraps where SHI is Pareto-optimal
- **Significance:** If SHI on frontier in >95% of bootstraps, reject null

**Visualization:**
- Scatter plot with 95% confidence ellipses
- Pareto frontier curve
- Iso-performance lines (constant accuracy × efficiency product)

### Analysis 2: Efficiency-Normalized Performance

**Objective:** Quantify accuracy per unit computational cost

**Metrics:**
- **Score per TFLOP:** Human-normalized score / (rollout FLOPs / 10¹²)
- **Score per second:** Human-normalized score / rollout latency
- **Score per GB:** Human-normalized score / peak memory

**Statistical Test:**
- **Null:** SHI and baselines have equal efficiency-normalized performance
- **Test:** Wilcoxon signed-rank test (paired by game/seed)
  - Pair: Same game, compare SHI vs. IRIS efficiency-normalized score
  - One-tailed: SHI > IRIS
  - Bonferroni correction for multiple baselines
- **Effect Size:** Hodges-Lehmann estimator (median difference)

**Reporting:**
- Table: Method | Score | FLOPs | Score/TFLOP | Rank
- Median improvement and 95% CI
- Win/tie/loss counts

### Analysis 3: Multi-Objective Optimization

**Objective:** Find optimal operating points for different deployment constraints

**Approach:** Constrained optimization
- **Scenario A (Latency-constrained):** Maximize accuracy subject to latency ≤ 10ms
- **Scenario B (Memory-constrained):** Maximize accuracy subject to memory ≤ 8GB
- **Scenario C (Energy-constrained):** Maximize accuracy subject to energy ≤ 100J per rollout

**Method:**
- For each method, check if constraints satisfied
- Among feasible methods, rank by accuracy
- Compute "regret": accuracy gap to unconstrained optimum

**Statistical Test:**
- **Null:** SHI and best baseline have equal accuracy under constraints
- **Test:** Permutation test
  - Statistic: Accuracy difference under constraint
  - Null distribution: 10,000 random label permutations
  - p-value: Proportion of permutations with difference ≥ observed

### Analysis 4: Correlation and Independence

**Objective:** Test whether accuracy and efficiency gains are independent

**Hypothesis:**
- **H0:** Accuracy and efficiency improvements are uncorrelated (ρ = 0)
- **H1:** Positive correlation (methods good at one are good at both)

**Procedure:**
1. For each method, compute:
   - Accuracy gain: (SHI score - baseline score)
   - Efficiency gain: (baseline FLOPs - SHI FLOPs) / baseline FLOPs
2. Compute Spearman correlation across games
3. Test significance via permutation test

**Interpretation:**
- **ρ > 0, p < 0.05:** Efficiency improvements drive accuracy (or vice versa)
- **ρ ≈ 0:** Independent mechanisms (hierarchy helps both separately)
- **ρ < 0:** Trade-off exists (efficiency hurts accuracy)

**Visualization:**
- Scatter: x = efficiency gain, y = accuracy gain
- Regression line with 95% CI
- Annotate outlier games

### Analysis 5: Hierarchical Regression

**Objective:** Decompose variance in accuracy into efficiency components

**Model:** Linear mixed-effects regression
```
Accuracy ~ Latency + Memory + FLOPs + (1|Game) + (1|Seed)
```

**Fixed Effects:**
- Latency (log-transformed)
- Memory (log-transformed)
- FLOPs (log-transformed)

**Random Effects:**
- Game (accounts for game difficulty)
- Seed (accounts for random variation)

**Analysis:**
- Standardized coefficients: which efficiency metric predicts accuracy best?
- Variance partitioning: % variance explained by efficiency vs. game vs. seed
- Interaction terms: Efficiency × Method (tests if SHI's efficiency helps more)

**Statistical Test:**
- Likelihood ratio test for each predictor
- AIC/BIC for model comparison
- Conditional R² (variance explained)

**Reporting:**
- Coefficient table with 95% CI
- Variance decomposition plot
- Marginal effects: predicted accuracy at different efficiency levels

### Analysis 6: Dominance Analysis

**Objective:** Determine if SHI statistically dominates baselines on both dimensions

**Procedure:**
For each baseline B:
1. **Accuracy dominance:** Test if SHI > B on accuracy (Wilcoxon signed-rank)
2. **Efficiency dominance:** Test if SHI > B on efficiency (Wilcoxon signed-rank)
3. **Joint dominance:** Both tests significant at α = 0.025 (Bonferroni)

**Dominance Matrix:**
```
         | IRIS | DART | STORM | Sparse Imagination
---------+------+------+-------+-------------------
Accuracy |  ✓   |  ✓   |   ✗   |        ✓
Efficiency|  ✓   |  ✗   |   ✓   |        ✓
Joint    |  ✓   |  ✗   |   ✗   |        ✓
```

**Interpretation:**
- ✓ = SHI significantly better (p < 0.025)
- ✗ = No significant difference or baseline better
- Joint ✓ = Pareto dominance

### Analysis 7: Sensitivity Analysis

**Objective:** Test robustness of trade-offs to measurement choices

**Variations:**
1. **Metric choice:** Repeat with different efficiency metrics (latency vs. FLOPs vs. memory)
2. **Aggregation:** Median vs. mean across games
3. **Normalization:** Raw scores vs. human-normalized vs. IQM (interquartile mean)
4. **Subset:** Easy games vs. hard games (split by median baseline score)

**Procedure:**
- Recompute Pareto frontier for each variation
- Check if SHI remains Pareto-optimal
- Compute rank correlation between variations

**Reporting:**
- Robustness table: % of variations where SHI is Pareto-optimal
- Sensitivity plot: Pareto frontier across variations
- Identify conditions where trade-offs change

## Experimental Design

### Factorial Design

**Factors:**
- **Method:** SHI-Hierarchical, SHI-Flat, IRIS, DART, STORM, Sparse Imagination (6 levels)
- **Game:** 26 Atari games (26 levels)
- **Seed:** 5 random seeds (5 levels)
- **Measurement:** Accuracy, Latency, Memory, FLOPs (4 dependent variables)

**Total Runs:** 6 × 26 × 5 = 780 runs

**Blocking:**
- Block by game (controls for game difficulty)
- Block by seed (enables paired tests)
- Randomize method order within blocks

### Data Collection Schedule

**Phase 1: Accuracy Replication (Weeks 1-4)**
- Run all methods on Atari 100k with original protocol
- Collect accuracy metrics only
- Validate against published baselines

**Phase 2: Efficiency Profiling (Weeks 5-6)**
- Load checkpoints from Phase 1
- Run profiling on fixed test set (1,000 rollouts per checkpoint)
- Collect latency, memory, FLOPs, energy

**Phase 3: Joint Analysis (Weeks 7-8)**
- Merge accuracy and efficiency data
- Run all statistical tests
- Generate visualizations

**Phase 4: Sensitivity Analysis (Week 9)**
- Repeat key analyses with variations
- Test robustness

**Phase 5: Reporting (Week 10)**
- Compile results
- Write manuscript sections

## Reporting Template

### Main Results Table

| Method | Score↑ | Latency↓ (ms) | Memory↓ (GB) | FLOPs↓ (T) | Score/TFLOP↑ | Pareto? |
|--------|--------|---------------|--------------|------------|--------------|---------|
| SHI-H  | 0.85±0.03 | 8.2±0.4 | 6.1±0.2 | 2.3±0.1 | 0.37 | ✓ |
| SHI-F  | 0.82±0.03 | 9.1±0.5 | 6.5±0.3 | 2.8±0.1 | 0.29 | ✗ |
| IRIS   | 0.79±0.04 | 12.3±0.6 | 8.9±0.4 | 4.1±0.2 | 0.19 | ✗ |
| DART   | 0.81±0.03 | 10.5±0.5 | 7.2±0.3 | 3.5±0.1 | 0.23 | ✗ |
| STORM  | 0.77±0.04 | 11.8±0.7 | 8.1±0.4 | 3.9±0.2 | 0.20 | ✗ |
| Sparse | 0.76±0.03 | 9.8±0.5 | 6.8±0.3 | 3.2±0.1 | 0.24 | ✗ |

*Values are median ± 95% CI across 26 games × 5 seeds. ↑ = higher is better, ↓ = lower is better.*

### Statistical Separation Table

| Comparison | Accuracy p-value | Efficiency p-value | Joint Dominance |
|------------|------------------|--------------------|--------------------|
| SHI-H vs. IRIS | 0.003** | <0.001*** | ✓ |
| SHI-H vs. DART | 0.041* | 0.002** | ✓ |
| SHI-H vs. STORM | 0.001** | <0.001*** | ✓ |
| SHI-H vs. Sparse | <0.001*** | <0.001*** | ✓ |
| SHI-H vs. SHI-F | 0.028* | 0.015* | ✓ |

*Wilcoxon signed-rank test, Bonferroni-corrected. * p<0.05, ** p<0.01, *** p<0.001*

### Visualization Suite

**Figure 1: Pareto Frontier**
- 2D scatter: FLOPs (x) vs. Score (y)
- Confidence ellipses per method
- Pareto curve highlighted
- Iso-performance contours

**Figure 2: Multi-Dimensional Trade-offs**
- Parallel coordinates plot
- Axes: Score, Latency, Memory, FLOPs, Energy
- Lines colored by method
- Highlights SHI's balanced profile

**Figure 3: Efficiency Breakdown**
- Stacked bar chart per method
- Components: Attention, FFN, Masking, VQ-VAE
- Shows where SHI saves computation

**Figure 4: Scaling Analysis**
- Line plots: Efficiency vs. rollout horizon K
- Separate panels for latency, memory, FLOPs
- Shows SHI's advantage grows with K

**Figure 5: Per-Game Analysis**
- Heatmap: Games (rows) × Methods (columns)
- Cell color: Efficiency-normalized score
- Identifies games where hierarchy helps most

**Figure 6: Correlation Analysis**
- Scatter: Efficiency gain (x) vs. Accuracy gain (y)
- Each point = one game
- Regression line with 95% CI
- Annotate outliers

## Advanced Analyses

### Conditional Efficiency Analysis

**Question:** Does efficiency advantage depend on game characteristics?

**Game Features:**
- Visual complexity: Number of distinct sprites
- Temporal dynamics: Frame-to-frame difference
- Reward sparsity: % of zero-reward transitions

**Analysis:**
- Cluster games by features (k-means, k=3)
- Compute efficiency gain per cluster
- Test interaction: Efficiency gain × Cluster (ANOVA)

**Interpretation:**
- If interaction significant: Hierarchy helps more in visually complex games
- Guides deployment decisions

### Ablation-Efficiency Interaction

**Question:** Which SHI component contributes most to efficiency?

**Ablations:**
- SHI-NoHierarchy (flat masking)
- SHI-NoSPARTAN (no causal graphs) [heirarchical-transformer.pdf:2]
- SHI-NoUncertainty (no uncertainty masking)
- SHI-NoMemory (no memory tokens)

**Analysis:**
- Measure efficiency for each ablation
- Compute marginal contribution: Efficiency(Full) - Efficiency(Ablation)
- Rank components by contribution

**Visualization:**
- Waterfall chart showing cumulative efficiency gains
- Identifies critical components

### Deployment Scenario Analysis

**Scenarios:**
1. **Real-time control:** Latency < 16ms (60 FPS)
2. **Edge deployment:** Memory < 4GB (mobile GPU)
3. **Batch processing:** Maximize throughput
4. **Energy-constrained:** Minimize energy per decision

**For each scenario:**
- Identify feasible methods
- Rank by accuracy
- Compute deployment cost ($/hour on cloud GPU)

**Output:**
- Decision matrix: Scenario → Recommended method
- Cost-benefit analysis

## Potential Confounds & Mitigations

**Confound 1: Implementation efficiency**
- *Risk:* SHI optimized code, baselines naive implementation
- *Mitigation:* Use official baseline repos; apply same optimizations (e.g., Flash Attention) to all

**Confound 2: Batch size effects**
- *Risk:* Methods scale differently with batch size
- *Mitigation:* Test multiple batch sizes {16, 32, 64, 128}; report scaling curves

**Confound 3: Hardware-specific optimizations**
- *Risk:* SHI tuned for A100, baselines for V100
- *Mitigation:* Test on multiple GPUs; report per-hardware results

**Confound 4: Measurement overhead**
- *Risk:* Profiling slows down training
- *Mitigation:* Separate profiling runs; validate accuracy unchanged

**Confound 5: Compiler optimizations**
- *Risk:* PyTorch JIT benefits some architectures more
- *Mitigation:* Test with/without torch.compile(); report both

## Reproducibility Checklist

- [ ] Pin all software versions (CUDA, PyTorch, Python)
- [ ] Document GPU driver version
- [ ] Provide Docker container with exact environment
- [ ] Release profiling scripts and measurement code
- [ ] Share raw efficiency data (CSV with all measurements)
- [ ] Include hardware specs in paper appendix
- [ ] Provide instructions to reproduce each figure
- [ ] Archive code on Zenodo with DOI
- [ ] Pre-register analysis plan on OSF

## Expected Outcomes

### Scenario 1: Pareto Dominance
- SHI significantly better on both accuracy and efficiency
- **Claim:** "SHI achieves 8% higher score with 44% fewer FLOPs (p<0.001)"
- **Implication:** Clear win, recommend for all use cases

### Scenario 2: Trade-off Exists
- SHI better on efficiency, slightly worse on accuracy
- **Claim:** "SHI reduces latency by 35% at cost of 3% score (p=0.04)"
- **Implication:** Recommend for latency-critical applications

### Scenario 3: Conditional Advantage
- SHI better on complex games, worse on simple games
- **Claim:** "SHI improves efficiency by 50% on visually complex games (p<0.001) with no accuracy loss"
- **Implication:** Adaptive method selection based on game features

### Scenario 4: Efficiency-Accuracy Correlation
- Strong positive correlation (ρ=0.72, p<0.001)
- **Claim:** "Efficiency improvements directly translate to accuracy gains"
- **Implication:** Hierarchy provides better inductive bias, not just computational savings

## Timeline & Resources

**Computational Budget:**
- Accuracy runs: 780 runs × 4 GPU-hours = 3,120 GPU-hours
- Profiling runs: 780 runs × 0.5 GPU-hours = 390 GPU-hours
- Ablations: 4 variants × 130 runs × 4 GPU-hours = 2,080 GPU-hours
- **Total:** ~5,600 GPU-hours (~$11,200 on cloud A100s)

**Personnel:**
- 1 researcher (implementation & analysis): 10 weeks
- 1 engineer (profiling infrastructure): 2 weeks
- 1 statistician (consultation): 1 week

**Deliverables:**
1. Extended results table with efficiency metrics
2. Statistical analysis report
3. Visualization suite (6 figures)
4. Reproducibility package (code + data + Docker)
5. Manuscript sections (Methods, Results, Discussion)

## Sources

This analysis plan is based on:
- [heirarchical-transformer.pdf] - "Sparse Hierarchical Imagination for Efficient Transformer World Models" - provides the original evaluation plan with rollout compute cost, rollout error, and final RL performance metrics [heirarchical-transformer.pdf:3], baseline comparisons against DART, IRIS, STORM [heirarchical-transformer.pdf:3], and the SPARTAN causal graph component [heirarchical-transformer.pdf:2]

For statistical methodology, I recommend:
- Dolan & Moré (2002) "Benchmarking optimization software with performance profiles" - for Pareto frontier analysis
- Zitzler et al. (2003) "Performance assessment of multiobjective optimizers" - for hypervolume indicator
- Agarwal et al. (2021) "Deep Reinforcement Learning at the Edge of the Statistical Precipice" (NeurIPS) - for RL-specific statistical testing and stratified bootstrap methods