Below is a drop-in “fairness/ethics audit layer” you can add to your existing Hierarchical Transformer evaluation without touching the original labels or pipelines, plus concrete uncertainty reporting. I anchor to your current plan to report rollout compute cost, rollout error, and final RL performance and to study uncertainty-based masking and causal guidance [heirarchical-transformer.pdf:3]. The audit attaches sidecar annotations and metrics to the same datasets, and slices your existing metrics by subgroups and intersectional factors, so your base benchmarks remain intact.

Scope and what to audit on the same datasets
- Slice existing metrics: For each dataset and split, report your current metrics (rollout error, final RL performance) stratified by subgroups and intersectional groups without changing ground-truth labels [heirarchical-transformer.pdf:3]. Examples: demographics (if present), domain/stressor factors (lighting, occlusion, sensor noise, rare states), difficulty bins, or scenario types. If human attributes are absent, use dataset-specific “risk factors” (rare/long-horizon states, edge-case dynamics) as ethical audit axes.
- Hierarchical-specific audits: Report subgroup gaps at each hierarchy level and across imagination/masking variants, since you already plan ablations on hierarchical masking and uncertainty-based masking [heirarchical-transformer.pdf:3].

How to add new annotations or audits without contaminating original labels
- Freeze dataset vX: Do not modify original files. Create a separate “audit sidecar” keyed by immutable sample_id.
- Sidecar schema: audit_v1.jsonl (or Parquet) with fields such as sample_id, audit_version, annotator_id (pseudonymous), timestamp, subgroup tags (e.g., gender, scenario_type), intersectional_group_id, notes, confidence, provenance.
- Blind and independent collection:
  - Annotate from a read-only copy with original labels hidden to avoid anchoring and label leakage.
  - Randomize item order and salt sample_ids for annotator UIs. Keep annotators blind to model predictions.
- Data governance:
  - Sensitive attributes: Minimize collection; obtain consent or use approved proxies. Store separately with access controls and k-anonymity thresholds for reporting small-N groups.
  - Log guidelines and decision rules; publish an audit codebook and example adjudication cases.
- Multi-annotator design for reliability:
  - Assign ≥2 annotators per item for sensitive tags; compute inter-annotator agreement (e.g., Krippendorff’s alpha; bootstrap CI).
  - Aggregate with a principled model (e.g., Dawid–Skene) and control for unreliable annotators using FDR where applicable [P5]. This avoids overfitting to one annotator’s bias and keeps original labels intact.
- Non-contamination in training/eval:
  - Training: Do not feed audit tags to the model unless explicitly running fairness-aware training ablations; keep the canonical baseline identical.
  - Evaluation: Load the sidecar only to define slices and weights. Maintain the original test set; create an “audit holdout” subset for exploratory analyses.
  - Reproducibility: Version the audit sidecar (audit_v1), record annotation tool versions, and publish a data card addendum.

Fairness and ethics metrics
- For classification-like outcomes (if applicable to rollouts or event detection):
  - Demographic parity difference, equalized odds gaps (FPR/FNR differences), subgroup calibration error (ECE per subgroup).
- For continuous or RL outcomes:
  - Mean and quantile of rollout error and final return per subgroup; worst-group performance; reward gap between groups; tail risk (CVaR) per subgroup; coverage of conformal intervals per subgroup (see below).
- Intersectional analysis: Report the same metrics across intersectional slices (e.g., subgroup A ∩ scenario type B), with small-N safeguards.

Uncertainty reporting (for both annotations and fairness metrics)
- For audit annotations:
  - Inter-annotator agreement: Report alpha/kappa with bootstrap CIs; adjudicate disagreements using Dawid–Skene or similar and report residual uncertainty as soft labels [P5]. Where disagreements encode intrinsic ambiguity, analyze whether it stems from annotators vs data factors [P2].
- For subgroup rates/gaps:
  - Use Wilson or Jeffreys binomial intervals for rates (FPR/FNR) with explicit n per subgroup; for differences, use stratified bootstrap with 5k+ replicates and report percentile or BCa intervals. Control multiple comparisons over subgroups via BH-FDR.
- For rollouts and RL:
  - Cluster bootstrap at the episode/trajectory level to respect temporal dependence; report mean difference in rollout error and return with 95% CIs. Aggregate across seeds; for paired comparisons (same seeds), use paired bootstrap or permutation tests.
- Calibration and coverage:
  - Report ECE and Brier score per subgroup with bootstrap CIs.
  - Provide split conformal prediction intervals/sets and report empirical coverage per subgroup; check “equalized coverage” and coverage gaps. Conformal methods maintain distribution-free coverage under exchangeability; under mild contamination, robust/split variants can preserve guarantees or quantify degradation [P4]. When calibration data may be noisy, discuss robustness to contamination and replicate with a trimmed or filtered calibration set [P4].
- Handling noisy or contaminated audits:
  - If you suspect mislabeled subgroup tags in calibration/reference sets, assess impact using robust conformal or contamination-aware calibration analyses and sensitivity checks [P4].

Reporting template (Fairness/Ethics Addendum)
- Methods: Who annotated, guidelines used, blinding, IAA statistics and CIs, aggregation method, governance and privacy controls.
- Metrics by slice: For each dataset and split, report subgroup and worst-group rollout error, return, calibration, and gap metrics with 95% CIs; include sample sizes. For intersectional slices, apply small-N warnings or suppress if n < threshold.
- Uncertainty: Clearly state seed ranges, bootstrap settings (replicates, clustering level), and whether CIs are paired or unpaired.
- Ablations: Report fairness metrics and CIs for hierarchy/masking variants (including uncertainty-based masking) alongside your existing metrics [heirarchical-transformer.pdf:3].
- Limitations and sensitivity: Note potential residual bias (missing attributes, unobserved confounders), and show sensitivity to different aggregation models and slice definitions [P2].

Three concrete, falsifiable experiments
1) Does uncertainty-based masking reduce subgroup error gaps?
- Hypothesis: Uncertainty-based masking will reduce the worst-group rollout error and shrink subgroup rollout-error gaps relative to no masking, without degrading average performance [heirarchical-transformer.pdf:3].
- Variables: Masking ∈ {off, uncertainty-based}; hierarchy level(s) ∈ {L1,…}; seeds ∈ {1,…,20}.
- Metrics: Mean rollout error per subgroup, worst-group rollout error, and gap (max−min) with cluster-bootstrap 95% CIs across episodes and seeds; final RL performance stratified by subgroup [heirarchical-transformer.pdf:3].
- Expected outcome: A measurable reduction in worst-group error and in gap CIs for the masked condition; if not observed, masking may concentrate uncertainty in minority slices (to be diagnosed via subgroup calibration and ECE).

2) Do conformal intervals equalize coverage across subgroups under label/calibration noise?
- Hypothesis: Split conformal prediction will achieve near-nominal coverage 1−α per subgroup and smaller coverage gaps than temperature scaling or isotonic regression baselines; robustness persists under mild calibration-set contamination [P4].
- Variables: Calibrator ∈ {split conformal, temperature scaling, isotonic}; contamination ∈ {0%, 5%} of calibration labels or subgroup tags; α ∈ {0.1, 0.2}.
- Metrics: Subgroup coverage, coverage gap (max−min), interval width; report 95% bootstrap CIs. Conduct sensitivity to trimming/robust variants [P4].
- Expected outcome: Split conformal shows coverage closer to nominal and smaller coverage gaps across subgroups; robust to small contamination.

3) Do multi-annotator aggregation and annotator-quality control change fairness conclusions?
- Hypothesis: Aggregating subgroup tags with Dawid–Skene plus FDR-based annotator quality control will reduce annotator-induced bias, stabilizing fairness metrics relative to single-annotator tags [P5], and help disentangle annotator vs data bias [P2].
- Variables: Tag source ∈ {single annotator, majority vote, Dawid–Skene+FDR}; sample fraction annotated ∈ {5%, 10%}.
- Metrics: IAA (alpha/kappa), stability of fairness gaps (absolute change and CI across bootstrap resamples), number of flagged low-quality annotators; downstream change in worst-group performance with 95% CIs.
- Expected outcome: Aggregated tags yield higher IAA and narrower CIs on fairness gaps; differences pinpoint whether prior conclusions were annotator-driven [P2].

Optional extension experiment
4) Fairness-aware reweighting trade-offs
- Hypothesis: Training-time subgroup reweighting (using audit sidecar, held out from baseline) reduces worst-group rollout error with bounded drop in average performance.
- Variables: Reweighting strength λ ∈ {0, 0.25, 0.5}, constraint on average performance.
- Metrics: Worst-group rollout error, average rollout error, Pareto frontier visualization with CIs.
- Expected outcome: A tunable trade-off curve; report whether any λ strictly improves worst-group error at ≤1% average performance cost.

Practical next steps
- Draft the audit codebook; define subgroup/slice taxonomy appropriate to your datasets.
- Spin a read-only data copy; generate audit_v1 sidecar via blinded multi-annotator protocol.
- Add slicing and CI computation to your eval: cluster bootstrap at episode level, per-subgroup ECE, and split conformal calibration for coverage checks.
- Integrate fairness tables and CIs into your existing reporting alongside rollout error and RL performance [heirarchical-transformer.pdf:3]; publish seeds and scripts.

Citations
- [P2] Characterizing Sources of Uncertainty to Proxy Calibration and Disambiguate Annotator and Data Bias — http://arxiv.org/abs/1909.09285v2
- [P4] Split Conformal Prediction under Data Contamination — http://arxiv.org/abs/2407.07700v2
- [P5] False Discovery Rate Control and Statistical Quality Assessment of Annotators in Crowdsourced Ranking — http://arxiv.org/abs/1605.05860v3

Additional anchors (optional)
- [P1] Robust Conformal Outlier Detection under Contaminated Reference Data — http://arxiv.org/abs/2502.04807v2

Notes on attachment grounding
- Your planned evaluation already includes rollout error, final RL performance, and uncertainty-based masking/ablation, which the audit will slice and augment, not alter [heirarchical-transformer.pdf:3].