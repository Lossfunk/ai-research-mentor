Below is a concrete plan to instrument your Hierarchical Transformer for real-time/streaming inputs (e.g., live notes), including (A) streaming-ready design changes, (B) logging/telemetry, (C) failure subtype capture, and (D) preregistered hypotheses and a longitudinal evaluation plan. I ground model- and metric-specific pieces in your attachment and add two literature anchors relevant to hierarchical/longitudinal streaming and dynamic chunking.

A) Streaming-ready model instrumentation
- Online tokenization and multi-scale buffers
  - For each incoming observation xt, run the hierarchical VQ-VAE/token clustering encoder online to produce discrete tokens incrementally rather than in full batches [heirarchical-transformer.pdf:2]. Maintain two rolling buffers:
    - Fine-scale token buffer: sliding window of recent discrete tokens.
    - Coarse-scale segment buffer: aggregated “macro-tokens” (e.g., sentence/paragraph boundaries for notes) updated at chunk boundaries. Dynamic chunking heuristics can follow punctuation/newline and size budgets; chunk-level flushing reduces latency variance [P8].
- Causal attention with segment/memory caches
  - Use streaming causal masks over the fine-scale window, plus a compressed memory of coarse-scale macro-tokens. Keep per-layer KV caches for the last W fine tokens, and a separate cache of K macro-tokens. The SPARTAN-style mechanism selects the subset of tokens most likely to influence future dynamics and informs masking decisions during imagination; use this to promote/drop tokens into long memory at runtime [heirarchical-transformer.pdf:2].
- Budget-aware dynamic masking
  - Enforce a per-step compute budget by modulating (i) hierarchical masking, (ii) uncertainty thresholds for retention, and (iii) causal-graph-guided inclusion of tokens. These knobs are already proposed for ablations (hierarchical masking, causal graph guidance, uncertainty-based masking) [heirarchical-transformer.pdf:3]; in streaming, treat them as adaptive controllers tied to latency targets and memory caps.
- Rollout interfaces
  - Provide two rollout modes per time step: (i) short-horizon single-step prediction (low-latency path), and (ii) extended imagination for planning, with early exit if latency budget nears violation. Instrument rollout compute cost and rollout error per time step to maintain parity with your offline metrics [heirarchical-transformer.pdf:3].

B) Telemetry and logging schema for streaming
- Tracing
  - One trace per input event with spans: ingest, tokenize, fine-scale attention, macro attention, SPARTAN selection, decode/postprocess. Add correlation IDs so spans can be joined with downstream outcomes (e.g., RL rewards or user feedback) [heirarchical-transformer.pdf:3].
- Core time-series metrics (export at 1–5 Hz and per-event):
  - Latency: end-to-end, model-forward, tokenization, queue time; p50/p95/p99; jitter (p95−p50).
  - Throughput: tokens/sec; macro-chunks/sec; dropped/skipped tokens.
  - Rollout metrics: rollout compute cost (FLOPs or ms per imagination step), rollout error (per-step and horizon-aggregated), and final task performance (e.g., RL reward) [heirarchical-transformer.pdf:3].
  - Memory/caching: KV cache size per layer; macro-memory size; cache hit rate; eviction count; SPARTAN-selected token count; mask sparsity and density [heirarchical-transformer.pdf:2].
  - Uncertainty: per token/segment predictive entropy or variance used for uncertainty-based masking [heirarchical-transformer.pdf:3].
  - Causal guidance: fraction of tokens admitted due to causal graph guidance, and their subsequent measured influence (e.g., gradient- or attention-based influence indices) [heirarchical-transformer.pdf:3].
- Decision/event logs (append-only, sampled if needed):
  - Tokenization decisions: boundaries, token IDs/centroids (hashed), codebook version [heirarchical-transformer.pdf:2].
  - Masking decisions: explain why tokens were retained/dropped (uncertainty threshold, budget pressure, causal guidance) [heirarchical-transformer.pdf:3].
  - Budget controller actions: target latency, current estimate, controller gain, action taken (e.g., decrease window W by 64).
  - Rollout sessions: horizon length, early exits, compute cost, error trajectory [heirarchical-transformer.pdf:3].
- Data contracts and replay
  - Persist a minimal, privacy-safe stream (timings, counts, decisions, seeds) to enable exact replay of streaming runs for reproducibility and failure analysis.

C) Failure subtype taxonomy and capture
- Latency SLO breach
  - Definition: p95 end-to-end latency > L ms for N consecutive windows.
  - Capture: emit “SLO_BREACH” with controller state, current W/K, queue depth; attach token/mask counts and SPARTAN selection stats to diagnose whether masking/budget control reacted sufficiently [heirarchical-transformer.pdf:3].
- Budget thrash
  - Rapid oscillation of masking parameters that causes alternating under/over-budget behavior. Detect via high variance in window size and retained tokens; record PID/controller parameters.
- Memory bloat
  - Macro-memory exceeds cap; eviction spike. Log causal reasons: e.g., high uncertainty causing over-retention [heirarchical-transformer.pdf:3].
- Tokenization drift
  - Shift in VQ centroids usage or OOD token rates; flag when distributional distance (e.g., JS divergence vs baseline) exceeds threshold. Attach codebook version and encoder stats [heirarchical-transformer.pdf:2].
- Masking misfire
  - SPARTAN/uncertainty drops tokens that later show high influence on rollout error spikes. Define “misfire” when rollout error rises > X% within T steps after a large drop; log the dropped tokens and predicted vs realized uncertainty [heirarchical-transformer.pdf:2][heirarchical-transformer.pdf:3].
- Causal guidance mismatch
  - Causal-graph-gated tokens underperform: tokens admitted via guidance have low measured influence or correlate with higher rollout error. Attach graph version and token list [heirarchical-transformer.pdf:3].
- Non-deterministic cache state
  - Mismatch in KV/macro-memory across replicas or replays; capture cache checksums and RNG seeds.

D) Preregistered hypotheses and longitudinal evaluation plan
- Primary endpoints (pre-specified)
  - Streaming endpoints: end-to-end p95 latency, jitter, and SLO violation rate.
  - Model endpoints (consistent with attachment): rollout compute cost, rollout error, and final task performance (e.g., RL cumulative reward) [heirarchical-transformer.pdf:3].
- Secondary endpoints
  - Cache-hit rate, macro-memory utilization, mask sparsity, uncertainty calibration (ECE/Brier), tokenization drift index [heirarchical-transformer.pdf:2][heirarchical-transformer.pdf:3].
- Experimental design
  - Randomized interleaving or canary A/B over streams; stratify by input length and topic; fixed analysis windows (e.g., weekly) with sequential monitoring and pre-specified stopping boundaries for harm (SLO breaches or performance regression > δ).
  - Predefine missing-data handling and multiple-testing correction (e.g., Holm) across the planned ablations [heirarchical-transformer.pdf:3].
- Preregistered hypotheses (examples)
  - H1: Enabling hierarchical masking at streaming time reduces rollout compute cost by ≥20% without increasing rollout error beyond +2% relative to baseline at matched latency [heirarchical-transformer.pdf:3].
  - H2: Uncertainty-based masking with threshold τ reduces p95 latency jitter by ≥15% while maintaining final task performance within ±1% of baseline [heirarchical-transformer.pdf:3].
  - H3: Causal-graph guidance decreases long-horizon rollout error by ≥5% on streams with frequent topic shifts (live notes) [heirarchical-transformer.pdf:3].
  - H4: Dynamic chunking (macro-boundaries) improves macro-memory cache-hit rate by ≥10% and lowers SLO breaches by ≥20% versus fixed-size chunking [P8].
- Analysis plan
  - Primary analyses on intent-to-treat; per-stream randomization keys. Report effects with CIs and pre-specified non-inferiority margins. Sensitivity analyses for bursty workloads. Archive the preregistration (e.g., OSF/internal registry) before deployment.

Concrete, falsifiable experiments
1) Streaming hierarchical masking effectiveness
- Hypothesis: Turning on hierarchical masking reduces rollout compute cost ≥20% with ≤2% change in rollout error at fixed throughput [heirarchical-transformer.pdf:3].
- Variables: Masking on/off; fine-window W and macro-memory K held constant; live notes stream.
- Metrics: Rollout compute cost, rollout error, p95 latency, final performance [heirarchical-transformer.pdf:3].
- Expected outcome: Compute cost drops with negligible error change.
- Guardrails: Abort if SLO breach rate > pre-set threshold.

2) Uncertainty threshold sweep for stability
- Hypothesis: Moderate uncertainty threshold τ∗ minimizes SLO breaches and jitter with no worse than −1% final performance vs baseline [heirarchical-transformer.pdf:3].
- Variables: τ ∈ {low, τ∗, high}; controller otherwise fixed.
- Metrics: Jitter, SLO breach rate, rollout error, final performance [heirarchical-transformer.pdf:3].
- Expected outcome: U-shaped trade-off; τ∗ balances retention and latency.

3) Causal guidance vs no guidance under topic shifts
- Hypothesis: Causal-graph guidance reduces long-horizon rollout error ≥5% on streams with abrupt topic changes [heirarchical-transformer.pdf:3].
- Variables: Guidance on/off; same W, K, τ; curate streams with synthetic or natural topic shifts.
- Metrics: Horizon-T rollout error, misfire rate (dropped-then-needed tokens), downstream performance [heirarchical-transformer.pdf:3].
- Expected outcome: Guidance reduces error during shifts.

4) Dynamic chunking for macro-memory efficiency
- Hypothesis: Dynamic chunking improves macro-memory cache-hit rate ≥10% and reduces p95 latency ≥10% vs fixed chunk size [P8].
- Variables: Chunking policy ∈ {fixed N tokens, dynamic by punctuation/newlines/size cap}.
- Metrics: Macro cache-hit rate, p95 latency, SLO breaches, final performance.
- Expected outcome: Dynamic chunking yields higher cache efficiency and lower latency.

Implementation checklist (pragmatic)
- Add streaming encoder path and expose token/macro buffers [heirarchical-transformer.pdf:2].
- Implement dual-cache attention (fine window + macro-memory) with budget-aware SPARTAN gating [heirarchical-transformer.pdf:2][heirarchical-transformer.pdf:3].
- Wire telemetry (metrics + traces), including rollout compute cost/error hooks [heirarchical-transformer.pdf:3].
- Build failure detectors for SLO breach, drift, misfire, mismatch (as above).
- Set up preregistration template with hypotheses, endpoints, analysis plan, and ablation matrix [heirarchical-transformer.pdf:3].

Notes on external anchors
- Longitudinal/hierarchical streaming is common in EHR modeling; the Hi-BEHRT line of work motivates multi-timescale representation learning for longitudinal data, analogous to “live notes” streams [P1].
- Dynamic chunking methods support lower-latency, segment-level incremental processing, fitting your macro-token policy [P8].

Citations
- [heirarchical-transformer.pdf:2] …first encoded into a sequence of discrete latent tokens via a hierarchical VQ-VAE or token clustering…; SPARTAN informs masking decisions during imagination.
- [heirarchical-transformer.pdf:3] …report rollout compute cost, rollout error, and final RL performance. Additional ablation studies will investigate hierarchical masking, causal graph guidance, and uncertainty-based masking. 4. Hypotheses and Evaluation.

- [P1] Hi-BEHRT: Hierarchical Transformer-based model for accurate prediction of clinical events using multimodal longitudinal electronic health records — http://arxiv.org/abs/2106.11360v1
- [P8] Dynamic Chunking for End-to-End Hierarchical Sequence Modeling — http://arxiv.org/abs/2507.07955v2

Sources
- Attachment: heirarchical-transformer.pdf (pp. 2–3) for architecture, masking, and evaluation metrics.
- [P1] and [P8] for external anchoring of longitudinal hierarchical modeling and dynamic chunking.