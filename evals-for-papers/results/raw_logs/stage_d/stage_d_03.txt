Below is a concrete plan to extend the MST taxonomy with a fourth top-level category, Efficiency Issues, and to validate that it is statistically distinct from the existing three categories (Specification, Inter-Agent Misalignment, Verification).

Context from the paper you cited
- The paper’s failure taxonomy has three top-level categories: Specification Issues (System Design), Inter-Agent Misalignment (Agent Coordination), and Task Verification (Quality Control), each with multiple failure modes and reported frequencies [Why Do Multi-Agent LLM Systems Fail_ - 2503.13657v2.pdf:3,5]. For example, modes include Step Repetition, Fail to Ask for Clarification, Reasoning–Action Mismatch, and Premature Termination [Why Do Multi-Agent LLM Systems Fail_ - 2503.13657v2.pdf:3]. The taxonomy and its distribution are summarized in the figures on pages 3 and 5 [Why Do Multi-Agent LLM Systems Fail_ - 2503.13657v2.pdf:3,5].
- Based on these figures, the existing taxonomy emphasizes correctness-related failures (mis-specification, coordination errors, and verification breakdowns) [Why Do Multi-Agent LLM Systems Fail_ - 2503.13657v2.pdf:3,5]. That motivates adding a separate, orthogonal category for efficiency when outcomes are correct but resource usage is poor.

Proposed new category: Efficiency Issues
Definition
- Failures where the final outcome is correct (or would be correct if judged by existing categories), but the multi-agent process exhibits excessive or avoidable resource use: tokens, wall-clock time, cost, turns, tool calls, or context bloat, relative to task-matched baselines.

Candidate sub-types
- Communication inefficiency
  - Verbosity inflation: long, redundant messages or repeated quoting of history without new information; excessive back-and-forth beyond what’s needed to complete the task.
  - Clarification overuse: repeated low-yield clarifications when the task was already sufficiently specified (distinct from “Fail to Ask for Clarification” in Misalignment) [Why Do Multi-Agent LLM Systems Fail_ - 2503.13657v2.pdf:3].
- Coordination inefficiency
  - Duplicated work: multiple agents independently perform the same subtask without consolidation.
  - Idle-wait patterns: poor task/role allocation causes agents to idle or wait unnecessarily while others overwork.
- Tool-use inefficiency
  - Redundant or unnecessary tool/API calls; over-verification loops that do not alter the final (correct) decision (distinct from Verification failures like “No or Incomplete Verification” or “Incorrect Verification”) [Why Do Multi-Agent LLM Systems Fail_ - 2503.13657v2.pdf:3].
- Memory/context inefficiency
  - Context bloat: repeatedly injecting the full conversation or irrelevant artifacts that inflate prompt length without improving correctness.
- Termination inefficiency
  - Late termination: the solution is reached earlier in the trace but the system continues iterating; distinct from “Unaware of Termination Conditions” (a correctness-oriented failure mode) [Why Do Multi-Agent LLM Systems Fail_ - 2503.13657v2.pdf:3].

Measurement and instrumentation
- Primary efficiency metrics (per trace)
  - Tokens: total input and output tokens; peak prompt length; fraction of quoted history.
  - Time: wall-clock latency; active compute time; queue/idle time.
  - Cost: API cost (estimated from tokens and model pricing).
  - Interaction structure: number of turns, messages, agents invoked; number of tool/API calls and unique tools; redo/retry loops.
- Baselines for normalization
  - Per-task single-agent baseline and best-of-N single-agent baseline.
  - Per-framework baseline (e.g., default AutoGen role setup) to control for system design effects.
  - Task difficulty controls (e.g., complexity ratings, length of gold solution).
- Define an “efficiency anomaly score”
  - Z-score or percentile of each metric vs. matched baseline; then aggregate (e.g., weighted sum or PCA) to produce an anomaly score. Threshold (e.g., ≥90th percentile) flags an Efficiency Issue trace. Use per-subtype signatures (e.g., tool-call density for Tool-use inefficiency; quote ratio for Memory/context inefficiency).

Annotation protocol
- Two-pass coding:
  - Pass 1: Annotate correctness-related taxonomy labels (existing three categories and their modes) using the paper’s codebook [Why Do Multi-Agent LLM Systems Fail_ - 2503.13657v2.pdf:3,5].
  - Pass 2: Annotate Efficiency Issues (present/absent) and sub-type(s). Provide decision rules and examples for each subtype (with instrumented metrics shown to annotators).
- Reliability
  - Compute inter-annotator agreement for each subtype (e.g., Krippendorff’s alpha) and for the overall Efficiency Issues label. Target alpha ≥ 0.67 before main data collection. Adjudicate disagreements.

Study design to validate distinctness
Goal: Demonstrate that Efficiency Issues is an empirically coherent construct, and statistically distinct from the existing three categories.

- Construct validity
  - Convergent: Efficiency Issues correlate with efficiency metrics (tokens, cost, time, tool-call count) after controlling for task difficulty and model choice.
  - Discriminant: Efficiency Issues remain weakly correlated with correctness failure incidence and labels from Specification, Misalignment, and Verification. For instance, a trace can be fully correct yet inefficient. Use partial correlations controlling for task and model.

- Factor structure
  - Exploratory factor analysis (EFA) on a matrix of binary labels for all failure modes plus Efficiency subtypes, augmented with normalized efficiency metrics. Hypothesis: Efficiency subtypes and metrics load predominantly on a distinct factor, separable from factors capturing Specification/Misalignment/Verification [Why Do Multi-Agent LLM Systems Fail_ - 2503.13657v2.pdf:3,5].
  - Confirmatory factor analysis (CFA) comparing:
    - Model A: three-factor structure (as in the paper).
    - Model B: four-factor structure adding Efficiency.
    - Evidence of distinctness: Model B fits significantly better (e.g., lower AIC/BIC, higher CFI/TLI; chi-square difference test).

- Predictive separation
  - Train a multinomial (or multi-label) classifier from trace features to predict top-level categories. Test whether the Efficiency label is predicted by a feature set dominated by efficiency signals (tokens, time, tool density) and not by signals diagnostic of correctness failures (e.g., contradiction markers, premature termination cues). Clear feature separation supports distinctness.
  - Nested model comparison for efficiency metrics:
    - Outcome variables: tokens, time, cost.
    - Model 1: regress on task, model, and the three existing category labels.
    - Model 2: add Efficiency Issues label.
    - Distinctness: Model 2 yields significant incremental R^2 and likelihood-ratio improvement for efficiency outcomes, showing Efficiency captures variance not explained by existing categories.

- Outcome orthogonality
  - Compare success rates: traces labeled only with Efficiency Issues vs. traces labeled with correctness failure categories. Expect success rates near baseline for the former, but not for the latter. This underscores that Efficiency is not primarily a correctness concept [Why Do Multi-Agent LLM Systems Fail_ - 2503.13657v2.pdf:3,5].

- Robustness checks
  - Across frameworks (e.g., CAMEL-style role-play, AutoGen), tasks, and models.
  - Sensitivity to thresholds for anomaly scores; re-run analyses with 80th/85th/90th percentile cutoffs.
  - Pre-registration of the analysis plan; power analysis to ensure stable factor solutions and classifier evaluation.

Three concrete, falsifiable experiments
1) Factor-structure test of a four-factor taxonomy
- Hypothesis: A four-factor model (Specification, Misalignment, Verification, Efficiency) fits annotation data and instrumented metrics significantly better than a three-factor model.
- Independent variables: Presence of Efficiency subtypes; existing failure-mode labels [Why Do Multi-Agent LLM Systems Fail_ - 2503.13657v2.pdf:3,5].
- Dependent variables: Model fit indices (AIC/BIC, CFI/TLI), chi-square difference between CFA models.
- Metric/analysis: EFA to propose structure; CFA to compare 3- vs 4-factor models; bootstrap confidence intervals for fit differences.
- Expected outcome: The 4-factor model shows significantly better fit and clean loadings of efficiency metrics and subtypes onto the Efficiency factor.

2) Incremental explanatory power for resource use
- Hypothesis: Efficiency Issues explain variance in tokens/time/cost beyond the three correctness categories.
- Independent variables: Task, model family, existing category labels, Efficiency Issues label.
- Dependent variables: Total tokens, wall-clock time, API cost.
- Metric/analysis: Nested linear mixed models (random effects for task and dataset). Likelihood ratio tests and ΔR^2 between models without vs. with Efficiency label.
- Expected outcome: Adding Efficiency label yields significant improvement in fit and positive coefficients on efficiency outcomes, while existing categories contribute little additional explanatory power for tokens/time/cost.

3) Outcome-orthogonality test
- Hypothesis: Traces labeled only with Efficiency Issues have success rates comparable to controls, unlike traces labeled with Specification/Misalignment/Verification failures.
- Independent grouping: Four groups—None, Efficiency only, Correctness only (any of the three), Both.
- Dependent variables: Task success (correctness), number of rework cycles.
- Metric/analysis: Proportion tests (e.g., χ²) and confidence intervals; logistic regression controlling for task difficulty.
- Expected outcome: Efficiency-only group’s success rate is statistically indistinguishable from None, and significantly higher than Correctness-only.

Optional fourth experiment: Intervention responsiveness
- Hypothesis: Efficiency-targeted interventions (prompt compression, agent role consolidation, tool-call budgeting) reduce efficiency metrics without worsening correctness.
- Design: A/B tests on the same tasks with and without interventions (e.g., LLMLingua compression; role constraints inspired by AutoGen and CAMEL setups).
- Outcomes: Tokens/time/cost and task success.
- Expected outcome: Significant reductions in tokens/time/cost with non-inferior correctness.

Data and power
- Data: ≥1,000 multi-agent traces across ≥5 task families; retain raw messages, tool calls, timestamps, and models used.
- Power: Target ≥200 traces with Efficiency labels to ensure stable factor loadings and reliable classification metrics; conduct a priori power analysis for CFA and mixed models.

Why this is distinct from existing taxonomy
- The paper’s categories and failure modes are correctness-centric and include items such as Step Repetition, Fail to Ask for Clarification, and Incorrect Verification [Why Do Multi-Agent LLM Systems Fail_ - 2503.13657v2.pdf:3]. Efficiency Issues, by construction, capture excessive resource use even when the final output is correct, thus adding an orthogonal evaluative dimension to the existing taxonomy [Why Do Multi-Agent LLM Systems Fail_ - 2503.13657v2.pdf:3,5].

Anchoring literature for efficiency interventions and testbeds
- AutoGen: multi-agent conversation framework often used to study agent coordination and can be instrumented for cost/latency analyses (Enabling Next-Gen LLM Applications via Multi-Agent Conversation).
- FrugalGPT: techniques for reducing LLM cost via model routing; relevant as interventions to test causal sensitivity of Efficiency Issues labels to cost- and token-reduction methods.
- LLMLingua: prompt compression to reduce tokens and latency; directly applicable to Memory/context inefficiency subtypes.

Citations
- [file:3] Why Do Multi-Agent LLM Systems Fail? — Inter-Agent Conversation Stages; Failure Categories and modes (Specification, Misalignment, Verification) with percentages.
- [file:5] Why Do Multi-Agent LLM Systems Fail? — Overview figure showing taxonomy categories, sub-modes, and distribution across categories.

Sources
- Why Do Multi-Agent LLM Systems Fail? (attached) — [file:3], [file:5]
- AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation — https://arxiv.org/abs/2308.08155
- FrugalGPT: How to Use Large Language Models While Reducing Cost and Increasing Efficiency — https://arxiv.org/abs/2305.05176
- LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models — https://arxiv.org/abs/2310.05736
- CAMEL: Communicative Agents for “Mind” Exploration of Large Scale Language Model Society — https://arxiv.org/abs/2303.17760