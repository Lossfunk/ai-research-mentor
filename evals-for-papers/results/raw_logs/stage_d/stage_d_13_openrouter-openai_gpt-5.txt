Below is an instrumentation and evaluation plan to run the Hierarchical Transformer in a real-time/streaming setting (e.g., live note-taking), while preserving the paper’s core evaluation axes—rollout compute cost, rollout error, and stability—and ablation framing (hierarchical masking, causal guidance, uncertainty masking) for comparability.

Streaming adaptation (minimal changes that preserve comparability)
- Stateful, incremental inference
  - Maintain rolling KV caches and hierarchical summaries across chunks; process input in time-sliced micro-batches (e.g., 256–1,024 tokens) with strict per-chunk latency SLAs.
  - Apply Sparse Hierarchical Imagination (SHI) incrementally: update retained-token sets and summary/memory tokens only within the latest window plus a small overlap to bound compute, keeping the same retained-token budget and rollout depth the paper reports [heirarchical-transformer.pdf:2–3].
- Stable rollouts under stream growth
  - Keep rollout error and compute accounting identical to the paper, reported per chunk and over time [heirarchical-transformer.pdf:3].
  - Run the same ablations (hierarchy off, causal off, uncertainty off) in streaming mode for attribution [heirarchical-transformer.pdf:3].

Logging schema (real-time telemetry)
- Session metadata
  - model_id, checkpoint_hash, ablation_flags (hierarchy/causal/uncertainty), tokenizer/version, device profile, quantization, retained_token_budget, rollout_depth, max_context.
  - stream_id, session_id, UTC timestamps; stream rate (tokens/sec).
- Chunk-level timing and compute
  - latency_ms (enqueue→response), queue_wait_ms, p50/p95 latency by rolling window.
  - rollout_compute_cost (same FLOPs/step accounting as paper), peak_mem_MB, tokens_in/out, retained_tokens_count and retention_ratio per level [heirarchical-transformer.pdf:3].
- Predictive and stability signals
  - per-token NLL and entropy; error-vs-horizon k-step teacher-forced rollout error curves per chunk (same definition as paper’s rollout error) [heirarchical-transformer.pdf:3].
  - uncertainty gating stats: retention thresholds hit rate; token influence/centrality histograms (diagnostic).
  - edit stability: fraction of tokens revised within the last N chunks; oscillation count (A→B→A reversions).
- Structural cues (for long docs/live notes)
  - predicted boundary indices (section/paragraph), summary/memory token states, and boundary deltas (add/move/remove).
- Event log (structured JSONL)
  - SLA_violation, Buffer_drop, Context_truncation, Mask_budget_exhausted, OOD_drift_flag, Privacy_flag (if enabled).

Failure subtype capture (automated detectors)
- SLA/throughput failures
  - Latency violation: chunk latency > τ (e.g., 150 ms). Buffer drop: input dropped due to backpressure.
- Masking/retention failures
  - Retention miss: model fails to reference evidence beyond L tokens that was present in context; detected via post-hoc “needle” checks on known anchors.
  - Budget saturation: retention_ratio → 1 at upper level for ≥K consecutive chunks (indicates hierarchy collapse).
- Stability failures
  - Oscillation: token subsequence edited back-and-forth within W chunks.
  - Drift/contradiction: new content contradicts a previously affirmed fact (regex/entailment checkers over entity–value pairs).
- Structure failures
  - Boundary churn: boundaries move by >M sentences across adjacent chunks without net content change.
  - Coherence drop: local perplexity improves but summary consistency (entailment with last summary) decreases.
- Safety/privacy (optional)
  - Sensitive-leak flag using a redaction dictionary and PI detectors (no feedback to the model; log-only).

Longitudinal evaluation design
- Units and timeline
  - Units: streams (live documents/notes). Each stream comprises T ≥ 50 chunks (minutes to hours). Track per-stream evolution weekly for 4–6 weeks (to capture drift).
- Primary metrics (comparable to the paper)
  - Rollout error per chunk and area under error–horizon curve; rollout compute cost; stability metrics (oscillations/1000 tokens) [heirarchical-transformer.pdf:3].
- Streaming-specific outcomes
  - Failure subtype rates (per 1k tokens) and time-to-first-failure; boundary churn rate; edit reversion rate; selective-latency compliance (p95, p99).
- Baselines/controls
  - Original hierarchical model in batch/offline mode replayed on the same streams (no SLA).
  - Flat transformer with sliding window.
  - Hierarchy ablations (causal/uncertainty off) [heirarchical-transformer.pdf:3].
  - Optional quantized/on-device variant to emulate deployment constraints.

Preregistered hypotheses (directional, falsifiable)
- H1 Efficiency parity: In streaming mode, SHI maintains non-inferior rollout error (±2% relative) at equal or lower rollout compute cost vs flat transformer across matched streams [heirarchical-transformer.pdf:3].
- H2 Stability under growth: As streams lengthen, hierarchical masking exhibits a smaller increase in oscillation and boundary churn than flat attention (negative Method×log(tokens) interaction on failure rates) [heirarchical-transformer.pdf:2–3].
- H3 Retention under load: Under elevated input rates (tokens/sec), SHI’s retention miss rate grows more slowly than the flat baseline at the same SLA (lower degradation slope).
- H4 Causal/uncertainty contribution: Turning off uncertainty or causal guidance increases long-horizon error AUC and oscillation rates without reducing compute [heirarchical-transformer.pdf:3].

Statistical analysis plan
- Mixed-effects models (per metric)
  - Metric ~ β0 + β1·Method + β2·log(StreamLen) + β3·InputRate + β4·Method×log(StreamLen) + (1|Stream) + (1|Day).
  - β1 tests main effect; β4 tests stability under growth. Report estimates, 95% CIs, partial R^2.
- Paired tests (within stream)
  - Wilcoxon signed-rank on per-stream differences in rollout error AUC, compute cost, oscillation rate; Benjamini–Hochberg FDR over metrics.
  - Time-to-failure: paired Prentice–Wilcoxon; Cox model with shared frailty by stream for H2/H3.
- Equivalence/non-inferiority
  - TOST margins: rollout error ≤ +2% relative; oscillation rate ≤ +0.5/1k tokens; p95 latency within ±10 ms of target SLA.
- Uncertainty reporting
  - Clustered bootstrap (resample streams, then chunks) for BCa 95% CIs on all aggregate rates/curves.

Three concrete, falsifiable experiments
1) Real-time typing simulation
- Setup: Convert long-doc corpora into keystroke-timed streams; run hierarchical vs flat with identical budgets and SLA.
- Metrics: rollout error AUC, compute cost, p95 latency compliance, oscillations/1k tokens.
- Hypotheses: H1 holds; hierarchical meets SLA with lower compute at parity error.
- Pass/fail: TOST for error non-inferiority and Wilcoxon showing lower compute; latency compliance within margin.

2) Stream-growth stress (needle tracking)
- Setup: Inject “needle” facts early; query usage after 10–50k new tokens. Measure retention miss rate and boundary churn as length increases.
- Metrics: retention_miss_rate(length), boundary_churn_rate.
- Hypotheses: H2/H3—shallower degradation slope for hierarchical.
- Pass/fail: Mixed-effects β4 (Method×log(length)) significantly favors hierarchical; AU of miss-rate vs length lower for hierarchical with CI < 0.

3) Input-rate ramp and ablations
- Setup: Sweep input rates to induce backpressure; compare full model vs uncertainty-off and causal-off ablations.
- Metrics: SLA violations, oscillations, error AUC, compute cost.
- Hypotheses: H3/H4—full model has fewer SLA-triggered regressions and lower oscillations at high rates; ablations increase long-horizon error without compute benefit.
- Pass/fail: Wilcoxon for SLA violations and oscillation increases in ablations (FDR); compute shows no significant reduction.

Implementation checklist
- Insert streaming controller around the existing rollout loop; keep compute accounting identical to the paper [heirarchical-transformer.pdf:3].
- Add evented logging with bounded overhead (<2% FLOPs); write-ahead buffers to avoid I/O blocking.
- Implement detectors as offline jobs over logs; do not feed them into the model during evaluation (to avoid confounds).
- Version every config and checkpoint in telemetry; predefine SLA thresholds and hypotheses before data collection.

Sources
- [heirarchical-transformer.pdf:2] Sparse Hierarchical Imagination applies level-specific sparsification to focus on control-relevant state aspects while improving compute efficiency.
- [heirarchical-transformer.pdf:3] The evaluation reports rollout compute cost, rollout error, and final performance; ablations on hierarchical masking, causal guidance, and uncertainty-based masking.
- Dai et al., 2019. Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context — https://arxiv.org/abs/1901.02860
- Ribeiro et al., 2020. CheckList: A Behavioral Testing Framework for NLP — https://arxiv.org/abs/2005.04118

Citations
- [heirarchical-transformer.pdf:2] …Sparse Hierarchical Imagination (SHI) applies structured, level-specific sparsification… to improve computational efficiency…
- [heirarchical-transformer.pdf:3] Report rollout compute cost, rollout error, and final performance; and ablate hierarchical masking, causal guidance, uncertainty-based masking.