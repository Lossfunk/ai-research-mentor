Below is a concrete plan to test whether adding a privacy/governance constraint—specifically offline inference (no external retrieval/networked calls; no shared persistent memory across episodes)—to your Hierarchical Transformer world model changes verification-like failure rates. I align evaluation to the paper’s imagination pipeline and metrics (rollout compute cost, rollout error, and final RL performance), and include ablations the paper already proposes (hierarchical masking, causal graph guidance, uncertainty-based masking) to probe mechanisms [heirarchical-transformer.pdf:3]. The verification-like failures are attached to multi-horizon imagination rollouts consistent with the paper’s rollout-centric evaluation [heirarchical-transformer.pdf:3].

What “offline inference” means here
- No network or retrieval calls at inference time (no RAG to knowledge bases, no cloud APIs), no filesystem reads beyond the loaded model weights and environment state, and no persistent cross-episode cache. The model may use only its parameters and within-episode ephemeral memory/state (e.g., KV cache, latent state).
- Training remains unchanged to isolate the inference-time governance constraint.

Baselines and model variants
- H-Transformer (unconstrained): Your model as described, with imagination rollouts and standard evaluation (reference: report rollout error, compute cost, final RL performance) [heirarchical-transformer.pdf:3].
- H-Transformer + Offline: Same architecture; governance guardrails enforce offline inference.
- Flat Transformer (no hierarchy): A non-hierarchical Transformer world model trained and evaluated under the same conditions.
- Optional H-Transformer + RAG: If allowed for contrast, a retrieval-augmented variant to highlight the delta between online knowledge and offline. This is useful because retrieval and symbolic checking are commonly paired for reliability in hierarchical planners [P1].

Datasets/tasks
Choose a mix that (a) supports ground-truth transition checking and (b) admits rule/specification checking.
- Transition-grounded: DMControl (e.g., Walker, Cheetah), Procgen (diverse procedural tasks), and Atari-100k subset for short- and long-horizon rollouts.
- Rule/specification-grounded: MiniGrid (goal-reaching with constraints), Sokoban (pushing constraints) for symbolic rule-checkers (no illegal pushes, keep-out zones, goal constraints).
- Long-horizon variants (e.g., sparse-reward Procgen or long-horizon MiniGrid) to stress the effect of offline constraints when external knowledge might otherwise help [P1], and to probe sequence-modeling limits where hierarchy helps [P7].

Verification-like failure signals
Compute these on imagined rollouts produced by the model’s hierarchical imagination pipeline [heirarchical-transformer.pdf:3]:
- A. Rollout mismatch failures: fraction of steps where |predicted next state − env next state| exceeds tolerance ε on key variables, over 1-step and D-step horizons. Report per-dimension and aggregated rates; include RMSE/MAE as secondary.
- B. Internal consistency failures: multi-horizon cross-consistency violations (e.g., “roll D then roll one more step” vs “roll D+1” are inconsistent beyond ε; or re-rolling from predicted latent yields divergent futures).
- C. Rule/spec failures: tasks with a symbolic checker flag violations (illegal moves, constraint breaks). This mirrors the pairing of hierarchical planning with symbolic verification in related work [P1].
Also report the paper’s standard metrics (rollout compute cost, rollout error, final RL performance) to connect failure rates to overall performance/efficiency [heirarchical-transformer.pdf:3].

Experimental protocol
- Training: Same training for all non-RAG variants; the offline constraint only affects inference. If using RAG as a contrast, its retrieval index is frozen before eval (no learning at eval).
- Evaluation: Fix seeds; use identical episode sets per variant for paired comparisons. Target ≥5–10 seeds per task; ≥200–1,000 episodes per condition (tune via pilot power analysis).
- Logging: For each episode and horizon D ∈ {1, 5, 10, 20}, log A/B/C failures, rollout error curves, runtime (compute cost), and task return. For rule/spec tasks, log exact violated rules and timestamps.
- Controls: Match context length, KV cache/window, and decoding budget across variants. If offline constraints change ephemeral memory budget, vary it explicitly (see Experiment 2).

Three concrete, falsifiable experiments

Experiment 1 — Does offline inference change failure rates?
- Hypothesis: Offline constraints change verification-like failure rates compared to unconstrained H-Transformer.
- Conditions: H-Transformer (unconstrained) vs H-Transformer + Offline.
- Tasks: DMControl (Walker/Cheetah), Procgen (e.g., CoinRun, Heist), MiniGrid (GoToObject, DoorKey).
- Metrics:
  - Primary: A/B/C failure rates with Wilson 95% CIs; absolute risk difference and odds ratio.
  - Secondary: rollout error (RMSE), runtime (compute cost), final RL return [heirarchical-transformer.pdf:3].
- Statistical tests:
  - For failure rates: paired McNemar test (same episodes) or mixed-effects logistic regression with random intercepts for seed and task; report odds ratios and cluster-robust SEs.
  - For continuous metrics (rollout error, return): paired t-test or Wilcoxon signed-rank; linear mixed models if multiple levels (seed/task).
  - Multiple comparisons: Holm-Bonferroni or Benjamini–Hochberg across tasks and horizons.
- Expected outcome: Evidence of increase/decrease/no-change in failure rates; quantify with CIs and effect sizes (no directional assumption a priori).

Experiment 2 — Memory budget sensitivity under offline constraint
- Rationale: Offline constraints often co-occur with stricter memory governance. The paper already reports compute-cost vs rollout-error trade-offs, which we extend to failure rates [heirarchical-transformer.pdf:3].
- Hypothesis: Under offline conditions, decreasing ephemeral memory (shorter context/KV window or fewer hierarchical slots) increases A/B failure rates.
- Conditions: H-Transformer + Offline with KV/context windows W ∈ {128, 256, 512} and latent/slot counts varied (low/med/high).
- Tasks: Procgen long-horizon and MiniGrid DoorKey (long paths).
- Metrics: A/B failure rates vs memory; rollout error; runtime.
- Tests: Trend tests via logistic regression with W as predictor; pairwise tests with Holm correction; report effect sizes per doubling of W.

Experiment 3 — Can structural guidance mitigate offline failures?
- Rationale: The paper proposes ablations on hierarchical masking, causal graph guidance, and uncertainty-based masking [heirarchical-transformer.pdf:3].
- Hypothesis: Adding causal graph guidance and/or uncertainty-based masking reduces A/B/C failure rates under offline constraints.
- Conditions: H-Transformer + Offline with:
  1) base hierarchy,
  2) + hierarchical masking,
  3) + causal graph guidance,
  4) + uncertainty-based masking.
- Tasks: MiniGrid tasks with rules (DoorKey, Lava), Sokoban levels.
- Metrics: Reduction in C-rule violations and A/B consistency failures; rollout error and runtime to detect trade-offs.
- Tests: Mixed-effects logistic regression across variants; planned contrasts vs base offline; report adjusted p-values and effect sizes.

Optional contrast experiment (if RAG allowed)
- Offline vs RAG: Compare H-Transformer + Offline to H-Transformer + RAG on tasks where external knowledge plausibly helps (e.g., symbolic hints, puzzle rules). Expect RAG to lower rule violations or long-horizon mismatch in some settings, consistent with hierarchical planning augmented by external knowledge and symbolic verification [P1]. Analyze as in Experiment 1.

Statistical design and power
- Unit of analysis: Episode-level binary failures for A/B/C; additionally step-level models with cluster-robust SEs by episode and seed.
- CIs and effect sizes: Report Wilson CIs for proportions; absolute risk difference, risk ratio, odds ratio. For continuous: mean difference with 95% CI; standardized effect size (Hedges’ g) or Cliff’s δ if non-normal.
- Multiple comparisons: Control FWER with Holm-Bonferroni across horizons (D) and tasks within each experiment; control FDR across secondary metrics with BH.
- Power: Run a small pilot (e.g., 100 episodes/condition) to estimate baseline failure rate p0 and clinically relevant difference Δ. Use a two-proportion power calculation to set target N per condition; inflate by design effect for clustering by seed/task. Prefer ≥5–10 seeds.

Reporting checklist
- Clearly define offline governance constraint and enforcement mechanism (network isolation, retrieval disabled, no persistent cache).
- Pre-register metrics A/B/C and ε-thresholds per variable; publish code for symbolic checkers (MiniGrid/Sokoban).
- Report: failure rates with CIs, effect sizes, adjusted p-values; rollout error, compute cost, final RL return [heirarchical-transformer.pdf:3]; and ablation outcomes (hierarchical masking, causal graph guidance, uncertainty-based masking) [heirarchical-transformer.pdf:3].
- Release seeds, episode lists, and evaluation harness to ensure replicability.

Why these choices
- The imagination rollout-centric evaluation and the specific ablations directly mirror the paper’s evaluation design, ensuring alignment to its hypotheses and metrics [heirarchical-transformer.pdf:3]. 
- Symbolic rule-checkers provide crisp verification-like signals for long-horizon hierarchical controllers, as used in hierarchical planning with symbolic verification [P1].
- Hierarchical sequence models are well-motivated for long-horizon decision making, so testing their reliability under governance constraints probes exactly where hierarchy provides benefits [P7].

Citations
- [P1] Hierarchical Planning for Complex Tasks with Knowledge Graph-RAG and Symbolic Verification — http://arxiv.org/abs/2504.04578v1
- [P7] Hierarchical Decision Transformer — http://arxiv.org/abs/2209.10447v1

Note: Model-specific claims above are framed as hypotheses to be tested. All metric definitions and ablations are grounded in the attached paper’s evaluation section and imagination pipeline [heirarchical-transformer.pdf:3].