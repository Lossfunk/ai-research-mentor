Below is a zero-shot, hierarchy-free variant of your Hierarchical Transformer (SHI) that removes hierarchy-specific pretraining while keeping evaluation metrics and conditions directly comparable to the original protocol.

Zero-shot, hierarchy-free variant (Flat-ZS)
Goal: Eliminate all hierarchy-specific components (no hierarchical VQ-VAE, no level-specific masking, no causal-graph guidance) while preserving the rollout-based world-model pipeline and reporting the same metrics.

1) Tokens and encoder (single-level, no hierarchy)
- Replace hierarchical tokenization with a single-level discrete tokenizer that yields exactly K tokens per frame. Concretely, use a non-hierarchical VQ-VAE with a single codebook or a non-hierarchical token clustering procedure—the original pipeline already allows clustering/VQ-VAE; we simply set L=1 so no level structure is used [heirarchical-transformer.pdf:2]. 
- No hierarchy-specific pretraining: train the encoder jointly with the world model on the task data, or use a simple single-level VQ-VAE objective without any hierarchical loss or multi-level codebooks. This preserves the “agent interacting with environment → encode → autoregressive rollout” loop, just without levels [heirarchical-transformer.pdf:2].

2) World model and rollout
- Keep the autoregressive Transformer world model unchanged in spirit (action-conditioned, latent imagination, teacher-forced next-step prediction) [heirarchical-transformer.pdf:2–3].
- Use a stochastic Transformer to expose token-wise uncertainty (e.g., dropout or latent noise), following the stochastic world-model idea referenced in STORM; this provides uncertainty estimates without requiring hierarchy [heirarchical-transformer.pdf:3–4].
- Include a single memory token (as in DART) to summarize context and stabilize rollouts; again, no level-specific operations are required [heirarchical-transformer.pdf:3–4].

3) Masking/controller (single-level, no causal graph)
- Replace the level-specific masking functions m(l)t with a single, global mask m_t ∈ {0,1}^K at each step. A small controller g outputs m_t = σ(g(h_t, u_t, c_t)), where:
  - h_t: context features (e.g., memory token),
  - u_t: time-step embedding,
  - c_t: auxiliary signals limited to uncertainty estimates and attention-to-memory scores only (retain STORM-like uncertainty and DART-like memory attention; omit SPARTAN causal-graph relevance) [heirarchical-transformer.pdf:3–4].
- Enforce a token-keep budget R to match the expected number of kept tokens per step used by SHI, ensuring comparable compute when we report FLOPs per rollout step [heirarchical-transformer.pdf:3].

4) Training objective and optimization
- Self-supervised next-token prediction on discrete latents (cross-entropy on token indices), optionally plus image reconstruction loss via the VQ-VAE decoder, identical to the standard world-model pipeline but without level-specific components [heirarchical-transformer.pdf:2–3].
- Train end-to-end on the same environment interaction budget as SHI; no additional pretraining phases that introduce hierarchical inductive biases.

Evaluation and metric comparability
We keep the same evaluation dimensions and match budgets so the numbers remain directly comparable.
- Report exactly the same three headline metrics: rollout compute cost (FLOPs), rollout error, and final RL performance [heirarchical-transformer.pdf:3].
  - Rollout compute cost: measure FLOPs per rollout step and per episode at the same rollout depth as SHI [heirarchical-transformer.pdf:3].
  - Rollout error: token-level cross-entropy and image-level reconstruction error at long horizons (e.g., 50–100 steps), as in the SHI hypotheses regarding stability and error [heirarchical-transformer.pdf:3].
  - RL performance: final median human-normalized scores on Atari 100k and Crafter under the same data budgets and training schedules [heirarchical-transformer.pdf:3].
- Matching conditions for fairness:
  - Token budget: Match total K tokens per frame to the sum over levels used by SHI; set the Flat-ZS keep-rate schedule so that E[|kept|] per step equals the SHI expected kept tokens [heirarchical-transformer.pdf:2–3].
  - Model capacity: Match parameter count (within ~1%), depth, d_model, number of heads.
  - Rollout protocol: Same rollout horizon, same action-conditioning, same planner/policy integration, and same environment interaction budget (e.g., 100k frames for Atari 100k) [heirarchical-transformer.pdf:3].
  - Seeds and logging: Same random seeds, identical logging cadence and checkpointing to ensure reproducibility of compute and returns.
- Baselines for comparison: Compare Flat-ZS to SHI and to a flat token-dropout baseline akin to “Sparse Imagination” at equal rollout depth, as noted in your hypotheses section [heirarchical-transformer.pdf:3].

Why metrics remain comparable
- The document’s evaluation focuses on rollout compute cost, rollout error, and final RL performance at matched rollout depths and against established baselines [heirarchical-transformer.pdf:3]. By holding constant K, the expected kept tokens per step, the rollout horizon, and parameter count, Flat-ZS preserves the main computational and difficulty axes that drive these metrics. The only change is the removal of hierarchy-specific machinery (levels, level-specific masking, and causal-graph guidance), which is the intended ablation. Thus, observed deltas can be attributed to the hierarchy rather than confounds [heirarchical-transformer.pdf:2–3].

Concrete training configuration (example)
- Encoder: Single-level VQ-VAE with one codebook (size e.g., 8k), patch size 8–16, producing K tokens per frame to match SHI’s total tokens across levels [heirarchical-transformer.pdf:2].
- World model: 12-layer Transformer, d_model and heads matched to SHI; stochastic variant via dropout to obtain uncertainty signals (as in STORM) [heirarchical-transformer.pdf:3–4].
- Controller: 2-layer MLP that ingests [memory token; time-step embedding; uncertainty per token; attention-to-memory score per token], outputs logits over K tokens → top-k or threshold to meet a per-step keep budget that mirrors SHI’s expected kept tokens [heirarchical-transformer.pdf:3].
- Losses: Next-token cross-entropy on latents; optional image reconstruction loss via decoder; entropy regularizer on mask to stabilize keep-rate; no hierarchy-specific losses [heirarchical-transformer.pdf:2–3].
- Optimization: Same schedules, batch sizes, updates as SHI; same rollout depth during training and evaluation; identical environment interaction budgets (Atari 100k, Crafter) [heirarchical-transformer.pdf:3].

Three falsifiable experiments
1) Flat-ZS vs SHI under matched token and compute budgets
- Hypothesis: With matched K, keep-rate schedule, rollout depth, and parameter count, Flat-ZS achieves comparable FLOPs per rollout step and exhibits modestly higher long-horizon rollout error, with final RL scores within a statistically bounded margin of SHI.
- Variables: Model type ∈ {SHI, Flat-ZS}; keep-rate schedule; rollout depth.
- Controls: K per frame, expected kept tokens per step, parameter count, seeds, training budget.
- Metrics: FLOPs/rollout step; token-level cross-entropy and image-level error at horizon H; final median human-normalized scores on Atari 100k and Crafter [heirarchical-transformer.pdf:3].
- Expected outcome: FLOPs within ±5%; rollout error possibly higher for Flat-ZS; RL performance within a defined margin (to be pre-registered). Deviations indicate hierarchy contributes beyond sparsification alone.

2) Importance-signal ablation within Flat-ZS
- Hypothesis: Uncertainty and memory-attention signals suffice to guide masking without hierarchy or causal graphs; removing uncertainty increases rollout error and decreases RL returns.
- Conditions: Flat-ZS with (a) uncertainty+memory attention; (b) memory attention only; (c) random flat dropout at same keep-rate.
- Controls: Same K, keep-rate schedule, parameter count, rollout depth, seeds.
- Metrics: FLOPs/step; rollout error at horizon H; Atari 100k and Crafter performance [heirarchical-transformer.pdf:3].
- Expected outcome: (a) outperforms (b) and (c) on rollout error and RL returns; if not, uncertainty signals are not pulling their weight without hierarchy.

3) Keep-rate schedule equivalence test
- Hypothesis: When matching expected kept tokens per step to SHI, Flat-ZS’s compute scales equivalently with keep-rate, but its error degrades faster at low keep-rates due to lack of semantic prioritization by levels.
- Conditions: Flat-ZS and SHI across 3–4 matched keep-rate schedules (e.g., high/medium/low).
- Controls: Same K, model capacity, rollout depth, seeds.
- Metrics: FLOPs/step vs. rollout error curves; area-under-curve; final RL performance per schedule [heirarchical-transformer.pdf:3].
- Expected outcome: Similar FLOPs scaling; steeper error increase for Flat-ZS as keep-rate decreases. If curves match, hierarchy may not be necessary for sparsification at given budgets.

Why this design respects your protocol
- It directly removes only hierarchy-specific components (multi-level latents, level-specific masking, causal-graph guidance) while retaining the same world-modeling pipeline, controller inputs that are permitted (uncertainty, memory attention), and benchmark metrics (compute, rollout error, RL returns) [heirarchical-transformer.pdf:2–4]. By matching rollout depth, token budgets, and parameter counts, we ensure any performance deltas reflect the effect of hierarchy rather than confounding factors [heirarchical-transformer.pdf:3].

Literature anchors (for design choices)
- Using stochastic Transformers for uncertainty in world models is consistent with STORM [Weipu Zhang et al., 2023] (https://arxiv.org/abs/2310.09615) [heirarchical-transformer.pdf:4].
- Sample-efficient Transformer world models and the Atari 100k setting are discussed in “Transformers are sample-efficient world models” [Micheli et al., 2023] (https://arxiv.org/abs/2209.00588) [heirarchical-transformer.pdf:4].

Citations
- [heirarchical-transformer.pdf:2] We consider an agent… hierarchical VQ-VAE or token clustering… L levels… goal of SHI is to apply structured, level-specific sparsification…
- [heirarchical-transformer.pdf:3] We hypothesize that SHI will reduce FLOPs and rollout error… report rollout compute cost, rollout error, and final RL performance… baselines and evaluation on Atari 100k and Crafter… masking/controller design with uncertainty and memory attention (and causal-graph relevance in SHI).
- [heirarchical-transformer.pdf:4] References including STORM and “Transformers are sample-efficient world models.”
- Weipu Zhang et al., 2023. STORM: Efficient stochastic transformer based world models for reinforcement learning — https://arxiv.org/abs/2310.09615
- Vincent Micheli et al., 2023. Transformers are sample-efficient world models — https://arxiv.org/abs/2209.00588

Sources
- Sparse Hierarchical Imagination manuscript excerpts as cited above.
- Weipu Zhang et al., 2023. STORM: Efficient stochastic transformer based world models for reinforcement learning. https://arxiv.org/abs/2310.09615
- Vincent Micheli et al., 2023. Transformers are sample-efficient world models. https://arxiv.org/abs/2209.00588