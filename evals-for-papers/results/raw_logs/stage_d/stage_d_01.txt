Below is a concise checklist to replicate the “LLM-as-a-Judge” pipeline in Why Do Multi‑Agent LLM Systems Fail? (Section 3.4) and to avoid leakage between taxonomy development and evaluation. I also include concrete validation experiments and literature anchors.

Key components to reproduce
- Failure taxonomy and rubric
  - Define a fixed set of failure categories with operational definitions and decision rules (e.g., mutually exclusive vs. multi-label; severity tiers). Lock the taxonomy before any test-time evaluation.
  - Provide a structured, unambiguous rubric and a strict output schema (JSON with required fields) to reduce judge variability [P3].
- Judge prompt and protocol
  - A single judging prompt (or small set) that instructs the LLM judge how to apply the rubric; includes: (a) the task, (b) the agent’s output, and (c) the rubric; optional: brief rationale requirement; no system identity or hyperparameter hints.
  - Fix model, temperature, decoding parameters, and sampling seeds; prefer a low-variance setting.
  - Consider multi-judge aggregation or a meta-judge for adjudication to reduce variance and bias [P2]. Blind the judge to system identity to reduce bias amplification [P1].
- Dataset split and sequencing
  - Separate dev vs. test tasks and outputs. Use dev only to draft/refine the taxonomy, prompts, and exemplars; use test only once, after freezing these.
  - Register a “prompt+rubric+taxonomy” version and do not change it once test evaluation begins.
- Scoring and aggregation
  - Per-instance labeling for each failure type, with clear aggregation rules (e.g., proportion of instances exhibiting each failure; CIs via bootstrap).
  - Reliability checks: compute agreement across multiple LLM judges and with a small human-labeled sample when possible.
- Reporting and reproducibility
  - Release: taxonomy text (versioned), judge prompts, judge model/version, decoding params, seeds, task lists, raw judgments, and code to re-run scoring.

Leakage risks and how to prevent them
- Using test data to shape the taxonomy or judge prompt
  - Freeze taxonomy and prompts based solely on dev; preregister or log a version hash and timestamp before touching test.
  - Keep all few-shot exemplars in the judge prompt drawn from dev only.
- Implicit leakage through examples in prompts
  - Remove any example that is identical or near-duplicate to test items; detect with fuzzy matching.
- Judge exposure to system identity or experimental condition
  - Blind the judge: do not include agent names, temperatures, or model families in the judging context; randomize output ordering and anonymize headers [P1].
- Format and verbosity biases
  - Normalize formatting for all systems (e.g., strip metadata, enforce a standard transcript format sent to the judge). Otherwise, the judge may pick up spurious cues [P3].
- Overfitting via prompt iteration on test
  - Disallow prompt edits after the first test pass. If any fix is unavoidable (e.g., JSON parsing bug), rerun all systems under the same updated prompt and mark the change as a new preregistered version.
- Single-judge instability
  - Use n≥3 independent samples or ≥2 distinct judge models (cross-family if possible) with a clear aggregation policy (majority vote or meta-judge) [P2]. Report inter-judge agreement.
- Family bias and self-preference
  - Avoid using the same model family as both agent and judge when possible; cross-family judging reduces self-preference and bias amplification [P1].
- Chain-of-thought contamination
  - If you require rationales, ensure they don’t leak reference solutions or evaluation hints into future stages; hide rationales from any subsequent generation models.

Minimal replication checklist (practical)
- Data: Disjoint dev/test task lists; dev-only exemplars bank; hashed splits.
- Taxonomy: Versioned rubric text with last-modified timestamp prior to test.
- Judge: Model ID/version, decoding params, frozen prompts, and a JSON schema.
- Blinding: Agent/system IDs removed; outputs normalized; randomized order.
- Reliability: Plan for multi-judge or repeated samples and a small human check set.
- Logs: All runs with seeds and prompt hashes; a public artifact for re-running.

Concrete, falsifiable validation experiments
1) Leakage stress test via exemplars
   - Hypothesis: If any leakage exists (dev exemplars overlap with test), failure rates will improve spuriously for overlapping cases.
   - Method: Intentionally create a “leaky” condition by inserting near-duplicate dev exemplars of certain test items into the judge prompt; compare failure rates vs. a clean prompt.
   - Metrics: Difference in failure rate per category; McNemar’s test on paired judgments; 95% CIs via bootstrap.
   - Expected outcome: A significant drop in judged failures for overlapped items would indicate sensitivity to leakage; in clean replication this drop should be near zero.

2) Blinding ablation
   - Hypothesis: Blinding reduces judge bias toward/against particular systems.
   - Method: Evaluate the same system outputs under blinded vs. unblinded prompts (unblinded includes system name or family); randomize order.
   - Metrics: Mean difference in failure rates and effect size (Cohen’s d); inter-judge agreement.
   - Expected outcome: Unblinded prompts produce larger variance and shifted failure rates; blinded prompts yield more stable estimates [P1].

3) Cross-judge reliability and human calibration
   - Hypothesis: A robust pipeline yields substantial agreement across judge models and with humans.
   - Method: Judge with two distinct models (e.g., two families) and a meta-judge adjudicator; obtain a 100–200 item human-labeled subset.
   - Metrics: Cohen’s kappa/Kendall’s tau between judges and human set; majority-vote vs. meta-judge consistency; 95% CIs.
   - Expected outcome: Moderate-to-strong agreement (e.g., κ≥0.6) and small deltas between majority vote and meta-judge [P2], with structured rubric helping consistency [P3].

4) Prompt version lock test
   - Hypothesis: Post-hoc prompt edits inflate apparent performance.
   - Method: Evaluate on test with v1 prompt, then (improperly) tweak prompt after seeing results; re-evaluate. Compare to a proper preregistered v2 evaluated fresh.
   - Metrics: Change in failure rates and the share of instances that flip labels.
   - Expected outcome: Large swings after post-hoc edits signal overfitting; properly preregistered v2 shows smaller, explainable changes.

Notes on evaluation rigor
- Prefer structured, rubric-driven judgments with JSON schemas and explicit definitions to reduce ambiguity [P3].
- Use multi-judge or adjudication frameworks to stabilize results and expose judge-specific biases [P2].
- Blind the judge and consider cross-family judges to mitigate bias amplification and self-preference [P1].

Literature anchors (for your replication report)
- Why Do Multi-Agent LLM Systems Fail? — arXiv PDF; see Section 3.4 for their LLM-as-a-Judge setup: https://arxiv.org/pdf/2503.13657.pdf
- Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena (Zheng et al., 2023): https://browse.arxiv.org/html/2306.05685v4
- A Survey on LLM-as-a-Judge (2024): https://arxiv.org/html/2411.15594v3

Citations
- [P1] Judging with Many Minds: Do More Perspectives Mean Less Prejudice? On Bias Amplifications and Resistance in Multi-Agent Based LLM-as-Judge — http://arxiv.org/abs/2505.19477v3
- [P2] Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments — http://arxiv.org/abs/2504.17087v1
- [P3] Multi-Agent LLM Judge: automatic personalized LLM judge design for evaluating natural language generation applications — http://arxiv.org/abs/2504.02867v1
- Why Do Multi-Agent LLM Systems Fail? — https://arxiv.org/pdf/2503.13657.pdf
- Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena — https://browse.arxiv.org/html/2306.05685v4

If you can share your current taxonomy draft and judge prompt, I can review them for leakage risks and suggest a locked version suitable for test-time evaluation.