Below are likely reviewer concerns about the validity of the “probably-rejected” arXiv labels in PeerRead, along with concrete mitigations you can commit to in the discussion. Where possible, we separate claims supported by the attached PeerRead paper from those grounded in the public description of the dataset.

What reviewers may question
- Construct validity of “rejection” on arXiv: A paper not found in target venue databases may have never been submitted, may still be under review, or may have been accepted later or in a different venue. The arXiv label is therefore a heuristic proxy rather than an observed decision [1]. Mitigation below.
- Time-lag bias: If the matching window is short, papers later accepted can be mislabeled as “rejected.” Many arXiv preprints get accepted months to years later [1].
- Entity-resolution/matching errors: Imperfect title/author matching to bibliographic sources (e.g., DBLP, proceedings) can create false negatives that inflate “probably-rejected” counts [1].
- Coverage/venue bias: “Probably-rejected” is often defined with respect to a limited set of focal venues; acceptance in journals, workshops, or adjacent conferences can be missed [1].
- Subfield/practice heterogeneity: arXiv posting and eventual publication rates vary by area and year; this can systematically bias the label.
- Downstream modeling bias: Treating noisy “probably-rejected” as true negatives can distort classifier training; models may overfit to arXiv-specific artifacts instead of review-relevant signals.
- Ethical/interpretive risk: The label name suggests a definitive outcome when it is, in fact, a weak/noisy inference; this can be misinterpreted by readers.

Commitments and mitigations to include
- Use arXiv labels only as weak supervision; prefer gold decisions for evaluation. We will restrict primary evaluations to conference subsets with observed accept/reject decisions (e.g., ACL 2016, ICLR 2017, which PeerRead explicitly includes) [PeerRead.pdf:2; PeerRead.pdf:15]. We will treat arXiv “probably-rejected” only for auxiliary training or analysis, never as ground truth for headline results.
- Reframe as PU learning instead of hard negatives. We will treat arXiv “probably-rejected” as unlabeled rather than negative and apply positive–unlabeled learning, which is designed for settings with known positives and ambiguous unlabeled data [3]. We will report performance vs. standard supervised baselines.
- Calibrate the label with multi-source matching and a longer window. We will re-implement and extend the original matching with DOIs, fuzzy title/author matching, and multiple bibliographic sources (DBLP, Crossref, OpenAlex), using a ≥24–36 month window from arXiv posting to reduce late-acceptance false negatives [1].
- Provide label-confidence scores, not binary flags. Each arXiv item will receive a probability of “unpublished in target venues by T months,” based on concordant matches across sources and text signals (e.g., “Accepted at …” in PDF or arXiv comments).
- Exclude ambiguous cases. We will remove arXiv papers that self-report “under review,” or that show any later acceptance indicator, from the “probably-rejected” pool.
- Sensitivity analyses. All key findings will be re-run (a) without arXiv data, (b) with arXiv included as weakly labeled (PU), and (c) with arXiv included as down-weighted negatives (e.g., 0.3–0.5 sample weights). We will explicitly quantify any changes.
- Manual audit to estimate noise. We will hand-verify a stratified random sample of “probably-rejected” papers to estimate precision and characterize common failure modes (late acceptance, venue coverage gaps, matching errors). We will report this as an error bar on any analysis using these labels.
- Transparent naming and documentation. In the paper, we will rename the label to “unmatched-to-target-venue-within-T-months (heuristic)” and clearly document construction, confidence, and limitations to avoid over-interpretation.

Three concrete, falsifiable experiments
1) Label audit and precision estimate
- Sample: 200 arXiv items labeled “probably-rejected” across years and subfields.
- Procedure: Search DBLP, Crossref, OpenAlex, Google Scholar, and paper PDFs/comments for later acceptances up to 36 months post arXiv posting; record match provenance and time-to-acceptance.
- Outcome: Estimate the false-negative rate of the matching (i.e., fraction actually accepted somewhere) with 95% CIs; report by year and venue-type.
- Falsifiable criterion: If precision <80%, we will not use these labels as negatives; we switch to PU-only analyses.

2) Robustness to label usage
- Train the same acceptance-prediction model under three conditions: (A) gold conference labels only (train and test on ACL/ICLR subsets) [PeerRead.pdf:2; PeerRead.pdf:15], (B) add arXiv as hard negatives, (C) add arXiv as PU (no negatives; PU risk estimator).
- Outcome: Compare AUC/F1 on a gold-labeled held-out test set. Pre-register a non-inferiority margin (e.g., ΔAUC ≤ 0.01 vs. A). 
- Falsifiable criterion: If (B) underperforms (A) by >0.01 AUC while (C) matches (A), we conclude hard-negative use is harmful and adopt PU.

3) Extended matching and impact analysis
- Implement extended matching (DOI+fuzzy title/author; DBLP+Crossref+OpenAlex) and a 36-month window.
- Outcome: Measure the fraction of “probably-rejected” reclassified as matched/accepted compared to the baseline heuristic; quantify the change in model results when using the updated labels.
- Falsifiable criterion: If ≥10% of “probably-rejected” become matched, we will revise all analyses and report the delta; otherwise, we justify the original heuristic as sufficiently precise for augmentation.

Optional fourth experiment (ablation)
- Down-weighting study: Vary sample weights on arXiv negatives from 0.1 to 1.0; plot performance on gold-labeled test. Pre-specify the weight chosen by validation performance stability.

How we will report this in the discussion
- We will explicitly state that arXiv “probably-rejected” are heuristic, not observed decisions, and present their estimated noise via the manual audit. Primary claims will rely on gold conference decisions (ACL/ICLR subsets) already present in PeerRead [PeerRead.pdf:2; PeerRead.pdf:15]. We will include sensitivity analyses demonstrating that conclusions are robust to excluding or reweighting the arXiv subset, and we will release our matching code, audit annotations, and label-confidence scores for transparency.

Citations
- [PeerRead.pdf:2] The dataset includes official conference-reviewed subsets (e.g., ACL 2016, ICLR 2017), which we use for gold-labeled evaluation.
- [PeerRead.pdf:15] The review form includes an explicit accept/reject recommendation, grounding our use of conference decisions as gold labels.
- [1] Kang, D., Ammar, W., Schwartz, R., & Cohen, W. W. (2018). A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications — https://aclanthology.org/N18-1149.pdf
- [2] allenai/PeerRead (dataset and construction details, including arXiv subset) — https://github.com/allenai/PeerRead
- [3] Elkan, C., & Noto, K. (2008). Learning classifiers from only positive and unlabeled data — https://dl.acm.org/doi/10.1145/1401890.1401920

Sources
[1] A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications — https://aclanthology.org/N18-1149.pdf
[2] allenai/PeerRead: Data and code — https://github.com/allenai/PeerRead
[3] Elkan & Noto (2008), Learning from Positive and Unlabeled Data — https://dl.acm.org/doi/10.1145/1401890.1401920