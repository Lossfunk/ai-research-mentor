Below I focus on threats that stem specifically from how PeerRead v1 was assembled and what that implies for analyses built on it, then list concrete analyses/experiments to proactively address likely reviewer critiques.

Most critical threats to validity in PeerRead’s data collection
- Coverage and representativeness across venues and years
  - PeerRead aggregates reviews from open-review venues (e.g., ICLR 2017) and selected NLP venues; coverage varies by venue and year, so distributions of decisions, review formats, and rubrics are not uniform across the corpus [P1]. This threatens external validity (models may capture venue/year idiosyncrasies rather than general peer-review signals).
- Missing-not-at-random (MNAR) reviews and decisions
  - Some venues in PeerRead have more complete decision/review availability than others (e.g., open-review venues expose rejected and accepted reviews; closed venues often expose accepted-paper information only), creating selection bias in which rejected papers may be underrepresented [P1]. If missingness depends on acceptance or quality, standard analyses become biased.
- Heterogeneous review types and rubrics
  - OpenReview includes assigned reviews, meta-reviews, and public comments with different purposes and styles; venues also differ in numerical rubrics (e.g., overall score, confidence, and aspect-specific dimensions) [P1]. Mixing these without stratification introduces measurement error and spurious correlations.
- Paper–review version mismatch
  - Open-review workflows allow iterative revisions and discussion; if reviews are linked to later versions or updated titles/abstracts, text–label associations can reflect post-review changes, inflating predictive performance or confounding causal interpretation [P1].
- Label noise and coarse decision labels
  - Acceptance is often encoded as a single binary/ordinal label, but desk rejects, withdrawals, and borderline decisions may be collapsed or inconsistently recorded across venues [P1]. This induces outcome noise and can mislead acceptance-prediction models.
- Limited domain and temporal scope
  - The dataset emphasizes ML/NLP venues and a narrow time window; linguistic and reviewing norms evolve over time. Without temporal/venue controls, results may not transfer outside the covered venues/years [P1].

Analyses and ablations to preempt reviewer critiques
- Quantify and model missingness
  - Report per-venue/year distributions: number of submissions, fraction with reviews, number with decisions, reviews-per-paper, and score coverage. Test whether missingness correlates with acceptance or simple text features (e.g., length, presence of code/data links). If MNAR is evident, perform sensitivity analyses (e.g., weighting or worst-case bounds) to show robustness.
- Disentangle review types and rubrics
  - Separate assigned reviews, meta-reviews, and public comments; replicate headline analyses within each subset and report discrepancies. Normalize or stratify by rubric (e.g., ICLR vs ACL) instead of pooling heterogeneous scores.
- Control for venue and time
  - Use venue- and year-fixed effects in statistical models; report cross-venue and cross-year generalization (train on venue A/year t, test on venue B/year t′). Provide results with and without fixed effects to show the extent of venue-specific signal.
- Guard against version leakage
  - Freeze a deterministic linkage between reviews and the exact paper version at the time of each review; rerun analyses with earlier vs later versions to show invariance. If unavailable, perform a “sanity” ablation using titles/abstracts retrieved at submission time only.
- Robustness to label noise
  - Estimate noise by identifying borderline submissions (e.g., narrow score ranges or conflicting reviewer scores) and re-run acceptance prediction excluding them. Calibrate predictive performance under simulated label flips near thresholds.
- Reviewer heterogeneity
  - Stratify by reviewer confidence and length; evaluate whether results hold across short vs long reviews and low- vs high-confidence reviews.
- Data statements and reproducibility packet
  - Release a brief datasheet summarizing sampling frames, inclusion/exclusion, missingness patterns, and preprocessing decisions; include code to reproduce splits and filters. This anticipates requests for transparency and auditability.

Three concrete, falsifiable experiments
1) Cross-venue/time generalization test
   - Hypothesis: Acceptance predictors trained on one venue/year substantially degrade when evaluated on a different venue/year, indicating limited external validity.
   - Setup: Train acceptance-prediction models on ICLR 2017 and test on another venue/year available in PeerRead; reverse the direction as well. Include simple baselines (bag-of-words) and stronger text models; include venue/year-fixed-effect baselines.
   - Metrics: AUROC, AUPRC, and calibration error with 95% CIs.
   - Expected outcome: Significant drop in performance across venues/years compared to in-venue splits; partial recovery when adding fixed effects or reweighting.
2) Review-type stratification and sensitivity
   - Hypothesis: Models trained on mixed review types (assigned reviews + public comments) behave differently than those trained on assigned reviews only; public comments introduce measurable bias.
   - Setup: Identify and separate assigned reviews vs public comments (using metadata available in OpenReview-derived records). Train/evaluate acceptance and score-prediction models in three conditions: assigned-only, comments-only, and mixed. Repeat per venue.
   - Metrics: Performance deltas, distributional shift diagnostics (e.g., average length, sentiment), and feature importances.
   - Expected outcome: Assigned-only models show different feature reliance and better calibration; mixed-condition results vary by venue.
3) Missingness mechanism and robustness analysis
   - Hypothesis: Missing decisions/reviews are MNAR with respect to acceptance and text features; reweighting or conservative imputations materially change conclusions.
   - Setup: Fit a logistic model predicting “observed vs missing” as a function of venue, year, abstract length, and simple lexical indicators; test MAR vs MNAR patterns. Then compare headline results under (a) complete-case analysis, (b) inverse propensity weighting, (c) worst-case bounds that treat missing as all accepted vs all rejected.
   - Metrics: Effect-size shifts (e.g., change in coefficients, AUROC), overlap diagnostics, and sensitivity curves.
   - Expected outcome: Evidence of MNAR; robustness bands that either support or qualify headline claims.

Optional analyses if aspect annotations are used
- Inter-annotator agreement and error analysis: Report kappa/alpha on a held-out double-annotated set; re-run models after removing low-agreement items to assess noise sensitivity.
- Temporal drift: Plot score distributions and review lengths over years to document shifts; test models trained on early years vs later years.

Literature anchors
- PeerRead: A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications. Use this to precisely describe included venues/years, review/score fields, and decision labels; cite when reporting coverage and rubrics [P1].
- Optional comparative anchor for context: Studies leveraging open-review data to analyze acceptance determinants (e.g., work analyzing arXiv preprinting and acceptance) can motivate controlling for confounders like time and publicity when using open-review data.

Citations
- [P1] A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications — http://arxiv.org/abs/1804.09635v1

Sources
- PeerRead paper [P1] for dataset scope, sources, and fields.