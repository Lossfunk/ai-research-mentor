Below are the most salient threats to validity in PeerRead’s data collection, followed by targeted analyses and falsifiable experiments to anticipate reviewer critiques.

Most critical threats to validity in data collection
- Venue and policy selection bias: A substantial fraction of PeerRead derives from open-review venues (e.g., ICLR) and a small set of NLP/ML conferences; this may not represent closed-review venues or other fields. Acceptance rates, reviewer behavior, score scales, and norms differ by venue and year, limiting external validity [Kang et al., 2018]. Open-review platforms themselves shape behavior (public visibility, early comments) in ways not comparable to closed review [Sethi et al., 2021].
- Self-selection and coverage bias: For non-open venues, reviews that appear publicly (or papers that appear on arXiv) are a selective subset; authors who share may be systematically different (topic, seniority, confidence), skewing the corpus [Kang et al., 2018].
- Label noise in acceptance decisions: For arXiv-linked papers, acceptance labels often require heuristic matching to venue programs; errors, ambiguity across versions, or desk rejections can introduce mislabeled instances [Kang et al., 2018].
- Version drift and construct validity: Reviews refer to the submission version; publicly available PDFs may be later revisions, creating mismatch between review text and paper content. Using acceptance as a proxy for “quality” is itself noisy given well-documented inconsistency in peer review decisions [Cortes & Lawrence, NeurIPS 2014 experiment].
- Heterogeneous and missing metadata: Score scales, rubric fields, and confidence definitions differ across venues/years; naive pooling without calibration can distort analyses (e.g., 1–10 vs 1–5 scales, missing confidence for some venues) [Kang et al., 2018].
- Topic/time confounds: Temporal shifts (policy changes, reviewer guidelines, acceptance rates) and topic composition (e.g., more deep learning after 2015) can act as confounders; models may learn era/venue signals instead of substantive review–decision relations [Kang et al., 2018].
- Text extraction/processing errors: PDF-to-text conversion, section segmentation, and deduplication can introduce systematic noise (e.g., math-heavy or scanned PDFs underrepresented), biasing downstream tasks [Kang et al., 2018].

Analyses to preempt common reviewer critiques
- Quantify representativeness: Compare topic distributions (e.g., venue categories, keywords), paper length, and acceptance rates in PeerRead vs. official venue statistics (when available). Report KS distances and Jensen–Shannon divergence. Stratify by venue and year.
- Score-scale harmonization: Provide per-venue, per-year z-score calibration for numerical fields (overall, confidence). Show that headline results are stable under: raw scores, per-venue z-scores, and ordinal-only encodings.
- Label noise audit: Manually verify a stratified random sample of arXiv–venue matches; estimate precision/recall of acceptance labels. Re-run key models with noise-robust losses and with labels corrected for the sample via EM/noise-transition estimation; report sensitivity bands.
- Version-mismatch assessment: For a sample, compute text similarity between the reviewed draft (if available) and the public PDF used; quantify revision magnitude and show robustness of results when restricting to low-drift cases.
- Missingness and selection: Report missing-data patterns for all fields by venue/year. Use inverse-probability weighting or multiple imputation to show that main findings are not artifacts of missingness.
- Temporal and cross-venue generalization: Train on earlier years/venues, test on later/held-out venues; report degradation to demonstrate domain shift and prevent overclaiming.
- Negative/sanity controls: Show that trivially spurious features (page length, citation count, section count) cannot alone achieve strong acceptance prediction; helps counter “data leakage” critiques.
- Inter-reviewer consistency: Where multiple reviews per paper exist, report within-paper variance and agreement (e.g., ICC, Krippendorff’s alpha); compare across venues to contextualize model ceilings.

Concrete, falsifiable experiments
1) Label-noise sensitivity for acceptance prediction
- Hypothesis: Acceptance-prediction performance is stable under plausible levels of label noise introduced by arXiv–venue matching.
- Design: Randomly flip y% of training labels (y in {2, 5, 10}); compare a baseline classifier vs. a noise-robust variant (e.g., bootstrap loss correction) on a clean, manually verified test set.
- Metrics: AUC/accuracy on verified test set; calibration error (ECE); confidence intervals via bootstrap.
- Falsifiable outcome: If performance drops >5 AUC points at y=5% for both models and noise-robust training does not recover performance, claims about model reliability are undermined.

2) Cross-venue and temporal robustness
- Hypothesis: Models trained on ICLR (open review) do not generalize without loss to ACL/EMNLP (closed or partially open) or to later years due to distribution shift.
- Design: Train acceptance prediction on ICLR-2017; test on ACL/EMNLP-2017 and ICLR-2018. Repeat with per-venue z-score normalization vs. no normalization.
- Metrics: Cross-domain AUC/accuracy; relative performance drop; significance via DeLong test.
- Falsifiable outcome: If cross-venue performance equals in-domain within ±1 AUC point, the claimed venue-induced bias is not supported.

3) Scale harmonization ablation
- Hypothesis: Pooling uncalibrated score scales distorts effects; per-venue z-scores restore consistency in regression linking scores to acceptance.
- Design: Fit logistic regressions predicting acceptance from review scores and confidence under three encodings: raw, per-venue z, and ordinal (rank). Include venue-year fixed effects.
- Metrics: Coefficient stability (CIs), pseudo-R^2, likelihood-ratio tests, and Hausman tests across encodings.
- Falsifiable outcome: If coefficients and fit are invariant across encodings, scale heterogeneity is not a material threat here.

4) Version-drift robustness
- Hypothesis: Using the public PDF instead of the reviewed draft introduces measurable drift that affects text-based models more than score-based baselines.
- Design: For papers with both draft and final versions, compute cosine similarity (TF-IDF or SBERT). Partition by similarity tertiles. Train text-only and score-only models on high-similarity vs. all papers; compare test performance on high-similarity subset.
- Metrics: AUC/accuracy differences; interaction term significance (model type × similarity tertile) in regression.
- Falsifiable outcome: If text-only performance does not improve when restricting to high-similarity pairs, version drift is unlikely to be driving results.

5) Inter-reviewer agreement and ceiling analysis
- Hypothesis: Low review agreement caps achievable prediction accuracy; observed model performance aligns with this ceiling.
- Design: Compute ICC/alpha across multiple reviews per paper (by venue). Derive a theoretical upper bound for acceptance prediction via a simple latent-variable model using reviewer variance. Compare model AUC to this bound.
- Metrics: ICC/alpha, bound vs. achieved AUC; goodness-of-fit for the latent model.
- Falsifiable outcome: If models exceed the bound implied by reviewer agreement, there may be leakage or mis-specification in the ceiling estimate.

One- to two-sentence literature anchors you can cite in the paper
- Kang et al., 2018, “A Dataset of Peer Reviews (PeerRead)” introduce the dataset, describe the venues used (notably open-review sources like ICLR), and discuss limitations such as coverage and heterogeneity across venues/years; use this to justify your bias and missingness audits. https://aclanthology.org/N18-1149.pdf
- Sethi et al., 2021, “What Have We Learned from OpenReview?” analyze behavioral and structural properties of open peer review, supporting claims that open-review dynamics differ from traditional venues and can affect data distributions. https://arxiv.org/abs/2103.05885
- Cortes & Lawrence (NeurIPS 2014 experiment) show decision inconsistency across independent reviewer panels, motivating label-noise and ceiling analyses. https://arxiv.org/pdf/2109.09774.pdf

Practical checklist for the camera-ready
- Include a “Threats to Validity” subsection with empirical evidence from the audits above (plots/tables), not just narrative.
- Report per-venue/year summary tables for scores, acceptance rates, missingness, and topic distributions.
- Provide robustness tables for: label-noise sensitivity, scale harmonization, cross-venue/time generalization, and version-drift restrictions.
- Release scripts for matching, calibration, and audits; log any manual corrections.

Sources
- Kang et al., 2018. A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications. EMNLP 2018. https://aclanthology.org/N18-1149.pdf
- Sethi, Sarkar, Knoth, 2021. What Have We Learned from OpenReview? https://arxiv.org/abs/2103.05885
- Shah et al., 2021 (revisit). Inconsistency in Conference Peer Review: Revisiting the 2014 NeurIPS Experiment. https://arxiv.org/pdf/2109.09774.pdf