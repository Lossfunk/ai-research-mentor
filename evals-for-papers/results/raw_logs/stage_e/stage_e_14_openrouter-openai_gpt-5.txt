Discussion addendum: socioeconomic bias and mitigation
PeerRead largely reflects English-language, well-resourced CS venues and years where access to publication pipelines, arXiv usage, and writing conventions are uneven across regions and institution types. Because we do not collect sensitive attributes, we cannot directly measure socioeconomic bias; nevertheless, spurious signals (e.g., acknowledgments/funding strings, affiliation cues, citation practices, stylistic markers of English proficiency) may correlate with author resources and be learned by models, risking amplification of historical inequities. We therefore limit claims about generalizability and fairness, and we will add a documented bias audit and concrete mitigations in the camera-ready version.

Mitigations to include for camera-ready
- Dataset documentation and transparency
  - Publish a Datasheet and Model Card describing sampling, known skews, and intended use/limitations; report per-venue-year distributions by region (country of affiliation), institution type (e.g., R1 vs. others), and arXiv availability.
- Proxy-based slice audits (no sensitive-attribute inference)
  - Derive coarse proxies from public metadata (country, institution category, venue region). Report per-slice performance and calibration (AUC/F1; Kendall’s tau/RMSE; ECE/Brier) and statistical parity of predicted acceptance rates.
  - Matched-pair analysis: within topic/length/venue-year matched pairs, test whether predictions differ systematically across proxies; include CIs and sign tests.
  - Residualization: regress model scores on content controls (topic, length, section mix); test whether proxies have significant residual effects.
- Robustness and de-leakage
  - Enforce anti-leakage masking of acknowledgments/funding/affiliation strings and reference sections; quantify metric deltas and retained-performance ratio (OOD/ID) before/after masking.
  - Adversarial/projection debiasing: minimize mutual information between representations and proxies; report the trade-off curves.
- Distributional correction
  - Importance reweighting or stratified subsampling to balance proxy distributions; report pre/post performance, calibration, and parity shifts with uncertainty.
- Ethics and governance
  - Do not infer individual socioeconomic status; limit analyses to coarse, aggregate proxies. Seek IRB/ethics review where required; provide an opt-out mechanism for authors. Clearly state that models are not for evaluating individual authors or institutions.

We will include the bias audit protocol, proxy construction details, matched-pair/residualization results with confidence intervals, and pre/post mitigation plots in the appendix. If disparities persist or performance degrades materially under masking/balancing, we will qualify claims and recommend masked, section-limited inputs and restricted-use contexts.

Sources
- Kang, D., Ammar, W., Schwartz, R., & Cohen, W. W. (2018). A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications — https://aclanthology.org/N18-1149.pdf
- Blodgett, S. L., Barocas, S., Daumé III, H., & Wallach, H. (2020). Language (Technology) is Power: A Critical Survey of “Bias” in NLP — https://aclanthology.org/2020.acl-main.485.pdf
- Gebru, T., et al. (2021). Datasheets for Datasets — https://dl.acm.org/doi/10.1145/3458723
- Mitchell, M., et al. (2019). Model Cards for Model Reporting — https://dl.acm.org/doi/10.1145/3287560.3287596