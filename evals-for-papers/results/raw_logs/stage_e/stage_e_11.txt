Most pointed critique to expect
“The error analysis is mostly anecdotal and does not rule out spurious correlations or dataset biases in PeerRead; without stratified, statistically supported analyses (venue/year/topic; document length; metadata confounds; label noise; leakage), it’s unclear whether the model learns genuine signals versus artifacts.”

Proactive rebuttal strategy
- Acknowledge scope: PeerRead’s construction and split choices can induce bias; we extend our analysis accordingly [Kang et al., 2018].
- Make it systematic, not anecdotal: Add stratified, significance-tested error breakdowns by venue, year, arXiv subject, and document length.
- Rule out artifacts: Compare to strong metadata-only baselines; match/control for confounds; add counterfactual and behavioral tests to probe reliance on superficial cues [Ribeiro et al., 2020].
- Address label noise and leakage: Estimate label issues; deduplicate near-duplicate papers; report post-cleaning results.
- Quantify uncertainty: Report confidence intervals and model calibration.

Concretely add to the paper (rebuttal-ready)
1) Stratified error tables and tests
- What: Report per-stratum accuracy/F1 with 95% bootstrap CIs across venue (ICLR/NeurIPS/ACL where available), year, arXiv category, and paper length quintiles.
- Why: PeerRead aggregates multiple venues/years; performance may reflect covariate shifts rather than general ability [Kang et al., 2018].
- Rebuttal angle: If differences are small and CIs overlap, we show robustness; if not, we identify where models fail and propose fixes.

2) Metadata-only and controlled baselines
- What: Train logistic regression/XGBoost using only metadata (year, venue, arXiv category, #words, #references, #figures/tables if available). Also evaluate matched subsets that equalize length and year across classes (propensity-score matching).
- Why: To test whether predictions can be made from superficial features rather than content [Kang et al., 2018].
- Rebuttal angle: Show text models outperform metadata baselines and retain gains after length/year matching; report deltas with CIs.

3) Behavioral and counterfactual tests
- What: Using CheckList-style probes, create minimal edits that remove or modify:
  - Explicit rating numbers in reviews (e.g., mask “7/10”).
  - Venue/time markers (e.g., “ICLR 2017”).
  - Surface sentiment cues (swap positive/negative adjectives) while preserving substance.
- Why: To test reliance on obvious shortcuts [Ribeiro et al., 2020].
- Rebuttal angle: If performance is stable under these perturbations (or degrades only when substantive content is altered), we demonstrate reduced shortcutting.

4) Label-noise and disagreement audit
- What: Estimate probable label issues (e.g., outlier detection on review vs. decision; identify cases where review sentiment sharply contradicts accept/reject). Re-train with suspected noisy examples down-weighted/removed; compare before/after metrics.
- Why: PeerRead decisions reflect committee outcomes; noise and mismatches with review text likely exist [Kang et al., 2018].
- Rebuttal angle: Show modest but consistent improvements and more coherent errors post-cleaning; discuss residual error types.

5) Leakage and duplication checks
- What: Deduplicate train/test by near-duplicate titles/abstracts (e.g., Jaccard/MinHash > 0.9) and by arXiv ID/version lineage; check author overlap if used as features; re-run evaluation on deduped splits.
- Why: PeerRead includes arXiv versions and conference variants; inadvertent leakage is plausible [Kang et al., 2018].
- Rebuttal angle: If results are stable post-dedup, we rule out leakage; otherwise, we report corrected numbers.

6) Calibration and uncertainty
- What: Report Expected Calibration Error and reliability diagrams; analyze whether high-confidence mistakes cluster in specific strata. Consider an abstention setting (coverage vs. risk).
- Why: Calibrated confidence contextualizes errors and supports safe use.

Three+ concrete, falsifiable experiments to include
1) Metadata vs. text ablation
- Hypothesis: The full text+review model significantly outperforms a metadata-only baseline; the gap persists after length/year matching.
- Variables: Model type (metadata-only vs. text+review), matching on length/year.
- Metrics: AUC, F1; performance gap with 95% bootstrap CIs.
- Pass/fail: If gap > 5 points AUC with non-overlapping CIs after matching, artifact reliance is unlikely.

2) Counterfactual masking of explicit cues
- Hypothesis: Masking numeric ratings and venue markers causes ≤2 AUC points drop; masking substantive contribution sentences causes larger drops.
- Variables: Perturbation type (ratings/venue/sentiment vs. content sentences).
- Metrics: ΔAUC/ΔF1; McNemar’s test on paired predictions.
- Pass/fail: Larger degradation for content edits than for cue masking supports genuine content use.

3) Cross-venue/year generalization
- Hypothesis: Training on venue A (e.g., ICLR) and testing on venue B (e.g., ACL) reduces performance but maintains >70% of in-venue AUC; error types shift systematically.
- Variables: Train/test venue-year splits.
- Metrics: AUC, per-class F1; error-rate differences with bootstrap CIs.
- Pass/fail: Retaining >70% relative AUC suggests partial robustness; report where it fails.

4) Label-noise mitigation
- Hypothesis: Removing/down-weighting top 5–10% suspected noisy instances improves AUC by ≥1 point and reduces ECE.
- Variables: Noise filtering rate, training regime.
- Metrics: AUC/F1/ECE with CIs.
- Pass/fail: Statistically significant improvements indicate noise was masking true performance.

5) Leakage audit
- Hypothesis: After deduplicating near-duplicates across splits, AUC changes by <1 point.
- Variables: Dedup on/off.
- Metrics: ΔAUC; title/abstract similarity distributions.
- Pass/fail: Minimal change indicates leakage is not inflating results.

Suggested phrasing for the rebuttal
“We appreciate the request for a more rigorous error analysis. In response, we added (i) stratified, significance-tested error breakdowns by venue/year/topic/length, (ii) strong metadata-only and matched baselines, (iii) CheckList-style counterfactual probes that remove superficial cues, (iv) a label-noise and duplication audit, and (v) calibration analyses. These additions show that our model’s gains are not explained by metadata artifacts or leakage: the text+review model outperforms metadata-only baselines by 6.4 AUC points after length/year matching (95% CI [3.1, 9.7]); performance degrades primarily under content-preserving counterfactuals rather than cue masking; and results remain within 0.4 AUC after deduplication. We will include the full tables and scripts in the camera-ready for reproducibility.” [CIs are placeholders—replace with your computed values.]

Key takeaways
- Preempt the artifact/bias critique by adding stratified, statistically supported analyses and counterfactual tests.
- Demonstrate robustness beyond metadata and rule out leakage.
- Acknowledge dataset limitations and show practical steps that mitigate them on PeerRead.

Sources
- Kang et al., 2018. A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications — https://aclanthology.org/N18-1149/
- Ribeiro et al., 2020. Beyond Accuracy: Behavioral Testing of NLP Models with CheckList — https://aclanthology.org/2020.acl-main.442/