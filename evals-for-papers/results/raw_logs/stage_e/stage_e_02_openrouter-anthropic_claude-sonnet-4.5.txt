Based on the PeerRead paper, I can see that the authors explicitly note their goal is "to demonstrate potential uses of PeerRead; rather than develop the best model" [PeerRead.pdf:7]. Let me help you frame the modest gains constructively and develop a robust contingency plan.

## Framing the Limitations

### 1. **Position as Dataset Contribution, Not SOTA Model**

**Framing Strategy:**
- Emphasize that the primary contribution is the **dataset and benchmark**, not achieving state-of-the-art performance
- Frame the models as "proof-of-concept" or "baseline demonstrations" that establish the feasibility of the task
- Explicitly state: "Our goal is to demonstrate potential uses of PeerRead rather than develop the best model to address this task" [PeerRead.pdf:7]

**Language to use:**
> "While our baseline models show modest improvements over simple heuristics, the primary contribution of this work is establishing PeerRead as a benchmark for peer review prediction tasks. These results demonstrate that aspect score prediction is feasible but remains challenging, opening opportunities for future methodological innovations."

### 2. **Reframe "Modest Gains" as "Challenging Task"**

**Framing Strategy:**
- Highlight the **inherent difficulty** of the task rather than model inadequacy
- Emphasize that even human reviewers show substantial disagreement on aspect scores
- Position modest gains as evidence that the task requires deeper understanding

**Language to use:**
> "The modest performance gains (X% over baseline) reflect the inherent complexity of predicting nuanced aspect scores from text alone. Peer review involves subjective judgment, domain expertise, and contextual factors not fully captured in paper text, suggesting that this task may represent an upper-bound challenge for text-only models."

### 3. **Highlight What Works**

**Framing Strategy:**
- Identify specific aspects or venues where models perform better
- Show that certain features or architectures provide consistent (if modest) improvements
- Demonstrate that the task is learnable, even if not yet solved

**Language to use:**
> "While overall gains are modest, we observe consistent improvements across all venues when incorporating [specific features], suggesting that [X] captures meaningful signals about review outcomes. Performance is notably higher for [specific aspect], indicating that some review dimensions are more predictable from manuscript text."

## Contingency Plan for Future Work

### **Tier 1: Low-Risk Extensions (If Reviewers Push Back)**

These require minimal additional work and strengthen the contribution without changing the core claims:

1. **Error Analysis & Qualitative Insights**
   - Manually analyze cases where models succeed vs. fail
   - Identify linguistic patterns associated with high/low aspect scores
   - Provide interpretable insights even if predictive performance is modest
   - **Deliverable:** "Our analysis reveals that papers scoring high on 'meaningful comparison' consistently use comparative language (e.g., 'outperforms', 'compared to'), while low-scoring papers lack explicit baselines."

2. **Feature Ablation Studies**
   - Show which features contribute most to performance
   - Demonstrate that your feature engineering is principled
   - Identify what information is most predictive
   - **Deliverable:** Table showing performance with/without each feature group

3. **Human Agreement Baselines**
   - Calculate inter-reviewer agreement on aspect scores
   - Show that model performance approaches human agreement levels
   - Reframe the ceiling for this task
   - **Deliverable:** "Our models achieve 65% of the performance gap between random baseline and inter-reviewer agreement, suggesting substantial progress on this inherently subjective task."

4. **Cross-Venue Generalization Analysis**
   - Train on one venue, test on another
   - Show that models learn generalizable patterns (or identify venue-specific biases)
   - **Deliverable:** Transfer learning results demonstrating that models capture venue-independent quality signals

### **Tier 2: Medium-Risk Extensions (For Revision or Follow-up)**

These require moderate additional experimentation:

5. **Ensemble Methods**
   - Combine multiple model architectures
   - Show that different models capture complementary signals
   - **Expected gain:** 2-5% improvement with minimal conceptual novelty

6. **Multi-Task Learning**
   - Jointly predict acceptance decision + aspect scores
   - Predict multiple aspects simultaneously with shared representations
   - **Rationale:** Aspect scores are correlated; joint modeling may improve performance

7. **Incorporate Review Text**
   - Use reviews as weak supervision or auxiliary input
   - Show that review language provides signals not present in papers
   - **Deliverable:** Upper-bound performance when review text is available

8. **Temporal Features**
   - Add submission timing, revision history, author response patterns
   - Show that non-textual features complement text-based models
   - **Expected gain:** Modest but demonstrates multi-modal potential

### **Tier 3: High-Risk, High-Reward Directions (For Future Work Section)**

These are substantial research directions that acknowledge current limitations while proposing ambitious solutions:

9. **Pre-trained Scientific Language Models**
   - Fine-tune SciBERT, SPECTER, or domain-specific transformers
   - **Rationale:** Generic embeddings may not capture scientific discourse patterns
   - **Expected gain:** 5-15% improvement, but requires significant compute

10. **Structured Argumentation Analysis**
   - Parse papers into claims, evidence, and reasoning
   - Model the logical structure of scientific arguments
   - **Rationale:** Aspect scores evaluate argumentation quality, not just topic or style

11. **Reviewer Expertise Modeling**
   - Model the match between paper content and reviewer expertise
   - Predict scores conditioned on reviewer background
   - **Rationale:** Scores depend on who reviews the paper, not just paper content

12. **Causal Inference Approaches**
   - Identify causal factors that influence aspect scores
   - Use counterfactual reasoning to understand what changes would improve scores
   - **Deliverable:** "Papers that add X are 2x more likely to score high on meaningful comparison"

13. **Active Learning for Data Efficiency**
   - Show that strategic sampling can achieve similar performance with less data
   - Address the data scarcity challenge directly
   - **Rationale:** Peer review data is expensive; efficient learning is critical

14. **Explainable Predictions**
   - Generate natural language explanations for predicted scores
   - Extract rationales that justify predictions
   - **Deliverable:** "Our model predicts a low 'substance' score because the paper lacks experimental validation (highlighted spans)"

## Proposed Discussion Section Structure

```markdown
### Limitations and Future Directions

**Modest Performance Gains.** Our baseline models achieve modest improvements 
over simple heuristics (X% RMSE reduction), reflecting the inherent difficulty 
of predicting subjective aspect scores from manuscript text alone. Peer review 
involves nuanced judgment, domain expertise, and contextual factors (e.g., 
reviewer background, conference standards) that are not fully captured in paper 
text. We view these results as establishing a challenging benchmark rather than 
a solved task.

**Task Difficulty.** To contextualize our results, we measured inter-reviewer 
agreement on aspect scores and found [Krippendorff's α = X / correlation = Y]. 
Our models achieve [Z%] of the performance gap between random baseline and 
human agreement, suggesting that the task ceiling is constrained by inherent 
subjectivity rather than model limitations alone.

**What Works.** Despite modest overall gains, we observe consistent patterns:
(1) [Feature X] provides reliable improvements across all venues, suggesting 
it captures meaningful quality signals; (2) Performance is notably higher for 
[Aspect Y] (RMSE = A) compared to [Aspect Z] (RMSE = B), indicating that some 
review dimensions are more predictable from text; (3) Cross-venue transfer 
experiments show that models learn generalizable patterns, not venue-specific 
artifacts.

**Primary Contribution.** We emphasize that our primary contribution is the 
PeerRead dataset and benchmark, not state-of-the-art models. Our baseline 
experiments demonstrate feasibility and establish performance benchmarks for 
future work. We anticipate that advances in scientific language understanding, 
structured argumentation analysis, and multi-modal modeling will yield 
substantial improvements.

**Promising Directions.** Future work could explore: (1) pre-trained scientific 
language models (SciBERT, SPECTER) that better capture domain-specific discourse; 
(2) structured representations of scientific argumentation (claims, evidence, 
reasoning); (3) multi-task learning that jointly predicts acceptance decisions 
and aspect scores; (4) explainable models that generate natural language 
rationales for predictions; and (5) causal inference approaches that identify 
actionable factors influencing review outcomes.
```

## Defensive Strategies for Rebuttal

If reviewers criticize the modest gains, use these responses:

**Response 1: Dataset Contribution**
> "We agree that model performance is modest, which is precisely why we are 
releasing PeerRead as a benchmark. Our goal is to enable the community to 
develop better models, not to claim we have solved the task. The dataset's 
value lies in enabling future research, as evidenced by [X subsequent papers 
citing PeerRead]."

**Response 2: Task Difficulty**
> "We respectfully note that even human reviewers show substantial disagreement 
on aspect scores (α = X). Our models achieve Y% of the human agreement ceiling, 
which we view as meaningful progress on an inherently subjective task. We have 
added analysis contextualizing our results relative to human performance."

**Response 3: Insights Over Performance**
> "While predictive performance is modest, our analysis reveals actionable 
insights: [specific finding]. These qualitative insights about what makes papers 
score well on different aspects may be more valuable to authors and reviewers 
than marginal performance improvements."

**Response 4: Baseline for Future Work**
> "We have added experiments with [Tier 1 extension], which improves performance 
by X%. However, we maintain that establishing a rigorous benchmark is more 
valuable than incremental model improvements. We expect that future work 
leveraging [Tier 3 directions] will substantially advance the state of the art."

## Key Takeaways

1. **Reframe the narrative**: Position the work as a dataset contribution with proof-of-concept models, not a modeling paper with a dataset byproduct
2. **Contextualize performance**: Compare against human agreement, not just baselines
3. **Extract insights**: Show that even modest models reveal interpretable patterns
4. **Provide clear runway**: Outline concrete, feasible directions for improvement
5. **Be proactive**: Address limitations explicitly rather than defensively

The PeerRead paper's strength is that it explicitly states its goal is demonstrating potential uses rather than achieving SOTA [PeerRead.pdf:7], which gives you strong precedent for this framing.

## Sources

1. Kang, D., Ammar, W., Dalvi, B., van Zuylen, M., Kohlmeier, S., Hovy, E., & Schwartz, R. (2018). A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications. *NAACL-HLT 2018*. [PeerRead.pdf]