Most pointed critique we anticipate
Reviewers may argue that our error analysis is anecdotal and attribution-heavy, relying on unvalidated saliency maps and cherry-picked cases rather than systematic tests that distinguish spurious lexical cues from review-relevant content. They may also note the absence of counterfactual checks, slice-wise statistics with uncertainty, and evidence that attributions survive standard sanity checks [2] or behavioral stress tests [1].

Proactive mitigation in the rebuttal
- Commit to a pre-registered, quantitative error-analysis addendum that (a) uses behavioral tests to probe shortcut use, (b) validates attributions with sanity checks, and (c) reports slice-wise failure rates with confidence intervals and actionable fixes.

Concrete, falsifiable analyses we will add
1) Behavioral counterfactual tests (CheckList-style)
- Design: For a held-out set, perform controlled edits that preserve content while removing likely shortcuts: mask venue/acknowledgment strings; replace topical n-grams with neutral synonyms; normalize citation/length. Measure prediction flip rate and Δ in Kendall’s tau/RMSE.
- Criteria: Accept model as content-reliant if flip rate ≤5% and |Δτ| ≤0.01; otherwise, adopt section-restricted inputs and/or feature removal and report the updated results [1].

2) Attribution sanity checks plus evidence overlap
- Design: Run gradient-based saliency (and attention baselines) and apply parameter/input randomization tests to ensure saliency depends on learned parameters. Align highlighted tokens with review evidence sentences that justify aspect judgments; report token/sentence F1 and its gap between correct vs. incorrect predictions.
- Criteria: Saliency must fail under parameter randomization and pass on the trained model; require evidence-overlap ≥0.3 and ≥0.1 higher for correct vs. incorrect predictions; if unmet, we treat saliency claims as unsupported and revise explanations accordingly [2].

3) Slice discovery and error decomposition
- Design: Partition errors by venue/year, topic embedding clusters, length/readability bins, and reviewer disagreement. Report per-slice tau/RMSE with 95% CIs; run permutation-importance and ablate feature families (e.g., reference/length cues) to test if slice errors are shortcut-driven.
- Criteria: If removing a feature family improves OOD tau by ≥0.02 with ≤0.01 in-domain loss (CI excludes 0), we drop that family and update main results and claims.

We will include the addendum (protocols, code, CIs) in the rebuttal and revise claims if thresholds are not met (e.g., qualify robustness to be in-venue only, adopt masked/section-limited inputs, or add uncertainty reporting).

Citations
[1] Beyond Accuracy: Behavioral Testing of NLP Models with CheckList — https://arxiv.org/abs/2005.04118
[2] Sanity Checks for Saliency Maps — https://www.semanticscholar.org/paper/Sanity-Checks-for-Saliency-Maps-Adebayo-Gilmer/8dc8f3e0127adc6985d4695e9b69d04717b2fde8