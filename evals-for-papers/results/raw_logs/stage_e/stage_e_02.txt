Below is a concise way to frame the limitations of aspect score prediction in PeerRead and a concrete contingency plan that anticipates modest gains and sets decision points.

How to frame the limitations
- Subjective, noisy labels cap the ceiling. Aspect scores (e.g., originality, clarity, impact) are subjective and reviewer-specific; inter-rater variability likely limits the maximum achievable performance from text alone. Explicitly position model gains against a noise ceiling estimated from reviewer disagreement and repeated reviews per paper when available [P2].  
- Signal–target mismatch. Some aspects depend on unobserved context (e.g., novelty vs. the broader literature, impact vs. community needs), which is not fully present in a single manuscript’s text. This structurally constrains predictability, even for strong models [P2].  
- Dataset size, heterogeneity, and shift. PeerRead spans venues and years and contains variable numbers of reviews/papers, which can induce distribution shifts and reviewer-style confounds that depress generalization [P2].  
- Metrics and problem formulation. Treating ordinal aspect scores as regression with point estimates ignores ordinal structure and uncertainty; small “modest” gains may partly reflect metric insensitivity (e.g., MSE vs. rank or calibration metrics) [P2].  
- Predictive models as surrogates for human judgments. Predictive systems may fail to capture latent goals and evaluation heuristics of human reviewers; even well-fit models can underperform on tasks requiring judgment under context mismatch [P1].

Contingency plan (phased, with go/no-go criteria)
Phase 0 — Measurement and ceilings (2–3 weeks)
- Reproduce PeerRead baselines and your current models on identical splits.  
- Estimate label noise and a ceiling: inter-reviewer agreement per aspect; bootstrap split-half reliability; reviewer-normalized z-scores; and an “oracle” ensemble using multiple reviews per paper (when available) [P2].  
- Go/no-go: If current performance is within 10–20% of the estimated ceiling, deprioritize model complexity and pivot to task reframing or label aggregation.

Phase 1 — Robust problem formulation and evaluation (3–5 weeks)
- Switch to ordinal/quantile modeling and report calibration: ordinal logistic or isotonic regression heads; evaluate Spearman/Kendall, ECE, Brier score, and correctness within ±1 bin.  
- Model reviewer/venue effects: mixed-effects or hierarchical heads; reviewer- or venue-normalized targets and leave-reviewer-out validation.  
- Robust splits: year- and venue-held-out splits; report in-domain vs. out-of-domain gaps [P2].  
- Go/no-go: If ordinal + calibration + hierarchical modeling yields <5–10% relative improvement vs. Phase 0, proceed to reframing.

Phase 2 — Reframing for utility and signal (4–8 weeks)
- Pairwise/ranking tasks: predict which of two papers scores higher on a given aspect; report pairwise accuracy and Kendall tau.  
- Coarser bins and triage: map scores to low/medium/high to reduce label noise and support practical triage; evaluate macro-F1 and calibration.  
- Retrieval-augmented context for subjective aspects: incorporate literature-aware features (citation overlap/novelty proxies) or retrieval-augmented encoders for originality/impact, with strict ablations to avoid leakage.  
- Go/no-go: If ranking or coarse-bin tasks show stronger and consistent improvements with better calibration, pivot project scope to triage/ranking as the primary contribution.

Phase 3 — Data and label quality (parallel/ongoing)
- Label aggregation: apply probabilistic label aggregation (e.g., per-reviewer reliability weighting) and compare to raw labels; quantify ceiling shifts.  
- Active curation: collect small, higher-consensus subsets (multiple annotators, adjudication) for a clean benchmark; show gap between noisy vs. clean supervision to justify future investment.  
- Error analyses: stratify by aspect, venue, paper length, section ablations to identify where signal lives (e.g., methods vs. intro).

Risk management and reporting
- Pre-register evaluation and negative results; report ceilings and calibration alongside accuracy to contextualize “modest” gains [P2].  
- Ethical and leakage controls: if using metadata (authors, citations), document and ablate; keep a “paper-text-only” track for fair comparability.  
- Resource gating: stop escalating model complexity once you are within the measured ceiling, and redirect to reframing or label quality.

Three falsifiable experiments
1) Noise ceiling and ordinal reformulation  
- Hypothesis: Ordinal objectives with calibration-aware evaluation yield significantly better rank consistency and calibration than MSE regression, even if raw RMSE improves modestly.  
- Variables: Loss (MSE vs. ordinal logistic), evaluation metrics (RMSE, Spearman, ECE), aspect type.  
- Metrics: ΔSpearman ≥ +0.05 and ΔECE ≤ −0.03 averaged across aspects; statistical tests via bootstrap CIs.  
- Expected outcome: Ordinal models improve rank consistency and calibration for subjective aspects (originality/impact) more than for clarity [P2].

2) Reviewer/venue normalization and generalization  
- Hypothesis: Reviewer- and venue-normalized targets with hierarchical modeling reduce out-of-domain degradation on venue-held-out splits.  
- Variables: Target normalization (none vs. z-score per reviewer/venue), head type (standard vs. hierarchical), split (random vs. venue-held-out).  
- Metrics: Relative drop from in-domain to held-out in Spearman/Kendall; calibration (ECE).  
- Expected outcome: Normalization + hierarchical heads reduce OOD drop by ≥20% relative compared to unnormalized baselines [P2].

3) Retrieval-augmented originality prediction  
- Hypothesis: Adding literature-aware features (e.g., retrieval of nearest related works and overlap-based novelty proxies) improves originality prediction more than clarity prediction, indicating task–signal alignment.  
- Variables: Context (paper-only vs. paper+retrieval snippets), aspect (originality vs. clarity), model (frozen vs. fine-tuned encoder).  
- Metrics: ΔSpearman and ΔBrier for originality vs. clarity; ablations to ensure gains are not due to leakage.  
- Expected outcome: Significant gains for originality (ΔSpearman ≥ +0.05) but not clarity, supporting the signal–target mismatch claim [P2].

How to write this up
- Emphasize measured ceilings and uncertainty to contextualize modest improvements (not a failure of modeling, but a property of the task and labels) [P2].  
- Position contributions around (i) principled evaluation and calibration, (ii) reframed tasks with clearer utility (ranking/triage), and (iii) data quality evidence.  
- Include negative results where improvements are within the noise ceiling, and redirect to practical use-cases where models add value despite modest absolute gains [P1][P2].

Citations
- [P1] Limitations of Agents Simulated by Predictive Models — http://arxiv.org/abs/2402.05829v1
- [P2] A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications — http://arxiv.org/abs/1804.09635v1

Sources
- Kang et al., 2018. A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications (PeerRead) [P2]
- Shah et al., 2024. Limitations of Agents Simulated by Predictive Models [P1]