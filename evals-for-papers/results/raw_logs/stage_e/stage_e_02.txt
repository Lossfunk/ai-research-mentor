Based on the PeerRead paper, I can see the aspect score prediction models showed only slight improvements over the mean baseline [PeerRead.pdf:7]. Let me help you frame these limitations constructively and develop a strong contingency plan.

## **Intuition**
Modest gains in predictive tasks often signal that the problem is either inherently difficult (high human disagreement, subjective judgments) or that the signal available in the features is limited. Rather than treating this as a failure, it's an opportunity to characterize *why* the task is hard and what that reveals about peer review itself.

## **Why this is principled**
Strong papers acknowledge limitations transparently and turn them into insights. The PeerRead paper explicitly states its goal is "to demonstrate potential uses of PeerRead; rather than develop the best model to address this task" [PeerRead.pdf:7], which is a defensible framing. This positions the dataset contribution as primary and the models as proof-of-concept baselines for future work.

---

## **Framing the Limitations**

### **1. Lead with the Dataset Contribution**
Position the modest model performance as *expected* given the task difficulty, not as a shortcoming:

**Suggested framing:**
- "Aspect score prediction represents a challenging task due to the inherently subjective nature of peer review judgments. Our models achieve modest improvements over the mean baseline (RMSE reduction of ~X%), which we attribute to..."
- Emphasize that establishing strong baselines and releasing the dataset enables future research to tackle this difficult problem

### **2. Diagnose Why the Task is Hard**
Turn the limitation into an analysis contribution. Add a subsection analyzing:
- **Inter-annotator agreement**: How much do reviewers disagree on aspect scores? If agreement is low, that's your ceiling
- **Feature informativeness**: Which features correlate most with scores? Are there aspects where text features simply don't capture the judgment?
- **Dataset size effects**: Learning curves showing whether more data would help or if you've plateaued

**Key questions to answer:**
1. What's the human agreement level on these aspect scores? (This sets your theoretical upper bound)
2. Do certain aspects (e.g., "clarity") predict better than others (e.g., "novelty")? Why might that be?
3. Is there evidence that the task requires reasoning beyond surface features?

### **3. Reframe as a Benchmark Challenge**
Position modest performance as establishing a difficult benchmark:
- "These results establish aspect score prediction as a challenging benchmark for future work in computational peer review analysis"
- Compare to other subjective prediction tasks (sentiment, argumentation quality) where baselines are also modest

---

## **Contingency Plan for Future Work**

### **Phase 0 (Next 2 weeks) – Diagnostic Analysis**

Before proposing future work, validate what's limiting performance:

**Deliverable 1: Agreement Analysis**
- Calculate inter-reviewer agreement (Krippendorff's α or ICC) for each aspect score
- If agreement < 0.6, you have a strong explanation for modest performance
- **Hypothesis**: Aspects with higher agreement will show better model performance

**Deliverable 2: Error Analysis**
- Sample 30-50 high-error predictions and manually categorize failure modes
- Look for patterns: Are errors random or systematic? Do models struggle with specific score ranges (e.g., borderline papers)?
- **Expected outcome**: Identify 2-3 concrete failure modes to guide future work

### **Future Work Directions (Conditional on Phase 0)**

**Direction 1: Multi-Task Learning with Auxiliary Signals**
- *Hypothesis*: Aspect scores correlate with each other and with acceptance decisions; joint prediction may improve individual aspect prediction
- *Minimal test*: Train a multi-task model predicting all aspects + acceptance simultaneously
- *Metrics*: Per-aspect RMSE, correlation between predicted aspects
- *Expected pattern*: If aspects share underlying factors, multi-task should outperform single-task by 5-10%
- *Follow-up*: Analyze learned representations to understand aspect relationships

**Direction 2: Incorporate Review Text as Supervision**
- *Hypothesis*: Reviewer comments contain richer signals about aspect judgments than paper text alone
- *Minimal test*: Extract sentences from reviews mentioning specific aspects (e.g., "clarity" mentions) and use as additional training signal
- *Metrics*: RMSE improvement, attention weight analysis
- *Expected pattern*: Models should attend to paper sections that reviewers explicitly critique
- *Pitfall*: Review text may not be available at submission time (clarify use case)

**Direction 3: Characterize Inherent Task Difficulty**
- *Hypothesis*: Some papers are inherently harder to score (e.g., interdisciplinary work, novel methods)
- *Minimal test*: Cluster papers by prediction error and analyze characteristics (topic diversity, citation patterns, linguistic complexity)
- *Metrics*: Cluster purity, feature importance for high vs. low error groups
- *Expected outcome*: Identify paper types where automated scoring is/isn't feasible
- *Contribution*: Practical guidance on when to trust automated aspect prediction

---

## **Recommended Framing in Paper**

**Limitations Section:**
```
While our models improve over the mean baseline, gains remain modest 
(RMSE reduction of X%). We attribute this to three factors: (1) the 
inherently subjective nature of peer review, evidenced by inter-reviewer 
agreement of α=Y on aspect scores; (2) the limited signal available in 
submission text alone, as many aspects (e.g., experimental soundness) 
require domain expertise and detailed evaluation; and (3) dataset size, 
with only Z labeled examples per aspect. These results establish aspect 
score prediction as a challenging benchmark for future work.
```

**Future Work Section:**
```
Future work could explore: (1) multi-task learning to leverage 
correlations between aspects; (2) incorporating review text to 
understand what signals reviewers use; (3) characterizing which 
paper types are amenable to automated scoring; and (4) developing 
confidence-aware models that abstain on difficult cases.
```

---

## **Next Actions (Choose 1-2)**

1. **Run agreement analysis** (4 hours): Calculate inter-reviewer agreement for each aspect; if low, this becomes your primary explanation
2. **Conduct error analysis** (6 hours): Sample high-error cases and categorize failure modes to guide future work
3. **Draft limitations section** (2 hours): Write 1-2 paragraphs using the framing above, then we can refine

**Critical question:** Do you have access to multiple reviews per paper to calculate inter-reviewer agreement? If not, that's a limitation to acknowledge but doesn't prevent the other analyses.

What's your biggest concern about how reviewers will perceive the modest gains—are you worried about novelty, contribution, or something else?