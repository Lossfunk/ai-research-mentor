Below is a focused checklist of targeted ablations and error analyses that reviewers typically expect for “paper acceptance” prediction on PeerRead, with special attention to hand-engineered features. Where possible, I tie suggestions to established concerns in this literature so you can proactively address likely reviewer questions [1][2].

A. Targeted ablations reviewers will expect
- Feature-group drop ablations
  - Remove each coherent group and report Δ in AUC/F1:
    - Length/size: pages, words, sentence count, figure/table/equation counts.
    - Reference/citation proxies: number of references, citation markers.
    - Readability/style: FK/Kincaid, avg sentence length, type–token ratio, POS/grammar proxies.
    - Structural/format cues: section headers, presence/length of abstract, presence of “acknowledgments,” appendix flags.
    - Topic/content representations: bag-of-words/TF–IDF vs. neural.
  - Purpose: quantify how much “surface” vs. “content” drives accuracy and show incremental benefit over text-only baselines [1].

- Incremental stacking curve
  - Add feature sets cumulatively (text → +length → +references → +readability → +structure) to show marginal gains at each step. This is clearer than a single “all vs. none” comparison and helps isolate which hand-crafted groups matter most.

- Randomization and permutation controls
  - Shuffle a feature group across papers while holding others fixed (permutation importance) to test reliance on that group.
  - Add noise-augmented versions of features (e.g., jitter page/word counts within realistic bounds) to test stability of effects.

- “Leakage-prone” feature ablations
  - Proactively remove cues that might correlate with camera-ready vs. submission formatting or post-acceptance edits (e.g., acknowledgments, appendix markers, certain LaTeX artifacts) and re-evaluate. Many acceptance classifiers pick up formatting cues rather than scientific merit; reviewers will expect you to rule this out [1].

- Residualization controls
  - Regress acceptance on obviously predictive surface features (e.g., length/readability/reference count), compute residuals, and train your model to predict residuals. If performance collapses, you’re likely capitalizing on superficial cues.

- Cross-representation consistency
  - Compare a text-only model (e.g., bag-of-words logistic regression) against: (i) hand-engineered only, (ii) hand-engineered + text, and (iii) neural text encoders. Show whether hand-crafted features still add signal beyond strong text baselines [1].

- Cross-venue and temporal generalization
  - Train on one venue/year and test on another (e.g., ICLR→ACL; 2017→2018) to quantify domain shift. Hand-engineered features that encode venue-specific formatting or limits (e.g., page caps) should degrade under such shifts [1].

B. Error analyses that surface confounds
- Slice-based performance
  - Break down metrics by:
    - Length and reference-count quantiles.
    - Topic clusters (LDA or embedding clusters).
    - Venue and year.
    - Number of authors.
  - Look for large TPR/FPR gaps indicating spurious reliance on particular slices (e.g., “long papers that are still rejected” as systematic FPs).

- Calibrated risk and confusion trends
  - Reliability diagrams and ECE; examine whether the model is overconfident on easy-to-cheat slices (long/reference-heavy papers).

- Matched-pairs analysis
  - Construct pairs matched on topic and length (or other surface stats), but with different labels. Examine whether the model still separates them; manually inspect top discordant pairs for qualitative failure modes.

- Qualitative probes for style cues
  - Inspect high-confidence FP/FN examples for stylistic markers (e.g., “we propose…,” rhetorical structure) that are known to correlate with ratings/acceptance in prior studies [2]. This helps argue you’re not purely picking up stylistic artifacts.

- Feature-attribution agreement checks
  - Use permutation/shapley to list top features on a held-out set; verify they align with plausible editorial criteria rather than dataset quirks (e.g., “presence of Acknowledgments” dominating importance is a red flag).

C. Pitfall checks reviewers often ask about
- Submission vs. camera-ready signals
  - Many datasets inadvertently mix versions; strip or ablate features that reflect post-acceptance edits (acknowledgments, supplemental section markers). Re-run with those removed to show robustness [1].

- Topic bias vs. merit
  - Control for topic by adding topic fixed effects or training within-topic folds; show the model is not simply preferring popular subfields.

- Label noise and borderline cases
  - If rating information is available, separate “borderline” from “clear accept/reject” and analyze performance stratified by confidence bands; acceptance is inherently noisy.

- Data leakage
  - Ensure no review text, meta-review, decision text, or submission IDs leak into features. Explicitly document filtering rules.

Three concrete, falsifiable experiments
1) Feature-group drop and incremental stacking
- Hypothesis: Acceptance accuracy gains are largely driven by length and reference-count features; readability adds little beyond text.
- Design: Train a text-only baseline; then add feature groups cumulatively and via leave-one-group-out ablations. Report AUC/F1/Δ.
- Expected outcome: Largest marginal gain from length/reference features; small or venue-dependent gains from readability. If not, hand-engineered design is contributing non-trivially.
- Risk control: Use stratified temporal splits (train earlier years, test later) and cross-venue tests to avoid overfitting to formatting.

2) Residualization to remove surface confounds
- Hypothesis: After regressing out length/reference/readability, a classifier trained on residuals will retain significantly above-chance performance, indicating content signal.
- Design: Fit logistic regression of label on the surface feature set; compute residuals; train models to predict residuals. Compare to original label prediction metrics.
- Success criterion: Residual AUC ≥ 0.60 with tight CIs would argue the model captures more than superficial cues; collapse toward 0.50 suggests spurious reliance.

3) Cross-venue and temporal generalization
- Hypothesis: Hand-engineered features that capture venue-specific constraints (page limits, sectioning) will not transfer across venues/years, whereas content-based text features will degrade less.
- Design: Train on Venue A Year t, test on Venue B Year t and Venue A Year t+1; compare performance drops by feature set.
- Success criterion: Smaller relative drop for text-only vs. hand-engineered-only; if hand-engineered remains strong, investigate for leakage.

4) Matched-pairs slice evaluation
- Hypothesis: When matched on topic and length, the model can still separate accepts from rejects better than chance.
- Design: Construct matched pairs by nearest neighbors in topic embedding and length; evaluate pairwise accuracy.
- Success criterion: Pairwise accuracy significantly > 50% with bootstrap CIs; if not, conclusions likely hinge on superficial proxies.

5) Permutation importance stress test
- Hypothesis: Permuting length/reference features yields larger performance degradation than permuting readability or POS features.
- Design: Permute one group at a time on held-out data; measure ΔAUC/F1 over 100 permutations.
- Success criterion: Rank-ordered degradation quantifies reliance; report CIs to address variance.

6) Qualitative audit of top errors
- Hypothesis: High-confidence FPs/FNs reflect substantive factors (novelty claims, clarity) rather than purely stylistic artifacts.
- Design: Manually inspect 50 top-confidence FPs/FNs; code failure reasons; report distribution and examples.
- Success criterion: If stylistic artifacts dominate (>50%), revise features/modeling and re-run ablations.

Literature anchors
- A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications (NAACL 2018). Introduces the acceptance prediction task and baselines; reviewers will expect comparisons and confound checks grounded in this setup (https://aclanthology.org/N18-1149/ or https://arxiv.org/abs/1804.09635) [1].
- Analyzing We-sentences in Scientific Writing to Predict Peer Review Ratings and Acceptance. Shows stylistic markers correlate with acceptance, underscoring the need to disentangle superficial cues from substantive content in feature design (https://ieeexplore.ieee.org/iel8/6287639/10820123/11151689.pdf) [2].

Citations
[1] A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications — https://aclanthology.org/N18-1149/
[2] Analyzing We-sentences in Scientific Writing to Predict Peer Review Ratings and Acceptance — https://ieeexplore.ieee.org/iel8/6287639/10820123/11151689.pdf

Key takeaways
- Expect to justify hand-engineered features with group-wise ablations, leakage checks, and cross-venue/time robustness.
- Show that any gains cannot be fully explained by surface proxies like length/reference counts/readability.
- Provide slice/error analyses that reveal and mitigate spurious correlations, and document controls clearly.