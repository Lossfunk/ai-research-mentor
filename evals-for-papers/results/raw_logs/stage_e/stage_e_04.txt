Below is a concise checklist of targeted ablations and error analyses that reviewers typically expect for PeerRead-style acceptance prediction, with special attention to hand-engineered (surface/structural) features. I anchor recommendations to PeerRead’s own setup and to closely related work that analyzes predictors of peer‑review outcomes.

Targeted ablations reviewers will expect (hand-engineered features)
- Feature-group ablations (remove-one-group-at-a-time):
  - Length/verbosity features (tokens per section, number of sections/pages) [1].
  - Structural counts (figures, tables, equations) [1].
  - Bibliography-related features (number of references, self-citation proxies if any) [1].
  - Readability/lexical statistics (e.g., Flesch, POS ratios) if used [4].
  - Topic/section features (e.g., LDA/section presence) if used [1].
- Within-group leave-one-out:
  - For structure: remove only figures vs only tables vs only equations to isolate which drives signal [1].
  - For length: ablate total length vs section-specific lengths (abstract-only length, intro-only length) [1].
- Normalization ablations:
  - Raw counts vs length-normalized (per 1k tokens) to test whether observed gains are simply proxies for length [1][4].
  - Log-scaling vs z-scoring to check sensitivity to distributional skew of counts [1].
- Section-scope ablations:
  - Title+abstract only vs body-only vs full paper to localize where hand-engineered features matter most (especially abstract length/readability) [1][3][4].
- Content vs surface disentanglement:
  - Remove all n-gram/embedding features and evaluate hand-engineered-only; then the reverse (no hand-engineered) to quantify incremental value [1][3].
- Venue/year/corpus ablations:
  - Train on one venue-year and test on another (e.g., ICLR→ACL, year-to-year) to ensure the hand-engineered signals are not venue-style artifacts [1].
- Confound controls:
  - Stratified evaluation by length deciles; also re-train after regressing out length from all other features to test whether structure counts still help beyond length [4].
  - Domain controls (e.g., arXiv subject area or venue track) because figure/equation density varies by field, which can spuriously track acceptance [1][4].

Leakage and robustness checks (often requested)
- PDF parsing/metadata leakage: verify that hand-engineered features are not inadvertently capturing acceptance cues (e.g., camera-ready artifacts, “accepted at …” footers, acknowledgments) [1].
- Style-template cues: explicitly test whether presence of specific style tokens (e.g., “ICLR 2017 template”) influences predictions; ablate such tokens if any [1].
- Duplicate/near-duplicate papers: deduplicate to avoid memorization; re-run ablations on deduped splits [1].
- Calibration: reliability diagrams and ECE after removing hand-engineered features to assess whether these features mainly improve calibration vs raw accuracy [1].

Error analyses reviewers will expect
- Slice-wise performance:
  - By length decile, number-of-references decile, figures/tables/equations bins, readability bins [4].
  - By venue/year and subject area/domain (e.g., theory vs applications) [1][4].
- Confusion inspection:
  - Examine top false positives/negatives with largest contribution from each hand-engineered feature using a linear model’s coefficients or SHAP on tree models; verify they reflect real quality, not spurious style/formatting [1][4].
- Counterfactual sensitivity:
  - Simple counterfactual edits to features (e.g., simulate +3 figures or +15 references by perturbing features) to test how sensitive the classifier is to incremental changes; report marginal effect curves [4].
- Human-in-the-loop sanity checks:
  - Present reviewers with anonymized pairs matched on length and topic where the model succeeds/fails; annotate whether failures arise from content understanding gaps vs surface cues [4].
- Cross-source robustness:
  - Train on ICLR/OpenReview subset and evaluate on ACL/NeurIPS subset to check if hand-engineered features generalize beyond a single reviewing culture [1]. 

Three concrete, falsifiable experiments
1) Leave-one-group-out and length-normalization ablation
   - Hypothesis: Structural counts (figures/tables/equations) provide predictive value beyond document length.
   - Setup: Train a strong baseline (e.g., linear or tree model) on all features. Evaluate drops when removing (a) length group, (b) structure group, and (c) structure group with features normalized per 1k tokens. Also regress out length from all non-length features and re-train.
   - Metrics: AUROC, accuracy, AUPRC; report 95% CIs. Stratify by length decile.
   - Expected outcome: If structure adds genuine signal, removing structure after controlling for length causes a statistically significant drop; if not, drops vanish after normalization/regression [1][4].

2) Section-scope and cross-venue generalization
   - Hypothesis: Hand-engineered features concentrated in abstracts and introductions generalize across venues; full-body surface features may be venue-specific.
   - Setup: Train three models: (A) title+abstract features only, (B) body-only features, (C) full paper. Evaluate in-domain (same venue-year) and cross-domain (train ICLR, test ACL/NeurIPS; swap directions).
   - Metrics: AUROC/accuracy; Δ performance cross-domain vs in-domain.
   - Expected outcome: If venue artifacts drive gains, cross-domain drops are larger for full-body features than for title+abstract features [1][3][4].

3) Counterfactual feature sensitivity curves
   - Hypothesis: Acceptance predictions should not change dramatically with small, plausible changes in structural counts (e.g., +1 figure, +5 references) once length is controlled.
   - Setup: For held-out papers, perturb hand-engineered features locally (± increments), keeping length fixed. Plot predicted acceptance probability vs each feature value; compute average marginal effect and elasticity.
   - Metrics: Slope and sensitivity (∆probability per unit change), compared before vs after length-normalization.
   - Expected outcome: Large sensitivities that shrink substantially after normalization/regression indicate confounding by length; stable small sensitivities suggest robustness [4].

Helpful anchors for baselines and expectations
- PeerRead reported that simple surface/structural features (length, figures/tables/equations, references) carry signal for acceptance prediction and were part of their feature sets and analyses [1]. Use this to justify feature-group ablations and cross-venue tests.
- Independent textual analyses of AI manuscripts similarly found outcome-associated signals in length, readability, and lexical indicators, reinforcing the need to disentangle content vs surface effects and to control for confounds via normalization and matched comparisons [4].
- Section-aware neural baselines (e.g., modularized hierarchical CNNs) show that content-focused models can outperform hand-crafted features; include such baselines and then quantify incremental value of hand-engineered features on top [3].

Key takeaways
- Reviewers will expect rigorous leave-one-group-out ablations, length control/normalization, and cross-venue tests to rule out spurious correlations.
- Provide slice-based error analysis and counterfactual sensitivity to demonstrate the model is not over-relying on superficial cues.
- Anchor to PeerRead’s original feature choices and to independent analyses of outcome-associated text features; be explicit about any potential leakage.

Citations
[1] A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications — https://aclanthology.org/N18-1149.pdf
[3] Automatic Academic Paper Rating Based on Modularized Hierarchical Convolutional Neural Network — https://www.semanticscholar.org/paper/Automatic-Academic-Paper-Rating-Based-on-Neural-Yang-Sun/1edb2cbdfcdd2c3258fe998b1b7ccd3611843016
[4] Textual analysis of artificial intelligence manuscripts reveals features associated with peer review outcome — https://arxiv.org/abs/1911.02648
[5] Multi-task Peer-Review Score Prediction — https://aclanthology.org/2020.sdp-1.14.pdf