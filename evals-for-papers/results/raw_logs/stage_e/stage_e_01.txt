Below are likely reviewer concerns about the “probably-rejected” labels for arXiv papers in PeerRead, followed by concrete mitigations you can include in your discussion/limitations. I also propose falsifiable experiments to quantify and reduce risk, and provide literature anchors.

Anticipated reviewer concerns
- Construct validity of the negative class: “Probably-rejected” is a weak proxy for true rejection. Many arXiv preprints are later accepted (often after substantial revision) or published in different venues; some were still under review at crawl time. This creates false negatives and a label mixture, not a clean “rejected” class. This concern aligns with broader findings that acceptance modeling requires careful outcome definition and controls for confounders [P2].  
- Selection bias in who posts to arXiv: arXiv posting is not random and varies by subfield, seniority, and norms. This can confound any acceptance-related analysis using arXiv-derived labels. Recent work studying arXiv posting vs. acceptance highlights such non-randomness and the need for careful observational controls [P1].  
- Temporal censoring/right-censoring: labels assigned too soon after the arXiv posting will misclassify papers that are eventually accepted (time-to-publication varies widely across venues).  
- Entity resolution/matching errors: imperfect linking from arXiv to published versions (title variants, author order, venue name changes) can inflate false negatives.  
- Domain/venue drift: arXiv categories and conference scopes differ; a “probably-rejected” label conflates venue-mismatch with quality signals.  
- Evaluation leakage: using “probably-rejected” as hard negatives for training and testing can bias both models and reported metrics if noise is not explicitly modeled.  

Mitigations to include in the paper (discussion/limitations)
- Explicitly reposition “probably-rejected” as noisy, weak labels: treat them as unlabeled or weak negatives, not ground-truth rejections. State that they capture “not matched to a target venue within a grace period” rather than “rejected.”  
- Add a conservative temporal grace window: only label as “probably-rejected” if no acceptance is found after a sufficiently long window (e.g., 12–24 months), and report sensitivity to this choice. This mitigates right-censoring.  
- Strengthen linking to published versions: augment arXiv-to-venue matching with DBLP, Crossref, Semantic Scholar, and fuzzy matching on titles/authors/DOIs; audit match quality on a stratified sample.  
- Use noise-robust learning: treat positives as accepted papers and “probably-rejected” as unlabeled; train with Positive–Unlabeled (PU) methods or apply label-noise estimation/cleaning (e.g., confident learning) to downweight suspected mislabeled negatives. Report both PU and naive supervised baselines to show robustness [P2].  
- Stratify and reweight: analyze by subfield, year, and arXiv adoption rates; consider inverse-propensity or post-stratification weights to reduce selection bias from arXiv usage heterogeneity [P1].  
- Sensitivity analyses: report performance when (a) excluding arXiv categories with very high mismatch to the target venue, (b) tightening/loosening the grace window, and (c) removing samples with low-confidence labels (per a de-noising model).  
- Human validation: manually verify a random, stratified subset of “probably-rejected” labels; report estimated noise rates and how they affect conclusions (e.g., via error-in-variables bounds).  
- Transparent reporting: document exact labeling rules, matching heuristics, and time windows; publish code, match tables, and a list of uncertain cases so others can replicate or refine the labels.

Concrete, falsifiable experiments
1) Manual audit and error-rate estimation of “probably-rejected”  
- Hypothesis: With a 24-month grace window and improved matching, the precision of “probably-rejected” as true negatives exceeds 85%.  
- Design: Randomly sample N=300 “probably-rejected” papers, stratified by year and arXiv category. Manually verify publication status (conference/journal) via DBLP/Crossref/Google Scholar author pages.  
- Metrics: Precision of the negative label (1 − false negative rate), inter-annotator agreement, error breakdown by field/time.  
- Expected outcome: Quantified noise rates; fields with high arXiv-to-journal paths may show lower precision.

2) Sensitivity to temporal censoring windows  
- Hypothesis: Increasing the grace window from 6→12→24 months monotonically increases negative-label precision but reduces coverage; model conclusions remain stable within CIs.  
- Design: Recompute labels under 6, 12, and 24 months; re-train acceptance prediction models; compare AUC/auPR, calibration, and top-k precision.  
- Metrics: Delta in performance and in label precision/coverage across windows.  
- Expected outcome: A Pareto curve showing the trade-off between label quality and sample size; conclusions robust for ≥12 months.

3) Positive–Unlabeled learning vs. naive supervised training  
- Hypothesis: Treating “probably-rejected” as unlabeled (PU) yields more stable generalization and better-calibrated probabilities than treating them as hard negatives.  
- Design: Train (a) standard supervised classifier with probably-rejected as negatives, (b) PU learner estimating class prior and risk, and (c) a confident-learning de-noising pipeline to flag/reweight suspected mislabeled examples.  
- Metrics: AUC/auPR on a gold test set built from confirmed accepts and human-validated negatives; Expected Calibration Error; robustness under label perturbations.  
- Expected outcome: PU and/or de-noised models outperform naive supervised on calibration and are less sensitive to label perturbations.

4) Enhanced entity resolution ablation  
- Hypothesis: Improved matching (DBLP+Crossref+fuzzy) reduces false negatives by ≥X% vs. exact-title matching only.  
- Design: Implement a multi-source, fuzzy resolver; compare discovered matches and validation precision/recall against a hand-labeled match set.  
- Metrics: Match precision/recall/F1; downstream change in negative-label precision.  
- Expected outcome: Significant gain in match recall with minimal precision loss.

5) Subfield-stratified robustness  
- Hypothesis: After stratifying by arXiv adoption, effect sizes and model rankings remain consistent across high- vs. low-adoption subfields.  
- Design: Partition by arXiv category and venue family; re-estimate metrics and compare with/post stratification weighting.  
- Metrics: Heterogeneity of effect estimates and performance gaps across strata.  
- Expected outcome: Some heterogeneity, but main conclusions persist; if not, qualify claims accordingly.

Where this ties to prior work
- Acceptance modeling requires careful outcome definition and controls; using weak labels without bias mitigation risks confounding and misleading conclusions [P2].  
- arXiv posting is non-random across fields and time; observational studies examining acceptance highlight selection issues and the need for robust methodology [P1].  

Literature anchors (titles with links)
- A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications (Kang et al., 2018) — https://aclanthology.org/N18-1149.pdf  
- What Makes a Scientific Paper be Accepted for Publication? (2021) — http://arxiv.org/abs/2104.07112v1 [P2]  
Optional methodology anchors for noisy/weak labels:
- Confident Learning: Estimating Uncertainty in Dataset Labels (Northcutt et al.) — https://arxiv.org/abs/1911.00068  
- Learning from Positive and Unlabeled Data (Elkan & Noto, 2008) — https://cseweb.ucsd.edu/~elkan/posonly.pdf

Key takeaways
- Treat “probably-rejected” as weak, noisy labels rather than ground truth; document rules and quantify noise.  
- Use temporal grace windows, stronger entity resolution, and human audits to estimate and reduce noise.  
- Prefer PU learning or label-noise–robust methods; report sensitivity across label definitions, windows, and subfields.  
- Make robustness the centerpiece of the discussion: show conclusions persist under multiple reasonable label constructions.

Citations
- [P1] Estimating the Causal Effect of Early ArXiving on Paper Acceptance — http://arxiv.org/abs/2306.13891v2  
- [P2] What Makes a Scientific Paper be Accepted for Publication? — http://arxiv.org/abs/2104.07112v1