Based on my knowledge of the PeerRead dataset and common concerns in peer review research, here are the key reviewer concerns you should anticipate and mitigations to address them:

## Anticipated Reviewer Concerns

### 1. **Label Validity: Assumption that arXiv = Rejected**

**Concern:** The fundamental assumption that papers appearing only on arXiv were rejected from conferences is unverifiable and potentially flawed. Authors may post to arXiv for many reasons unrelated to rejection:
- Preprints never submitted to the target venue
- Work-in-progress shared for feedback
- Papers submitted to different venues not tracked in the dataset
- Authors who prefer open access without formal publication

**Mitigations:**
- **Acknowledge the limitation explicitly**: State that "probably-rejected" is a noisy proxy label with unknown false positive rate
- **Temporal filtering**: Describe any temporal constraints used (e.g., only including arXiv papers posted within X months before the conference deadline)
- **Validation subset**: If possible, manually verify a random sample of "probably-rejected" papers by checking author publication records or contacting authors
- **Sensitivity analysis**: Report how your findings change under different assumptions about label noise (e.g., 10%, 20%, 30% mislabeling rates)
- **Conservative interpretation**: Frame findings as lower bounds or trends rather than definitive conclusions

### 2. **Selection Bias in arXiv Posting**

**Concern:** Authors who post rejected papers to arXiv may differ systematically from those who don't, creating selection bias.

**Mitigations:**
- **Characterize the bias**: Discuss which author populations are more/less likely to use arXiv (e.g., field-specific norms, institutional culture, career stage)
- **Subgroup analysis**: If metadata permits, analyze whether patterns hold across different author demographics or institutions
- **Comparison with known rejected papers**: If you have access to any ground-truth rejected papers (from venues that release them), compare characteristics to validate the proxy

### 3. **Confounding by Paper Quality**

**Concern:** Papers on arXiv might be systematically lower quality than those never submitted, making comparisons with accepted papers unfair.

**Mitigations:**
- **Quality controls**: Describe any filtering for obvious spam, non-English papers, or formatting issues
- **Matched sampling**: Use propensity score matching or similar techniques to compare accepted vs. probably-rejected papers with similar characteristics (length, author count, citation count at time of submission)
- **Multiple baselines**: Compare against both accepted papers and other negative classes if available

### 4. **Venue-Specific Differences**

**Concern:** ArXiv usage varies dramatically across venues and fields, making cross-venue comparisons problematic.

**Mitigations:**
- **Stratified analysis**: Report results separately for each venue or venue type
- **ArXiv penetration rates**: Document the proportion of submissions that appear on arXiv for each venue
- **Field norms**: Discuss how arXiv culture differs (e.g., CS theory vs. NLP vs. ML)

### 5. **Temporal Dynamics**

**Concern:** ArXiv posting behavior has changed over time, and papers may be revised or eventually accepted elsewhere.

**Mitigations:**
- **Snapshot timing**: Clearly specify when arXiv data was collected relative to conference decisions
- **Version control**: Note whether you used the first arXiv version or a specific version
- **Longitudinal tracking**: If feasible, check whether "probably-rejected" papers were later accepted elsewhere (using DBLP, Google Scholar, or Semantic Scholar)

### 6. **Ground Truth Availability**

**Concern:** Why use noisy proxy labels when some venues provide actual rejection data?

**Mitigations:**
- **Justify the trade-off**: Explain that arXiv-based labels enable larger-scale analysis across venues that don't release rejection data
- **Hybrid approach**: Use ground-truth rejected papers where available to validate your proxy, then extend to other venues
- **Transparency**: Provide statistics on how many papers fall into each category and confidence levels

## Proposed Discussion Section Structure

```markdown
### Limitations and Validity of Probably-Rejected Labels

**Label Noise.** Our "probably-rejected" labels are based on the heuristic 
that papers appearing only on arXiv (and not in conference proceedings) were 
likely rejected. This is necessarily a noisy proxy with several sources of 
error: (1) authors may post preprints without submitting to the target venue, 
(2) papers may have been submitted to different venues, and (3) posting 
behavior varies by field and author demographics. We estimate the false 
positive rate at [X%] based on [manual validation/temporal filtering/author 
surveys]. Our findings should be interpreted as trends rather than definitive 
statements about individual papers.

**Selection Bias.** Authors who post rejected work to arXiv may differ 
systematically from those who do not. In our dataset, [X%] of submissions 
appear on arXiv, with higher rates in [subfield/institution type]. To 
mitigate this, we [stratify by venue/use matched sampling/report subgroup 
analyses].

**Validation.** We validated our labeling approach by [manual inspection of 
N papers/comparison with ground-truth rejected papers from venue X/longitudinal 
tracking showing Y% were never published]. [Report agreement statistics or 
validation results].

**Generalizability.** Our findings are most applicable to venues and fields 
with high arXiv adoption rates (>X%). Results may not generalize to fields 
where preprint culture is less established.

**Future Work.** More reliable labels could be obtained through [venue 
partnerships/author surveys/multi-venue tracking]. We release our labeling 
code and encourage community validation.
```

## Concrete Validation Experiments

1. **Manual validation study**: Sample 100-200 "probably-rejected" papers and verify through author publication records, Google Scholar, or direct author contact. Report precision.

2. **Temporal consistency check**: For papers posted to arXiv before conference deadlines, track whether they appear in proceedings within 6-12 months. Papers that never appear strengthen the rejection hypothesis.

3. **Cross-venue validation**: For authors with multiple submissions, check if "probably-rejected" papers from venue A were later accepted at venue B, providing indirect validation.

## Key Takeaways

- **Be proactive**: Address label validity upfront rather than waiting for reviewers to raise it
- **Quantify uncertainty**: Provide concrete estimates of label noise and sensitivity analyses
- **Validate where possible**: Even small-scale manual validation adds credibility
- **Frame appropriately**: Position findings as exploratory or trend-based rather than definitive
- **Provide value despite noise**: Argue that even noisy labels at scale can reveal useful patterns unavailable from small ground-truth datasets

## Sources

1. Kang, D., Ammar, W., Dalvi, B., van Zuylen, M., Kohlmeier, S., Hovy, E., & Schwartz, R. (2018). A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications. *NAACL-HLT 2018*. https://aclanthology.org/N18-1149.pdf
2. PeerRead Dataset Repository. https://github.com/allenai/PeerRead