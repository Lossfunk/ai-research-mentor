## Comprehensive Response Framework for Domain Transferability Concerns

Here's a structured response to reviewers questioning domain transferability, with concrete experiments to address their concerns.

---

## Response Template for Rebuttal/Discussion

### **Opening: Acknowledge the Concern**

```markdown
We thank the reviewer for raising this important concern about domain 
transferability. We agree that our models are trained on ML/AI venues 
(NIPS, ICLR) and may not generalize to other disciplines, journals, or 
review cultures. This is a fundamental limitation of the PeerRead dataset 
and an important direction for future work.

To address this concern, we have conducted additional experiments [if 
revision allows] / propose the following analyses [if rebuttal only] to 
characterize the scope and limits of transferability.
```

### **Strategy 1: Acknowledge and Bound the Claim**

```markdown
**Revised Claims.** We have revised our claims to be more precise about 
the scope of our findings:

BEFORE: "Our models can predict aspect scores for scientific papers"
AFTER: "Our models can predict aspect scores for ML/AI conference submissions 
to venues with similar review processes and standards"

We explicitly state in the limitations (Section X.X) that:
1. Transferability to other CS subfields is untested
2. Transferability to non-CS disciplines is unlikely without retraining
3. Transferability to journals (vs. conferences) is unknown
4. Transferability across review cultures (e.g., single-blind vs. double-blind) 
   is uncertain
```

### **Strategy 2: Provide Evidence of Within-Domain Robustness**

```markdown
**Within-Domain Generalization.** While we cannot test cross-discipline 
transfer without additional data, we demonstrate robust generalization 
within the ML/AI domain:

1. **Cross-venue transfer**: Models trained on NIPS achieve F1 = X.XX on 
   ICLR (vs. F1 = Y.YY on NIPS), representing only a Z% performance drop 
   [New Experiment 1]

2. **Cross-year transfer**: Models trained on 2013-2015 data achieve F1 = 
   X.XX on 2017 data, demonstrating temporal stability [New Experiment 2]

3. **Cross-topic transfer**: Models trained on deep learning papers achieve 
   F1 = X.XX on reinforcement learning papers (vs. F1 = Y.YY in-domain) 
   [New Experiment 3]

These results suggest that our models learn generalizable quality signals 
rather than venue-specific artifacts, at least within ML/AI.
```

### **Strategy 3: Analyze What Transfers and What Doesn't**

```markdown
**Transfer Analysis.** To understand the limits of transferability, we 
analyzed which features and aspects transfer across venues [New Analysis 1]:

**Aspects that transfer well** (ΔF1 < 0.05 across venues):
- Clarity: F1 = X.XX (NIPS) vs. Y.YY (ICLR)
- Soundness: F1 = X.XX (NIPS) vs. Y.YY (ICLR)
- Reproducibility: F1 = X.XX (NIPS) vs. Y.YY (ICLR)

**Aspects that transfer poorly** (ΔF1 > 0.15 across venues):
- Novelty: F1 = X.XX (NIPS) vs. Y.YY (ICLR)
- Significance: F1 = X.XX (NIPS) vs. Y.YY (ICLR)

**Interpretation:** Aspects grounded in objective textual features (clarity, 
organization) transfer better than those requiring domain knowledge (novelty, 
significance). This suggests that partial transfer is possible: clarity 
assessment may generalize across domains, while novelty assessment requires 
domain-specific training.
```

---

## New Experiments to Include (Priority Order)

### **Experiment 1: Cross-Venue Transfer Matrix (ESSENTIAL)**

**What to do:**
```python
# Train on each venue, test on all venues
venues = ['NIPS', 'ICLR', 'arXiv']
results = {}

for train_venue in venues:
    for test_venue in venues:
        model = train(data[train_venue])
        results[train_venue][test_venue] = evaluate(model, data[test_venue])
```

**Present as transfer matrix:**

| Train ↓ / Test → | NIPS | ICLR | arXiv | Avg Transfer Gap |
|------------------|------|------|-------|------------------|
| **NIPS** | 0.XX (in-domain) | 0.YY | 0.ZZ | -A.AA |
| **ICLR** | 0.YY | 0.XX (in-domain) | 0.ZZ | -A.AA |
| **arXiv** | 0.YY | 0.ZZ | 0.XX (in-domain) | -A.AA |
| **All venues** | 0.YY | 0.ZZ | 0.XX | - |

**Key metrics:**
- **In-domain performance**: Diagonal entries
- **Transfer gap**: Difference between in-domain and cross-domain
- **Symmetric transfer**: Is NIPS→ICLR similar to ICLR→NIPS?

**Interpretation scenarios:**

**Scenario A: Small transfer gap (<10%)**
```markdown
Cross-venue transfer is strong (average gap = X%), suggesting our models 
learn venue-agnostic quality signals. This provides evidence that transfer 
to similar venues (ML/AI conferences with comparable review processes) is 
feasible.
```

**Scenario B: Moderate transfer gap (10-20%)**
```markdown
Cross-venue transfer shows moderate degradation (average gap = X%), indicating 
venue-specific biases exist but models capture some generalizable signals. 
Domain adaptation techniques (Section X.X) reduce the gap to Y%.
```

**Scenario C: Large transfer gap (>20%)**
```markdown
Cross-venue transfer is poor (average gap = X%), confirming the reviewer's 
concern. However, aspect-level analysis (Table Y) shows that clarity and 
soundness transfer well (gap < 10%), while novelty and significance do not 
(gap > 30%). This suggests selective transfer is possible for objective aspects.
```

---

### **Experiment 2: Aspect-Level Transfer Analysis (ESSENTIAL)**

**What to do:**
```python
# For each aspect separately
aspects = ['clarity', 'soundness', 'novelty', 'significance', 
           'reproducibility', 'originality']

for aspect in aspects:
    # Train on NIPS, test on ICLR
    model = train_aspect_model(aspect, data['NIPS'])
    nips_f1 = evaluate(model, data['NIPS'][aspect])
    iclr_f1 = evaluate(model, data['ICLR'][aspect])
    transfer_gap = nips_f1 - iclr_f1
```

**Present as:**

| Aspect | NIPS (in-domain) | ICLR (transfer) | Transfer Gap | Transferability |
|--------|------------------|-----------------|--------------|-----------------|
| Clarity | 0.XX | 0.YY | -Z.ZZ | **High** |
| Soundness | 0.XX | 0.YY | -Z.ZZ | **High** |
| Reproducibility | 0.XX | 0.YY | -Z.ZZ | **Medium** |
| Originality | 0.XX | 0.YY | -Z.ZZ | **Medium** |
| Novelty | 0.XX | 0.YY | -Z.ZZ | **Low** |
| Significance | 0.XX | 0.YY | -Z.ZZ | **Low** |

**Critical insight:**
```markdown
Aspects vary dramatically in transferability. Clarity and soundness show 
minimal transfer gaps (<10%), suggesting these aspects are evaluated 
consistently across venues. In contrast, novelty and significance show 
large gaps (>25%), reflecting venue-specific standards for what constitutes 
"novel" or "significant" work.

**Implication for cross-domain transfer:** Clarity assessment may transfer 
to non-ML domains (e.g., biology, physics), while novelty assessment likely 
requires domain-specific retraining.
```

---

### **Experiment 3: Feature-Level Transfer Analysis (STRONGLY RECOMMENDED)**

**What to do:**
```python
# Identify which features transfer
# Train on NIPS, analyze feature importance on NIPS vs. ICLR

features_nips = get_feature_importance(model, data['NIPS'])
features_iclr = get_feature_importance(model, data['ICLR'])

# Compute correlation between feature importances
correlation = correlate(features_nips, features_iclr)

# Identify features with consistent vs. inconsistent importance
```

**Present as:**

| Feature Category | Importance Correlation | Interpretation |
|------------------|------------------------|----------------|
| Readability metrics | r = 0.85 | **Transfers well** |
| Structural features | r = 0.72 | **Transfers moderately** |
| N-gram features | r = 0.45 | **Venue-specific** |
| Citation features | r = 0.38 | **Venue-specific** |
| Topic keywords | r = 0.21 | **Highly venue-specific** |

**Specific features that transfer:**
- Sentence length (r = 0.89)
- Flesch-Kincaid score (r = 0.83)
- Paper length (r = 0.78)
- Reference count (r = 0.71)

**Specific features that don't transfer:**
- Topic-specific keywords (r = 0.15)
- Citation velocity (r = 0.28)
- Author h-index (r = 0.33)

**Interpretation:**
```markdown
Features based on writing quality (readability, structure) show high 
importance correlation across venues (r > 0.7), while content-specific 
features (keywords, citations) are venue-dependent (r < 0.4). This suggests 
that models relying on surface features may transfer better than those 
relying on semantic content.

**Design implication:** For cross-domain applications, prioritize 
readability and structural features over topic-specific features.
```

---

### **Experiment 4: Domain Adaptation Techniques (RECOMMENDED)**

**What to do:**
```python
# Test domain adaptation methods
methods = [
    'No adaptation (baseline)',
    'Fine-tuning on target domain',
    'Domain-adversarial training',
    'Multi-task learning (venue as auxiliary task)',
    'Instance weighting'
]

for method in methods:
    # Train on NIPS, adapt to ICLR
    model = train_with_adaptation(method, source='NIPS', target='ICLR')
    performance = evaluate(model, data['ICLR'])
```

**Present as:**

| Adaptation Method | ICLR F1 | Improvement over Baseline | Training Data Needed |
|-------------------|---------|---------------------------|----------------------|
| No adaptation | 0.XX | - | 0 ICLR papers |
| Fine-tuning (10% ICLR) | 0.YY | +Z.ZZ | 50 ICLR papers |
| Fine-tuning (50% ICLR) | 0.YY | +Z.ZZ | 250 ICLR papers |
| Domain-adversarial | 0.YY | +Z.ZZ | 0 ICLR papers |
| Multi-task learning | 0.YY | +Z.ZZ | 0 ICLR papers |
| Instance weighting | 0.YY | +Z.ZZ | 0 ICLR papers |

**Critical finding:**
```markdown
Fine-tuning on just 10% of target domain data (50 papers) recovers 80% of 
the transfer gap, achieving F1 = X.XX vs. Y.YY for full in-domain training. 
This suggests that modest amounts of target domain data enable effective 
adaptation.

Domain-adversarial training improves transfer without target labels (F1 = 
X.XX vs. Y.YY baseline), demonstrating that unsupervised adaptation is 
possible.
```

---

### **Experiment 5: Zero-Shot Transfer to Related Domains (AMBITIOUS)**

**What to do:**
If you can obtain data from related domains (even without labels):

```python
# Test on related CS venues (if available)
related_venues = ['ACL', 'CVPR', 'ICML', 'KDD']

for venue in related_venues:
    # Use model trained on NIPS
    predictions = model.predict(data[venue])
    
    # Validation strategies without labels:
    # 1. Correlation with acceptance (if known)
    # 2. Correlation with citations (proxy for quality)
    # 3. Human evaluation on sample
```

**Present as:**

| Target Domain | Similarity to NIPS | Predicted F1* | Validation Method |
|---------------|-------------------|---------------|-------------------|
| ICML (ML) | High | 0.XX | Acceptance correlation |
| CVPR (Vision) | Medium | 0.YY | Citation correlation |
| ACL (NLP) | Medium | 0.YY | Human evaluation (n=50) |
| KDD (Data Mining) | Low | 0.ZZ | Human evaluation (n=50) |

*Estimated via proxy metrics

**Interpretation:**
```markdown
Zero-shot transfer to closely related domains (ICML) shows promising results 
(predicted F1 = X.XX), while transfer to more distant domains (KDD) is poor 
(F1 = Y.YY). This suggests a gradient of transferability based on domain 
similarity.
```

---

### **Experiment 6: Universal vs. Domain-Specific Features (RECOMMENDED)**

**What to do:**
```python
# Separate features into universal and domain-specific
universal_features = ['readability', 'structure', 'length']
domain_features = ['keywords', 'citations', 'methods']

# Train models with different feature sets
model_universal = train(universal_features, data['NIPS'])
model_domain = train(domain_features, data['NIPS'])
model_full = train(all_features, data['NIPS'])

# Test transfer
for model in [model_universal, model_domain, model_full]:
    nips_f1 = evaluate(model, data['NIPS'])
    iclr_f1 = evaluate(model, data['ICLR'])
```

**Present as:**

| Feature Set | NIPS F1 | ICLR F1 | Transfer Gap | Interpretation |
|-------------|---------|---------|--------------|----------------|
| Universal only | 0.XX | 0.YY | -Z.ZZ | **Best transfer** |
| Domain-specific only | 0.XX | 0.YY | -Z.ZZ | **Worst transfer** |
| Full model | 0.XX | 0.YY | -Z.ZZ | **Best in-domain** |

**Critical insight:**
```markdown
Models using only universal features (readability, structure) show minimal 
transfer gap (X%) but lower absolute performance. Models using domain-specific 
features achieve higher in-domain performance but poor transfer (gap = Y%).

**Design trade-off:** For applications requiring cross-domain deployment, 
universal features provide more robust (if less accurate) predictions. For 
single-domain applications, domain-specific features are preferable.
```

---

### **Experiment 7: Human Evaluation of Transfer Quality (GOLD STANDARD)**

**What to do:**
```python
# Sample predictions on target domain
# Have domain experts rate quality

sample_size = 50-100 papers from ICLR
for each paper:
    model_prediction = model.predict(paper)
    expert_rating = get_expert_rating(paper)
    
# Calculate agreement
agreement = correlation(model_predictions, expert_ratings)
```

**Present as:**

| Evaluation Setting | Spearman ρ | Interpretation |
|--------------------|------------|----------------|
| NIPS (in-domain) | 0.XX | Model-expert agreement |
| ICLR (transfer) | 0.YY | Transfer quality |
| Human-human (ICLR) | 0.ZZ | Inter-rater agreement |

**Critical comparison:**
```markdown
Model-expert agreement on ICLR (ρ = X.XX) is lower than in-domain agreement 
(ρ = Y.YY) but remains above chance (ρ = 0) and approaches human-human 
agreement (ρ = Z.ZZ). This suggests the model provides useful (if imperfect) 
predictions on the transfer domain.

Qualitative analysis of disagreements reveals that the model struggles with 
[specific aspects], suggesting targeted improvements for cross-domain transfer.
```

---

## Ablation Studies to Address Transferability

### **Ablation 1: Vocabulary Overlap Analysis**

**What to do:**
```python
# Measure vocabulary overlap between domains
vocab_nips = get_vocabulary(data['NIPS'])
vocab_iclr = get_vocabulary(data['ICLR'])

overlap = len(vocab_nips & vocab_iclr) / len(vocab_nips | vocab_iclr)

# Identify domain-specific terms
nips_only = vocab_nips - vocab_iclr
iclr_only = vocab_iclr - vocab_nips
```

**Present as:**

| Metric | Value | Interpretation |
|--------|-------|----------------|
| Vocabulary overlap | X% | Lexical similarity |
| NIPS-specific terms | Y terms | Domain divergence |
| ICLR-specific terms | Z terms | Domain divergence |

**Top NIPS-specific terms:** [list]
**Top ICLR-specific terms:** [list]

**Interpretation:**
```markdown
Vocabulary overlap is X%, indicating [high/moderate/low] lexical similarity. 
NIPS-specific terms include [examples], while ICLR-specific terms include 
[examples], reflecting different research emphases.

Low vocabulary overlap (X < 60%) suggests that models relying heavily on 
lexical features will transfer poorly. This motivates the use of semantic 
embeddings or domain-agnostic features.
```

---

### **Ablation 2: Embedding Space Analysis**

**What to do:**
```python
# Visualize paper embeddings from different domains
embeddings_nips = get_embeddings(data['NIPS'])
embeddings_iclr = get_embeddings(data['ICLR'])

# Reduce dimensionality (t-SNE or UMAP)
# Plot and measure separation

# Quantify domain shift
mmd = maximum_mean_discrepancy(embeddings_nips, embeddings_iclr)
```

**Visualization:** t-SNE plot showing NIPS and ICLR papers

**Metrics:**
- Maximum Mean Discrepancy (MMD): X.XX
- Classifier accuracy (NIPS vs. ICLR): Y%

**Interpretation:**
```markdown
Embedding space analysis reveals [moderate/significant] domain shift (MMD = 
X.XX). A classifier can distinguish NIPS from ICLR papers with Y% accuracy, 
indicating distributional differences.

However, accepted and rejected papers from both venues show similar patterns 
in embedding space, suggesting that quality signals are consistent across 
domains despite topical differences.
```

---

### **Ablation 3: Aspect Correlation Across Domains**

**What to do:**
```python
# Measure how aspects correlate in different domains
for domain in ['NIPS', 'ICLR']:
    correlations = compute_aspect_correlations(data[domain])
    
# Compare correlation matrices
difference = correlations['NIPS'] - correlations['ICLR']
```

**Present as correlation heatmaps:**

**NIPS Aspect Correlations:**
|  | Clarity | Soundness | Novelty | Significance |
|--|---------|-----------|---------|--------------|
| Clarity | 1.00 | 0.XX | 0.YY | 0.ZZ |
| Soundness | 0.XX | 1.00 | 0.YY | 0.ZZ |
| ... | ... | ... | ... | ... |

**ICLR Aspect Correlations:**
[Similar table]

**Interpretation:**
```markdown
Aspect correlations are similar across venues (Frobenius norm of difference = 
X.XX), suggesting consistent relationships between aspects. For example, 
clarity and soundness correlate at r = Y.YY in NIPS and r = Z.ZZ in ICLR.

This consistency supports the hypothesis that aspect relationships are 
universal, even if absolute scoring standards differ.
```

---

## Proposed Future Work Section

```markdown
### 6.X Domain Transferability: Limitations and Future Directions

**Current Scope.** Our models are trained and evaluated on ML/AI conference 
submissions (NIPS, ICLR). While we demonstrate robust within-domain 
generalization (Section X.X), transferability to other disciplines remains 
an open question.

**Partial Transfer Hypothesis.** Our analysis (Section X.X) suggests that 
transferability varies by aspect:
- **High transfer potential**: Clarity, soundness, reproducibility (objective, 
  text-based aspects)
- **Low transfer potential**: Novelty, significance (subjective, domain-
  knowledge-dependent aspects)

This suggests a tiered approach to cross-domain deployment:
1. **Tier 1 (high confidence)**: Use clarity/soundness models across domains
2. **Tier 2 (medium confidence)**: Adapt models with small amounts of target 
   domain data
3. **Tier 3 (low confidence)**: Retrain novelty/significance models for each 
   domain

**Recommended Validation Strategy.** Before deploying to new domains, we 
recommend:
1. Collect 50-100 labeled papers from target domain
2. Measure transfer gap via cross-domain evaluation
3. If gap > 20%, fine-tune on target domain data
4. Validate with domain expert evaluation (n ≥ 50)

**Data Requirements for New Domains.** Based on our domain adaptation 
experiments (Section X.X), we estimate:
- **Related domains** (e.g., ICML, NeurIPS): 50-100 labeled papers for 
  fine-tuning
- **Distant CS domains** (e.g., systems, theory): 200-500 labeled papers
- **Non-CS domains** (e.g., biology, physics): 500-1000 labeled papers + 
  domain-specific feature engineering

**Open Research Questions:**
1. Can we identify universal quality signals that transfer across all 
   scientific domains?
2. What is the minimum target domain data needed for effective adaptation?
3. Can unsupervised domain adaptation (using unlabeled target papers) reduce 
   data requirements?
4. How do review cultures (single-blind vs. double-blind, conference vs. 
   journal) affect transferability?

**Call for Collaboration.** We invite researchers from other disciplines to 
collaborate on cross-domain validation. We will release our models and 
provide support for adaptation to new domains.
```

---

## Rebuttal-Specific Response (If No Revision Possible)

```markdown
We thank the reviewer for this important concern. We acknowledge that our 
models are trained on ML/AI venues and may not transfer to other domains 
without adaptation. We have revised our claims to be explicit about this 
limitation (see revised abstract, introduction, and limitations section).

However, we respectfully argue that this limitation does not diminish the 
contribution of our work:

1. **Within-domain value**: Our models provide immediate value for ML/AI 
   venues, which represent a large and active research community. Even 
   domain-specific tools can have significant impact.

2. **Transferability analysis**: We provide extensive analysis of what 
   transfers and what doesn't (Section X.X), offering insights for future 
   cross-domain work. This analysis is itself a contribution.

3. **Methodological contribution**: Our feature engineering, model 
   architecture, and evaluation methodology can be applied to other domains, 
   even if the trained models cannot. We provide code and documentation to 
   facilitate this.

4. **Baseline for future work**: Our work establishes baselines and 
   methodology that future work can build upon for cross-domain transfer. 
   Perfect cross-domain generalization is not required for a paper to make 
   a valuable contribution.

We have added a "Domain Transferability" subsection to the discussion 
(Section X.X) that:
- Explicitly acknowledges the limitation
- Analyzes which aspects/features are most likely to transfer
- Proposes validation strategies for new domains
- Estimates data requirements for adaptation
- Calls for cross-domain collaboration

We believe this addresses the reviewer's concern while being honest about 
the current scope of our work.
```

---

## Key Messaging Points

### **What to Emphasize:**

1. **Honesty about limitations**: "We explicitly state that our models are trained on ML/AI venues and may not transfer to other domains"

2. **Evidence of robustness**: "We demonstrate strong within-domain generalization across venues, years, and topics"

3. **Actionable insights**: "Our analysis reveals which aspects and features transfer, guiding future cross-domain work"

4. **Practical value**: "Even domain-specific models provide value to the ML/AI community"

5. **Path forward**: "We provide concrete recommendations for adapting to new domains, including data requirements and validation strategies"

### **What to Avoid:**

1. **Overpromising**: Don't claim your models will work on biology papers without evidence

2. **Dismissing the concern**: Don't say "transferability doesn't matter"

3. **Vague future work**: Don't just say "we'll test on other domains later"—be specific

4. **Defensive tone**: Acknowledge the limitation gracefully

---

## Summary Table: Experiments by Feasibility and Impact

| Experiment | Feasibility | Impact | Time Required | Priority |
|------------|-------------|--------|---------------|----------|
| Cross-venue transfer matrix | **High** | **High** | 2-3 hours | **MUST DO** |
| Aspect-level transfer | **High** | **High** | 3-4 hours | **MUST DO** |
| Feature-level transfer | **High** | **Medium** | 4-5 hours | **SHOULD DO** |
| Domain adaptation | **Medium** | **High** | 1-2 days | **SHOULD DO** |
| Vocabulary overlap | **High** | **Low** | 1-2 hours | **NICE TO HAVE** |
| Embedding space analysis | **Medium** | **Medium** | 3-4 hours | **NICE TO HAVE** |
| Zero-shot to related domains | **Low** | **High** | Weeks | **FUTURE WORK** |
| Human evaluation | **Low** | **Very High** | Weeks | **FUTURE WORK** |

---

## Concrete Action Plan

### **If You Have Time for Revision (1 week):**

**Day 1-2:**
- Run cross-venue transfer matrix (Experiment 1)
- Run aspect-level transfer analysis (Experiment 2)
- Draft revised limitations section

**Day 3-4:**
- Run feature-level transfer analysis (Experiment 3)
- Run domain adaptation experiments (Experiment 4)
- Create visualizations

**Day 5-6:**
- Write new "Domain Transferability" section
- Revise abstract/intro to bound claims
- Update discussion with insights

**Day 7:**
- Proofread and finalize

### **If You Only Have Time for Rebuttal (1-2 days):**

**Priority 1 (Must do):**
- Run cross-venue transfer matrix (2-3 hours)
- Draft rebuttal response acknowledging limitation
- Revise paper claims to be more precise

**Priority 2 (If time permits):**
- Run aspect-level transfer analysis (3-4 hours)
- Add brief transferability discussion to limitations

### **If No Time for New Experiments:**

**Rebuttal strategy:**
- Acknowledge limitation gracefully
- Argue that domain-specific contributions are still valuable
- Propose specific future work with concrete validation plan
- Revise claims to be explicit about scope
- Emphasize methodological contributions that transfer even if models don't

---

## Key Takeaways

1. **Don't fight the reviewer**—acknowledge that domain transferability is a legitimate concern

2. **Provide evidence where possible**—cross-venue and aspect-level transfer experiments are quick and high-impact

3. **Be specific about scope**—clearly state that your models are for ML/AI venues, not all scientific domains

4. **Offer actionable insights**—even if your models don't transfer, your analysis of what transfers and what doesn't is valuable

5. **Propose concrete future work**—don't just say "we'll test on other domains"; specify data requirements, validation strategies, and collaboration opportunities

6. **Emphasize partial transfer**—clarity and soundness may transfer even if novelty doesn't

7. **Frame it as a research question**—"what transfers across domains?" is itself an interesting scientific question

The goal is to turn a criticism into an opportunity to demonstrate thoughtful analysis and set a research agenda for future work.

## Sources

- [A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications](https://aclanthology.org/N18-1149.pdf) - Kang et al., NAACL 2018
- [Domain Adaptation for Peer Review Score Prediction](https://arxiv.org/abs/2010.12395) - Domain adaptation techniques
- Standard practices in transfer learning evaluation from NLP and ML conferences