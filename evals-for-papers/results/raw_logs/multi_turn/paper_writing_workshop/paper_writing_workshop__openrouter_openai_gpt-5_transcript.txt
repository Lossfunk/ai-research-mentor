[Turn 0] SYSTEM:
You're preparing to write your first conference paper and need help structuring the narrative, deciding what results to include, and understanding how to frame your contributions clearly.

[Turn 1] USER:
How do I even start deciding which experiments or findings belong in the paper versus what stays in the lab notebook?

[Turn 1] ASSISTANT:
Stage: C — Research plan/results curation

Quick note on sources: The mentorship-guidelines tool returned no specific entries for this query; I’m grounding advice in high-quality writing/reproducibility guidance instead [1][2].

Strategic questions
- What is your single-sentence central claim (cause-and-effect or performance statement) you want the paper to stand on?
- What venue and timeline are you targeting (e.g., NeurIPS short paper vs. journal)? This changes how much goes in main vs. appendix.
- Which experiments do you already have, and which are confirmatory vs. exploratory?
- Biggest current bottleneck: analysis clarity, missing controls, or too many figures?

How to decide what goes where
- Main text (must carry the paper): Only experiments that are necessary and sufficient to support the central claim and its mechanism. This typically means: (a) a clear main result, (b) a minimal ablation that shows “why it works,” and (c) a robustness/reliability check. Every figure should directly answer a reader’s question about the claim [1]. Put reproducibility-critical details (datasets, seeds, hardware, hyperparams summary) either succinctly here or pushed to Methods if the venue prefers [2].
- Appendix (supports credibility and reuse): Extended ablations, sensitivity analyses, additional datasets/tasks showing generality, full hyperparameter tables, training curves, per-seed results, and computational budget details [2].
- Repository/supplementary: Full code, exact configs, logs, and any large tables/plots that would clutter the paper but are needed for reproduction [2].
- Lab notebook only (do not include): Early pilot/exploratory attempts that don’t change conclusions; duplicative variants that don’t sharpen mechanism; dead ends unless they preempt a likely reviewer alternative and you can document them cleanly in an appendix.
- Edge case: Negative results belong if they falsify a plausible alternative explanation or bound the scope of the claim (often as a compact appendix figure with a one-line takeaway), not if they are unrelated failures.

Three concrete, falsifiable experiments to include (templates)
1) Minimal main-effect test
- Objective & hypothesis: Your method M outperforms baseline B on target task T by ≥X% under a pre-registered metric (e.g., accuracy, AUROC, F1). Hypothesis: M > B on T.
- Setup: Choose one canonical dataset; fix protocol (data splits, preprocessing); run ≥3 seeds for each method under identical training budgets.
- Metrics & success: Report mean±95% CI; pre-specify success as a statistically significant improvement with a simple test (e.g., paired t-test or bootstrap).
- Interpretation: If significant, the core claim is supported; if not, either the effect is small or confounded—revise claim or examine failure modes.
- Follow-ups: Add a second dataset of similar difficulty to check generality; if compute-limited, add a smaller synthetic dataset to validate effect direction.

2) Mechanism ablation
- Objective & hypothesis: Removing component C (or replacing it with a naïve alternative) reduces performance by ≥Y%, indicating C is causally responsible for gains.
- Setup: Keep everything constant; compare M vs. M–C vs. “C-only naive.” Control parameters and training steps across variants.
- Metrics & success: Same primary metric; success if M–C degrades as predicted and “C-only naive” does not exceed B.
- Interpretation: If ablation has little effect, your proposed mechanism may be wrong; re-evaluate attribution and update narrative.
- Follow-ups: Swap C for a functionally similar alternative to test whether gains are due to a general property vs. a specific implementation.

3) Robustness and stability
- Objective & hypothesis: The main effect is stable across seeds and within reasonable hyperparameter neighborhoods; hypothesis: variance does not erase the mean gain.
- Setup: Grid or random search over 3–5 plausible hyperparameter settings; 3 seeds per setting for M and B under equal budgets.
- Metrics & success: Report best/median/mean and interquartile range; success if M’s median ≥ B’s upper quartile or similar pre-registered threshold; include calibration plots where relevant.
- Interpretation: If the gain disappears under small perturbations, the result is fragile; tighten controls or adjust claim to reflect sensitivity.
- Follow-ups: Report compute cost and memory to contextualize trade-offs; move expanded tables to appendix and code/configs to repo for reproducibility [2].

Decision rubric you can apply today
- Relevance: Does this figure/experiment directly test the central claim or a key alternative? If not, move to appendix or notebook [1].
- Necessity: If you remove it, does the argument break? If no, relegate to appendix/notebook.
- Sufficiency: With the main result, one mechanism ablation, and one robustness check, could a skeptical reviewer verify the claim without guessing missing steps? If yes, you have the minimal main body [1][2].
- Reproducibility: Are details sufficient for another lab to re-run it? If details are long, keep a concise pointer in the paper and place the rest in Methods/appendix/repo [2].

Next actions (1–2 days)
- Write your one-sentence central claim and a 1-paragraph “Results storyline.”
- Inventory all existing experiments in a sheet; tag each as Main, Mechanism, Robustness, Generality, or Exploratory. Apply the rubric to assign Main vs. Appendix vs. Notebook.
- Draft three experiment cards (one for each template above) with hypothesis, falsifier, variables, metrics, and a stop rule. Use these to finalize what survives into the paper.

Intuition
Papers persuade by telling one clear story: show the effect, explain why it happens, and demonstrate it’s real and not a fluke. Everything else either supports credibility (appendix/repo) or stays internal.

Why this is principled
Strong papers are structured around a single contribution with figures that directly support it; excess exploration dilutes clarity [1]. Reproducibility norms require enough procedural detail and stability evidence that others can re-run your work, which often means moving the heavy specifics to appendices and code while keeping the argument tight in the main text [2].

Resources
- Ten simple rules for structuring scientific papers (Mensh & Kording, 2017) — guidance on keeping one central contribution and aligning figures to claims [1].
- Machine Learning Reproducibility Checklist (Pineau et al., v2.0) — what details belong in paper/appendix/repo for credible results [2].

Citations
[1] Ten simple rules for structuring papers — https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005619
[2] The Machine Learning Reproducibility Checklist (v2.0) — https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf

Before we proceed: what’s your target venue/timeline, your one-sentence central claim, and which experiments you’ve already run? Also, what’s your weekly time/compute budget and whether you have co-authors or mentors reviewing drafts?

[Turn 2] USER:
How do I pick the one storyline that feels like the real contribution when I have two equally promising angles?

[Turn 2] ASSISTANT:
Stage: B — Idea selection (narrowing to one storyline); aim to move to Stage C with a discriminative pilot

Strategic questions
- Give me 1–2 sentences for each storyline (Angle A vs. Angle B). What’s the core claim and mechanism for each?
- What venue and timeline are you aiming for?
- What preliminary evidence do you already have (numbers, plots, ablations), and what’s the biggest bottleneck right now (compute, data, or clarity)?
- What’s your weekly time/compute budget and do you have co-authors to help run pilots?

Decision scorecard (rate each angle 0–3; pick the one with the higher total; break ties with the first criterion)
- Importance if true: Would this meaningfully change how others do things or what they believe? [G4]
- Tractability in 2 weeks: Can you run a clean, end-to-end demo and at least one ablation quickly? [G14]
- Mechanism clarity: Can you state a falsifiable “because X, Y improves via Z” and design a surgical test? [G14]
- Generality: Likely to hold across 2 datasets/scales without heroic tuning? [G7]
- Reviewer defensibility: Can you pre-empt the most obvious alternative explanation with one compact test? [G6][G1]

Three discriminative experiments you can run to choose the storyline
1) Head-to-head effect-size and cost-adjusted benefit
- Objective & hypothesis: On the same dataset/protocol, one angle (A or B) yields a larger, more reliable improvement per unit compute. Hypothesis: effect-size(A)/cost(A) > effect-size(B)/cost(B).
- Setup: Fix data, preprocessing, training budget; run ≥3 seeds for each angle and a strong baseline; log wall-clock/GPU-hours.
- Metrics & success: Report mean±CI and an efficiency score (Δmetric per GPU-hour). Pick the storyline with a statistically larger efficiency and lower variance. [G14]
- Interpretation: If both are similar, prefer the one you can ablate cleanly (Experiment 2). If neither beats baseline reliably, revisit claims.
- Follow-ups: Replicate the winner on a second dataset with minimal retuning to sanity-check generality. [G7]

2) Mechanism ablation and falsifier
- Objective & hypothesis: Removing the key component for each angle causes a predictable drop (≥Y%) unique to that angle; whichever shows a clearer, causal mechanism wins. Hypothesis: removing C_A degrades A more than removing C_B degrades B.
- Setup: For each angle, run M vs. M–C vs. naïve-C; hold all else constant; pre-register the expected direction/size. [G6]
- Metrics & success: Choose the angle with a sharper, repeatable causal signal (larger drop, tighter CI, correct direction across seeds).
- Interpretation: If ablations don’t move the needle, your mechanism story is weak; deprioritize that angle. [G14]
- Follow-ups: Swap in a functionally similar alternative to test whether gains come from a general property vs. a specific trick.

3) Breadth and reviewer-alternative test
- Objective & hypothesis: The better storyline survives small distribution shifts and pre-empts a plausible reviewer alternative. Hypothesis: Winner maintains ≥X% of its gain on 2 datasets/scales and passes one pre-registered “skeptic test.” [G6]
- Setup: Evaluate both angles across 2 datasets (or 1 dataset at 2 scales) with matched hyperparameters; add one targeted test that would support a reviewer’s alternative explanation if true (e.g., data leakage, overfitting to a quirk).
- Metrics & success: Median performance across seeds, interquartile range, and pass/fail on the skeptic test. Prefer the angle with stable medians and a passed skeptic test. [G1][G6]
- Interpretation: If the effect evaporates under mild changes, scope down or treat that angle as appendix material; pick the other as the main story. [G7]

Writing tactic to finalize the choice
- Draft a 6-sentence abstract and a one-figure sketch for each storyline; if one reads crisper and needs fewer caveats, that’s your main thread. Write to a single skeptical reviewer and ensure every sentence is supported by a concrete test. [G6][G14]

Phase 0 (≤14 days) gates
- Deliverable 1: Three experiment cards (above) with pre-registered hypotheses, falsifiers, metrics, and stop rules.
- Deliverable 2: A prediction log (≥14 entries) and one reproduced figure showing the head-to-head result within target fidelity.
- If gates aren’t met, stay in Phase 0 and iterate on the smallest discriminative test. [G7]

Intuition
When two angles feel equally promising, the “real contribution” is the one you can demonstrate cleanly, causally, and repeatedly with minimal caveats—something a skeptical reviewer can’t easily knock down.

Why this is principled
High-impact work targets important questions and picks battles it can win decisively; choose based on importance and tractability, not seduction or sunk cost [G4][G1]. Tight, falsifiable experiments and figure-first writing keep you honest and make one storyline clearly stronger under scrutiny and time constraints [G14][G6][G7].

Resources
- Hamming: You and Your Research — focus on important problems; don’t confuse activity with impact [G4].
- Schulman: Opinionated Guide to ML Research — design crisp, falsifiable experiments; pick tractable wins [G14].

Next actions
- Send me the 1–2 sentence claims for Angle A and B, your target venue/timeline, and your compute/time budget. I’ll help tailor the three pilots and the score thresholds.

Citations
[G1] How Not To Choose A Research Project — https://www.lesswrong.com/posts/kDsywodAKgQAAAxE8/how-not-to-choose-a-research-project
[G4] 1986 Hamming — https://gwern.net/doc/science/1986-hamming
[G6] Highly Opinionated Advice On How To Write Ml Papers — https://www.alignmentforum.org/posts/Xt8tMtwfsLo2jRCEj/highly-opinionated-advice-on-how-to-write-ml-papers
[G7] Principles Of Effective Research — http://michaelnielsen.org/blog/principles-of-effective-research/
[G14] Opinionated Guide Ml Research.Html — http://joschu.net/blog/opinionated-guide-ml-research.html

[Turn 3] USER:
If I run the head-to-head pilot and the effect sizes are nearly identical, what tie-breaker should I apply next?

[Turn 3] ASSISTANT:
Stage: B → C — Tie-breaking between two storylines; goal is a discriminative pilot you can finish this week

Strategic questions
- Which angle has the clearer falsifier you’re willing to pre-register (a specific ablation or skeptic test that, if it fails, you’ll drop it)? [G6][G3]
- Which angle would be easier for a third party to reproduce end-to-end next month (fewer moving parts, simpler configs)? [G2][G6]
- Which angle better matches your target venue’s norms (mechanistic clarity vs. benchmark wins) and your 2–3 week compute/time budget? [G3][G9]

Tie-breaker ladder (apply in order; stop at the first decisive win)
1) Mechanism clarity and falsifiability: Prefer the angle with a cleaner causal story and a surgical ablation that predictably drops performance in the expected direction across seeds. [G6][G3]
2) Robustness/generalization: Prefer the angle whose gains persist with minimal retuning across a second dataset or scale and pass a pre-registered skeptic test (e.g., anti-leakage control). [G6][G9]
3) Efficiency and stability: Prefer higher effect-per-unit compute and lower variance across seeds/hyperparameters; reproducibility beats a hairline average win. [G2][G6]
4) Narrative crispness: Draft a 6-sentence abstract and one-figure storyboard for each; pick the one that needs fewer caveats. [G6]
5) Program potential: Choose the angle that naturally unfolds into 2–3 follow-ups you can execute; don’t just optimize for today’s metric. [G4][G13]

Three concrete, falsifiable tie-breaker experiments
1) Mechanism discriminant ablation
- Objective & hypothesis: Removing the core component of each angle causes a predictable, ≥Y% drop unique to that angle; the angle with the larger, tighter causal effect wins. Hypothesis: Δ(A−A−C_A) > Δ(B−B−C_B).
- Setup: Same dataset/protocol; run M_A vs. M_A−C_A and M_B vs. M_B−C_B with identical budgets; ≥3 seeds; pre-register expected direction and threshold. [G6][G3]
- Metrics & success: Mean±CI drop, paired tests/bootstraps; success if one angle shows a larger drop with tighter CI and correct direction across seeds.
- Interpretation: Clearer causal sensitivity indicates a stronger, defensible mechanism; if both are weak, downgrade both or refine the mechanism. [G6]
- Follow-ups: Swap C with a functionally similar alternative to test if the property (not the exact implementation) drives gains.

2) Breadth and skeptic test
- Objective & hypothesis: The winner keeps ≥X% of its gain on a second dataset or at a different scale and passes a targeted skeptic test (e.g., shuffled-label control, feature randomization). Hypothesis: A or B maintains stable medians and passes the pre-registered check. [G6][G9]
- Setup: Evaluate both angles on dataset D2 or smaller/larger model; minimal retuning; include one skeptic control chosen to target the most plausible alternative explanation.
- Metrics & success: Median across seeds, IQR, pass/fail on skeptic control; pick the angle with stable medians and a passed skeptic test.
- Interpretation: Stability under mild shifts and targeted controls signals genuine effect rather than overfitting quirks. [G6]
- Follow-ups: If both pass, proceed to efficiency tie-break; if both fail, scale down scope or reframe.

3) Compute/sample-efficiency sweep
- Objective & hypothesis: One angle achieves equal performance with fewer GPU-hours or fewer labeled samples; efficiency dominates marginal average gains. Hypothesis: Efficiency(A) > Efficiency(B) measured as Δmetric per log compute or per labeled sample. [G3][G2]
- Setup: Run a short compute sweep (e.g., 0.5×, 1×, 2× budget) or a low-data curve (5–10–20–50% data) for both angles; 2–3 seeds each.
- Metrics & success: Area under performance–compute (or performance–data) curve; slope at low budget; select the angle with higher AUC and better low-budget slope.
- Interpretation: Better scaling at practical budgets is more valuable and easier for others to adopt and reproduce. [G3][G2]
- Follow-ups: Validate the winner at one additional nearby budget to confirm the slope advantage.

Quick 48-hour next actions
- Pre-register the falsifier and thresholds for the ablation and skeptic test; write one-paragraph predictions for each angle. [G6]
- Run the smallest ablation+sko test combo first; if still tied, run the 0.5× vs. 1× compute sweep for efficiency. [G3]
- Share seed-level results and configs so we can sanity-check variance and reproducibility before committing. [G2]

Intuition
When top-line gains tie, the contribution that survives is the one with a cleaner causal story, broader stability, and lower adoption friction; those properties travel better across reviewers, datasets, and time.

Why this is principled
Impact follows from important problems pursued with decisive, falsifiable experiments and tight framing; tractable wins that are reproducible and extensible beat marginal average gains with caveats [G4][G3]. Pre-registered skeptic tests and robustness checks protect against overfitting narratives and make the result reviewer-defensible and reusable [G6][G2][G9].

Citations
[G2] Manifesto For Doing Good Science — https://letters.lossfunk.com/p/manifesto-for-doing-good-science
[G3] Opinionated Guide Ml Research.Html — http://joschu.net/blog/opinionated-guide-ml-research.html
[G4] 1986 Hamming — https://gwern.net/doc/science/1986-hamming
[G6] Highly Opinionated Advice On How To Write Ml Papers — https://www.alignmentforum.org/posts/Xt8tMtwfsLo2jRCEj/highly-opinionated-advice-on-how-to-write-ml-papers
[G9] Principles Of Effective Research — http://michaelnielsen.org/blog/principles-of-effective-research/

[Turn 4] USER:
Which experiment card should I draft first if my compute budget only lets me run one this week?

[Turn 4] ASSISTANT:
Stage: B → C — Picking the single lowest-compute, highest-information pilot

Quick check-in
- Do you already have trained checkpoints for both angles? If yes, can you disable the key component at inference or do a short warm-start fine-tune without full retraining?
- What’s your exact weekly budget (e.g., X GPU-hours) and the suspected mechanism difference between the two angles?
- Are configs modular enough to toggle one component without changing everything else?

Recommendation: draft this experiment card first
1) Mechanism discriminant ablation (warm-start, minimal compute) — do this first
- Objective & hypothesis: Removing the hypothesized causal component from each angle causes a predictable drop; whichever shows a larger, tighter drop has the clearer mechanism. Hypothesis: Δ(A − A−C_A) > Δ(B − B−C_B).
- Setup: Reuse existing checkpoints. For each angle, either (a) disable component C at inference if possible, or (b) fine-tune 1–3 epochs from the same checkpoint with C removed, keeping all else fixed; run 2–3 seeds. This is typically the best information-per-FLOP, and it directly tests falsifiability. [1]
- Metrics & success: Primary metric mean±CI; paired test or bootstrap on per-seed differences. Success = one angle shows a larger, consistent drop in the predicted direction across seeds.
- Interpretation: A sharper, repeatable drop signals a defensible causal story; a weak or noisy drop means the mechanism is overstated—deprioritize that angle.
- Follow-ups: Swap C for a simple alternative to test “general property vs. specific trick”; move extended runs to later if needed.

If warm-start ablation isn’t feasible this week, pick one of these
2) Low-budget compute-efficiency sweep (0.25× budget)
- Objective & hypothesis: One angle achieves equal or higher performance at a fraction of the budget. Hypothesis: AUC over performance–compute curve (using 0.25× and 1× budgets) is higher for one angle.
- Setup: Train both angles on the same dataset for 0.25× and 1× steps with 2 seeds; keep hyperparameters fixed; log GPU-hours.
- Metrics & success: Δmetric per GPU-hour and small-sample slope; choose the angle with better low-budget slope and higher AUC.
- Interpretation: Better early scaling is often more useful and reproducible than a marginal tie at full budget. [1]

3) Evaluation-only skeptic control (no retraining)
- Objective & hypothesis: The better angle survives a targeted skeptic test that would expose a spurious effect (e.g., input perturbations, subpopulation shift) using existing checkpoints. Hypothesis: Winner maintains ≥X% of its gain and passes the control.
- Setup: Construct a cheap distribution shift or negative control tied to the suspected alternative explanation (e.g., simple input corruptions or subpopulation slices). Evaluate both existing models identically.
- Metrics & success: Median across seeds (if available), IQR, and pass/fail on the control. Prefer the angle that remains stable and passes the skeptic test.
- Interpretation: Passing a targeted control reduces the risk of “storytelling” wins that don’t generalize. [2]

Next actions (this week)
- Confirm whether you can warm-start and toggle the key component; if yes, draft the ablation card and pre-register the expected direction and minimal drop size. [1]
- If not, draft the compute-efficiency card; otherwise use the evaluation-only skeptic control card and run it today. [2]
- Share seed-level numbers and configs; I’ll help sanity-check variance and decide which storyline graduates to main.

Intuition
With tight compute, the best experiment is the one that most directly tests the causal claim or reveals spuriousness while reusing what you’ve already trained. That’s usually a warm-start mechanism ablation; if unavailable, early-budget scaling or a skeptic control gives high information cheaply.

Why this is principled
Crisp, falsifiable tests maximize learning per unit compute and keep you honest about mechanisms and scope. They’re also easier to reproduce and defend to reviewers than broad, expensive sweeps. [1][2]

Citations
[1] An Opinionated Guide to ML Research — http://joschu.net/blog/opinionated-guide-ml-research.html
[2] Troubling Trends in Machine Learning Scholarship — https://arxiv.org/abs/1807.03341

[Turn 5] USER:
If my checkpoints are frozen and I can’t warm-start, which single inference-only test still gives the clearest signal—shuffled-label, input-noise, or subpopulation slice?

[Turn 5] ASSISTANT:
Stage: B → C — Picking one inference-only skeptic test under tight compute

Quick note on sources: The mentorship-guidelines tool returned no specific entries; I’m grounding the tie-breaker in robust evaluation literature [1][2][3].

Short answer
- Prefer subpopulation slice if your hypothesized difference is “handles spurious correlations or group shift” (choose the angle with higher worst-group accuracy and smaller group gap) [2]. If you don’t have reliable group labels or your story is general robustness, prefer input-noise corruptions (e.g., ImageNet-C/CIFAR-C) as the single inference-only test (choose the lower corruption error/mCE) [1]. Use shuffled-label only as a sanity check for leakage or evaluation bugs; it rarely discriminates between two trained models at inference-only [3].

Strategic questions
- Do you have trustworthy group/slice labels or easy-to-define slices aligned with your suspected spurious feature?
- Which angle claims what mechanism: subgroup invariance vs. general robustness to perturbations?
- What domain/dataset are you on (so we can pick the right corruption set or define slices)?

Three concrete, falsifiable inference-only experiments (pick one to run now)
1) Corruption robustness (default if no slices)
- Objective & hypothesis: The winning angle is more robust to common corruptions without retuning. Hypothesis: Its mean corruption error (mCE) across severities is lower. [1]
- Setup: Apply a standard corruption suite (e.g., Gaussian noise, blur, brightness, JPEG; severities 1–5) to the test set; evaluate both frozen models identically.
- Metrics & success: mCE or average error increase vs. clean; success if one model has a statistically lower mCE or a consistent margin across severities.
- Interpretation: Lower mCE indicates the effect is not a narrow data quirk but transfers under realistic perturbations. If results are mixed, report per-corruption wins and pick the model with the better median across severities. [1]
- Follow-ups: Add a tiny, cheap subset of corruptions at an unseen severity to confirm the ordering.

2) Subpopulation worst-group test (use if your claim targets spuriousness/group shift)
- Objective & hypothesis: The winner improves worst-group accuracy and reduces group disparity. Hypothesis: Higher worst-group accuracy and smaller max group gap at fixed threshold. [2]
- Setup: Define groups by metadata or simple heuristics (e.g., background color, demographics, spurious token presence); evaluate both models per-group on the same clean test inputs.
- Metrics & success: Worst-group accuracy, average accuracy, and group gap (max − min); success if one model has higher worst-group accuracy with a non-inferior average. [2]
- Interpretation: A better worst-group trade-off supports a subgroup-invariance mechanism; if both collapse on the same group, your narrative may not be about spuriousness—pivot to corruption robustness. [2]
- Follow-ups: Try one cheap reweighting of the decision threshold per group to ensure the outcome isn’t a threshold artifact.

3) Shuffled-label negative control (sanity check, not a tie-breaker)
- Objective & hypothesis: Ensure no label leakage or evaluation bug. Hypothesis: Both models drop to chance-level accuracy/AUC with randomly permuted labels on the same inputs. [3]
- Setup: Permute the label-to-example mapping once; re-evaluate both models without retraining.
- Metrics & success: Top-1 accuracy near 1/K (K classes) or AUC ~0.5; any substantial deviation flags leakage or leakage-like artifacts.
- Interpretation: If one model exceeds chance notably, fix the evaluation pipeline before making tie-break decisions. This test won’t typically separate two valid models. [3]
- Follow-ups: None unless it fails; then debug data leakage or caching/mismatch.

Recommendation for this week
- If you have meaningful group labels/slices aligned with your suspected mechanism, run the subpopulation worst-group test. Otherwise, run corruption robustness as your single inference-only tie-break. Keep the shuffled-label test as a 5-minute sanity check.

Next actions (48 hours)
- Confirm whether slices are available and aligned with your claim; if not, proceed with a corruption suite at one mid severity (e.g., severity=3) to get a quick read, then expand if needed. [1]
- Pre-register a simple success criterion (e.g., “Model A’s mCE < Model B’s by ≥2 points” or “worst-group accuracy improves by ≥3 points without lowering average”).
- Share the per-corruption or per-group table; I’ll help interpret variance and decide which storyline graduates.

Intuition
Pick the skeptic test that most directly targets your claimed mechanism: subgroup invariance → subpopulation worst-group; general robustness → corruption suite. Shuffled-label is only a safeguard against leakage and won’t meaningfully separate two trained models.

Why this is principled
Standard corruption benchmarks provide a cheap, reviewer-familiar proxy for generalization under perturbations [1]. Worst-group accuracy operationalizes robustness to group shifts and spurious correlations, which is exactly what subgroup-invariance methods should improve [2]. Shuffling labels probes memorization during training; applied only at evaluation, it’s a sanity check and seldom a discriminative signal [3].

Citations
[1] Hendrycks & Dietterich (2019), Benchmarking Neural Network Robustness to Common Corruptions and Perturbations — https://arxiv.org/abs/1903.12261
[2] Sagawa et al. (2020), Distributionally Robust Neural Networks for Group Shifts (GroupDRO) — https://cs.stanford.edu/~ssagawa/assets/slides/groupDRO_ICLR2020.pdf
[3] Zhang et al. (2017), Understanding Deep Learning Requires Rethinking Generalization — https://arxiv.org/abs/1611.03530

Before you run it: do you have group labels/slices for your dataset? If not, I’ll help you pick a minimal corruption set tailored to your domain.

[Turn 6] USER:
I don’t have group labels—should I still run the corruption suite or is there a cheaper slice I can define without metadata?

[Turn 6] ASSISTANT:
Stage: B → C — Inference-only tie-break with no metadata

Quick note on sources: No sources found for “cheap slicing without metadata; inference-only skeptic tests” in the mentorship-guidelines tool. Below draws on standard robustness/evaluation practice; resources at the end.

Strategic questions
- What’s your domain (vision, text, audio, tabular) and dataset size? This determines which metadata-free slices are easy to compute.
- Is there a concrete nuisance you suspect (e.g., brightness, blur, length, negation, noise level)?
- How many evaluations can you afford (e.g., 1–2 passes over the test set vs. 5–10)?

Decision rule
- If you can name a plausible nuisance you can estimate from inputs alone, define one metadata-free slice aligned to it and run a worst-slice test. If not, run a small corruption suite at a single mid severity as your default.

How to define metadata-free slices (pick one aligned to your suspected mechanism)
- Vision: brightness (mean pixel intensity), blur (variance of Laplacian), colorfulness (chroma std), texture complexity (local entropy). Bin examples into quartiles by the chosen statistic; the bottom quartile is your “hard slice.”
- Text: length (tokens), rare-word fraction (share of tokens below frequency threshold), OOV rate, negation presence via simple regex (e.g., “not|n’t|never”). Use quartiles (or a boolean for negation).
- Audio: SNR proxy (spectral flatness), RMS energy, tempo estimate. Quartile bins.
- Tabular: pick 1–2 continuous features central to the task; bin by quartiles (or missingness pattern if relevant).

Pre-register the statistic, binning rule (e.g., bottom 25%), and success criterion before looking at model outcomes to avoid slice cherry-picking.

Three concrete, falsifiable inference-only experiments (run exactly one first)
1) Corruption subset robustness (default if you can’t define a good slice)
- Objective & hypothesis: One model is more robust to common perturbations without retuning; hypothesis: its average corruption error at mid severity is lower.
- Setup: Choose 5 common corruptions (e.g., Gaussian noise, blur, brightness, JPEG, contrast) at a single mid severity. Evaluate both frozen models on the corrupted test set plus clean.
- Metrics & success: Report clean accuracy and the mean corruption error (average error increase vs. clean). Success if one model’s mean corruption error is ≥2 points lower and the ordering is consistent across at least 3/5 corruptions.
- Interpretation: A lower mean corruption error suggests broader robustness rather than overfitting to clean quirks. If mixed, prefer the model with the better median across corruptions.
- Follow-ups: Spot-check one additional unseen severity to confirm the ordering.

2) Metadata-free worst-slice test (use if you suspect a specific nuisance)
- Objective & hypothesis: The better model maintains higher worst-slice accuracy with no drop in overall accuracy. Hypothesis: higher accuracy on the bottom quartile of a pre-registered statistic (e.g., lowest brightness) while overall accuracy is non-inferior (≤1 point lower).
- Setup: Compute the statistic on the clean test set; split into quartiles; evaluate both models per quartile.
- Metrics & success: Worst-slice accuracy, overall accuracy, and the gap between worst and best quartile. Success if one model has higher worst-slice accuracy and a smaller gap without losing overall accuracy.
- Interpretation: Improved worst-slice performance supports a robustness mechanism aligned with that nuisance. If both degrade on the same slice, your story may not be about that nuisance—switch to corruption robustness.
- Follow-ups: Repeat with a second, orthogonal statistic (e.g., blur if you started with brightness) to triangulate.

3) Consistency under lightweight augmentations (invariance proxy)
- Objective & hypothesis: The better model is more prediction-consistent under benign transforms. Hypothesis: higher consistency (same predicted label) under two benign transforms and a smaller accuracy drop.
- Setup: On the clean test set, apply two label-preserving transforms (vision: small translation and brightness jitter; text: synonym swap with a controlled list and punctuation normalization; audio: small gain and time-shift). Evaluate both models on original and transformed inputs.
- Metrics & success: Consistency rate (fraction of examples with unchanged prediction) and accuracy delta relative to clean. Success if one model has higher consistency on both transforms and smaller accuracy drops.
- Interpretation: Higher invariance with stable accuracy is a strong, inference-only robustness signal. If consistency improves but accuracy collapses, it signals overconfident invariance—treat cautiously.
- Follow-ups: Add a third transform only if the first two are inconclusive.

What to run first this week
- If you can define a single, credible statistic tied to your suspected failure mode (e.g., blur for images, length for text), run the metadata-free worst-slice test. Otherwise, run the 5-corruption mid-severity robustness test. Keep the consistency test as a backup if results are tied.

Success criteria to pre-register (pick one)
- Corruption: “Model A’s mean corruption error is at least 2 points lower; tie-break by better median across corruptions.”
- Slice: “Model A’s worst-slice accuracy is ≥3 points higher with overall accuracy within 1 point.”
- Consistency: “Model A’s consistency is ≥3 points higher on both transforms with accuracy drop ≤1 point.”

Next actions (48 hours)
- Tell me your domain and suspected nuisance; I’ll help you lock one statistic/transform and exact thresholds.
- Pre-register the slice/corruption choice and success criterion in a short note.
- Run the single chosen test and share per-corruption or per-quartile numbers (seed-level if available).

Intuition
If you can target a plausible nuisance with a simple, input-derived statistic, worst-slice performance gives a direct, causal-leaning signal about your story. If not, a compact corruption suite is the most reviewer-familiar, inference-only proxy for general robustness.

Why this is principled
Pre-registered, model-agnostic slices avoid post-hoc cherry-picking and align the test with your hypothesized mechanism. Standard corruption evaluations provide a low-cost, broadly recognized stress test when metadata is absent, and consistency checks operationalize invariance without retraining.

Resources
- Hendrycks & Dietterich (2019), Benchmarking Neural Network Robustness to Common Corruptions and Perturbations — https://arxiv.org/abs/1903.12261
- Ribeiro et al. (2020), CheckList: Behavioral Testing of NLP Models — https://arxiv.org/abs/2005.04118
- Zeiler & Fergus (2014), Visualizing and Understanding Convolutional Networks (occlusion-style analysis inspiration) — https://arxiv.org/abs/1311.2901
