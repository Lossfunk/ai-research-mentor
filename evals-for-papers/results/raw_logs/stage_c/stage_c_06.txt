1. Problem framing and goals
- Context: In low-resource countries, federated learning (FL) must contend with intermittent/slow connectivity, heterogeneous and low-capability devices, limited energy, and partial client participation. These factors increase communication cost, destabilize convergence, and complicate fairness across clients [P2]. Semi-asynchronous aggregation and resource-aware client selection can reduce stragglers and idle waiting in heterogeneous IoT-like networks [P4]. Communication-minimizing paradigms (e.g., single-/one-shot FL) can reduce rounds to accommodate sparse connectivity [P5][P8].
- Primary goal: Deliver and evaluate a robust, communication- and energy-efficient FL pipeline that maintains acceptable accuracy and fairness under real-world constraints (low bandwidth, device heterogeneity, client churn).
- Secondary goals:
  - Quantify trade-offs between communication, energy, time-to-accuracy, and model utility under partial participation and dropouts [P2].
  - Validate semi-asynchronous aggregation and resource-aware client selection to improve wall-clock convergence in unreliable networks [P4].
  - Evaluate single-/one-shot FL viability for extremely constrained settings [P5][P8].
- Anchors: Adaptive/semi-asynchronous FL for resource-constrained devices [P4]; lightweight/single-shot FL for severely limited communication [P5].

2. Experiments
Experiment 1: Communication-efficient FL under low bandwidth
- Hypothesis: Gradient/weight compression (e.g., quantization to 8/4 bits; top-k sparsification) reduces upstream bytes by ≥10× with ≤2–3% absolute accuracy drop compared to uncompressed baselines under cross-device heterogeneity. This is motivated by the centrality of communication bottlenecks in FL and the promise of fewer/leaner updates in constrained settings [P2][P8].
- Setup:
  - Task: Image or text classification representative of local applications (e.g., low-resolution image classification or SMS intent). Start with publicly available small-scale proxies to ensure reproducibility.
  - Setting: Cross-device simulation with 1–5k clients, non-IID shards, participation rate 1–10%, 10–30% random dropouts, emulated uplink bandwidth 10–200 kbps.
  - Methods: FedAvg baseline without compression; with 8-/4-bit quantization; with top-k (e.g., 1% sparsity) and error feedback; single-round “one-shot” model aggregation as an extreme variant [P8].
- Baselines: Uncompressed FedAvg; periodic local SGD with full-precision updates; single-shot lightweight approach (e.g., k-NN voting-style single-shot from [P5]) to set a lower-communication floor.
- Evaluation metrics:
  - Utility: Top-1 accuracy, AUROC (if imbalanced), convergence rounds and wall-clock time.
  - Efficiency: Upload/download bytes per client and total; number of communication rounds.
  - Robustness/fairness: 10th-percentile client accuracy; variance across clients.
  - Energy proxy: CPU time and radio TX/RX time per round.
- Expected outcomes: Compression strategies should reduce communication by an order of magnitude with modest utility loss; single-/one-shot FL will minimize rounds but may yield lower peak accuracy or require stronger local models [P2][P5][P8].

Experiment 2: Semi-asynchronous aggregation for intermittent connectivity
- Hypothesis: Semi-asynchronous or staleness-aware aggregation improves time-to-target-accuracy by ≥20–30% over synchronous FedAvg under 20–50% client dropouts, with comparable final accuracy [P4].
- Setup:
  - Task: Same as Experiment 1 for comparability.
  - Network: Introduce variable latency and sporadic outages, with clustered outages to mimic grid/power patterns.
  - Methods: Synchronous FedAvg vs semi-asynchronous server that accepts partial sets each round, caps staleness, and re-weights stale updates; bounded-delay rules as in semi-async designs [P4].
- Baselines: Synchronous FedAvg with straggler tolerance (timeouts); buffered aggregation without staleness control.
- Evaluation metrics:
  - Time-to-X% accuracy; final accuracy after fixed wall-clock budget.
  - Effective client utilization (fraction of selected clients whose updates are accepted).
  - Bytes/accuracy and energy/accuracy ratios.
- Expected outcomes: Semi-async reduces idle waiting and improves wall-clock efficiency while preserving accuracy when staleness is bounded [P4].

Experiment 3: Resource-aware client selection to improve throughput and fairness
- Hypothesis: Selecting clients by a simple utility-per-time (estimated update value divided by expected completion time/energy) reduces time-to-accuracy and increases low-percentile client performance relative to random selection in heterogeneous environments [P4].
- Setup:
  - Client metadata: Lightweight telemetry (device class, historical completion time, last-success timestamps).
  - Policies: Random selection; availability-first; resource-aware utility-per-time selection; inclusion quotas for underrepresented client groups to mitigate bias.
- Baselines: Random and availability-only selection.
- Evaluation metrics:
  - Throughput: successful updates per wall-clock hour.
  - Fairness: 10th-percentile client accuracy; worst-group accuracy.
  - Robustness: Performance with diurnal availability patterns and sudden outages.
- Expected outcomes: Resource-aware selection improves throughput and reduces straggler-induced slowdowns; quota mechanisms can mitigate neglected-client underperformance without large global utility loss [P4]. 

Experiment 4: Single-/one-shot FL feasibility in ultra-low-connectivity settings
- Hypothesis: One-shot or very-few-shot FL can meet minimum utility thresholds (e.g., >60–70% of multi-round baseline accuracy) when multi-round communication is infeasible, especially with well-regularized local training and strong pretraining [P5][P8].
- Setup:
  - Methods: One-shot aggregation from locally trained models; few-shot (2–3 rounds) with large local epochs; compare to multi-round compressed FL.
- Baselines: Multi-round compressed FedAvg; centralized training on a small proxy if permissible (for upper bound).
- Evaluation metrics:
  - Utility: Accuracy gap to multi-round baseline.
  - Communication: Rounds and total bytes.
  - Stability: Sensitivity to local epoch length and non-IID severity.
- Expected outcomes: One-/few-shot approaches deliver dramatic comms savings at the cost of some utility; may be acceptable under extreme constraints or as a bootstrapping stage [P5][P8].

Note: Privacy/security (DP-SGD, secure aggregation) are essential in deployment, but the retrieved sources do not provide concrete, implementation-level guidance on these aspects. We will incorporate them and source canonical references (e.g., DP-SGD and secure aggregation) before pilot testing; see “Risks and mitigations” and evidence-gathering plan.

3. Timeline for the next 6 months with milestones
- Month 1: Requirements and simulation scaffolding
  - Define 2–3 target tasks and success metrics with stakeholders.
  - Build reproducible cross-device simulator with bandwidth/latency/outage models; add energy proxies.
  - Implement baselines: synchronous FedAvg; logging for bytes, time, fairness.
  - Milestone: Baseline runs and data pipeline ready; first accuracy vs communication curve [P2].
- Month 2: Communication-efficiency sweep
  - Implement quantization (8/4-bit) and sparsification with error feedback; add one-/few-shot variants.
  - Run grid across participation rates (1–20%), dropouts (10–50%), and non-IID levels.
  - Milestone: Report showing accuracy/bytes trade-offs; select top 2 methods for further study [P2][P8].
- Month 3: Semi-asynchronous and staleness control
  - Add semi-async aggregator with staleness caps and reweighting.
  - Compare wall-clock time-to-accuracy vs synchronous baselines.
  - Milestone: Demonstrate ≥20% wall-clock improvement in at least one regime or adjust design [P4].
- Month 4: Client selection and fairness
  - Implement resource-aware selection and quotas; evaluate throughput and low-percentile accuracy.
  - Start small on-device pilot (5–20 Android devices or SBCs) to validate telemetry and energy measurements.
  - Milestone: Evidence that resource-aware selection improves throughput without fairness regression [P4].
- Month 5: Integrated best configuration + robustness
  - Combine best compression + semi-async + selection; stress test with outage bursts and diurnal patterns.
  - Sensitivity analyses: model size, local epochs, participation rate.
  - Milestone: Integrated system surpasses predefined communication and time targets with acceptable accuracy.
- Month 6: Privacy, documentation, and dissemination
  - Integrate DP/secure aggregation prototypes; quantify utility cost (subject to sourcing canonical methods).
  - Prepare paper with ablations and negative results; create open-source artifacts and reproducibility package.
  - Milestone: Submission-ready manuscript with complete experimental appendix.

4. Resources (compute, tools, datasets)
- Compute:
  - Server: 1–2 modest GPUs (e.g., RTX 3090/A5000) or CPU-only if models are small; 64–128 GB RAM.
  - Clients: 10–30 low-end Android phones or Raspberry Pi-class devices for pilot; larger simulated cohorts.
- Tools:
  - FL frameworks: Flower or FedML for cross-device orchestration and custom strategies (semi-async, selection).
  - Network emulation: Linux tc/netem or Mahimahi for bandwidth/latency/outages.
  - Telemetry: Lightweight client timers and OS battery stats; server-side logging of bytes/time.
- Datasets/tasks:
  - Start with small, mobile-suitable public datasets (e.g., low-res images or text classification) to keep on-device compute low; move to domain-specific data with partners as available.
  - Note: The retrieved sources do not specify standard cross-device LMIC datasets; we will curate or adapt small public datasets as proxies and document distributional assumptions to avoid overclaiming external validity.

5. Risks and mitigations table
- Communication bottlenecks cause poor convergence — Use quantization/sparsification and few-/one-shot training; limit model size; measure bytes/accuracy trade-offs rigorously [P2][P5][P8].
- Stragglers and outages stall synchronous rounds — Adopt semi-asynchronous aggregation with staleness bounds and reweighting [P4].
- Heterogeneity harms fairness (some clients underperform) — Track 10th-percentile and worst-group metrics; apply quotas during client selection; allow light personalization heads; report disparities transparently [P2].
- Energy constraints reduce participation — Shorten local epochs for low-end devices; schedule training during charging/Wi‑Fi windows; reward designs requiring fewer rounds [P2][P4].
- Privacy and security gaps — Integrate secure aggregation and DP; quantify utility/privacy trade-offs; seek domain approvals. Limitation: current citations set lacks detailed DP/SA guidance; we will bring in canonical methods before pilot.
- External validity (sim-to-real gap) — Run on-device micro-pilots early (Month 4) to calibrate emulator parameters and energy proxies; adjust models accordingly.
- Limited datasets representative of LMIC contexts — Partner with local orgs; define data governance; start with proxies and document assumptions; build synthetic stress tests for non-IID and outages.

6. Stretch ideas or follow-up directions
- Opportunistic/one-shot FL: Use one-shot or very-few-shot FL for areas with sporadic connectivity; combine with strong pretraining and local regularization [P5][P8].
- Semi-async + adaptive compression: Dynamically adjust quantization/sparsity based on instantaneous link quality for better QoS under volatility [P4].
- Federated continual learning for evolving domains: Study memory/compute–performance trade-offs over time as data drifts, aligning with resource-constrained continual FL needs [P1].
- Extremely lightweight learners: Explore non-parametric or tiny models for initial deployment (e.g., k-NN/majority-vote single-shot baselines) and distill into larger models when connectivity allows [P5].
- Policy-aware client selection: Incorporate fairness constraints or service-level guarantees into selection objective (e.g., minimum update frequency per group) [P4].

Limitations and evidence-gathering plan
- The retrieved sources provide strong support for: (i) communication bottlenecks and the need for comm-efficient or one-/few-shot methods [P2][P5][P8]; and (ii) semi-asynchronous aggregation and resource-aware selection under heterogeneous IoT-like constraints [P4]. 
- Gaps: Concrete, authoritative references for differential privacy in FL and secure aggregation are not present in this set. Before pilot deployment, we will add canonical citations and implementations (e.g., DP-SGD and secure aggregation) via targeted searches and established libraries, and re-run key experiments with privacy enabled.

Citations
- [P1] Resource-Constrained Federated Continual Learning: What Does ... — https://arxiv.org/html/2501.08737v1
- [P2] Issues in federated learning: some experiments and preliminary ... — https://www.nature.com/articles/s41598-024-81732-0
- [P4] Adaptive federated learning for resource-constrained IoT devices ... — https://www.nature.com/articles/s41598-024-78239-z
- [P5] Lightweight Federated Learning Approach for Resource-Constrained Internet of Things — https://pmc.ncbi.nlm.nih.gov/articles/PMC12473279/
- [P8] Towards One-shot Federated Learning: Advances, Challenges, and Future Directions — https://arxiv.org/html/2505.02426v1

Optional next step
- If you share your target application (e.g., health, agriculture, education) and the typical client device/network profile, I can instantiate the above with concrete models, datasets, and success thresholds and draft the exact ablation grid.