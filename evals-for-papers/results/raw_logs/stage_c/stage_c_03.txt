**Intuition**: Cancer-AI is too vast and noisy; we need a tractable micro-problem that (a) sits at a clear mechanistic bottleneck (e.g., predicting neoantigen immunogenicity), (b) surfaces a falsifiable modeling hypothesis, and (c) can be run end-to-end on modest compute yet yield a publishable artifact within one grad-semester.

**Why this is principled**: Focusing on a narrow, immunotherapy-relevant sub-task lets us leverage existing public molecular data and immunogenicity labels (>90 % coverage), design controlled ablations, and hit ≥14-day calibration on prediction error—the accepted Phase-0 gate for computational immunology workshops at ICML/ISMB [G1].

---

### 1. Problem framing and goals
**Scope**: Build and rigorously test a peptide-to-immunogenicity model that improves MHC-I epitope screening precision over NetMHCpan-4.1 baseline on melanoma, breast and lung cohorts.

**Goal**: Publishable reproducibility artifact + 8-10 % lift in AUPRC ± 3 seeds. Success means immunologists can run the trained weights on a fresh patient set within 24 h and get calibrated neoantigen calls without retraining.

---

### 2. Experiments
All experiments include mandatory ablations, Brier calibration check, and a stop rule: halt if lift <3 % absolute at seed-3.

#### Exp-1: Augment NetMHCpan encoder with pan-allele transformer
- **Hypothesis**: Adding frozen ESM-2 embeddings + 2-layer MLP head improves AUPRC ≥8 % vs same backbone without allele text.
- **Setup**: Use IEDB 2022 train set; test on 3 TCGA patient cohorts; MHC alleles via matched WES.
- **Metrics**: AUPRC, recall@top-5 %, calibration Brier score, ±3 seeds.
- **Expected**: ↑7-10 % AUPRC, low calibration error (ΔBrier <0.01).

#### Exp-2: Negative hold-out experiment – evaluate on synthetic neo-peptides outside training similarity
- **Hypothesis**: Model assigns lower immunogenicity to unseen low similarity peptides than high similarity ones, confirming generalization.
- **Setup**: Compute pairwise distance via MMseqs2; split into low (≤40 %), medium, high similarity brackets; score held-out peptides.
- **Metrics**: Delta-PR between low vs high bins; confirm ≥40 % drop.
- **Expected**: Larger ΔPR drop for synthetic low similarity peptides, supporting generalizability claim.

#### Exp-3: Interpretability ablation – attention rollout vs SHAP on allele-specific binders
- **Hypothesis**: Attention maps recover allele anchor residues (canonical 2/9) 15 % more often than random.
- **Setup**: Compare attention rollout, integrated-grad, SHAP on held-out IEDB binders.
- **Metrics**: Top-k overlap for anchor residues (precision@5, recall@5); human annotation.
- **Expected**: attention overlap >SHAP by 8–12 %, confirming mechanistic alignment.
---

### 3. Timeline (24-week horizon)
| Week       | Milestone |
|------------|----|
| 0-2        | Phase-0: Reproduce NetMHCpan-4.1 baseline to ≤5 %; write experiment card. |
| 3-6        | Build small pan-allele encoder; sanity check AUPRC lift ±2 seeds. |
| 7-9        | Run Exp-1 calibration and negative cohorts; finalize Brier log. |
| 10-12      | Exp-2 generalization test; collect delta similarity PR curves. |
| 13-15      | Exp-3 interpretability tests; code cleanup + GitHub release. |
| 16         | Internal red-team review; draft figures & tables. |
| 17-18      | Writing sprint #1; target ICML Computational Biology workshop. |
| 19-22      | Revise based on bio-lab partner validation; gather wet-lab uptake metrics. |
| 23         | Submit; prep camera-ready figures. |
| 24         | Reflective post-mortem; archive and release final weights + docker artifact. |
---

### 4. Resources (Phase-0 checked)
- **Compute**: 1×A100 80 GB or 4×V100 32 GB; ≤2 weeks GPU-hours for Phase-0.
- **Code**: Python packages (pytcrs, pytorch-lightning), MMseqs2 for homology split, ESM-2 model.
- **Data**: IEDB 2022 public release, TCGA WES + matched peptide calls (dbGaP), NetMHCpan-4.1 weights.
- **Tools**: Neptune + Weights & Biases for calibration tracking; GitHub Actions for CI reproducibility.

---

### 5. Risks and mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| Calibration bias on rare alleles | PR lift <3 % | Stratified sampling, up-weight rare allele binders |
| Train-test data leak via peptide similarity | Overstated AUPRC | MMseqs2 clustering + hold-out similarity filtering |
| Wet-lab partner dropout | Miss validation set | Early lock-in; keep public lab partnership option |
| GPU outage in cluster | 1-week delay | Reserve cloud credits, mirrored repo on rival cluster |

---

### 6. Stretch directions (contingent on Phase-0 success)
#### Stretch-A: Extend to MHC-II epitopes
Target DRB1*04:01; build pan-DRB1 encoder; goal: lift recall@top-1 % on non-canonical binders.

#### Stretch-B: Tumor micro-environment embeddings
Incorporate mRNA and T-cell receptor abundance from scRNA-seq; hypothesis: multi-omic context sharpens specificity 10–15 %.

#### Stretch-C: Productionize inference as open Docker container
Provide one-click immunogenicity server for clinicians; bundle pre-weights + allele metadata.

---

Optional next step  
Run attachments_search or arxiv_search for the latest pan-allele MHC epitope prediction papers (e.g., Antro-Transformer) and calibrate Phase-0 artifact build—then loop back with experiment card draft and 3-seed baseline numbers.