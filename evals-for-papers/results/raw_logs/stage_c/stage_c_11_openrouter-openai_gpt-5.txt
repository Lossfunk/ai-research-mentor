1. Problem framing and goals
- Objective: Orchestrate a cross-lab evaluation of large-scale climate downscaling methods with rigorous uncertainty quantification (UQ), producing transparent, reproducible rankings and governance-ready artifacts. Methods to include: classical/statistical downscaling (e.g., LOCA; BCSD), operational/gridded products (e.g., NEX-GDDP-CMIP6), and deep learning approaches (e.g., DeepSD; hard-constrained DL) [2][3][4][8][1].
- Variables, domains, scales: Daily precipitation and temperature, at 0.25°→0.1° and 0.1°→1–4 km (regional), across at least two climatically distinct regions (e.g., U.S. West Coast and Southeast Asia), with transfer to an out-of-region test [8][6].
- Uncertainty pillars:
  - Model UQ: deep ensembles, heteroscedastic regression (predictive mean/variance), quantile regression, and distribution-free calibration via conformal prediction [7].
  - Evaluation UQ: coverage of predictive intervals, sharpness, CRPS, Brier score/reliability diagrams, and PIT diagnostics [9][10].
- Cross-lab roles
  - Lab A (Coordination & Infra): Pangeo data plumbing (Intake-ESM/xarray/Dask), containerized pipelines, governance and preregistration [5].
  - Lab B (Classical baselines): LOCA/BCSD implementations; NEX-GDDP-CMIP6 ingestion and baselines [2][3][8].
  - Lab C (DL & UQ): DeepSD/UNet variants; hard-constrained physics-aware models; ensembles/quantiles/conformal [1][4][7].
  - Lab D (Evaluation & Impacts): Metrics suite (CRPS, reliability, extremes), optional hydrologic sensitivity checks with established toolkits (note: hydrology tools are ancillary to downscaling; use cautiously) [9][10].
- Success criteria (6 months)
  - Reproducibility: Cross-lab replication of headline metrics within agreed tolerances via shared containers and seed control [5].
  - Accuracy and UQ: On held-out regions, top model achieves statistically significant CRPS improvement over LOCA/BCSD and NEX-GDDP-CMIP6; 90% conformal intervals achieve ~90% empirical coverage with competitive sharpness [2][3][7][9].
  - Governance: Evaluation plan, risk register, and dataset/model cards aligned to community tooling (ESMValTool) and verification best practices [6][9].

2. Experiments (each with hypothesis, setup, baselines, evaluation metrics, and expected outcomes)
Experiment 1: Cross-method baseline accuracy on daily T/P
- Hypothesis: Recent deep learning downscalers (e.g., DeepSD; hard-constrained DL) outperform LOCA/BCSD and NEX-GDDP-CMIP6 on bias/RMSE and correlation, especially for complex terrains [1][2][3][4][8].
- Setup:
  - Data: CMIP6 GCMs (historical and SSP2-4.5) via Intake-ESM; targets: Daymet V4 daily 1-km (CONUS) and one non-CONUS high-res product (or national meteorological analysis) for out-of-region testing [5][6].
  - Methods: LOCA, BCSD, NEX-GDDP-CMIP6 baseline ingestion; DeepSD and a hard-constrained DL variant trained per region; no UQ yet [1][2][3][4][8].
- Baselines: LOCA, BCSD, NEX-GDDP-CMIP6 [2][3][8].
- Metrics: RMSE, mean bias, Pearson/Spearman correlation, spatial skill (xskillscore); computational footprint (GPU-hours), and wall-time per decade processed [5][8].
- Expected outcomes: DL methods reduce RMSE/bias vs. LOCA/BCSD, with trade-offs in compute; results recorded in a pre-registered evaluation plan [1][2][4].

Experiment 2: Probabilistic calibration and coverage (UQ core)
- Hypothesis: Conformal prediction wraps (split-conformal or aggregated variants) around point or quantile forecasts achieve target nominal coverage (e.g., 90%) across space/time, with sharper intervals than naive Gaussian assumptions; deep ensembles improve sharpness but may under-cover extremes without calibration [7][9][10].
- Setup:
  - Methods: Deep ensembles (M=5–7), heteroscedastic regression (predictive variance), quantile regression (τ∈{0.05,0.5,0.95}); conformal calibration on a held-out calibration period, evaluated on the test period [7].
  - Regions: In-region and out-of-region (transfer) tests.
- Baselines: Empirical climatology intervals; uncalibrated ensemble spread.
- Metrics: Empirical coverage at 50/80/90%, average interval width (sharpness), CRPS, Brier score, reliability diagrams (with stabilized plotting) [9][10].
- Expected outcomes: Conformalized intervals achieve nominal coverage with competitive sharpness; ensembles under-cover without calibration, especially for precipitation tails [7][9][10].

Experiment 3: Extremes fidelity (tails and return levels)
- Hypothesis: DL downscalers may smooth extremes; hard constraints and quantile/conformal methods better preserve upper-tail behavior (e.g., 99th percentile daily precipitation), improving return level estimation vs. LOCA/BCSD [2][4].
- Setup:
  - Compute annual maxima and high quantiles (Q95, Q99) at station-matched pixels; fit GEV on targets and predictions; compare return levels and exceedance probabilities.
- Baselines: LOCA, BCSD, NEX-GDDP-CMIP6 [2][3][8].
- Metrics: Bias in return levels (20- and 50-year), tail RMSE, reliability of exceedance probabilities (Brier, reliability diagrams); spatial hit/miss rates of extreme events [9][10].
- Expected outcomes: Hard-constrained and calibrated DL reduce tail underestimation vs. unconstrained DL and LOCA/BCSD; if evidence is mixed, document where each method fails (acknowledging that extreme-value evaluation literature is broad and not downscaling-specific in our quick scan; we will augment with targeted hydrometeorological tail-evaluation references in Month 3–4).

Experiment 4: Physical consistency and mass/energy constraints
- Hypothesis: Hard-constrained DL reduces physical inconsistency metrics (e.g., negative precipitation probabilities, water/energy balance artifacts) without sacrificing accuracy [4].
- Setup:
  - Implement/ingest hard-constrained loss terms per reference; compute diagnostics (non-negativity; conservation proxies at basin-scale with regridding).
- Baselines: Unconstrained DL, LOCA/BCSD [2][3].
- Metrics: Rate of physical violations; basin-scale closure residuals; accuracy metrics from Exp. 1 to quantify trade-offs [4].
- Expected outcomes: Significant reduction in violation rates with modest accuracy trade-offs [4].

Experiment 5: Scalability and reproducibility across labs
- Hypothesis: A shared Pangeo+Dask pipeline with Intake-ESM, standardized containers, and deterministic seeds yields reproducible metrics across labs; throughput scales near-linearly with cluster size on commodity cloud/HPC [5].
- Setup:
  - Each lab runs the same containerized pipeline on a common CMIP6 subset and shared targets; compare metrics and wall-time; record costs.
- Baselines: Non-containerized ad hoc code path.
- Metrics: Inter-lab metric deltas (e.g., RMSE differences < 1e-6 after regridding/IO harmonization), throughput (GB/hour), and job failure rates [5].
- Expected outcomes: Reproducibility within tolerance and documented cost/latency profiles; any discrepancies trigger a root-cause postmortem.

Experiment 6: Benchmark against operational downscaled products
- Hypothesis: For widely used downscaled products (NEX-GDDP-CMIP6), modern DL+UQ configurations can achieve better accuracy and calibrated uncertainty on held-out historical periods, providing evidence for method updates [8].
- Setup:
  - Ingest NEX-GDDP-CMIP6 for overlapping models/periods; evaluate side-by-side with our top methods on the same targets [8].
- Baselines: NEX-GDDP-CMIP6 [8].
- Metrics: RMSE/bias, CRPS, coverage/sharpness, extremes metrics (as above).
- Expected outcomes: Clear trade-offs; if operational baselines win in certain regimes (e.g., tropical convection), document where DL/UQ needs improvement.

3. Timeline for the next 6 months with milestones
- Month 1: Governance, data, and pipelines
  - Draft preregistered evaluation protocol (metrics, splits, seeds); finalize regions/variables; set up Pangeo/Intake-ESM catalog, storage, and container images [5].
  - Milestones: Protocol v1; CMIP6/Daymet ingestion and QC complete; ESMValTool evaluation plan drafted (for consistency of diagnostics) [6].
- Month 2: Baselines and first DL runs
  - Stand up LOCA/BCSD and ingest NEX-GDDP-CMIP6; train DeepSD on Region A; run Exp. 1 on Region A [1][2][3][8].
  - Milestones: Baseline report (RMSE/bias/corr, compute cost); container published.
- Month 3: UQ integration
  - Implement ensembles/heteroscedastic/quantile; add conformal calibration; run Exp. 2 on Region A; start Region B [7][9][10].
  - Milestones: Calibration report (coverage/sharpness/CRPS); reliability plots.
- Month 4: Extremes and constraints
  - Run Exp. 3 (extremes) and Exp. 4 (constraints) on both regions; refine losses/hyperparameters [4][9][10].
  - Milestones: Extremes fidelity report; physical consistency diagnostics.
- Month 5: Cross-lab reproducibility and scalability
  - Execute Exp. 5 across labs; conduct cost/throughput study; finalize CI tests for determinism [5].
  - Milestones: Reproducibility pass with tolerances; performance/cost brief.
- Month 6: Operational benchmark and consolidation
  - Run Exp. 6 vs NEX-GDDP-CMIP6; integrate all results; compile governance artifacts (risk register, model/data cards); submit preprint [8].
  - Milestones: Final comparative report; open repository with reproducible notebooks/containers; governance package v1.

4. Resources (compute, tools, datasets)
- Compute and storage
  - Shared object storage (cloud/HPC) for CMIP6 and targets; Dask clusters sized to process ≥10 TB within project timelines; GPU nodes for DL training (modest, scaled scheduling).
- Tools
  - Pangeo ecosystem: xarray, Dask, Intake-ESM for CMIP6; evaluation helpers (xskillscore) [5].
  - Downscaling: LOCA/BCSD implementations; DeepSD code; hard-constrained DL per reference [1][2][4].
  - UQ: Ensemble wrappers; quantile/heteroscedastic heads; conformal prediction library (split-conformal) [7].
  - Evaluation: ESMValTool for standardized model diagnostics; CRPS/Brier/reliability diagram implementations with stable plotting [6][9][10].
  - Regridding: Use the project’s standard regridder (document choice) to ensure comparability (note: specific regridding package citation not retrieved; we will document and cite in the repo).
- Datasets
  - CMIP6 models via Intake-ESM catalogs (historical, SSP2-4.5) [5].
  - Observation/analysis: Daymet V4 daily 1-km (CONUS) [6]. Non-CONUS region to use an equivalent national analysis (to be documented).
  - Operational downscaled: NEX-GDDP-CMIP6 v2 Tech Note and datasets [8].
  - Literature baselines: LOCA documentation; DeepSD (Vandal et al.); hard-constrained DL [1][2][4].

5. Risks and mitigations table
- Heterogeneous preprocessing breaks reproducibility
  - Mitigation: Single containerized pipeline; Intake-ESM catalogs; fixed seeds; regridding and masking standardized with version pins; cross-lab CI [5].
- UQ miscalibration under distribution shift (Region B)
  - Mitigation: Region-specific conformal calibration; adaptive/aggregated conformal methods; evaluate transfer coverage separately and report gaps [7][9].
- Extremes poorly captured by all methods
  - Mitigation: Tail-focused training (quantile losses), hard constraints, and targeted post-processing; transparently report limitations; seek additional tail-focused references (acknowledged gap).
- Compute/storage overruns
  - Mitigation: Prioritize subset of CMIP6 models for initial runs; schedule Dask jobs; monitor cost/throughput; compress intermediates.
- Baseline method fidelity (LOCA/BCSD variants differ)
  - Mitigation: Document parameterizations, use canonical references/software where available, and run sensitivity analyses [2][3].
- Governance misalignment and unclear acceptance criteria
  - Mitigation: Preregister metrics/thresholds; map diagnostics into ESMValTool-style reports; maintain a risk register; schedule mid-project audits [6].

6. Stretch ideas or follow-up directions
- Diffusion-based or transformer spatiotemporal downscalers with physics-informed losses; compare UQ to ensembles/conformal (subject to compute).
- Multi-model superensembles: combine CMIP6 sources with Bayesian model averaging; quantify ensemble-of-opportunity gains (use CRPS/Brier).
- Impact-model coupling: propagate UQ through a hydrologic model to test decision relevance (note: hydrology coupling not the core aim; if pursued, use established toolkits and document assumptions).
- Conformal risk control for extremes: conformalized quantile regression focused on upper tails; benchmark coverage-sharpness trade-offs [7].

Citations
[1] DeepSD: Generating High Resolution Climate Change Projections — https://arxiv.org/abs/1703.03126  
[2] LOCA Statistical Downscaling (Localized Constructed Analogs) — https://loca.ucsd.edu/what-is-loca/  
[3] Application of the Bias Correction and Spatial Downscaling (BCSD) Method — https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2019EA000995  
[4] Hard-Constrained Deep Learning for Climate Downscaling — https://arxiv.org/html/2208.05424v9  
[5] Pangeo/Intake-ESM example for CMIP6 — https://gallery.pangeo.io/repos/pangeo-gallery/cmip6/intake_ESM_example.html  
[6] Daymet V4: Daily Surface Weather Data on a 1‑km Grid — https://daac.ornl.gov/DAYMET/guides/Daymet_Daily_V4.html  
[7] Quantifying uncertainty in climate projections with conformal ensembles — https://arxiv.org/abs/2408.06642  
[8] NEX‑GDDP‑CMIP6 v2 Technical Note — https://www.nccs.nasa.gov/sites/default/files/NEX-GDDP-CMIP6-v2-Tech_Note.pdf  
[9] Decomposition of the Continuous Ranked Probability Score for Ensembles (CRPS) — https://journals.ametsoc.org/view/journals/wefo/15/5/1520-0434_2000_015_0559_dotcrp_2_0_co_2.xml  
[10] Stable reliability diagrams for probabilistic classifiers — https://pmc.ncbi.nlm.nih.gov/articles/PMC7923594/

Optional next step
- Approve Month 1 protocol and data plumbing: we will preregister metrics/splits, stand up the shared Pangeo/Intake-ESM catalog, and deliver a containerized baseline pipeline so all labs can reproduce Exp. 1 on Region A within two weeks.