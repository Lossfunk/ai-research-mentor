You can do meaningful, publishable AI research without writing Python by focusing on (a) data/evaluation questions, (b) no‑code or hosted tools, and (c) careful experimental design. Below is a practical 8–10 week plan with concrete, falsifiable experiments, ablations, and deliverables.

Scope and setup (no-code/low-code stack)
- Problem types to target
  - Data-centric classification or QA where you can control data quality and evaluation.
  - Prompting and retrieval-augmented generation (RAG) using hosted LLM UIs.
- Tools (no code)
  - Orange or KNIME (drag‑and‑drop ML pipelines) for tabular/text classification.
  - Hugging Face AutoTrain (web UI) for training text/image classifiers without code.
  - ChatGPT/Claude web UIs for prompting studies; use spreadsheets for evaluation.
  - Optional: Google Colab or Kaggle notebooks to run pre-made notebooks by editing parameters only.

Three+ concrete, falsifiable experiments
1) Data cleaning improves accuracy without changing the model
- Task: Binary or multi-class text classification (e.g., sentiment; pick an open dataset or assemble a small domain dataset).
- Procedure: Train a baseline classifier in Orange/KNIME with cross‑validation. Then clean data (remove near-duplicates, fix label issues via simple rules or reviewer passes). Retrain.
- Hypothesis: Cleaning yields ≥1.5–3.0 percentage‑point test accuracy (or F1) improvement at the same model/hyperparameters. Falsify if gain <1.0 point.
- Metrics: Accuracy/F1, calibration (expected calibration error if available), confusion matrix. Track time spent cleaning vs. performance gain.
- Ablations: Clean top 5%, 10%, 20% most suspicious examples; measure marginal returns.
- Rationale: Label-error reduction often improves generalization [Northcutt et al., 2021].

2) No‑code fine‑tuning vs. prompt‑only baseline
- Task: Small text classification (e.g., topic or intent).
- Procedure: Evaluate a hosted LLM’s zero‑shot and few‑shot prompts in the UI on a 200–500 example test set. Then train a classifier with HF AutoTrain on the same training data; compare.
- Hypothesis: AutoTrain fine‑tuning beats few‑shot prompting by ≥3–5 points in accuracy/F1 at similar inference latency for short texts. Falsify if improvement <2 points.
- Metrics: Accuracy/F1, latency per 100 examples, confusion matrix, robustness to class imbalance.
- Ablations: Training set size (100, 500, 2,000); class‑balanced vs. imbalanced.

3) Prompting patterns for reasoning tasks
- Task: Short multi-step reasoning questions (e.g., a 100‑item subset of GSM8K‑style arithmetic or your domain’s procedural tasks).
- Procedure: In ChatGPT/Claude UI, compare three prompt types on the same questions: (a) direct answer; (b) chain‑of‑thought (“think step by step”); (c) few‑shot exemplars with rationales.
- Hypothesis: Chain‑of‑thought or exemplar prompts increase exact match by ≥5–10 points over direct prompting. Falsify if ≤3 points.
- Metrics: Exact match, time per question, error typology.
- Ablations: Length‑limited CoT vs. freeform; number of exemplars (1, 3, 5).
- Rationale: Reasoning improves with structured prompting in many settings [Wei et al., 2022].

4) Lightweight, no‑code RAG vs. no‑RAG baseline
- Task: Build a small corpus (10–30 PDFs/web pages) in a niche topic. Use an LLM UI with file upload/knowledge features to answer 100 factual questions you author.
- Procedure: Compare answers with and without attached knowledge files; hold questions constant.
- Hypothesis: With knowledge files, factual accuracy increases by ≥10 points and hallucinations decrease by ≥30% (manual rubric). Falsify if gains <5 points.
- Metrics: Accuracy (exact/partial match), citation presence, hallucination rate.
- Ablations: Corpus size (10 vs 30 docs) and question difficulty.

8–10 week timeline
- Week 1: Choose a domain and target tasks; collect or curate a small, rights‑cleared dataset (500–5,000 examples). Set up Orange/KNIME and AutoTrain accounts. Define metrics and a scoring rubric.
- Weeks 2–3: Run Experiment 1 (cleaning vs. baseline). Document cleaning criteria and time budget. Produce cross‑val plots and error analysis.
- Weeks 3–4: Run Experiment 2 (AutoTrain vs. prompt‑only). Vary training set sizes; record accuracy/latency; build a simple leaderboard of configurations.
- Weeks 5–6: Run Experiment 3 (prompting patterns). Pre‑register the rubric and hypotheses; collect blind scores in a spreadsheet to reduce bias.
- Weeks 6–7: Run Experiment 4 (no‑code RAG). Measure gains and failure modes; compile examples of good/bad retrieval effects.
- Weeks 8–10: Synthesize results into a short paper/report: problem, setup, methods, results, ablations, limitations, and a reproducibility package (datasets, exported Orange/KNIME workflows, AutoTrain config, evaluation spreadsheet).

Measurement and reporting
- Always report: dataset size, class balance, train/test splits, exact prompts/configs, number of evaluation items, and compute (e.g., “no local GPU; hosted UI