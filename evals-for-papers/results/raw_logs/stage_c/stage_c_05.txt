1. Problem framing and goals
- Core research question: How reliable, robust, and transparent are no‑code AutoML systems on tabular datasets, and how do no‑code large language model (LLM) prompting strategies compare across public reasoning/QA benchmarks?
- Motivation and grounding:
  - Prior work has compared capabilities and performance of AutoML tools, but systematic, user-centered evaluation of no-code settings (time-to-result, usability, documentation quality) remains underexplored [P1]. Benchmarking best practices highlight leakage control, dataset curation, and lifecycle governance, which many ad‑hoc evaluations omit [P2]. Reporting standards and documentation (e.g., model cards/datasheets) improve rigor and reproducibility [P3], [P9]. For LLM prompting, step-by-step reasoning evaluation and its pitfalls are actively studied, motivating careful protocols beyond raw accuracy [P4]; reproducibility frameworks stress variance across runs/tools [P6].
- Primary objectives:
  1) Quantify performance, robustness, and cost/time-to-result of GUI-based AutoML on standard tabular datasets with strict leakage controls.
  2) Assess transparency and documentation quality (explanations, model cards/datasheets) produced using only built-in no-code features.
  3) Evaluate no-code prompting strategies (e.g., chain-of-thought, self-consistency) on public QA/reasoning subsets with reproducible, low-friction procedures.
- Success criteria:
  - A reproducible benchmark report (preregistered) with at least 8 tabular datasets covering classification/regression, clear leakage controls, and variance estimates; and a prompting study on at least 3 QA datasets, each with ≥200 items, with protocol preregistration, inter-annotator reliability, and transparent reporting [P2], [P3], [P9].
  - An open replication package: protocol, dataset cards, model cards, prompts, results sheets, and instructions.

2. Experiments
Experiment 1: No-code AutoML on tabular data (baseline capability and performance)
- Hypothesis: Across diverse tabular datasets, multiple no-code AutoML platforms achieve comparable performance to strong tree-based baselines within a small margin, with variation by dataset type; time-to-first-solution and cost differ substantially by platform [P1], [P2].
- Setup (no coding):
  - Tools: H2O Driverless AI (GUI trial), Google Vertex AI AutoML Tables (GUI), Azure ML Studio AutoML (UI), MLJAR (UI). Record platform versions.
  - Datasets (public, classic, low-risk): from OpenML/UCI (e.g., OpenML Adult, Credit-g, Higgs-small, Covertype, Churn, California Housing, YearPredictionMSD, Bike Sharing). Prepare datasheets for each dataset before training [P9].
  - Protocol: For each dataset, do a fixed 80/20 holdout with an additional internal validation via platform defaults; disable target leakage by removing obvious post-outcome features and applying time-aware splits when timestamps exist [P2]. Cap training time uniformly per tool (e.g., 1 hour/dataset).
- Baselines:
  - Simple baselines configured in-tool: logistic regression/GLM and single Gradient Boosted Trees (or XGBoost default if available).
  - AutoML full autopilot mode for each tool with identical time budget.
- Evaluation metrics:
  - Classification: ROC-AUC, PR-AUC (for imbalance), F1, log loss, calibration error (ECE).
  - Regression: RMSE, MAE, R^2, calibration plot (prediction vs observed).
  - Operational: time-to-first-solution, total runtime, cost (cloud pricing), clicks/steps, UI errors encountered.
- Expected outcomes:
  - Performance clustering with small but significant differences across platforms; calibration and log loss may differentiate tools beyond accuracy; operational metrics reveal notable differences in speed/cost [P1], [P2].
  - Documented dataset cards and model cards improve interpretability and reproducibility [P3], [P9].

Experiment 2: Robustness to distribution shift and data hygiene
- Hypothesis: Under temporal or covariate shifts, autopilot ensembles degrade more than simpler baselines on certain datasets; stricter leakage controls reduce apparent performance but increase external validity [P2].
- Setup (no coding):
  - For time-stamped datasets, use train on earlier period, test on later period; for others, produce stratified splits that intentionally vary feature distributions (e.g., geography-based split).
  - Apply identical time budgets and settings as Experiment 1.
- Baselines:
  - Same simple baselines as Experiment 1 and “best” AutoML per platform from Experiment 1.
- Evaluation metrics:
  - Primary: generalization gap (validation vs shifted test), relative performance drop (%), calibration drift (change in ECE).
  - Secondary: SHAP/feature-importance stability across splits if provided in-platform; otherwise, compare top-k features surfaced by each tool.
- Expected outcomes:
  - Measurable performance drop under shift; simpler models may show smaller degradation on some datasets; findings illustrate the importance of benchmark design and leakage checks [P2].

Experiment 3: Transparency, documentation, and usability assessment (human study)
- Hypothesis: Tools differ in quality/completeness of explanations and documentation; providing standardized model cards/datasheets improves user trust and facilitates replication [P3], [P9].
- Setup (no coding):
  - Participants: 5–10 graduate-level readers (or domain practitioners). Materials: exported model reports/explanations from each platform for 3 datasets; a checklist derived from REFORMS/reporting standards and documentation practices (model cards/datasheets) [P3], [P9].
- Baselines:
  - Each tool’s default report vs the same report augmented with a standardized model card/datasheet you complete.
- Evaluation metrics:
  - Likert scales (clarity, sufficiency, actionability), task time (how long to locate key info: performance, limitations, data splits), error rates (misinterpretations), inter-annotator agreement (Cohen’s kappa or Krippendorff’s alpha).
- Expected outcomes:
  - Augmented documentation improves comprehension/time-to-answer; tool-level differences surface in explanation clarity and coverage [P3], [P9].

Experiment 4: No-code LLM prompting strategies on reasoning/QA tasks
- Hypothesis: Chain-of-thought (CoT) with self-consistency improves accuracy on multi-step reasoning compared to direct answers, but with variance across models and datasets; majority-vote self-consistency stabilizes results [P4], [P6].
- Setup (no coding):
  - Use hosted LLM UIs (e.g., ChatGPT, Claude, Gemini). Select 3 public datasets/subsets: arithmetic (GSM8K small subset), commonsense (CSQA subset), open-domain (TriviaQA subset). Use fixed random subsets (e.g., first 200 items) to keep manual evaluation feasible. Document prompts and instructions precisely.
  - Conditions: Direct-answer baseline; CoT prompt; CoT + self-consistency (e.g., 5 samples, majority vote).
- Baselines:
  - Direct-answer prompts without reasoning steps.
- Evaluation metrics:
  - Exact match / accuracy; macro-F1 (if applicable); calibration proxy via self-reported confidence; cost and time per 100 questions; variance across 3 sessions/days.
- Expected outcomes:
  - CoT + self-consistency improves accuracy on reasoning-heavy tasks with nontrivial variance; careful evaluation and reporting needed to avoid overclaiming [P4], [P6].

3. Timeline for the next 6 months with milestones
- Month 1: Preregistration and setup
  - Finalize research questions, datasets list, evaluation metrics; create preregistration on OSF.
  - Build templates: datasheets (for each dataset), model cards (per model), experiment log sheets [P3], [P9].
  - Pilot 1 dataset across all AutoML tools to validate procedure; refine leakage checks and time budgets [P2].
- Month 2: AutoML benchmarking (core datasets)
  - Run Experiment 1 across 4–5 datasets; collect performance, calibration, cost/time; draft preliminary tables/plots.
  - Begin Experiment 2 on 2 datasets with natural time splits.
- Month 3: Complete robustness and documentation pass
  - Finish Experiment 2 across all applicable datasets; analyze generalization gaps.
  - Generate tool-native reports; draft model cards/datasheets; start Experiment 3 protocol and materials [P3], [P9].
- Month 4: Human study + LLM prompting pilot
  - Conduct Experiment 3 with 5–10 evaluators; compute IAA (kappa/alpha).
  - Pilot Experiment 4 on one QA dataset; refine prompts and sampling [P4], [P6].
- Month 5: LLM prompting main runs and analysis
  - Complete Experiment 4 on all 3 QA datasets; collect variance across sessions; compile results.
  - Integrate all findings; stress-test claims against benchmarking best practices [P2].
- Month 6: Write-up, artifact packaging, and submission
  - Finalize figures, tables, documentation; assemble replication package (protocol, data splits, model cards/datasheets, prompts, raw outputs).
  - Internal peer review; target venue selection; submission.

4. Resources (compute, tools, datasets)
- Compute
  - Cloud credits for AutoML (estimate: $300–$1,000 depending on datasets/time budgets). Track cost per run.
  - LLM UI usage: allocate ~$100–$300 for multi-sample self-consistency runs across 3 models.
- Tools (no/low-code)
  - AutoML: H2O Driverless AI (GUI), Google Vertex AI AutoML Tables (GUI), Azure ML Studio AutoML (UI), MLJAR (UI) [P1].
  - Workflow/annotation: KNIME or RapidMiner for data prep if needed (drag-and-drop), Label Studio (UI) for human study materials.
  - Documentation: OSF for preregistration; templates for datasheets/model cards [P9].
- Datasets
  - Tabular: OpenML/UCI classic tasks (classification and regression), with attention to leakage and license notes [P2].
  - QA/reasoning: GSM8K subset, CSQA subset, TriviaQA subset; confirm licenses/usage permissions and sampling protocol [P4].

5. Risks and mitigations
- Data leakage or benchmark artifacts inflate results
  - Mitigation: Strict split policies (time-aware when available), pre-registered feature exclusion rules, auditing per BetterBench guidance [P2].
- Cross-platform configuration mismatch
  - Mitigation: Fix uniform time budgets, enable/disable similar features across tools, log all defaults; report any unavoidable differences transparently [P1], [P2].
- Cost/time overruns on cloud AutoML
  - Mitigation: Pilot small time budgets; early stopping; prioritize a representative subset of datasets; cap per-dataset spend.
- Limited interpretability/export restrictions
  - Mitigation: Use tools with built-in explainability; supplement with standardized model cards and datasheets documenting gaps [P3], [P9].
- Human study bias/low reliability
  - Mitigation: Clear rubrics, blinded materials, multiple raters; compute inter-annotator agreement and revise rubric if kappa/alpha < 0.6.
- LLM variance and nondeterminism
  - Mitigation: Multiple sessions, fixed prompts, self-consistency aggregation; report variability and seeds where configurable [P6].
- Tool/version drift during study
  - Mitigation: Freeze versions during runs; archive screenshots/configs; report exact build numbers [P2].

6. Stretch ideas or follow-up directions
- Fairness and calibration audits: Add datasets with sensitive attributes (e.g., Adult) and compute demographic parity difference, equalized odds; calibrate with temperature scaling in tools that support it [P2].
- Cross-tool ensembles: Evaluate whether simple ensembling of the best models across tools (where export is possible) yields gains; document feasibility/usability constraints.
- Tabular benchmark expansion: Incorporate emerging tabular benchmark suites to broaden coverage (e.g., living benchmarks) if licensing and access allow [P2].
- Prompting generalization: Compare prompting strategies across multiple LLM providers and tasks (math vs commonsense) and add error taxonomy analysis informed by reasoning evaluation surveys [P4].
- Reproducibility checklists: Map results to REFORMS and produce a compliance summary; release public templates others can adapt [P3], [P9].

Citations
- [P1] Evaluation and Comparison of AutoML Approaches and Tools — https://ar5iv.labs.arxiv.org/html/1908.05557
- [P2] BetterBench: Assessing AI Benchmarks, Uncovering Issues, and Establishing Best Practices — https://arxiv.org/html/2411.12990
- [P3] REFORMS: Consensus-based Recommendations for Machine-learning-based Science — https://reforms.cs.princeton.edu/
- [P4] Evaluating Step-by-step Reasoning Traces: A Survey — https://arxiv.org/html/2502.12289v1
- [P6] An Analyst-Inspector Framework for Evaluating Reproducibility of LLMs in Data Science — https://arxiv.org/abs/2502.16395
- [P9] The Responsible Foundation Model Development Cheatsheet — https://arxiv.org/html/2406.16746v4

Optional next step
- If you want, I can generate a preregistration template and a one-click checklist (datasheet/model card forms, run logs, cost tracker) tailored to the exact tool mix you choose.