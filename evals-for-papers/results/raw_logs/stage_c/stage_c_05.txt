Turnkey research plan: Practical, publication-grade evaluation of compression + fine-tuning for 7B LLMs without Python coding

Scope: Develop and evaluate a reproducible recipe that combines post-training quantization (PTQ), parameter-efficient fine-tuning (QLoRA-style adapters on a quantized base), optional pruning, and preference alignment to achieve high accuracy retention with 4–8× memory compression and lower latency on 7B instruction-tuned LLMs. We anchor to recent evidence that (a) PTQ can be combined with QLoRA to recover quality [P1], (b) joint distillation + quantization is effective for compact models [P2], and (c) preference alignment can restore conversational ability for quantized models [P7]. We use these as literature anchors and design falsifiable experiments around them.

1) Problem framing and goals
- Target models and tasks
  - Base: 7B instruction-tuned LLMs (e.g., Llama-2-7B-chat, Mistral-7B-Instruct). Tasks: general instruction-following, commonsense, and reasoning subsets (ARC-C, HellaSwag, BoolQ), plus open-ended chat scoring (MT-Bench/AlpacaEval-equivalent), and perplexity on WikiText-2. We will report accuracy/EM/ROUGE as appropriate and standardized chat pairwise win rates.
- Objectives with numeric targets
  - Memory compression: Achieve 4–8× model memory reduction vs FP16 baselines through 4-bit PTQ; secondary target: explore 3-bit settings.
  - Accuracy retention: ≥95% of FP16 baseline average score across a fixed task basket (ARC-C, HellaSwag, BoolQ; plus perplexity within +5% relative). Success if each task retains ≥92% and average ≥95% of FP16.
  - Latency: ≥1.8× lower single-token latency vs FP16 on the same GPU at batch size 1; throughput: ≥1.6× tokens/sec at batch size 8.
  - Tuning efficiency: QLoRA-on-PTQ recovers ≥1.5 percentage points absolute of any PTQ-only drop on the task average with ≤20% of full fine-tuning time [P1].
  - Conversational quality: For chat evaluations, quantized+aligned models achieve ≥90% of the FP16-aligned model’s pairwise win rate; quantized preference alignment follows [P7] guidance.
- Success criteria
  - Primary: Meet all four targets above for at least one 4-bit configuration; demonstrate Pareto frontier where at least one point strictly dominates PTQ-only baselines in both quality and latency [P1][P2].
  - Secondary: Show that 3-bit settings can achieve average ≥90% retention post fine-tuning on at least two tasks.

2) Experiments (five core experiments, each with ablations)
General evaluation harness
- Use lm-eval-harness or OpenCompass presets; report means over 3 seeds; 95% CIs via bootstrap. Fix a Task Basket (ARC-C, HellaSwag, BoolQ; perplexity on WikiText-2; 100–200 question sampled chat set scored by GPT-4 classifer or MT-Bench-like rubric). Keep prompts, decoding, and temperature fixed across variants.

Experiment 1. PTQ baselines on 7B models
- Hypothesis H1: 4-bit PTQ (activation-aware or smooth quantization variants) retains ≥95% of FP16 baseline accuracy across the task basket; falsify if average <95% or any task <92%.
- Setup
  - Models: Llama-2-7B-chat and Mistral-7B-Instruct (FP16 baselines).
  - Quantization: 4-bit PTQ using activation-aware quantization (AWQ-like) and a smooth calibration approach (SmoothQuant-like). Group sizes g ∈ {32, 64, 128}; calibration set 1280 tokens sampled from task-disjoint text.
  - Hardware: 1× A100 40GB or 1× L40S 48GB; batch sizes 1 and 8 for latency/throughput profiling.
  - Decoding: greedy for accuracy tasks; chat sampling temp 0.2, top-p 0.9; max gen length 128.
- Ablations (2–4)
  - Bitwidth: 3-bit vs 4-bit.
  - Group size: 32 vs 64 vs 128.
  - Calibration size: 256 vs 1280 vs 4096 tokens.
  - Quant method: activation-aware vs smooth calibration.
- Baselines
  - FP16 original model (no compression).
  - 8-bit weight-only baseline.
- Metrics and thresholds
  - Task accuracy/score: ≥95% of FP16 average; per-task ≥92%.
  - Perplexity: ≤+5% relative to FP16.
  - Latency/throughput: ≥1.8× latency reduction; ≥1.6× throughput increase.
- Expected outcomes
  - 4-bit: average retention 95–99%; 3-bit: 88–95% depending on group size and calibration.

Experiment 2. QLoRA-style fine-tuning on a 4-bit PTQ base
- Hypothesis H2: Adding low-rank adapters and brief SFT on a 4-bit PTQ base recovers ≥1.5 points absolute of the PTQ-only drop and meets ≥95% FP16 retention with ≤20% of full SFT tokens [P1]. Falsify if recovery <1.5 pts or tokens exceed 20% of full SFT budget [P1].
- Setup
  - Base: Best 4-bit model from Exp1.
  - Data: 50–100k instruction-response pairs (general domain), deduplicated, non-overlapping with eval. 1–3 epochs max.
  - Adapters: rank r ∈ {8, 16, 32}; α ∈ {16, 32}; dropout 0.05; target modules: attention and MLP projections.
  - Optimizer: AdamW (lr ∈ {1e-4, 2e-4}, wd 0), cosine decay, warmup 3%.
  - Batch: global 128 sequences × 512 tokens; gradient accumulation to fit VRAM.
  - Hardware: 1× A100 80GB or 2× L40S 48GB.
- Ablations
  - Adapter rank r and α.
  - Training tokens: 10%, 20%, 40% of a full SFT budget surrogate.
  - Freeze pattern: attention-only vs attn+MLP.
- Baselines
  - PTQ-only from Exp1.
  - LoRA on FP16 baseline with same token budget (upper-bound quality w/out compression).
- Metrics and thresholds
  - Same task basket; retention ≥95%, per-task ≥92%; chat win rate vs FP16-LoRA ≥90%.
  - Stability: training loss monotonicity and no divergence.
- Expected outcomes
  - 4-bit + adapters meets target and narrows gap to FP16-LoRA with 10–20% SFT budget [P1].

Experiment 3. Joint distillation + quantization for compact recovery
- Hypothesis H3: Distillation from a stronger teacher (e.g., 13B/34B) into the 4-bit 7B student improves average accuracy by ≥2 points over SFT-only QLoRA-on-PTQ at similar token budgets [P2]. Falsify if gain <2 points.
- Setup
  - Teacher: FP16 13B or 34B instruction model.
  - Student: 7B best 4-bit checkpoint from Exp2 (or re-init quantized base).
  - Distillation: mix of soft targets (KL on logits) and supervised signals; temperature τ ∈ {1, 2}; loss λ_KL ∈ {0.3, 0.5}.
  - Data: 100k–200k mixed-domain prompts; optionally augment with teacher-generated rationales.
  - Optimizer/hparams: as Exp2; 1–2 epochs.
- Ablations
  - Teacher size: 13B vs 34B.
  - Distill weighting λ_KL and τ.
  - Student init: PTQ base vs PTQ+LoRA-adapted.
- Baselines
  - Exp2 best model.
  - PTQ-only.
- Metrics/thresholds
  - Average score ≥ Exp2 +2 points with same or fewer tokens; retention ≥95% FP16; perplexity non-degraded (>−2% relative).
- Expected outcomes
  - Distillation yields consistent 2–4 point improvements for compact quantized students [P2].

Experiment 4. Structured pruning on quantized students
- Hypothesis H4: Up to 30% structured pruning of attention/MLP channels on the 4-bit student reduces latency by ≥15% with ≤3% absolute average accuracy loss after brief adapter re-tuning. Falsify if latency gain <15% or accuracy loss >3%.
- Setup
  - Pruner: channel/head pruning guided by magnitude or saliency (e.g., LLM-Pruner-style structural pruning), target sparsity s ∈ {10%, 20%, 30%}.
  - Post-prune recovery: 5–20k steps adapter re-tuning on instruction data.
  - Hardware: as Exp2.
- Ablations
  - Sparsity levels s.
  - Prune target: attention-only vs MLP-only vs both.
  - Recovery budget: 5k vs 10k vs 20k steps.
- Baselines
  - Unpruned Exp2/Exp3 best.
- Metrics/thresholds
  - Same task basket; latency/throughput on identical hardware and batching; retention ≥92% per-task, ≥95% average optional.
- Expected outcomes
  - 10–20% structured pruning yields clear latency gains with small drops; 30% requires careful recovery.

Experiment 5. Preference alignment on quantized models
- Hypothesis H5: Direct preference alignment (e.g., DPO/DPA on quantized models) improves chat win-rate by ≥5 points over SFT-only quantized models while preserving non-chat accuracy within −1 point [P7]. Falsify if win-rate gain <5 points or non-chat average drops >1 point.
- Setup
  - Start from best Exp2/3 4-bit checkpoint.
  - Data: 30–50k preference pairs (public or synthetically generated), with safety filtering.
  - Method: DPO-style objective with low-rank adapters; lr 5e-5; 1 epoch; β ∈ {0.05, 0.1}.
- Ablations
  - Preference data size: 10k vs 30k vs 50k.
  - β temperature; adapter rank r ∈ {8, 16}.
- Baselines
  - SFT-only quantized model.
  - FP16-aligned model upper bound.
- Metrics/thresholds
  - Chat win-rate vs FP16-aligned ≥90%; general-task average within −1 point; toxicity and refusal rates monitored.
- Expected outcomes
  - Alignment substantially improves conversational quality without degrading core tasks [P7].

3) Timeline (6 months with bi-weekly checkpoints)
- Month 1: Reproducible PTQ baselines
  - Weeks 1–2: Environment setup; obtain models; run FP16 baselines; define fixed eval harness. Deliverable: FP16 scores and profiling; acceptance: stable reproducible metrics ±0.3 points across seeds.
  - Weeks 3–4: Implement 4-bit PTQ variants; profile latency/throughput. Deliverable: PTQ report with ablations; decision: proceed if ≥93% average retention (go), else expand calibration or try alternate PTQ.
- Month 2: QLoRA-on-PTQ tuning
  - Weeks 5–6: Run adapter sweeps (r, α, tokens). Deliverable: recovery curve vs token budget; checkpoint: meets ≥95% retention on average?
  - Weeks 7–8: Consolidate best config; robustness check on held-out prompts. Go/no-go: if <95% retention, escalate token budget or broaden modules.
- Month 3: Distillation
  - Weeks 9–10: Distill from 13B teacher; sweep λ_KL, τ. Deliverable: +2 points over Exp2?
  - Weeks 11–12: Optional 34B teacher trial; choose best. Go/no-go: if no consistent gain ≥2 points, deprioritize distillation for remaining months.
- Month 4: Pruning for speed
  - Weeks 13–14: Apply 10–30% structured pruning; post-prune adapter recovery. Deliverable: latency vs accuracy tradeoffs; checkpoint: ≥15% latency gain at ≤3 points loss?
  - Weeks 15–16: Integrate best prune with Exp3; finalize Pareto set.
- Month 5: Preference alignment on quantized
  - Weeks 17–18: DPO/DPA training with 30–50k pairs. Deliverable: chat win-rate improvement ≥5 points; safety metrics.
  - Weeks 19–20: Sensitivity analysis on β and data size; finalize aligned model.
- Month 6: Scaling, sensitivity, and paper-ready assets
  - Weeks 21–22: Full sensitivity/Pareto analysis over bitwidth (3/4/8), group size, adapter rank, token budget; build Pareto frontiers for quality vs latency and memory.
  - Weeks 23–24: Reproducibility pack: scripts, configs, seeds, docker; thorough error bars; write paper draft and release artifacts. Go/no-go for submission based on primary success criteria.

4) Resources
- Compute
  - Minimal: 1× L40S 48GB or A100 40GB for PTQ and evaluation; 2× L40S or 1× A100 80GB for QLoRA/distillation sweeps; optional 1× 4090 24GB for smaller runs (lower batch sizes).
  - Estimated hours:
    - Month 1: 80–120 GPU-hours (PTQ sweeps + eval).
    - Month 2: 150–250 GPU-hours (adapter sweeps).
    - Month 3: 200–300 GPU-hours (distillation).
    - Month 4: 120–180 GPU-hours (pruning + recovery).
    - Month 5: 120–200 GPU-hours (preference alignment).
    - Month 6: 120–200 GPU-hours (sensitivities, final eval).
- Datasets (versions/splits)
  - ARC-Challenge dev/test; HellaSwag validation; BoolQ dev/test; WikiText-2 for perplexity; curated instruction SFT set (50–100k pairs) from public sources; preference pairs (30–50k).
  - Strict deduplication and contamination checks; fixed splits and seeds.
- Tooling and libraries
  - PyTorch 2.3+, CUDA 12.1; Hugging Face Transformers 4.44+; bitsandbytes 0.43+; AutoAWQ or SmoothQuant implementation; peft 0.12+; TRL for alignment; lm-eval-harness 0.4.2 or OpenCompass 0.2+.
  - Docker image frozen from base nvidia/cuda:12.1-cudnn8-runtime + conda env lockfile. No-code friendly: all runs via shell scripts or Makefile targets; no Python coding required.

5) Risks and mitigations
- Risk: PTQ degradation on reasoning tasks
  - Probability: Medium; Impact: Medium
  - Mitigation: Increase calibration set; adjust group size; try smooth vs activation-aware quantization; fall back to 8-bit for sensitive layers [P1].
- Risk: QLoRA-on-PTQ instability at 3–4 bits
  - Probability: Medium; Impact: High
  - Mitigation: Lower lr, increase warmup, freeze more modules; modest rank increase; gradient clipping; early stopping [P1].
- Risk: Distillation yields limited gains
  - Probability: Medium; Impact: Medium
  - Mitigation: Increase teacher size; tune τ and λ_KL; add rationale supervision; expand domain coverage [P2].
- Risk: Pruning harms calibration and throughput not realized
  - Probability: Medium; Impact: Medium
  - Mitigation: Prefer structured/channel pruning; re-tune adapters; profile with the same kernel stack; ensure sparsity patterns align with kernels.
- Risk: Evaluation variance obscures differences
  - Probability: Medium; Impact: Medium
  - Mitigation: 3-seed average; CIs; fixed decoding; longer test sets; preregister metrics.
- Risk: Data contamination inflates scores
  - Probability: Low–Medium; Impact: High
  - Mitigation: Deduplication against model pretraining snapshots where possible; hold-out blind eval; report contamination checks.

6) Integrated recipe and scaling study
- Integration logic
  - Stage A: Establish high-quality 4-bit PTQ foundations (Exp1).
  - Stage B: Add low-rank adapters on top of PTQ for recovery at low token budgets (Exp2) [P1].
  - Stage C: If needed, apply distillation from larger teacher to boost compact student (Exp3) [P2].
  - Stage D: Apply modest structured pruning to push latency gains with bounded accuracy loss (Exp4).
  - Stage E: Run preference alignment on the quantized (and possibly pruned) model to restore conversational quality (Exp5) [P7].
- Scaling/sensitivity
  - Bitwidth vs accuracy: 3/4/8 bits × group size sensitivity; identify when QLoRA closes the gap at lower bits [P1].
  - Adapter rank vs tokens: r ∈ {8,16,32} × token budgets 10–40% of full SFT; map diminishing returns.
  - Distillation hyperparameters: λ_KL, τ, teacher size; quantify incremental benefit relative to compute.
- Pareto analysis
  - Construct Pareto frontiers of (memory, latency, accuracy) for: FP16, PTQ-only, PTQ+QLoRA, PTQ+QLoRA+distill, and +prune. Report which points strictly dominate PTQ-only and approach FP16 upper bounds [P1][P2].
- Reproducibility
  - Publish exact configs, seeds, commit hashes, and docker image; release scripts to run everything via shell-only commands; provide a one-click runbook.

How this plan helps a non-Python user
- All steps are executable via curated shell scripts and configuration files (YAML/JSON) for models, datasets, and hyperparameters. We will provide:
  - A Makefile with targets (make ptq, make lora, make distill, make prune, make align, make eval).
  - Predefined config templates for each experiment and ablation set.
  - Dockerized environment to avoid package conflicts.
  - A runbook with copy/paste commands for each milestone.

Key takeaways
- The plan methodically tests whether adapter-based fine-tuning on quantized models can close the PTQ quality gap at low cost [P1], whether adding distillation further helps compact 7B students [P2], and whether preference alignment on quantized models restores chat quality [P7], all while building a Pareto frontier for practical deployment.

Citations
- [P1] Can Post-Training Quantization Benefit from an Additional QLoRA Integration? — http://arxiv.org/abs/2502.10202v1
- [P2] DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization — http://arxiv.org/abs/2203.11239v1
- [P6] Distill or Annotate? Cost-Efficient Fine-Tuning of Compact Models — http://arxiv.org/abs/2305.01645v3
- [P7] Improving Conversational Abilities of Quantized Large Language Models via Direct Preference Alignment — http://arxiv.org/abs/2407.03051v2

Note: We anchored claims about combining QLoRA with PTQ [P1], joint distillation + quantization [P2], and alignment on quantized models [P7]. Some baseline techniques (e.g., specific PTQ variants, pruning) are standard but not individually cited here due to tool limits; during execution, we will expand the bibliography to include method-specific implementations evaluated (e.g., activation-aware and smooth PTQ variants, and structural pruning for LLMs) and ensure precise attributions in the final paper.