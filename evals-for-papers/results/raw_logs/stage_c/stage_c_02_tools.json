[
  {
    "tool_name": "web_search",
    "run_id": "run-web_search-1761050475106",
    "status": "success",
    "started_ms": 1761050475106,
    "ended_ms": 1761050512652,
    "duration_ms": 37546,
    "metadata": {
      "score": 3.4,
      "inputs_keys": [
        "mode",
        "page_size",
        "query",
        "response_format",
        "topic"
      ],
      "tool_state": "HEALTHY",
      "tool_state_label": "healthy",
      "backoff_count": 0,
      "num_sources": 3
    },
    "events": [
      {
        "timestamp_ms": 1761050512652,
        "event_type": "final_result",
        "payload": {
          "summary": [
            "- GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers",
            "- SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models",
            "- LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"
          ],
          "sources": [
            "https://arxiv.org/abs/2210.17323",
            "https://www.alphaxiv.org/abs/2211.10438",
            "https://arxiv.org/abs/2208.07339"
          ]
        }
      }
    ]
  },
  {
    "tool_name": "legacy_arxiv_search",
    "run_id": "run-legacy_arxiv_search-1761050475018",
    "status": "success",
    "started_ms": 1761050475018,
    "ended_ms": 1761050475105,
    "duration_ms": 87,
    "metadata": {
      "score": 0.7000000000000002,
      "inputs_keys": [
        "limit",
        "query"
      ],
      "tool_state": "HEALTHY",
      "tool_state_label": "healthy",
      "backoff_count": 0,
      "num_sources": 3
    },
    "events": [
      {
        "timestamp_ms": 1761050475105,
        "event_type": "final_result",
        "payload": {
          "summary": [
            "- Towards Efficient Post-training Quantization of Pre-trained Language Models",
            "- AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models",
            "- OutlierTune: Efficient Channel-Wise Quantization for Large Language Models"
          ],
          "sources": [
            "http://arxiv.org/abs/2109.15082v1",
            "http://arxiv.org/abs/2311.01305v3",
            "http://arxiv.org/abs/2406.18832v1"
          ]
        }
      }
    ]
  },
  {
    "tool_name": "legacy_arxiv_search",
    "run_id": "run-legacy_arxiv_search-1761050473730",
    "status": "success",
    "started_ms": 1761050473730,
    "ended_ms": 1761050475017,
    "duration_ms": 1287,
    "metadata": {
      "score": 0.7000000000000002,
      "inputs_keys": [
        "limit",
        "query"
      ],
      "tool_state": "HEALTHY",
      "tool_state_label": "healthy",
      "backoff_count": 0,
      "num_sources": 3
    },
    "events": [
      {
        "timestamp_ms": 1761050475017,
        "event_type": "final_result",
        "payload": {
          "summary": [
            "- Towards Efficient Post-training Quantization of Pre-trained Language Models",
            "- AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models",
            "- OutlierTune: Efficient Channel-Wise Quantization for Large Language Models"
          ],
          "sources": [
            "http://arxiv.org/abs/2109.15082v1",
            "http://arxiv.org/abs/2311.01305v3",
            "http://arxiv.org/abs/2406.18832v1"
          ]
        }
      }
    ]
  },
  {
    "tool_name": "web_search",
    "run_id": "run-web_search-1761050333943",
    "status": "success",
    "started_ms": 1761050333943,
    "ended_ms": 1761050366815,
    "duration_ms": 32872,
    "metadata": {
      "score": 3.9999999999999996,
      "inputs_keys": [
        "mode",
        "page_size",
        "query",
        "response_format",
        "topic"
      ],
      "tool_state": "HEALTHY",
      "tool_state_label": "healthy",
      "backoff_count": 0,
      "num_sources": 3
    },
    "events": [
      {
        "timestamp_ms": 1761050366815,
        "event_type": "final_result",
        "payload": {
          "summary": [
            "- SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models",
            "- KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization",
            "- LLMCBench: Benchmarking Large Language Model Compression for Efficient Deployment"
          ],
          "sources": [
            "https://www.catalyzex.com/paper/smoothquant-accurate-and-efficient-post",
            "https://arxiv.org/abs/2401.18079",
            "https://arxiv.org/html/2410.21352v2"
          ]
        }
      }
    ]
  },
  {
    "tool_name": "legacy_arxiv_search",
    "run_id": "run-legacy_arxiv_search-1761050333857",
    "status": "success",
    "started_ms": 1761050333857,
    "ended_ms": 1761050333943,
    "duration_ms": 86,
    "metadata": {
      "score": 1.3,
      "inputs_keys": [
        "limit",
        "query"
      ],
      "tool_state": "HEALTHY",
      "tool_state_label": "healthy",
      "backoff_count": 0,
      "num_sources": 3
    },
    "events": [
      {
        "timestamp_ms": 1761050333943,
        "event_type": "final_result",
        "payload": {
          "summary": [
            "- Low-Rank Quantization-Aware Training for LLMs",
            "- QuantMoE-Bench: Examining Post-Training Quantization for Mixture-of-Experts",
            "- AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization Method for LLMs"
          ],
          "sources": [
            "http://arxiv.org/abs/2406.06385v3",
            "http://arxiv.org/abs/2406.08155v2",
            "http://arxiv.org/abs/2405.13358v1"
          ]
        }
      }
    ]
  },
  {
    "tool_name": "legacy_arxiv_search",
    "run_id": "run-legacy_arxiv_search-1761050333104",
    "status": "success",
    "started_ms": 1761050333104,
    "ended_ms": 1761050333856,
    "duration_ms": 752,
    "metadata": {
      "score": 1.3,
      "inputs_keys": [
        "limit",
        "query"
      ],
      "tool_state": "HEALTHY",
      "tool_state_label": "healthy",
      "backoff_count": 0,
      "num_sources": 3
    },
    "events": [
      {
        "timestamp_ms": 1761050333856,
        "event_type": "final_result",
        "payload": {
          "summary": [
            "- Low-Rank Quantization-Aware Training for LLMs",
            "- QuantMoE-Bench: Examining Post-Training Quantization for Mixture-of-Experts",
            "- AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization Method for LLMs"
          ],
          "sources": [
            "http://arxiv.org/abs/2406.06385v3",
            "http://arxiv.org/abs/2406.08155v2",
            "http://arxiv.org/abs/2405.13358v1"
          ]
        }
      }
    ]
  },
  {
    "tool_name": "web_search",
    "run_id": "run-web_search-1761050214399",
    "status": "success",
    "started_ms": 1761050214399,
    "ended_ms": 1761050273186,
    "duration_ms": 58787,
    "metadata": {
      "score": 2.1999999999999997,
      "inputs_keys": [
        "mode",
        "page_size",
        "query",
        "response_format",
        "topic"
      ],
      "tool_state": "HEALTHY",
      "tool_state_label": "healthy",
      "backoff_count": 0,
      "num_sources": 3
    },
    "events": [
      {
        "timestamp_ms": 1761050273186,
        "event_type": "final_result",
        "payload": {
          "summary": [
            "- AI Model Compression: Reducing Model Size While Maintaining Performance for Efficient Deployment",
            "- Efficient Compressing and Tuning Methods for Large Language Models",
            "- Model Compression and Efficient Inference for Large Language Models"
          ],
          "sources": [
            "https://www.runpod.io/articles/guides/ai-model-compression-reducing-model-size-while-maintaining-performance-for-efficient-deployment",
            "https://dl.acm.org/doi/10.1145/3728636",
            "https://arxiv.org/html/2402.09748v1"
          ]
        }
      }
    ]
  },
  {
    "tool_name": "web_search",
    "run_id": "run-web_search-1761050166211",
    "status": "success",
    "started_ms": 1761050166211,
    "ended_ms": 1761050199552,
    "duration_ms": 33341,
    "metadata": {
      "score": 2.4,
      "inputs_keys": [
        "mode",
        "page_size",
        "query",
        "response_format",
        "topic"
      ],
      "tool_state": "HEALTHY",
      "tool_state_label": "healthy",
      "backoff_count": 0,
      "num_sources": 3
    },
    "events": [
      {
        "timestamp_ms": 1761050199552,
        "event_type": "final_result",
        "payload": {
          "summary": [
            "- SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models",
            "- Benchmarking Post-Training Quantization in LLMs: Comprehensive Taxonomy, Unified Evaluation, and Comparative Analysis",
            "- A Comprehensive Survey of Compression Algorithms for Language Models"
          ],
          "sources": [
            "https://proceedings.mlr.press/v202/xiao23c/xiao23c.pdf",
            "https://arxiv.org/abs/2502.13178",
            "https://arxiv.org/abs/2401.15347"
          ]
        }
      }
    ]
  },
  {
    "tool_name": "legacy_arxiv_search",
    "run_id": "run-legacy_arxiv_search-1761050166120",
    "status": "success",
    "started_ms": 1761050166120,
    "ended_ms": 1761050166210,
    "duration_ms": 90,
    "metadata": {
      "score": 0.5,
      "inputs_keys": [
        "limit",
        "query"
      ],
      "tool_state": "HEALTHY",
      "tool_state_label": "healthy",
      "backoff_count": 0,
      "num_sources": 3
    },
    "events": [
      {
        "timestamp_ms": 1761050166210,
        "event_type": "final_result",
        "payload": {
          "summary": [
            "- On the Impact of Calibration Data in Post-training Quantization and Pruning",
            "- LLM-QAT: Data-Free Quantization Aware Training for Large Language Models",
            "- EAQuant: Enhancing Post-Training Quantization for MoE Models via Expert-Aware Optimization"
          ],
          "sources": [
            "http://arxiv.org/abs/2311.09755v2",
            "http://arxiv.org/abs/2305.17888v1",
            "http://arxiv.org/abs/2506.13329v2"
          ]
        }
      }
    ]
  },
  {
    "tool_name": "legacy_arxiv_search",
    "run_id": "run-legacy_arxiv_search-1761050165398",
    "status": "success",
    "started_ms": 1761050165398,
    "ended_ms": 1761050166119,
    "duration_ms": 721,
    "metadata": {
      "score": 0.5,
      "inputs_keys": [
        "limit",
        "query"
      ],
      "tool_state": "HEALTHY",
      "tool_state_label": "healthy",
      "backoff_count": 0,
      "num_sources": 3
    },
    "events": [
      {
        "timestamp_ms": 1761050166119,
        "event_type": "final_result",
        "payload": {
          "summary": [
            "- On the Impact of Calibration Data in Post-training Quantization and Pruning",
            "- LLM-QAT: Data-Free Quantization Aware Training for Large Language Models",
            "- EAQuant: Enhancing Post-Training Quantization for MoE Models via Expert-Aware Optimization"
          ],
          "sources": [
            "http://arxiv.org/abs/2311.09755v2",
            "http://arxiv.org/abs/2305.17888v1",
            "http://arxiv.org/abs/2506.13329v2"
          ]
        }
      }
    ]
  },
  {
    "tool_name": "web_search",
    "run_id": "run-web_search-1761050113294",
    "status": "success",
    "started_ms": 1761050113294,
    "ended_ms": 1761050134783,
    "duration_ms": 21489,
    "metadata": {
      "score": 3.1999999999999997,
      "inputs_keys": [
        "mode",
        "page_size",
        "query",
        "response_format",
        "topic"
      ],
      "tool_state": "HEALTHY",
      "tool_state_label": "healthy",
      "backoff_count": 0,
      "num_sources": 3
    },
    "events": [
      {
        "timestamp_ms": 1761050134783,
        "event_type": "final_result",
        "payload": {
          "summary": [
            "- A Survey on Model Compression for Large Language Models",
            "- Memory-Efficient Double Compression for Large Language Models",
            "- A Survey of Model Compression Techniques: Past, Present, and Future"
          ],
          "sources": [
            "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00704/125482/A-Survey-on-Model-Compression-for-Large-Language",
            "https://aclanthology.org/2024.findings-emnlp.988.pdf",
            "https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2025.1518965/full"
          ]
        }
      }
    ]
  },
  {
    "tool_name": "legacy_arxiv_search",
    "run_id": "run-legacy_arxiv_search-1761050113210",
    "status": "success",
    "started_ms": 1761050113210,
    "ended_ms": 1761050113294,
    "duration_ms": 84,
    "metadata": {
      "score": 0.5,
      "inputs_keys": [
        "limit",
        "query"
      ],
      "tool_state": "HEALTHY",
      "tool_state_label": "healthy",
      "backoff_count": 0,
      "num_sources": 3
    },
    "events": [
      {
        "timestamp_ms": 1761050113294,
        "event_type": "final_result",
        "payload": {
          "summary": [
            "- AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration",
            "- LLM-QAT: Data-Free Quantization Aware Training for Large Language Models",
            "- Empirical Evaluation of Post-Training Quantization Methods for Language Tasks"
          ],
          "sources": [
            "http://arxiv.org/abs/2306.00978v5",
            "http://arxiv.org/abs/2305.17888v1",
            "http://arxiv.org/abs/2210.16621v1"
          ]
        }
      }
    ]
  },
  {
    "tool_name": "legacy_arxiv_search",
    "run_id": "run-legacy_arxiv_search-1761050112430",
    "status": "success",
    "started_ms": 1761050112430,
    "ended_ms": 1761050113209,
    "duration_ms": 779,
    "metadata": {
      "score": 0.5,
      "inputs_keys": [
        "limit",
        "query"
      ],
      "tool_state": "HEALTHY",
      "tool_state_label": "healthy",
      "backoff_count": 0,
      "num_sources": 3
    },
    "events": [
      {
        "timestamp_ms": 1761050113209,
        "event_type": "final_result",
        "payload": {
          "summary": [
            "- AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration",
            "- LLM-QAT: Data-Free Quantization Aware Training for Large Language Models",
            "- Empirical Evaluation of Post-Training Quantization Methods for Language Tasks"
          ],
          "sources": [
            "http://arxiv.org/abs/2306.00978v5",
            "http://arxiv.org/abs/2305.17888v1",
            "http://arxiv.org/abs/2210.16621v1"
          ]
        }
      }
    ]
  },
  {
    "tool_name": "web_search",
    "run_id": "run-web_search-1761050020312",
    "status": "success",
    "started_ms": 1761050020312,
    "ended_ms": 1761050050442,
    "duration_ms": 30130,
    "metadata": {
      "score": 3.0,
      "inputs_keys": [
        "mode",
        "page_size",
        "query",
        "response_format",
        "topic"
      ],
      "tool_state": "HEALTHY",
      "tool_state_label": "healthy",
      "backoff_count": 0,
      "num_sources": 3
    },
    "events": [
      {
        "timestamp_ms": 1761050050442,
        "event_type": "final_result",
        "payload": {
          "summary": [
            "- A Survey on Model Compression for Large Language Models",
            "- Neural Weight Compression for Language Models",
            "- Compression Laws for Large Language Models"
          ],
          "sources": [
            "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00704/125482/A-Survey-on-Model-Compression-for-Large-Language",
            "https://arxiv.org/html/2510.11234v1",
            "https://www.arxiv.org/pdf/2504.04342"
          ]
        }
      }
    ]
  },
  {
    "tool_name": "legacy_arxiv_search",
    "run_id": "run-legacy_arxiv_search-1761050020221",
    "status": "success",
    "started_ms": 1761050020221,
    "ended_ms": 1761050020312,
    "duration_ms": 91,
    "metadata": {
      "score": 0.5,
      "inputs_keys": [
        "limit",
        "query"
      ],
      "tool_state": "HEALTHY",
      "tool_state_label": "healthy",
      "backoff_count": 0,
      "num_sources": 3
    },
    "events": [
      {
        "timestamp_ms": 1761050020312,
        "event_type": "final_result",
        "payload": {
          "summary": [
            "- XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer Compression",
            "- Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression",
            "- Aggressive Post-Training Compression on Extremely Large Language Models"
          ],
          "sources": [
            "http://arxiv.org/abs/2510.11236v1",
            "http://arxiv.org/abs/2407.04965v3",
            "http://arxiv.org/abs/2409.20094v1"
          ]
        }
      }
    ]
  },
  {
    "tool_name": "legacy_arxiv_search",
    "run_id": "run-legacy_arxiv_search-1761050019482",
    "status": "success",
    "started_ms": 1761050019482,
    "ended_ms": 1761050020220,
    "duration_ms": 738,
    "metadata": {
      "score": 0.5,
      "inputs_keys": [
        "limit",
        "query"
      ],
      "tool_state": "HEALTHY",
      "tool_state_label": "healthy",
      "backoff_count": 0,
      "num_sources": 3
    },
    "events": [
      {
        "timestamp_ms": 1761050020220,
        "event_type": "final_result",
        "payload": {
          "summary": [
            "- XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer Compression",
            "- Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression",
            "- Aggressive Post-Training Compression on Extremely Large Language Models"
          ],
          "sources": [
            "http://arxiv.org/abs/2510.11236v1",
            "http://arxiv.org/abs/2407.04965v3",
            "http://arxiv.org/abs/2409.20094v1"
          ]
        }
      }
    ]
  }
]