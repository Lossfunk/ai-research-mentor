1. Problem framing and goals
- Objective: Build and rigorously ablate federated reinforcement learning (FRL) for smart-grid control with explicit privacy guarantees and reliability/robustness under realistic non-IID clients, partial participation, and adversaries. Target two representative tasks:
  - Transmission operations (Grid2Op/L2RPN): topology reconfiguration and redispatch to survive contingencies, minimize curtailment, and respect safety limits [4]. Key metrics include timesteps survived, cumulative reward, line disconnections, and rule violations [4][5].
  - Distribution/DER coordination (CityLearn): building energy management and demand response to reduce cost/emissions while meeting comfort; metrics include energy cost, emissions, ramping/peak and comfort violations [6].
- Privacy motivation: Even when combining differential privacy (DP) and secure aggregation, model updates can leak information; recent work reconstructs individual data points from FL under DP+secure aggregation, motivating empirical privacy audits rather than assuming safety by design [P1].
- Reliability motivation: Federated training is vulnerable to malicious clients and non-IID drift; robust aggregation and asynchronous schemes are needed to sustain performance under partial participation, stragglers, and adversaries [P6][3].
- Deliverables at 9 months:
  1) A reproducible FRL benchmark suite for Grid2Op and CityLearn with privacy and robustness test harnesses.
  2) A comprehensive ablation of algorithmic choices (sync vs async, aggregation, personalization), privacy knobs (epsilon, clipping, secure aggregation), and robustness defenses (Byzantine-robust aggregators).
  3) Quantified privacy-utility and reliability-utility frontiers (e.g., epsilon vs timesteps survived; adversary fraction vs load not served).
  4) Open-source code and a draft paper with statistically robust results and privacy accounting.

2. Experiments
All experiments report mean±95% CI over ≥5 seeds per setting and include significance tests (paired t-test with Holm–Bonferroni). Unless noted: 20 clients; 20% participation per round; 200 rounds; actor-critic (PPO/A2C) with clipped policy gradient; shared policy/value across clients; FedAvg baseline. Grid2Op: l2rpn_case14_sandbox or equivalent; CityLearn: ≥10-building scenario with diverse schedules.

E1. FRL baselines vs centralized and local
- Hypothesis: Federated actor-critic (FedAvg) achieves within 5–10% of centralized RL performance on Grid2Op and CityLearn under mild non-IID, and outperforms purely local RL on average client utility.
- Setup: Train centralized RL (pooled data), local-only RL per client, and FRL (FedAvg). Non-IID generated by stratifying load/weather profiles (Dirichlet α∈{0.1, 0.5, 1.0}). 1000 episodes/client total budget.
- Baselines: Centralized PPO; Local PPO; FRL: FedAvg + PPO; Personalization via FedAvg+local fine-tune (last 10% rounds).
- Metrics: Grid2Op timesteps survived, curtailment MWh, rule violations [4][5]; CityLearn cost, emissions, ramping, comfort [6]; sample efficiency (return vs environment steps).
- Expected: FRL ≈ centralized within 5–10% at α≥0.5; FRL > local by ≥10% on average. Personalization boosts worst-client metrics under strong heterogeneity [2][1].

E2. Communication/aggregation ablation (sync vs async)
- Hypothesis: Buffered asynchronous aggregation (FedBuff) preserves ≥90% of synchronous FRL utility under 10–30% participation with 2–4× lower wall-clock, and is less sensitive to stragglers [P6].
- Setup: Compare Sync FedAvg (all selected clients per round) vs FedBuff (buffer size b∈{10, 20}, staleness cap S∈{5, 10}). Inject stragglers (Pareto-distributed delays).
- Baselines: FedAvg, FedProx (μ∈{0.001, 0.01}), FedOpt (Yogi/Adam).
- Metrics: Same as E1 + wall-clock to target return; staleness vs return curves; communication rounds to convergence.
- Expected: FedBuff reaches target return with fewer synchronization stalls; small degradation under extreme staleness [P6].

E3. Non-IID heterogeneity stress test
- Hypothesis: Client-drift mitigations (FedProx, proximal clipping) stabilize FRL under strong non-IID (α=0.1), improving worst-client outcomes by ≥10% vs FedAvg.
- Setup: Dirichlet non-IID on exogenous drivers; personalization (head finetuning) vs global model; local steps E∈{1, 5, 10}.
- Baselines: FedAvg, FedProx, FedAvgM; personalization layer vs none.
- Metrics: Mean/worst-client metrics; fairness (90th–10th percentile gap); instability (policy oscillation rate).
- Expected: FedProx reduces oscillation and fairness gap at E≥5.

E4. Privacy: DP and secure aggregation ablation
- Hypothesis: With per-step clipping c∈{0.1, 0.5, 1.0} and noise σ chosen for ε∈{2, 4, 8} (Rényi DP accounting), FRL retains ≥80% of non-DP performance on CityLearn, ≥70% on Grid2Op; secure aggregation adds negligible utility loss but mitigates server-side inference risk [P1][P8].
- Setup: Implement DP-SGD on policy/value gradients with global clipping; compare DP-FTRL with BLTs accountant (practical private training) adapted for policy gradients [P8]. Secure aggregation (TFF/PySyft) on updates.
- Baselines: No-DP; DP-SGD; DP-FTRL (BLT).
- Metrics: Utility (E1 metrics); privacy (ε, δ), empirical leakage via membership inference and reconstruction: adapt update-inversion attack to policy/value gradients, following the reconstruction methodology indicating leakage despite DP+secure aggregation [P1].
- Expected: Monotonic privacy-utility trade-off; BLTs accountant yields improved utility at same ε vs naive tree aggregation [P8]. Reconstruction success decreases with lower ε; if attacks succeed under DP, document conditions [P1].

E5. Robustness to Byzantine clients (poisoning/Byzantine-resilient aggregation)
- Hypothesis: Under 10–30% malicious clients performing update-scaling/sign-flip or targeted backdoors, robust aggregators (median/trimmed mean/Krum) and inversion-based defenses (FedInv) reduce performance degradation by ≥50% vs FedAvg, but sophisticated adaptive attacks remain effective, consistent with prior findings [3].
- Setup: Attack types: (a) sign-flip/scale; (b) Gaussian noise; (c) backdoor via reward shaping to trigger unsafe switching; (d) drift injection. Defenses: coordinate-wise median, trimmed mean (20%), Krum, FedInv [3].
- Baselines: FedAvg; robust variants.
- Metrics: Utility; safety incidents per episode; backdoor success rate; gradient anomaly scores; convergence stability.
- Expected: Median/trimmed mean robust to simple attacks; Krum brittle under non-IID; adaptive attacks can bypass some defenses as shown in prior FL work [3].

E6. Reliability and safety layer ablation
- Hypothesis: Adding a safety layer (action projection using power-flow constraints) reduces rule violations by ≥80% with ≤10% utility loss on Grid2Op; improves survival under N-1 contingencies [4][5].
- Setup: Compare pure policy vs policy+action projection (OPF-feasible projection or rule-based safety filter). Evaluate contingency scenarios and fault injections (line outages, load spikes).
- Baselines: Policy-only; safety layer variants.
- Metrics: Violations, disconnections, curtailment, timesteps survived [4][5].
- Expected: Strong violation reduction; minor performance drop.

E7. Partial participation and client sampling
- Hypothesis: Sampling 10–20% clients per round with weighted aggregation retains ≥90% utility vs full participation, with DP accounting adjusted for sampling amplification [P6][P8].
- Setup: Vary participation p∈{0.1, 0.2, 0.5, 1.0}; re-account DP with sampling; compare sync vs FedBuff.
- Metrics: Utility; ε for fixed σ; convergence speed.
- Expected: Utility robust down to p≈0.2; async helps at small p [P6][P8].

E8. Personalization and fine-tuning under privacy
- Hypothesis: Post-FRL local fine-tuning under DP (smaller σ, tighter c) improves worst-client performance by ≥5–10% without raising global ε substantially (account locally).
- Setup: 10% rounds local-only fine-tuning; per-client ε tracking.
- Baselines: No personalization; non-private personalization.
- Metrics: Worst-client metrics; per-client ε; fairness.
- Expected: Gains concentrated in hardest clients; privacy budget manageable with few local steps.

E9. End-to-end privacy- and robustness-aware FRL recipe
- Hypothesis: Combining FedBuff + DP-FTRL (ε≈4) + trimmed mean + safety layer yields the best privacy-reliability-utility balance.
- Setup: Grid2Op and CityLearn; sweep ε∈{2,4,8}, malicious rate m∈{0,0.1,0.2,0.3}.
- Metrics: Utility, violations, ε, adversary degradation.
- Expected: “Knee” at ε≈4; robust agg ≥50% attack mitigation [P6][P8][3].

3. Timeline for the next 9 months with milestones
- Month 1
  - Stand up simulators and baselines: Grid2Op (l2rpn_case14), CityLearn multi-building; centralized and local PPO baselines.
  - Implement federated scaffolding (Flower or FedML) with synchronous FedAvg + PPO.
  - Milestone: Reproduce centralized vs local vs FRL gap on at least one environment (E1).
- Month 2
  - Non-IID data pipeline and personalization hooks; add FedProx/FedOpt; run E2/E3 small-scale sweeps.
  - Milestone: Heterogeneity sensitivity curves (α vs performance).
- Month 3
  - Integrate FedBuff asynchronous pipeline; straggler emulation; partial participation; run E2/E7 full.
  - Milestone: Async vs sync wall-clock and utility comparison.
- Month 4
  - Add DP training: clipping, Opacus/TF-Privacy, Rényi DP accountant; secure aggregation prototype (TFF/PySyft).
  - Implement privacy audits (membership inference, update inversion adapted to policy/value gradients).
  - Milestone: Privacy-utility curves for ε∈{2,4,8} on CityLearn (E4).
- Month 5
  - Extend DP experiments to Grid2Op; evaluate secure aggregation overhead and correctness.
  - Milestone: Privacy results on both environments; reproducible DP accounting logs.
- Month 6
  - Byzantine attack suite (sign-flip, scaling, Gaussian, backdoor); robust aggregators (median, trimmed mean, Krum) and FedInv.
  - Milestone: Attack/defense matrix with degradation and mitigation rates (E5).
- Month 7
  - Safety layer/action projection integration; contingency testbed (line outages, load spikes).
  - Milestone: Reliability metrics improvement with safety layer (E6).
- Month 8
  - Compose best-of: FedBuff + DP (ε≈4) + robust aggregation + safety; personalization under DP.
  - Milestone: End-to-end recipe and ablation summary (E8/E9).
- Month 9
  - Robustness/privacy stress tests, ablations cleanup, statistical validation, prepare artifacts (code, configs, seeds, scripts) and paper draft.
  - Milestone: Submission-ready results with full reproducibility package.

4. Resources (compute, tools, datasets)
- Compute
  - Simulation-heavy workloads are CPU-bound; GPU accelerates policy nets. Recommended: 1–2 nodes with 32–64 vCPU and 1–2 GPUs (A100 or V100) each; 128–256 GB RAM total. For 20–50 clients simulated, target ~1e7–3e7 environment steps across 9 months.
  - Storage: ~200 GB for logs, checkpoints, traces. Networking sufficient for intra-node FL.
- Tools
  - FL/FRL: Flower or FedML (FL orchestration), Ray RLlib or CleanRL/Stable-Baselines3 (RL), Opacus or TensorFlow Privacy (DP), TFF/PySyft or OpenFL (secure aggregation prototypes).
  - Robustness: Implement median/trimmed mean/Krum; integrate FedInv code if available [3]. Async: FedBuff-like buffered server [P6].
  - Evaluation: Grid2Op, CityLearn; power-flow backends; DP accounting (RDP, BLTs) [P8].
- Datasets/environments
  - Grid2Op + L2RPN cases with provided scenarios/contingencies and metrics [4][5].
  - CityLearn benchmark datasets and metrics [6].
  - Optionally synthetic non-IID schedules via weather/load generators.

5. Risks and mitigations
- DP degrades utility at low ε
  - Mitigation: Use adaptive clipping, per-layer noise, and DP-FTRL with BLTs accountant to improve utility at fixed ε; tune ε≈4 as “knee” [P8]. Report privacy-utility frontier rather than a single point.
- Reconstruction/leakage persists despite DP+secure aggregation
  - Mitigation: Empirically audit with inversion/membership attacks; reduce ε, increase clipping; consider client-side privacy filtering; document limits clearly [P1].
- Robust aggregators fail under non-IID or adaptive attacks
  - Mitigation: Combine robust aggregation with anomaly detection and client weighting; evaluate multiple defenses (median/trimmed mean/Krum/FedInv); simulate adaptive attackers and report failures [3].
- Async staleness harms convergence
  - Mitigation: Bound staleness; buffer-size tuning; server-side momentum/variance reduction; use FedProx in async [P6].
- Safety violations in Grid2Op despite improved reward
  - Mitigation: Enforce safety layer/action projection; add constraint penalties; evaluate under N-1 contingencies and report violations [4][5].
- Compute/runtime overrun
  - Mitigation: Progressive scaling from 10→20→50 clients; early stopping; curriculum scenarios; budget episodes per round; profile simulators.
- Reproducibility gaps
  - Mitigation: Fix seeds, log configs, pin versions; release scripts and exact checkpoints; CI to rerun key tests.
- Simulator-to-reality gap
  - Mitigation: Use multiple environments (Grid2Op + CityLearn); stress-test with realistic disturbances; collaborate for limited historical validation if possible.

6. Stretch ideas or follow-up directions
- Personalized FRL: Hypernetwork-based personalization or meta-learning to accommodate diverse clients (e.g., buildings) while sharing a core policy; evaluate fairness and worst-client outcomes.
- Hierarchical FRL: Two-level control (transmission-distribution); local RL at feeders coordinated via federated critic updates.
- Safety-first FRL: Constrained RL (primal–dual) with explicit reliability constraints; certified safe action shields informed by AC power flow.
- Communication-efficient privacy: Event-triggered updates, sparsification, and secure aggregation; adaptive participation to exploit DP’s sampling amplification [P8].
- Task-driven robustness: Adversarial training with policy-gradient regularizers; client-level adversary detection via update attribution.

Limitations and how to gather better references
- Direct, large-scale studies of FRL specifically in power grids are sparse; most privacy/robustness results come from general FL and supervised settings [1][2]. We mitigate by:
  - Adapting known FL attacks/defenses to policy gradients and reporting empirical gaps.
  - Grounding reliability metrics in Grid2Op and CityLearn official documentation [4][5][6].
  - Running replicable, simulator-based evaluations that can later be mapped to real data.

Optional next step
- Spin up a minimal FRL + DP prototype on CityLearn with 10 clients and ε∈{4,8} to get an early privacy-utility curve within two weeks, de-risking Months 4–5.

Citations
- [P1] Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation — http://arxiv.org/abs/2301.04017v2
- [P6] Federated Learning with Buffered Asynchronous Aggregation — http://arxiv.org/abs/2106.06639v4
- [P8] A Hassle-free Algorithm for Private Learning in Practice: Don't Use Tree Aggregation, Use BLTs — http://arxiv.org/abs/2408.08868v3
- [1] Federated Learning for Smart Grid: A Survey on Applications and ... — https://arxiv.org/pdf/2409.10764
- [2] Federated Reinforcement Learning: Techniques, Applications, and Open Challenges — https://arxiv.org/abs/2108.11887
- [3] Local Model Poisoning Attacks to Byzantine-Robust Federated Learning (USENIX Security 2020) — https://www.usenix.org/conference/usenixsecurity20/presentation/fang
- [4] Grid2Op documentation — https://grid2op.readthedocs.io/
- [5] Grid2Op utility/metrics — https://grid2op.readthedocs.io/en/latest/user/utils.html
- [6] CityLearn References (metrics and benchmarks) — https://www.citylearn.net/references.html