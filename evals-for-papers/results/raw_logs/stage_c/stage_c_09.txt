Stage: B → C (shaping a concrete plan and evaluation protocol)

Intuition: Interpretability claims only matter if they’re faithful to model internals and decision use, not just visually plausible. For autonomous driving, robust evaluation requires pressure-testing explanations under perturbations (e.g., point or pixel dropping), sanity checks, and counterfactuals while tying results back to safety governance (model/data cards and risk checkpoints). Combining 2D detection, segmentation, and 3D lidar models gives a comprehensive picture of where interpretability helps— and where it fails—across sensing modalities. 

Why this is principled: Saliency methods can pass visual sniff tests while failing causal faithfulness; parameter randomization and data perturbation tests expose these failures [Adebayo et al., “Sanity Checks for Saliency Maps”][1]. Quantitative faithfulness metrics (infidelity/sensitivity) assess whether attributions align with model behavior under input perturbations [2]. Black-box evaluation for detectors (D-RISE) enables method comparisons when gradients are unreliable [3]. For 3D, point-dropping saliency validates whether attributions match performance impact [4]. Governance artifacts (Model Cards, Datasheets) and the NIST AI RMF ensure evidence is translated into risk-aware practice [9][10][11].

1) Problem framing and goals
- Scope: Build a one-year, publication-grade benchmark and governance workflow for evaluating interpretability tools on autonomous driving perception models across three tasks: (a) 2D object detection, (b) semantic segmentation, and (c) 3D lidar detection. 
- Methods covered: Gradient-based (Grad-CAM variants), attribution (Integrated Gradients, SmoothGrad), black-box saliency for detectors (D-RISE) [3], concept-based (TCAV) [5], 3D saliency (point-dropping) [4], and object-aware counterfactuals (OCTET) [P1].
- Core goals:
  - Faithfulness: Do explanations causally track model behavior? Use infidelity/sensitivity, deletion/insertion curves, and retraining stress tests [2][12].
  - Reliability: Do explanations survive sanity checks and are they robust to distributional shifts (weather, time of day, occlusions)? [1]
  - Task relevance: Do method rankings transfer across 2D/3D/fusion baselines (e.g., CenterPoint, BEVFusion)? [6][8]
  - Governance: Produce Model Cards and Datasheets, and run NIST AI RMF-aligned checkpoints at each milestone [9][10][11].

2) Experiments
Each experiment includes hypothesis, setup/baselines, evaluation metrics, and expected outcomes, with follow-up variations.

E1. Saliency sanity checks across modalities
- Hypothesis: Methods that truly reflect model internals will fail (change drastically) under weight randomization or label shuffling; methods that remain visually similar are unfaithful [1].
- Setup: Models: 2D detector (e.g., YOLO/RetinaNet), segmentation (Mask2Former or DeepLab), 3D detector (CenterPoint) on nuScenes and KITTI; run Grad-CAM/IG/SmoothGrad/GuidedBP and D-RISE for detectors [3][6]. Apply parameter randomization and data-label randomization (Adebayo tests) [1].
- Baselines: Random and uniform attribution, edge detectors.
- Metrics: Spearman/SSIM similarity of maps before/after randomization; change in explanation rank-order per pixel/point; pass/fail rate of sanity checks [1].
- Expected outcomes: Faithful methods degrade under randomization; any near-invariance flags method unreliability. If a method passes for segmentation but fails for 3D, it motivates modality-specific guidance [1][4].
- Follow-ups: Ablate normalization and smoothing; compare white-box vs black-box detectors (Grad-CAM vs D-RISE) [3].

E2. Faithfulness via infidelity/sensitivity and deletion/insertion
- Hypothesis: Methods with lower infidelity and higher sensitivity better capture causal importance [2]; deletion/insertion curves will correlate with infidelity metrics.
- Setup: For each model/task, compute infidelity/sensitivity (Captum implementations for IG/Grad-CAM), and deletion/insertion AUC (progressive removal/insertion of top-attributed pixels/points). Include BEVFusion to test multi-sensor fusion [8].
- Baselines: Random attribution; gradient magnitude.
- Metrics: Infidelity (lower better), sensitivity (higher better), deletion AUC (lower better), insertion AUC (higher better) [2].
- Expected outcomes: Convergent ranking (e.g., IG > Grad-CAM variants for segmentation), but detectors may favor D-RISE due to proposal-specific dynamics [3]. Divergences indicate metric-task mismatch.
- Follow-ups: Robustness under noise/weather stratification; per-class analysis (pedestrian vs vehicle) on nuScenes [5][7].

E3. Black-box explanation for detectors (D-RISE) vs gradient methods
- Hypothesis: For object detectors, black-box D-RISE saliency better predicts changes in detection scores under mask perturbations than gradient-based methods [3].
- Setup: 2D detectors (RetinaNet, YOLOv5/7) and 3D detectors (CenterPoint) if extended with point-masking analog; compute D-RISE maps and Grad-CAM. Apply targeted mask perturbations to top-K salient regions and measure score/IoU drop [3][6].
- Baselines: Random region masks; uniform masks.
- Metrics: Δscore/ΔmAP per percent area/points removed; Kendall τ between attribution rank and score drop [3].
- Expected outcomes: D-RISE exhibits stronger correlation between attribution and observed detector degradation than Grad-CAM (2D); mixed outcomes in 3D encourage specialized point-saliency approaches [3][4].
- Follow-ups: Proposal-aware ablations and cross-sensor masking in BEVFusion (image vs lidar contributions) [8].

E4. 3D point-cloud saliency and point-dropping stress tests
- Hypothesis: Point-saliency maps that the model depends on will show large performance drops when top-salient points are removed; non-salient removal will minimally affect metrics [4].
- Setup: CenterPoint on nuScenes and Waymo Open; compute saliency via gradients or learned point-importance; progressively drop top-N% points globally and within instance boxes [4][7].
- Baselines: Random point dropping; geometric heuristics (e.g., distance-based).
- Metrics: mAP/NDS degradation trajectories vs %points removed; area under degradation curve [7].
- Expected outcomes: Faithful 3D saliency exhibits steep degradation after a small fraction of salient points are removed; if degradation equals random, attribution is uninformative [4][7].
- Follow-ups: Occlusion-specific tests (simulate LiDAR shadowing), sensor dropout to test BEVFusion attribution consistency [8].

E5. Concept-based explanations (TCAV) for driving factors
- Hypothesis: TCAV scores for concepts (e.g., “night,” “rain,” “pedestrian clothing color”) will correlate with performance stratification; spurious concepts will not [5].
- Setup: Curate concept datasets from nuScenes metadata (time-of-day, weather) and synthetic overlays; compute TCAV for segmentation and detection heads [5][7].
- Baselines: Random concept vectors; non-semantic textures.
- Metrics: TCAV directional derivatives; correlation with per-slice performance (mAP/NDS) across weather/time; statistically test via permutation [5][7].
- Expected outcomes: Legitimate concepts (night, rain) show significant TCAV and align with performance drops; if not, either concepts are ill-posed or model is robust to that factor [5][7].
- Follow-ups: Intervene via counterfactuals (style transfer/lighting augmentation) to validate causal effect; compare with OCTET object-aware counterfactuals [P1].

E6. Object-aware counterfactuals for detectors (OCTET)
- Hypothesis: Object-aware counterfactual edits that minimally alter the scene but flip detections give more faithful rationales than unconstrained perturbations [P1].
- Setup: Implement or reproduce OCTET on 2D detectors (YOLO/RetinaNet) and extend to 3D via rendered point insertions/removals; measure success rate and minimality [P1].
- Baselines: Patch-based counterfactuals; unconditional diffusion edits.
- Metrics: Counterfactual success rate (target class score flip), edit distance (L2/LPIPS/point-count), realism checks; downstream effect on mAP/NDS [P1].
- Expected outcomes: OCTET achieves higher success with smaller, object-localized edits than baselines; failures suggest missing object priors or over-regularization [P1].
- Follow-ups: Video counterfactuals (e.g., LD‑ViCE) to test temporal consistency on short clips [P2] (note: video counterfactual methods are evolving rapidly; verify maturity before committing).

3) Timeline for the next 12 months with milestones (first 6 months detailed)
- Phase 0 (Weeks 1–6) — Gates before scaling
  - Deliverables: (1) Prediction log with ≥14 entries across models and explainers; (2) One reproduced figure/metric (infidelity or D-RISE Δscore curve) within ≤10% of reported numbers; (3) An experiment card and one ablation or negative result with post-mortem.
  - Governance: Draft Dataset Card (nuScenes/KITTI datasets used) and Model Card templates; initialize NIST AI RMF “Map” and “Measure” activities [9][10][11].
- Month 2
  - Complete E1 sanity checks for segmentation and 2D detection on a 5k image slice; pre-register metrics and stop rules.
  - Milestone: Sanity check report; go/no-go on method set (drop unfaithful).
- Month 3
  - Run E2 infidelity/deletion-insertion on 2D and segmentation; begin E3 D-RISE on detectors [2][3].
  - Governance checkpoint: First risk review (data slices, failure taxonomies), update Model/Dataset Cards [9][10].
- Month 4
  - Extend to 3D: run E4 point-dropping saliency on CenterPoint (nuScenes); start E5 TCAV concept curation [4][6][7].
  - Milestone: 3D saliency stress-test results with ablation curves; interim paper draft figures.
- Month 5
  - Complete E3 and E5; start E6 OCTET reproduction and pilot counterfactuals [P1].
  - Governance checkpoint: NIST RMF “Manage”: define model change policy and monitoring KPIs [11].
- Month 6
  - Integrate cross-modality comparison; write up evaluation protocol and results for submission-ready draft sections.
  - Milestone: Internal preprint + artifact repository; externalized Model/Dataset Cards v1 [9][10].
- Months 7–9
  - Scale to BEVFusion; cross-sensor attribution and dropout tests [8]. Add Waymo Open for 3D generalization [7]. Begin video counterfactual pilot on short sequences [P2].
  - Governance: Perform shift and impact assessment (night/rain) and bias audit slices; update RMF “Measure/Manage” artifacts [11].
- Months 10–12
  - Consolidate benchmark: release code, standardized metrics, and governance templates; run ROAR-style remove-and-retrain stress tests where feasible (note caveats on DPI and compute) [12].
  - Submission and camera-ready; plan industry-facing report on governance-ready interpretability workflows.

4) Resources (compute, tools, datasets)
- Compute
  - Recommended: 4×A100 40–80GB or 8×A6000; storage 5–10 TB; mixed precision to control costs. For ROAR retraining, budget additional GPU-weeks [12].
- Tools
  - Modeling: MMDetection/MMDetection3D; OpenPCDet; BEVFusion repo [8].
  - Explainability: Captum (IG/infidelity/sensitivity) [2]; Grad-CAM libs; D-RISE implementation [3]; TCAV code [5]; Open3D for point ops; evaluation harness for deletion/insertion.
  - Governance: Model Card and Datasheet templates [9][10]; NIST AI RMF worksheets [11].
- Datasets and benchmarks
  - nuScenes (multimodal; metrics mAP, NDS) [7]; KITTI for 2D/3D baselines; Waymo Open Dataset for scale and generalization [6][7].

5) Risks and mitigations table
- Method appears faithful but fails sanity checks — Require E1 before any downstream claims; drop methods that fail [1].
- Metrics disagree (infidelity vs deletion/insertion) — Report multi-metric results; analyze disagreements per class/slice; pre-register tie-breakers [2].
- Detector-specific artifacts (proposal dependence) — Use D-RISE for detectors; treat Grad-CAM results as exploratory [3].
- 3D saliency unstable under sparsity — Use instance-local dropping and stratify by range; aggregate over seeds [4].
- Counterfactuals look realistic but are out-of-distribution — Enforce realism constraints and quantify minimality (LPIPS/point-count); human-in-the-loop screening [P1].
- Retraining compute for ROAR too high — Use scaled subsets and report DPI caveats; prioritize deletion/insertion and infidelity; document limits [12].
- Governance drift (missed documentation) — Calendarized checkpoints tied to milestones; RMF “Govern/Manage” ownership and sign-off [11].
- Dataset bias or slice undercoverage — Use Datasheets to document coverage; add targeted test slices (night/rain/rare classes) [10][7].

6) Stretch ideas or follow-up directions
- Video counterfactuals for temporal faithfulness (e.g., LD‑ViCE—verify maturity; methods are evolving) [P2].
- Closed-loop simulation: Drive scenario generation where attribution guides scene edits; measure planner safety metrics downstream.
- Concept bottleneck or intermediate factor attribution in BEV space (lane boundaries, drivable area).
- Human-grounded evaluation: Labeler studies comparing explanation utility for error triage; couple with time-to-diagnosis metrics.
- Benchmark release: A standardized interpretability-for-AD suite with governance templates (Model Cards, Datasheets, RMF checklists) [9][10][11].

Literature anchors
- Adebayo et al., “Sanity Checks for Saliency Maps” (NeurIPS 2018) — https://arxiv.org/abs/1810.03292 [1]
- Yeh et al., “On the (In)fidelity and Sensitivity of Explanations” (NeurIPS 2019) — https://arxiv.org/abs/1901.09392 [2]
- Petsiuk et al., “Black-box Explanation of Object Detectors via Saliency Maps (D-RISE)” (CVPR 2021) — https://openaccess.thecvf.com/content/CVPR2021/html/Petsiuk_Black-Box_Explanation_of_Object_Detectors_via_Saliency_Maps_CVPR_2021_paper.html [3]
- Zheng et al., “PointCloud Saliency Maps” (ICCV 2019) — https://openaccess.thecvf.com/content_ICCV_2019/papers/Zheng_PointCloud_Saliency_Maps_ICCV_2019_paper.pdf [4]
- Kim et al., “TCAV: Testing with Concept Activation Vectors” (ICML 2018) — https://arxiv.org/abs/1711.11279 [5]
- Yin et al., “CenterPoint” (2021) — https://arxiv.org/abs/2006.11275 [6]
- Caesar et al., “nuScenes” (CVPR 2020) — https://openaccess.thecvf.com/content_CVPR_2020/papers/Caesar_nuScenes_A_Multimodal_Dataset_for_Autonomous_Driving_CVPR_2020_paper.pdf [7]
- Liu et al., “BEVFusion” (2022) — https://ar5iv.labs.arxiv.org/html/2205.13542 [8]
- Mitchell et al., “Model Cards for Model Reporting” (FAccT 2019) — https://arxiv.org/abs/1810.03993 [9]
- Gebru et al., “Datasheets for Datasets” (CACM 2021; arXiv 2018) — https://arxiv.org/abs/1803.09010 [10]
- NIST AI Risk Management Framework 1.0 — https://nvlpubs.nist.gov/nistpubs/ai/nist.ai.100-1.pdf [11]
- ROAR benchmark and caveats — “A Benchmark for Interpretability Methods…” (NeurIPS 2019) + DPI caveats https://papers.neurips.cc/paper/9167-a-benchmark-for-interpretability-methods-in-deep-neural-networks.pdf; DPI critique https://export.arxiv.org/pdf/2304.13836v3.pdf [12]

Notes on evidence coverage: Our mentorship-guidelines tool did not surface domain-specific governance guidance for interpretability in autonomous driving; we therefore rely on general but authoritative frameworks (NIST AI RMF, Model/Dataset Cards) and peer-reviewed method papers for evaluation protocols. If deeper industry-specific standards are needed (e.g., ISO 34502/ISO 21448 SOTIF intersections with ML interpretability), we would run a targeted standards search and engage with OEM documentation.

Optional next step: Spin up Phase 0 on a 5k-image nuScenes slice: reproduce D-RISE Δscore-vs-area curves for a RetinaNet baseline and compute infidelity for IG on a segmentation model, then run parameter-randomization sanity checks; package these as your first Experiment Card and governance artifacts (Dataset/Model Card drafts). 

Citations
[1] Sanity Checks for Saliency Maps — https://arxiv.org/abs/1810.03292
[2] On the (In)fidelity and Sensitivity of Explanations — http://papers.neurips.cc/paper/9278-on-the-infidelity-and-sensitivity-of-explanations
[3] Black-Box Explanation of Object Detectors via Saliency Maps (D-RISE) — https://openaccess.thecvf.com/content/CVPR2021/html/Petsiuk_Black-Box_Explanation_of_Object_Detectors_via_Saliency_Maps_CVPR_2021_paper.html
[4] PointCloud Saliency Maps — https://openaccess.thecvf.com/content_ICCV_2019/papers/Zheng_PointCloud_Saliency_Maps_ICCV_2019_paper.pdf
[P1] OCTET: Object-aware Counterfactual Explanations — http://arxiv.org/abs/2211.12380v2
[P2] LD-ViCE: Latent Diffusion Model for Video Counterfactual Explanations — http://arxiv.org/abs/2509.08422v2
[5] TCAV: Testing with Concept Activation Vectors — https://arxiv.org/abs/1711.11279
[6] Center-based 3D Object Detection and Tracking (CenterPoint) — https://arxiv.org/abs/2006.11275
[7] nuScenes: A Multimodal Dataset for Autonomous Driving — https://openaccess.thecvf.com/content_CVPR_2020/papers/Caesar_nuScenes_A_Multimodal_Dataset_for_Autonomous_Driving_CVPR_2020_paper.pdf
[8] BEVFusion — https://ar5iv.labs.arxiv.org/html/2205.13542
[9] Model Cards for Model Reporting — https://arxiv.org/abs/1810.03993
[10] Datasheets for Datasets — https://arxiv.org/abs/1803.09010
[11] NIST AI Risk Management Framework 1.0 — https://nvlpubs.nist.gov/nistpubs/ai/nist.ai.100-1.pdf
[12] ROAR benchmark and DPI caveats — https://papers.neurips.cc/paper/9167-a-benchmark-for-interpretability-methods-in-deep-neural-networks.pdf; https://export.arxiv.org/pdf/2304.13836v3.pdf