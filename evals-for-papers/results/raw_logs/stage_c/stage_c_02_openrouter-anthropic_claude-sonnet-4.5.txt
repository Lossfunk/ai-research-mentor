## Research Plan: Compressible LLMs (6 Months)

### 1. **Problem Definition & Research Questions**

**Core Objective:** Develop methods to compress large language models while maintaining performance, enabling deployment on resource-constrained devices.

**Key Research Questions:**
- What compression techniques (pruning, quantization, distillation, low-rank factorization) provide the best accuracy-efficiency tradeoffs?
- Can we identify and preserve "critical" parameters/structures during compression?
- How does compression affect different capabilities (reasoning, factual recall, instruction-following)?
- Can compression be task-adaptive or learned end-to-end?

**Target Metrics:**
- Model size reduction (parameters, memory footprint)
- Inference speedup (latency, throughput)
- Task performance retention (perplexity, downstream task accuracy)
- Energy consumption

---

### 2. **Core Experimental Track**

#### **Month 1: Baseline Establishment & Infrastructure**

**Experiment 1: Compression method benchmarking**
- **Hypothesis:** Different compression methods have complementary strengths across model sizes and tasks
- **Method:** Apply standard techniques to a base model (e.g., LLaMA-7B or Mistral-7B):
  - **Magnitude pruning:** 30%, 50%, 70% sparsity (unstructured and structured)
  - **Post-training quantization:** INT8, INT4, mixed-precision
  - **Knowledge distillation:** Distill to 50% width/depth variants
  - **Low-rank decomposition:** SVD on weight matrices with rank reduction
- **Evaluation:** 
  - Perplexity on WikiText-103, C4
  - Zero-shot on MMLU, HellaSwag, ARC
  - Inference latency on CPU/GPU
- **Expected outcome:** Establish Pareto frontier of compression-performance tradeoffs; identify which methods excel at different compression ratios

**Deliverables:**
- Reproducible benchmark suite
- Baseline performance table across 8-10 compression configurations

---

#### **Month 2: Structured Pruning & Importance Scoring**

**Experiment 2: Attention head and FFN neuron importance**
- **Hypothesis:** Not all attention heads and FFN neurons contribute equally; targeted pruning outperforms random/magnitude-based
- **Method:**
  - Compute importance scores via:
    - Gradient-based (Taylor expansion approximation)
    - Activation-based (mean activation magnitude)
    - Attention pattern analysis (entropy, sparsity)
  - Prune 25%, 50%, 75% of heads/neurons based on scores
  - Compare with random pruning and magnitude pruning
- **Metrics:** Task accuracy retention, layer-wise pruning patterns
- **Expected outcome:** Identify which layers/components are most compressible; validate importance metrics

**Experiment 3: Layer-wise compression sensitivity**
- **Hypothesis:** Early and late layers are more sensitive to compression than middle layers
- **Method:**
  - Apply varying compression rates per layer (e.g., 20% for layers 1-4, 60% for layers 5-20, 30% for layers 21-32)
  - Use grid search or evolutionary algorithms to find optimal per-layer budgets
- **Metrics:** Overall compression ratio vs. performance degradation
- **Expected outcome:** Layer-specific compression recipe that outperforms uniform compression

**Ablation A1:** Compare importance metrics (gradient vs. activation vs. hybrid)  
**Ablation A2:** Effect of calibration data size (1K, 10K, 100K samples) on importance estimation

---

#### **Month 3: Quantization & Mixed-Precision**

**Experiment 4: Quantization-aware training vs. post-training quantization**
- **Hypothesis:** QAT recovers more accuracy than PTQ at aggressive bit-widths (≤4-bit)
- **Method:**
  - PTQ: GPTQ, AWQ, or SmoothQuant on frozen model
  - QAT: Fine-tune with quantization simulation for 5-10% of original training steps
  - Test at INT8, INT4, INT3, INT2
- **Metrics:** Perplexity, task accuracy, quantization error distribution
- **Expected outcome:** Quantify QAT benefit vs. computational cost; identify minimum viable bit-width

**Experiment 5: Mixed-precision allocation**
- **Hypothesis:** Allocating higher precision to sensitive layers/parameters improves efficiency
- **Method:**
  - Use sensitivity analysis to assign bit-widths (e.g., 8-bit for embeddings/final layer, 4-bit for middle layers)
  - Compare with uniform quantization at same average bit-width
  - Explore automated search (e.g., Pareto-optimal bit allocation via NSGA-II)
- **Metrics:** Model size, accuracy, hardware efficiency (if possible)
- **Expected outcome:** Mixed-precision strategy that beats uniform quantization by 2-5% accuracy at same size

**Ablation A3:** Quantization granularity (per-tensor, per-channel, per-group)  
**Ablation A4:** Calibration dataset domain (in-domain vs. diverse corpora)

---

#### **Month 4: Hybrid Compression & Joint Optimization**

**Experiment 6: Pruning + quantization combinations**
- **Hypothesis:** Sequential application (prune-then-quantize) is suboptimal; joint optimization yields better results
- **Method:**
  - **Sequential:** Prune to 50% → quantize to INT4
  - **Joint:** Optimize pruning mask and quantization parameters together
  - **Iterative:** Alternate pruning and quantization steps
- **Metrics:** Final model size, accuracy, convergence speed
- **Expected outcome:** Demonstrate 3-7% accuracy improvement with joint optimization

**Experiment 7: Dynamic vs. static compression**
- **Hypothesis:** Input-adaptive compression (dynamic) can maintain quality with higher average compression
- **Method:**
  - Static: Fixed pruning mask/quantization
  - Dynamic: Early-exit mechanisms, adaptive depth/width based on input difficulty
  - Implement simple difficulty predictor (e.g., perplexity-based routing)
- **Metrics:** Average compression ratio, accuracy on easy vs. hard examples, latency distribution
- **Expected outcome:** Dynamic approach achieves 10-15% better efficiency on mixed-difficulty datasets

**Ablation A5:** Order of compression operations (prune→quantize vs. quantize→prune)  
**Ablation A6:** Fine-tuning budget (0%, 1%, 5% of original training)

---

#### **Month 5: Task-Specific & Emergent Capability Analysis**

**Experiment 8: Compression impact on emergent abilities**
- **Hypothesis:** Reasoning and multi-step tasks degrade faster than simple pattern matching under compression
- **Method:**
  - Evaluate compressed models on capability spectrum:
    - **Simple:** Sentiment analysis, NER
    - **Moderate:** Summarization, QA
    - **Complex:** Multi-hop reasoning (HotpotQA), math (GSM8K), code (HumanEval)
  - Track performance degradation curves across compression ratios
- **Metrics:** Per-task accuracy vs. compression ratio
- **Expected outcome:** Identify capability-specific compression limits; inform task-adaptive compression

**Experiment 9: Task-adaptive compression**
- **Hypothesis:** Fine-tuning compression parameters on target task outperforms generic compression
- **Method:**
  - Start with generically compressed model
  - Fine-tune pruning masks/quantization on task-specific data (e.g., code for programming tasks)
  - Compare with generic compression at same ratio
- **Metrics:** Task accuracy, transfer to related tasks
- **Expected outcome:** 5-10% task-specific improvement; insights on compression transferability

**Ablation A7:** Compression impact on different model sizes (1B, 7B, 13B parameters)  
**Ablation A8:** Effect on instruction-following vs. base model completion

---

#### **Month 6: Advanced Techniques & Integration**

**Experiment 10: Low-rank adaptation during compression**
- **Hypothesis:** Combining compression with parameter-efficient fine-tuning (LoRA) maintains adaptability
- **Method:**
  - Compress base model aggressively (70% pruning + INT4)
  - Add LoRA adapters and fine-tune on downstream tasks
  - Compare with less-compressed model without LoRA
- **Metrics:** Task accuracy, total parameter count (base + adapters), fine-tuning cost
- **Expected outcome:** Demonstrate that compressed+LoRA matches larger models with fewer total parameters

**Experiment 11: Compression-aware architecture search**
- **Hypothesis:** Models designed for compressibility outperform post-hoc compression of standard architectures
- **Method:**
  - Train smaller models with compression-friendly inductive biases:
    - Grouped convolutions in FFN
    - Shared attention heads
    - Mixture-of-experts with prunable experts
  - Compare with compressed standard architecture at same parameter count
- **Metrics:** Accuracy, compression ratio, training cost
- **Expected outcome:** Identify architectural patterns that enable better compression

**Ablation A9:** Compression stability across random seeds  
**Ablation A10:** Long-context performance degradation (test on 4K, 8K, 16K token sequences)

---

### 3. **Evaluation Framework**

**Benchmark Suite:**
- **Language modeling:** WikiText-103, C4, The Pile
- **Understanding:** MMLU (57 tasks), BoolQ, PIQA
- **Reasoning:** HellaSwag, ARC-Challenge, GSM8K
- **Generation:** Summarization (CNN/DM), MT (WMT), code (HumanEval)
- **Instruction-following:** MT-Bench, AlpacaEval

**Efficiency Metrics:**
- Model size (MB), parameter count
- Inference latency (ms/token) on target hardware (A100, CPU, edge device)
- Memory footprint (peak RAM)
- Energy consumption (if measurable)

**Statistical Rigor:**
- Run each experiment with 3 random seeds
- Report mean ± std for all metrics
- Use paired t-tests for significance testing

---

### 4. **Timeline (6 Months)**

| Month | Focus | Key Deliverables |
|-------|-------|------------------|
| **1** | Baselines & infrastructure | Benchmark suite, baseline table, codebase |
| **2** | Structured pruning | Importance scoring methods, layer-wise recipes |
| **3** | Quantization | QAT vs PTQ comparison, mixed-precision strategies |
| **4** | Hybrid methods | Joint optimization framework, dynamic compression |
| **5** | Task analysis | Capability degradation study, task-adaptive compression |
| **6** | Advanced techniques | LoRA integration, architecture insights, paper draft |

**Weekly milestones:**
- Week 1-2: Literature review, environment setup
- Week 3-4: Experiment 1 (baselines)
- Week 5-8: Experiments 2-3 (pruning)
- Week 9-12: Experiments 4-5 (quantization)
- Week 13-16: Experiments 6-7 (hybrid)
- Week 17-20: Experiments 8-9 (task-specific)
- Week 21-24: Experiments 10-11 (advanced)
- Week 25-26: Paper writing, code release

---

### 5. **Expected Contributions**

**Empirical:**
- Comprehensive comparison of compression techniques across model scales
- Identification of optimal compression recipes for different deployment scenarios

**Methodological:**
- Novel joint optimization framework for pruning + quantization
- Task-adaptive compression strategies

**Practical:**
- Open-source toolkit for LLM compression
- Deployment-ready compressed models (e.g., 7B → 2GB model with <10% degradation)

**Scientific:**
- Insights into which model components/capabilities are most compressible
- Understanding of compression's impact on emergent abilities

---

### 6. **Resource Requirements**

**Compute:**
- 4-8 A100 GPUs (or equivalent) for training/fine-tuning
- CPU nodes for inference benchmarking
- Estimated: ~5,000 GPU-hours over 6 months

**Data:**
- Pre-training corpora: The Pile, C4 (publicly available)
- Evaluation benchmarks: MMLU, GSM8K, etc. (publicly available)
- Calibration data: 10K-100K samples from diverse sources

**Software:**
- PyTorch, Hugging Face Transformers
- Compression libraries: GPTQ, AutoGPTQ, bitsandbytes
- Evaluation: lm-evaluation-harness

---

### 7. **Risk Mitigation**

**Risk:** Compressed models fail to maintain acceptable performance
- *Mitigation:* Start with moderate compression (30-50%); use strong baselines; explore hybrid methods

**Risk:** Computational budget insufficient for large-scale experiments
- *Mitigation:* Focus on 7B models; use smaller models (1-3B) for ablations; leverage pre-compressed checkpoints

**Risk:** Difficulty reproducing SOTA compression results
- *Mitigation:* Engage with open-source community; use established libraries; document all hyperparameters

**Risk:** Limited access to diverse hardware for deployment testing
- *Mitigation:* Prioritize CPU and single-GPU inference; collaborate with industry partners if possible

---

### 8. **Success Criteria**

**Minimum viable:**
- 50% model size reduction with <5% accuracy drop on MMLU
- 2× inference speedup on CPU
- Reproducible benchmark and open-source code

**Target:**
- 70% size reduction with <10% accuracy drop
- Identify task-adaptive compression strategies that beat generic compression by 5%
- Publication at ML venue (NeurIPS, ICML, ICLR) or systems venue (MLSys)

**Stretch:**
- 80% reduction with <15% drop via novel joint optimization
- Demonstrate deployment on edge devices (Raspberry Pi, mobile)
- Industry adoption or integration into popular frameworks

---

### 9. **Suggested Ablations Summary**

1. **A1:** Importance metrics (gradient vs. activation vs. hybrid)
2. **A2:** Calibration data size (1K, 10K, 100K)
3. **A3:** Quantization granularity (per-tensor, per-channel, per-group)
4. **A4:** Calibration dataset domain
5. **A5:** Compression operation order
6. **A6:** Fine-tuning budget (0%, 1%, 5%)
7. **A7:** Model size scaling (1B, 7B, 13B)
8. **A8:** Base vs. instruction-tuned models
9. **A9:** Stability across random seeds
10. **A10:** Long-context performance

---

### 10. **Literature Anchors**

Key recent work to build upon:
- **Pruning:** SparseGPT, Wanda (pruning without retraining)
- **Quantization:** GPTQ, AWQ, SmoothQuant (post-training quantization)
- **Distillation:** DistilBERT, TinyBERT principles applied to LLMs
- **Hybrid:** LLM.int8(), QLoRA (quantization + efficient fine-tuning)
- **Analysis:** Scaling laws for compressed models, lottery ticket hypothesis for LLMs

---

This plan provides a structured 6-month roadmap with 11 core experiments, 10 ablations, clear hypotheses, and measurable outcomes. The experiments build progressively from baselines to advanced techniques, with monthly milestones. Would you like me to elaborate on any specific experiment, suggest specific model architectures to test, or provide implementation details for any technique?