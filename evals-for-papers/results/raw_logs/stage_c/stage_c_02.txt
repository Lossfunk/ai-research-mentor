1. Problem framing and goals
- Objective: Develop and evaluate compressible LLMs that retain strong task performance and calibration while reducing memory, latency, and energy. Focus on post-training quantization (PTQ), pruning/sparsity, quantization-aware finetuning (QAT) via low-rank adapters, and KV-cache compression, with systematic ablations and reproducible protocols.
- Primary methods to test:
  - Weight-only PTQ (GPTQ, AWQ, SqueezeLLM, ZeroQuant) [1][2][8][10]
  - Activation-aware/activation-smoothing quantization (SmoothQuant) [3]
  - Low-bit finetuning on quantized backbones (QLoRA) [5]
  - One-shot sparsity (SparseGPT, Wanda) [6][7]
  - KV-cache quantization (KVQuant) [9]
- Tasks and metrics:
  - Core benchmarks: MMLU, GSM8K, HellaSwag, ARC-c, TruthfulQA, WinoGrande, LAMBADA; code/L2 metrics: perplexity on WikiText-2 or C4 shards; coding (HumanEval) if resources permit.
  - Efficiency metrics: latency/throughput (batch=1 and server batches), memory footprint (model weights + KV), energy (per 1k tokens).
  - Reliability: calibration (ECE/Brier), robustness (prompt paraphrases), alignment drift checks on toxicity/harms (e.g., RealToxicityPrompts).
- Success criteria (for a 7B–13B model):
  - ≥3–4× memory reduction and ≥1.7× speedup with ≤2 points absolute drop on MMLU and ≤2–3% relative drop on GSM8K pass@1; near-parity calibration vs. full-precision baseline, and no statistically significant increase in toxicity.
- Literature anchors:
  - GPTQ: Accurate PTQ for LLMs [1]
  - QLoRA: Efficient finetuning of 4-bit quantized LLMs [5]

2. Experiments
Experiment 1: Weight-only PTQ comparison (GPTQ vs. AWQ vs. ZeroQuant vs. SqueezeLLM)
- Hypothesis: Calibrated weight-only PTQ can achieve 3–4 bit quantization with <2 pt accuracy loss on knowledge and reasoning benchmarks; AWQ will be robust to activation outliers; SqueezeLLM’s dense-and-sparse quantization improves low-bit regimes [1][2][8][10].
- Setup:
  - Models: LLaMA-2 7B/13B or Mistral 7B (base or instruct).
  - Methods: GPTQ (2–4 bit, group sizes 32/64), AWQ (4 bit), ZeroQuant(-V2) PTQ pipelines, SqueezeLLM “dense-and-sparse” configs.
  - Calib set: 512–4k sentences (C4/RefinedWeb) for PTQ calibration; vary size for ablation.
- Baselines: FP16; 8-bit weight quantization (if available) as a strong baseline.
- Metrics: MMLU, HellaSwag, ARC-c, TruthfulQA, LAMBADA; perplexity; latency, memory; ECE/Brier.
- Expected outcomes: AWQ/GPTQ at 4-bit close to FP16 with minor drops; 3-bit may degrade reasoning more; SqueezeLLM improves under aggressive compression [1][2][8][10].

Experiment 2: Activation-aware quantization via SmoothQuant
- Hypothesis: Smoothing/redistributing outliers enables joint weight+activation quantization to 8/4-bit with small degradation, improving throughput on hardware requiring activation quantization [3].
- Setup:
  - Apply SmoothQuant to same models as E1; test INT8 weights+activations and mixed INT8/INT4 activations on GEMM-friendly kernels.
- Baselines: E1 best models; pure weight-only quantization.
- Metrics: Same as E1, plus activation memory and end-to-end speedup under realistic batching.
- Expected outcomes: Mixed-precision activations reduce memory and improve speed with small accuracy loss vs. weight-only [3].

Experiment 3: QAT via QLoRA on PTQ backbones
- Hypothesis: Low-rank adapter finetuning on top of 4-bit quantized weights recovers (or surpasses) accuracy lost from PTQ and improves calibration on instruction-tuned tasks [5].
- Setup:
  - Start from 4-bit GPTQ or AWQ checkpoints; apply QLoRA (NF4 + 16-bit adapters) on instruction datasets (e.g., OpenOrca subset).
  - Sweep LoRA rank (r=8/16/32), α, dropout; ablate calib set size used by PTQ.
- Baselines: Original FP16 instruct model; PTQ-only models.
- Metrics: MT-Bench, MMLU, GSM8K, TruthfulQA; ECE/Brier; latency/memory.
- Expected outcomes: QLoRA recovers 50–100% of lost accuracy and improves calibration vs. PTQ-only; may approach FP16 on instruction metrics [5].

Experiment 4: One-shot pruning (SparseGPT, Wanda) and hybrid with quantization
- Hypothesis: 30–50% sparsity via one-shot pruning with minimal degradation; combined with 4-bit PTQ yields multiplicative gains if kernels exploit sparsity [6][7][8].
- Setup:
  - Prune with SparseGPT (layer/block-wise sparsity schedules) and Wanda (activation-aware magnitude), sparsity targets 30/50/60%.
  - Evaluate alone and combined with 4-bit GPTQ/AWQ. Test structured vs unstructured if kernels support.
- Baselines: E1 best PTQ; FP16.
- Metrics: Same; plus throughput on sparse-enabled kernels; storage footprint.
- Expected outcomes: Up to 30–40% sparsity tolerable with limited drops; hybrid improves memory and may improve speed if kernels are efficient [6][7][8].

Experiment 5: KV-cache quantization and long-context stress
- Hypothesis: Mixed-precision or low-bit KV-cache quantization preserves next-token accuracy and QA at long context with substantial memory savings [9].
- Setup:
  - Apply KVQuant recipes (e.g., per-layer mixed precision, channel-wise scaling) at context lengths 8k–32k; vary attention mechanisms if available.
- Baselines: FP16 KV; naive 8-bit KV.
- Metrics: Long-context perplexity (PG19, Books), RULER/Needle-in-a-Haystack retrieval accuracy; latency, memory.
- Expected outcomes: KVQuant retains accuracy with 2–4× KV memory reduction; mild slowdown if extra dequant ops are needed [9].

Experiment 6: Calibration, robustness, and alignment drift under compression
- Hypothesis: Aggressive compression harms calibration and robustness; QAT (E3) mitigates this.
- Setup:
  - Take best from E1–E5; evaluate ECE/Brier on MMLU/GSM8K; adversarial paraphrases; toxicity (RealToxicityPrompts).
- Baselines: FP16; PTQ-only.
- Metrics: ECE, Brier; toxicity rate; robustness deltas.
- Expected outcomes: PTQ-only degrades calibration modestly; QLoRA restores; no significant toxicity increase if instruction data used.

Experiment 7: Calibration data size and selection ablation for PTQ
- Hypothesis: PTQ sensitivity to calibration data distribution is high at 3–4 bits; modest improvements from domain-matched calibration selection [1][2][10].
- Setup: Vary calib size (128–8192 samples) and distribution (instruction, knowledge, math).
- Baselines: E1 default.
- Metrics: MMLU, GSM8K; ECE; latency; memory.
- Expected outcomes: Diminishing returns after ~2k samples; domain-matched calibration helps domain-specific tasks slightly.

Experiment 8: Hardware-aware mixed precision search
- Hypothesis: Layer-wise mixed precision guided by sensitivity yields near-FP16 accuracy with better speed/memory than uniform 4-bit [1][2][3][8].
- Setup: Per-layer Hessian/loss-sensitivity proxies to assign 8/6/4 bit weights; INT8/INT4 activations; grid search constrained by memory budget.
- Baselines: Uniform 4-bit; 8-bit.
- Metrics: Accuracy, ECE; latency; memory; energy.
- Expected outcomes: Mixed precision beats uniform 4-bit at same memory; offers smoother accuracy–efficiency Pareto.

3. Timeline for the next 6 months with milestones
- Month 1: Reproducible baselines
  - Stand up eval harness for MMLU, GSM8K, HellaSwag, ARC-c, TruthfulQA, LAMBADA; perplexity pipeline; calibration metrics.
  - Implement GPTQ, AWQ, ZeroQuant, SmoothQuant; gather calibration sets; profiling scripts.
  - Milestone: FP16 baselines + first 4-bit GPTQ/AWQ results on 7B; profiling dashboard [1][2][3][10].
- Month 2: Broad PTQ sweep and ablations
  - Full grid for bits (2–4), group sizes, calib size; activation quant via SmoothQuant.
  - Milestone: Pareto curves (accuracy vs memory/latency) for 7B, draft technical report [1][2][3].
- Month 3: QAT/QLoRA recovery and calibration study
  - QLoRA on best PTQ models; calibration/robustness suite; early instruction-task results.
  - Milestone: QAT closes most gaps; internal write-up with calibration analysis [5].
- Month 4: Sparsity and hybrid compression
  - Apply SparseGPT/Wanda; hybrid with 4-bit; explore structured sparsity if kernels available.
  - Milestone: Demonstrate additional 1.3–1.6× memory reduction at similar accuracy; kernel feasibility assessment [6][7][8].
- Month 5: KV-cache quantization and long-context
  - Integrate KVQuant; evaluate up to 32k tokens; measure latency/memory scaling.
  - Milestone: KV memory reduced ≥2× with minimal accuracy loss on long-context tasks [9].
- Month 6: Consolidation, system integration, and paper draft
  - Final Pareto front; best model release; ablation summary; reproducibility artifacts.
  - Milestone: Submission-ready paper with code; artifact evaluation checklist.

4. Resources (compute, tools, datasets)
- Compute:
  - 7B–13B experiments: 2–8× A100/H100 80GB or 4× A6000/RTX 4090 (quantization fits in less); QLoRA finetuning: 1–4 GPUs with 4-bit weights [5].
  - Storage: 2–4 TB; CPU RAM: 128–256 GB for offline calibration/pruning.
- Tools:
  - Quantization: AutoGPTQ (GPTQ), AWQ implementations, SmoothQuant, ZeroQuant; SqueezeLLM code; bitsandbytes; llama.cpp for deployment kernels [1][2][3][8][10].
  - Pruning: SparseGPT, Wanda repos [6][7].
  - Evaluation: lm-eval-harness, EleutherAI; custom ECE/Brier scripts; latency/energy profilers (nsys, nvml, CodeCarbon).
- Datasets/benchmarks:
  - MMLU, HellaSwag, ARC-c, GSM8K, TruthfulQA, WinoGrande, LAMBADA; WikiText-2 or C4 shards for perplexity; RealToxicityPrompts for safety.

5. Risks and mitigations
- Accuracy collapse at low bits (≤3-bit)
  - Mitigations: Mixed precision per layer; larger calibration sets; QAT/QLoRA recovery; revert sensitive layers to 8-bit [1][2][3][5].
- Activation outliers degrade INT4 activations
  - Mitigations: SmoothQuant smoothing and per-channel scaling; pre-layer norm fixes [3].
- Sparsity speedups not realized
  - Mitigations: Use kernels supporting un/structured sparsity; constrain to 2:4 or block-sparse; profile before committing [6][8].
- Calibration drift and increased hallucinations
  - Mitigations: Include ECE/Brier in selection; QAT with instruction data; safety checks; temperature calibration.
- KV-cache quantization harms long-context retrieval
  - Mitigations: Mixed-precision KV, layer-wise tuning; avoid quantizing attention heads most sensitive [9].
- Reproducibility gaps (seed/config sensitivity)
  - Mitigations: Deterministic seeds; config pinning; publish exact calib sets and preprocessing.
- Hardware/kernel mismatch
  - Mitigations: Early target-hardware profiling; fallbacks to widely available kernels (cuBLAS, CUTLASS); CI across GPUs.

6. Stretch ideas or follow-up directions
- Compression-aware scaling laws: extend accuracy–efficiency tradeoff modeling across bits/sparsity and model sizes; validate on 7B→34B; derive predictive curves for target accuracy under memory budgets.
- Progressive compression schedules: iterative prune→quantize→QAT cycles; autoML to pick layer-wise bit-widths under latency constraints.
- Robustness-first compression: integrate calibration/robustness losses during QAT; multi-objective tuning to minimize ECE at fixed accuracy.
- Hardware co-design: structured N:M sparsity with vendor kernels; fuse dequant with GEMM; KV-aware attention kernels.
- KV cache distillation: train small KV adapters to mimic FP16 attention states under low-bit KV.

Optional next step
- Stand up a minimal repro of Experiment 1 on a 7B model this week, producing a two-dimensional Pareto (MMLU vs. RAM) for GPTQ vs. AWQ at 4-bit with a fixed calibration set, then share the plot and configs for early feedback.

Citations
[1] Frantar et al., GPTQ: Accurate Post-Training Quantization for Generative Pretrained Transformers — https://arxiv.org/abs/2210.17323
[2] Lin et al., AWQ: Activation-aware Weight Quantization for LLM Compression — https://ar5iv.org/html/2306.00978
[3] Xiao et al., SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models — https://arxiv.org/abs/2211.10438
[4] (Not used)
[5] Dettmers et al., QLoRA: Efficient Finetuning of Quantized LLMs — https://arxiv.org/pdf/2305.14314
[6] Frantar & Alistarh, SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot — https://arxiv.org/abs/2301.00774
[7] Sun et al., Wanda: A Simple and Effective Pruning Approach for Large Language Models — https://arxiv.org/pdf/2306.11695
[8] Kim et al., SqueezeLLM: Dense-and-Sparse Quantization — https://arxiv.org/abs/2306.07629
[9] Shao et al., KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization — https://proceedings.neurips.cc/paper_files/paper/2024/file/028fcbcf85435d39a40c4d61b42c99a4-Paper-Conference.pdf
[10] Yao et al., ZeroQuant / ZeroQuant-V2: Efficient and Affordable Post-Training Quantization for Transformers — https://arxiv.org/abs/2206.01861 and https://arxiv.org/abs/2303.08302