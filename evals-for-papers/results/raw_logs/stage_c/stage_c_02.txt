Before I develop a detailed 6-month plan, I need to understand your specific context to tailor the roadmap effectively:

## Essential Context Questions

1. **Resources & Time**: What GPU access do you have (local GPUs, cloud credits, cluster allocation)? How many hours per week can you dedicate to this project?

2. **Current Position**: Are you a PhD student, master's student, or independent researcher? Do you have advisor support or are you working solo?

3. **Specific Focus**: "Compressible LLMs" is broad—are you most interested in:
   - Quantization methods (binary/ternary weights, low-bit precision)?
   - Structural compression (pruning, modularization, knowledge distillation)?
   - Novel compression paradigms (wavelet transforms, learned projections)?
   - Deployment-focused (edge devices, latency constraints)?

4. **Target Venue & Timeline**: Are you aiming for a specific conference (NeurIPS, ICML, ICLR, ACL) or workshop? What's your submission deadline?

5. **Current Bottleneck**: What's blocking you right now—choosing a specific angle, setting up baselines, or something else?

---

## Provisional Phase 0 (Weeks 1-2): Calibration & Reproduction

While you provide those details, here's a **gated starter plan** [G7][G12] that will build essential skills regardless of your specific direction:

### **Intuition**
Compression research requires deep familiarity with the performance-efficiency tradeoff landscape. Starting with reproduction ensures you understand what "good" compression looks like and builds intuition for where current methods fail—this grounds later novelty in empirical reality rather than speculation.

### **Why this is principled**
Effective research begins with crisp problem framing and early feedback loops [G7][G12]. Reproduction establishes baseline competence, reveals hidden implementation details that papers omit, and generates prediction logs that calibrate your intuition about what interventions might work [G19].

### Deliverables (Must complete before advancing):

**Deliverable 1: Reproduction Artifact**
- Pick ONE baseline from recent work: either extreme quantization [P1], modular transformers [P3], or projected compression [P5]
- Reproduce at least one key result (e.g., perplexity on WikiText-2 or accuracy on GLUE task) within ≤10% relative error across ≥3 random seeds
- Document all deviations from the paper in a 1-page "reproduction log"

**Deliverable 2: Prediction Log + Experiment Card**
- Maintain a daily prediction log with ≥14 entries over 2 weeks (predict: "Will this hyperparameter change improve perplexity by >5%?" Record outcome and Brier score)
- Draft one experiment card using this template:
  - **Hypothesis**: [specific, directional claim]
  - **Falsifier**: [what result would disprove it]
  - **Minimal Test**: [smallest experiment to run]
  - **Variables**: [independent/dependent/controls]
  - **Expected Patterns**: [confirmatory vs. disconfirmatory results]
  - **Analysis Plan**: [metrics, tests, visualizations]
  - **Stop Rule**: [when to halt or pivot]

---

## Three Concrete Experiments (Expanded)

Here are three falsifiable experiments grounded in the recent literature [P1][P3][P5], each scoped for different resource levels:

### **Experiment 1: Compression-Aware Fine-Tuning vs. Post-Training Compression**

**Objective & Hypothesis**: Test whether training a model with compression-aware objectives (e.g., quantization-aware training) from scratch outperforms compressing a pre-trained model post-hoc. Hypothesis: Compression-aware training will yield ≥15% better perplexity at the same compression ratio because it allows the model to adapt its representations during learning [P1].

**Setup & Resources**: Use a small transformer (e.g., GPT-2 small, 117M parameters) on WikiText-103. Train two variants: (1) standard pre-training followed by 4-bit quantization, and (2) quantization-aware training from initialization. Requires 1-2 GPUs (A100 or V100) for ~3-5 days. Use existing QAT libraries (e.g., PyTorch's quantization toolkit).

**Evaluation Metrics**: Measure validation perplexity, compression ratio (original size / compressed size), and inference latency on CPU. Track training curves to identify when compression-aware models diverge from standard training. Success criterion: ≥15% perplexity improvement at 4-bit precision.

**Expected Results & Interpretation**: If QAT wins, it suggests compression should be integrated early in the training pipeline—explore whether this holds for larger models or different compression ratios (2-bit, 8-bit). If post-training compression is competitive, investigate whether the gap closes with better calibration data or more sophisticated quantization schemes (e.g., mixed-precision).

**Follow-up Variations**: Test on different model sizes (350M, 1.3B parameters), vary compression ratios (2-bit, 3-bit, 8-bit), and explore task-specific fine-tuning (GLUE benchmarks) to see if the advantage persists beyond language modeling.

---

### **Experiment 2: Modular Layer Pruning with Adaptive Depth**

**Objective & Hypothesis**: Investigate whether dynamically selecting which transformer layers to execute per input (adaptive depth) can maintain accuracy while reducing compute. Hypothesis: A learned router that skips 30-50% of layers will preserve ≥95% of original accuracy on diverse inputs because many examples don't require full model depth [P3].

**Setup & Resources**: Start with a pre-trained BERT-base or GPT-2 medium model. Implement a lightweight router (small MLP) that predicts per-layer importance scores for each input. Train the router on a held-out calibration set (10K examples from your target dataset) using a combined loss: task accuracy + efficiency penalty (number of layers executed). Requires 1 GPU for ~2-3 days.

**Evaluation Metrics**: Measure task accuracy (F1 on SQuAD or accuracy on MNLI), average number of layers executed per input, and wall-clock inference time. Plot accuracy vs. compute tradeoff curves. Success criterion: ≥95% accuracy retention with ≤60% average layer execution.

**Expected Results & Interpretation**: If the router successfully identifies "easy" vs. "hard" examples, you'll see a bimodal distribution of layer usage. Analyze which input characteristics correlate with shallow vs. deep execution (e.g., sentence length, syntactic complexity). If accuracy drops sharply, investigate whether certain layer types (early vs. late, attention vs. FFN) are more critical.

**Follow-up Variations**: Extend to token-level adaptive computation (skip layers for some tokens but not others), test on out-of-distribution data to assess router robustness, and compare learned routers vs. heuristic rules (e.g., skip layers for short inputs).

---

### **Experiment 3: Compression Lottery Ticket Hypothesis**

**Objective & Hypothesis**: Test whether compressed subnetworks found early in training (lottery tickets) can match the performance of fully-trained compressed models. Hypothesis: A subnetwork identified at 10-20% of training will achieve ≥90% of the final compressed model's performance when trained in isolation, suggesting compression can be front-loaded [P1][P4].

**Setup & Resources**: Train a small transformer (GPT-2 small or distilBERT) on a language modeling or classification task. At checkpoints (10%, 20%, 50% of training), apply magnitude-based pruning to identify sparse subnetworks (50-70% sparsity). Rewind these subnetworks to their initialization and train to convergence. Requires 2-3 GPUs for ~1 week (parallel training of multiple subnetworks).

**Evaluation Metrics**: Compare final perplexity/accuracy of lottery ticket subnetworks vs. the full model compressed post-training. Measure training efficiency (FLOPs to convergence) and final model size. Success criterion: Early lottery tickets (≤20% training) achieve ≥90% of post-training compression performance.

**Expected Results & Interpretation**: If early tickets succeed, it implies compression decisions can be made before expensive full training—explore whether this generalizes to larger models or different compression methods (quantization, distillation). If tickets fail, investigate whether the pruning criterion (magnitude) is suboptimal or whether transformers lack the lottery ticket property observed in CNNs.

**Follow-up Variations**: Test different pruning criteria (gradient-based, activation-based), vary sparsity levels (30%, 50%, 80%), and explore iterative magnitude pruning (IMP) to refine ticket quality. Extend to structured pruning (entire attention heads or FFN dimensions).

---

## Problem Selection Rubric (Score before proceeding)

Evaluate your chosen compression angle using this 0-3 rubric (0 = poor, 3 = excellent):

1. **Importance**: If solved, how much does it matter for deployment or understanding? (0-3)
2. **Tractability**: Can you get signal within ~3 weeks given your resources? (0-3)
3. **Surprise**: Does it challenge a common belief or reveal something unexpected? (0-3)
4. **Generality**: Does it apply across models, datasets, or tasks? (0-3)
5. **Mechanistic Payoff**: Is there a clear "why" hypothesis to test? (0-3)

**Proceed only if your total score is ≥10/15.** If below, iterate on scope or pivot to a higher-scoring angle.

---

## Next Actions (1-3 days)

1. **Answer the context questions above** so I can tailor the full 6-month timeline to your situation.
2. **Pick one reproduction target** from [P1], [P3], or [P5] and locate the official code repository.
3. **Start your prediction log** today—make one prediction about your reproduction attempt (e.g., "I predict the baseline will achieve X perplexity within Y hours of training").

Once you provide your context, I'll expand this into a full 6-month roadmap with monthly milestones, ablation studies, venue-specific positioning, and risk mitigation strategies [G8][G13].

---

## Citations

- [G7] Opinionated Guide Ml Research — http://joschu.net/blog/opinionated-guide-ml-research.html
- [G8] 1986 Hamming — https://gwern.net/doc/science/1986-hamming
- [G12] How I Think About My Research Process Explore Understand — https://www.alignmentforum.org/posts/hjMy4ZxS5ogA9cTYK/how-i-think-about-my-research-process-explore-understand
- [G13] How Not To Choose A Research Project — https://www.lesswrong.com/posts/kDsywodAKgQAAAxE8/how-not-to-choose-a-research-project
- [G19] Taste — https://colah.github.io/notes/taste/
- [P1] Extreme Compression for Pre-trained Transformers Made Simple and Efficient — http://arxiv.org/abs/2206.01859v1
- [P3] Modular Transformers: Compressing Transformers into Modularized Layers for Flexible Efficient Inference — http://arxiv.org/abs/2306.02379v1
- [P4] A Survey on Transformer Compression — http://arxiv.org/abs/2402.05964v2
- [P5] Projected Compression: Trainable Projection for Efficient Transformer Compression — http://arxiv.org/abs/2506.22255v1