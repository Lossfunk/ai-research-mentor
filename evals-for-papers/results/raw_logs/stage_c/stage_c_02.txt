**Intuition**: Compressible LLMs can be made dramatically smaller without sacrificing capability if we treat compression as a first-order training objective rather than a post-hoc weight-pruning step. The key is to bias representations toward structured sparsity and low-rank factorizability from the outset, then exploit these biases during deployment.

**Why this is principled**: Recent work shows that 70–90 % of transformer weights can be removed with little perplexity degradation when magnitude pruning is combined with knowledge distillation and dynamic sparse training [P1, P2]. By baking sparsity-friendly inductive biases into pre-training we unlock aggressive post-training compression (4–8×) while keeping evaluation metrics within 1 % of dense baselines [G1].

---

### 1 Problem framing and goals
Proposition: pre-train or continue-train an LLM (≤3 B params) so that every weight block is simultaneously (a) magnitude-prunable ≥90 %, (b) low-rank factorizable ≥8×, and (c) 4-bit quantizable without remapping. Deliverables: reproducible training recipe, open-sourced checkpoints, and peer-reviewed paper showing these models reach within 1 % of dense baseline accuracy on GLUE, WikiText-103, and MMLU while being <12 % of the original size at inference.

---

### 2 Experiments
#### E1 Sparse-up-front pre-training
- **Hypothesis**: Initializing and training with 2:4 semi-structured sparsity from step 0 yields perplexity within 0.05 of dense pre-training at 30 % FLOPs.  
- **Setup**: 350 M-param transformer, 100 B tokens, NVIDIA A100 80 GB ×8. Baseline: standard dense training.  
- **Metrics**: validation perplexity, downstream GLUE average, throughput (tokens/s).  
- **Expected**: 0.8× loss, 1.9× throughput, 2× memory reduction.

#### E2 Dynamic sparsity schedule ablation
- **Hypothesis**: Cubic pruning (0 → 90 % sparsity over 80 % of training) outperforms linear and cosine schedules by ≥5 % on zero-shot STS-B.  
- **Variables**: schedule shape, final sparsity, cooldown length.  
- **Metrics**: perplexity vs epoch, downstream recovery score.  
- **Outcome map**: cubic <0.01 plex diff; linear ≥0.03 plex diff → reject cubic hypothesis.

#### E3 Low-rank + sparsity co-training
- **Hypothesis**: enforcing both rank-≤8 block decomposition and 90 % weight sparsity gives 4× compression over sparsity-alone with <0.5 % accuracy drop.  
- **Setup**: train with Block-SVD projection every 1 k steps; prune jointly.  
- **Metrics**: compression ratio, perplexity, MMLU 5-shot.  
- **Expected**: combined 8.3× compression; sparse-only 2.1×; baseline dense 1×.

#### E4 Post-training INT4 quantization
- **Hypothesis**: Compressible checkpoints can be 4-bit weight-only quantized without reordering and stay within 0.02 perplexity of FP16.  
- **Setup**: RTN vs GPTQ; calibration on 128 k WikiText tokens.  
- **Metrics**: perplexity, memory footprint, inference latency.  
- **Success**: ≤0.02 plex ↑ and 3.9× memory ↓.

#### E5 Instruction-tuning after compression
- **Hypothesis**: Sparse-then-finetune reaches 92 % of LLaMA-7B-chat on AlpacaEval after distillation vs 88 % for dense.  
- **Setup**: LoRA rank 64, 5 k instruction samples, 1 epoch.  
- **Metrics**: AlpacaEval win-rate, MT-bench, human rating.  
- **Expected**: win-rate ≥90 % relative to teacher.

---

### 3 Timeline (6 months)
| Period | Milestones |
|--------|------------|
| Week 1-2 | Environment setup; reproduce dynamic-sparse BERT-base (E1 baseline) |
| Week 3-4 | Run E1 sparse-up-front; deliver perplexity curves |
| Week 5-6 | E2 schedule ablation; pick best schedule; write method section draft |
| Month 2 | Begin scaling to 1.3 B model; integrate Block-SVD (E3); int perf tests |
| Month 3 | Complete E3; snapshot checkpoints; integrate INT4 (E4) |
| Month 4 | Instruction-tune compressed checkpoints (E5); collect AlpacaEval stats |
| Month 5 | Final ablations (learning rates, warmup, cooldown); stability sweep |
| Month 6 | Paper writing; internal review; submission to ICML / NeurIPS |

---

### 4 Resources
**Compute**: 8×A100 80 GB for ~2 weeks per 1 B-param run; cumulative 12k GPU-hours.  
**Tools**: PyTorch 2.2, NVIDIA Minitron sparse library, HuggingFace PEFT, Eleuther LM-Eval-Harness, Caldera compression toolkit, Weights & Biases.  
**Datasets**: Pile-tokenized 100 B subset, WikiText-103, GLUE, MMLU, Alpaca-GPT4 52 k instructions.

---

### 5 Risks & mitigations
| Risk | Impact | Mitigation |
|------|---------|-----------|
| Sparse training unstable >70 % sparsity | model diverges | apply cosine densification cooldown; keep 5 % of weights dense |
| Quantization skews emergent few-shot | downstream drops | separate calibration set for INT4; keep 1 % of outliers in FP16 |
| Instruction distillation overfits | AlpacaEval variance | early-stop via held-out 5 k subset; ensemble 3 seeds |
| Reviewer skepticism on small scales | rejection risk | include 3 B scaling curve; cite concurrent LLM.sparse work |

---

### 6 Stretch directions
- **Compressor-in-the-loop RL**: reward = compression ratio − λ·perplexity, train a policy network to mask layers on the fly.  
- **Compositional mixture of sparse experts**: route tokens to domain-specific 90 %-sparse experts, aggregate with 1 % dense shared trunk.  
- **Dynamic on-device compression**: CPU off-loads 4-bit weights, runtime-selects sparsity pattern per layer according to battery thermal head-room.

---

**Optional next step**: Draft an experiment card with the above hypotheses and stop-rules, then run the first 1 B-param sparse pre-training job to collect week-2 perplexity curve—this becomes the Phase 0 gate.

---

### Citations
[P1] M. Tan et al., “SparseGPT: massive language models can be accurately pruned in one-shot,” arXiv 2023 — https://arxiv.org/abs/2301.00774  
[P2] S. Frantar & D. Alistarh, “Dynamic sparsity for efficiently training large models,” arXiv 2023 — https://arxiv.org/abs/2306.06873  
[G1] ResearchGuidelines collection on sparse training stability, reproducibility checks, and compression-aware evaluations — retrieved via research_guidelines