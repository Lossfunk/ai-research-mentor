**1. Problem framing and goals**  
You can do meaningful zero-compute research by:
- Re-analyzing existing public datasets with new hypotheses (≤1–2 week download, ≤5 GB RAM) — e.g. probing TinyStories, GLUE, or CIFAR-10.
- Mining GitHub-released models/datasets and re-running authors’ inference notebooks on CPU-only (many finish in hours on a laptop, GPU optional).
- Carving a reproducibility niche: pick 5–10 recently claimed papers and reproduce the exact experiment cards in the author repo under identical random seeds with provided data.  
Target objective: deliver one fully reproducible paper + one fresh “no-compute” hypothesis within 6 months, publishable as a tiny-compute or survey track at ACL/ICLR/ICML workshops.

**Intuition**: Most progress in ML comes from reframing problems, not bigger GPUs. You’re weaponizing rigor, not flops.  
**Why this is principled**: Multiple meta-surveys show 30–50 % of empirical claims degrade under reproduction; your audit path is finite and gate-able [G1].

**2. Experiments** (all CPU/laptop, ≤8 GB RAM, single-threaded OK)  
**E1 TinyStories vs. Llama-2 scaling law**  
Hypothesis: TinyStories (tiny models, curated data) achieves the same emergent behavior slope as Llama-2 if plotted along FLOPs-to-performance, implying emergence is not unique to scale.  
Setup: Re-use authors’ released checkpoints (≤300 MB each) and loss traces; replicate scatter plot with identical token count normalization.  
Evaluation: log-log Brier curve vs. compute; compute R² (goodness-of-fit) compared to Llama-2 scaling curve.  
Baseline: Llama-2 scaling numbers as reported in Touvron et al. 2023.  
Success metric: R² ≥0.80 with visually overlapping trend lines.  
Interpretation: If curves align, emergence is scale-agnostic → supports tiny-data modeling narrative. If not, scale still matters → supports Chinchilla-style resource scaling.  
Follow-up: Add Phi or Orca data to check if “tiny but curated” rewrites the curve.

**E2 Can a 2-layer MLP memorize 2-bit XOR on CPU?**  
Hypothesis: Memorization without over-parameterization can occur if we feed exact one-hot representation of bits.  
Setup: Generate synthetic 2-bit XOR dataset (4 samples, 2 classes). Train a MLP (2-10-10-1) with ReLU; record test accuracy, param count, and epochs to convergence.  
Baseline: published “memorization threshold” claims by Zhang et al. 2017 showing >100k params are needed for random labels.  
Evaluation: plot parameters vs. epochs to convergence; stop at 100% train + test accuracy.  
Outcomes: If <50 params suffice, it falsifies the “massive needed” claim on tiny data; if not, confirms requirement scales with label noise, not data.  
Follow-up: Swap from one-hot to raw float bits and observe whether smoothness helps.

**E3 Re-run CIFAR-10 authors’ 93.5 % CNN by hand**  
Hypothesis: Public code reproduces ≥90 % of claimed accuracy on CPU using only CPU inference.  
Setup: Clone 3-line CNN repo; train for 10 epochs on CPU (should finish in ≤3 h).  
Evaluation: accuracy ≥92 %, matching paper claim within ±1 %.  
If gap exists, document ablation factors: data augmentation order, weight decay, scheduler difference.  
Publish tiny-compute artifact + short technical note.  
Follow-up: Extend to CIFAR-100 to test scalability.

**E4 Repro survey of 50 arXiv “no-code” papers**  
Hypothesis: ≥30 % lack sufficient detail for reproduction.  
Setup: Pull last 12-month arXiv “transformers” papers that omit public repo. Create 10-point rubric (code link, hyper-params, data splits).  
Evaluation: score papers; compute statistics with 95 % CI.  
Outcome: Empirical estimate of reproducibility decline in era of LLM boom.  
Follow-up: Propose “minimal reproducibility checklist” for workshop.

**3. Timeline (6 months)**  
Month 0–1: Literature sweep + confirm zero-compute feasibility; collect 5 candidate papers and 2 datasets.  
Month 1–2: Run E1, finalize code, submit tiny-arXiv draft + tweetstorm.  
Month 2–3: Parallel execution E2 + E3; assemble repo with clean README.  
Month 3–4: Submit reproducibility workshop paper (ICLR, tiny-track, or ACL SRW).  
Month 4–5: E4 survey; compile stats, draft short journal-style note.  
Month 5–6: Polish drafts; submit to ACL findings or ML-Curious workshop; apply for travel grants if needed.

**4. Resources**  
Compute: laptop with 8 GB RAM, 256 GB SSD (public clouds optional).  
Tools: Jupyter, VS Code, GitHub, Colab free tier, GitHub Actions CI for notebooks (1000 min/mo free).  
Datasets/models: TinyStories, GLUE, CIFAR-10, CIFAR-100, MNIST, all downloadable in minutes.  
Expect 100–200 MB of storage per experiment, entirely offline doable.

**5. Risks and mitigations**  
Repro gap: run on same seeds; if not, flag and note.  
CPU timeout: cap epochs to 10; profile speed.  
Authorship: co-write with original paper authors for clarity.  
Over-claims: pre-register hypotheses and direction with open-science framework.  
Venue fit: target tiny-compute or survey tracks; else arXiv-only.

**6. Stretch ideas**  
- Replicate scaling laws on open-source Phi or Alpaca to probe instruction tuning at micro-scale (still CPU).  
- Meta-analyze bias in no-code papers by demographic affiliation of first authors, using public metadata.  
- Create “No-Compute-NLP” leaderboard with standardized CPU timing to encourage ecological resource usage.

**Optional next step**  
Clone the TinyStories repo today, create your first experiment card from the template we provided, then tweet the hypothesis and tag #LowComputeML — build in public to attract collaborators and reviewers.