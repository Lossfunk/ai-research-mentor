Turnkey plan: zero-compute, black-box LLM research on token-efficiency, robustness, and calibration

1) Problem framing and goals

Project scope
- Black-box, training-free evaluation and methods for improving token efficiency, robustness to prompt paraphrase, uncertainty calibration, and safety for LLMs accessed via public APIs. Benchmarks: MMLU, TruthfulQA, MT-Bench, BBQ, RealToxicityPrompts. We follow HELM-style, multi-metric evaluation to avoid single-number overfitting [1].

Objectives with numerical targets
- Token-efficiency: Achieve ≥95% task performance retention versus uncompressed prompts while reducing input tokens by 40–60% on MMLU and TruthfulQA (macro-averaged across subjects) [8][4][3].
- Self-consistency under fixed budget: Under a fixed total token budget B, majority-vote self-consistency with k small CoT generations (k≥5) outperforms a single long CoT by ≥2.0 absolute points on MMLU and MT-Bench, with ≤10% cost overhead relative to the long-CoT baseline [7][2][4].
- Paraphrase robustness: Reduce worst-case paraphrase sensitivity (max − min accuracy across N paraphrases per task) by ≥50% relative to single-prompt baseline on MMLU and TruthfulQA; target absolute variance ≤3 points [1][4][3].
- Calibration with abstention: Using black-box conformal prediction built from model scores or verbalized confidence, achieve 90% marginal coverage with ≤15% abstention rate on multiple-choice subsets (MMLU, TruthfulQA MC) [P5][P2].
- Safety and bias: Reduce average toxicity probability on RealToxicityPrompts by ≥50% via system-prompt interventions while keeping task accuracy on MMLU/TruthfulQA within −1.0 point of baseline; reduce stereotypical bias on BBQ (Δ-bias) by ≥25% without harming overall accuracy [5][6][1].

Success criteria
- At least three of the five objectives above are met simultaneously across two families of models (e.g., one frontier closed model and one open model hosted via an API), with pre-registered metrics and standardized evaluation protocols [1][2].

2) Experiments (5 total; 2–4 ablations each)

Experiment 1: Training-free prompt compression for token efficiency
- Hypothesis H1: Prompt compression achieves ≥95% performance retention with 40–60% token reduction on MMLU and TruthfulQA. Falsify if retention <95% or token reduction <40% [8][4][3].
- Setup:
  - Models: Two black-box models with logprobs support if available (e.g., GPT-4o-mini, Claude 3.5 via API; one open model via hosted API). Temperatures: 0.0 and 0.7.
  - Datasets: MMLU (public test split; report per-subject and macro average) [4]; TruthfulQA MC and generation sets [3].
  - Methods: 
    - Baseline: Original prompts (no compression).
    - Heuristic compression: rule-based deletion of redundant descriptors, instruction deduplication, and few-shot example pruning.
    - LLMLingua-style compression pipeline (zero-train) applied via an API or CPU-only execution; target 30–70% compression [8].
  - Hyperparameters: compression ratio targets r ∈ {0.3, 0.5, 0.7}; temperature ∈ {0.0, 0.7}; max_tokens fixed across methods.
- Ablations:
  1) Compression ratio r sweep and effect on accuracy and token cost.
  2) With vs without chain-of-thought (CoT) prompting under equal total token budget.
  3) Few-shot pruning: 0/1/3/5 examples.
  4) Model family sensitivity (closed vs open).
- Baselines: No compression; naive truncation (limit instruction to K tokens); published LLMLingua settings where applicable [8].
- Metrics: Accuracy (MMLU macro; TruthfulQA MC), token usage (input/output), cost; retention = method accuracy / baseline accuracy.
- Expected outcomes: 40–60% input token reduction with 95–98% accuracy retention; if CoT is used, retention ≥95% under matched token budget; failure indicates over-aggressive compression [8][4][3].

Experiment 2: Self-consistency vs single-shot under fixed budget
- Hypothesis H2: Under a fixed total token budget B, k-sample self-consistency (k∈{5,10,20}) achieves ≥2.0 points higher accuracy than single long-CoT generation on MMLU and MT-Bench QA categoricals. Falsify if gain <2.0 points [7][2][4].
- Setup:
  - Models: Same as Exp1. Temperature ∈ {0.6, 0.8}; top_p=0.95.
  - Datasets: MMLU (MC) [4]; MT-Bench single-turn QA subsets compatible with automatic correctness checks (e.g., math/code categories where reference exists) [2].
  - Methods:
    - Baseline: One CoT generation with max_output_tokens=T.
    - Self-consistency: k generations with shorter outputs; majority voting (MC) or answer string normalization.
  - Budgeting: Ensure total tokens (input+output) match within ±10% across conditions.
- Ablations:
  1) k ∈ {5,10,20} vs single-shot.
  2) Temperature ∈ {0.0, 0.6, 0.8}.
  3) With vs without CoT phrasing (e.g., “Let’s think step by step”).
- Baselines: Single-shot no-CoT; single-shot with CoT [7].
- Metrics: Accuracy; token cost; win rate on MT-Bench sub-questions if automated; tie-breaker rules.
- Expected outcomes: +2–5 points average on MMLU; diminishing returns beyond k=10; optimal temperature around 0.6–0.8 [7][4][2].

Experiment 3: Paraphrase-robust prompting and ensemble smoothing
- Hypothesis H3: Prompt-ensemble (N paraphrases, majority vote) reduces worst-case paraphrase drop by ≥50% and halves variance versus single-prompt baseline on MMLU and TruthfulQA. Falsify if worst-case drop reduction <50% [1][4][3].
- Setup:
  - Models: Same as Exp1.
  - Datasets: MMLU (subset of 10 representative subjects for throughput) [4]; TruthfulQA MC+gen [3].
  - Methods:
    - Generate N=10 semantically equivalent paraphrases of the instruction and exemplars using a separate LLM; audit with lexical overlap and semantic similarity filters.
    - Inference via each paraphrase; aggregate via majority vote (MC) or confidence-weighted voting (if logprobs available).
  - Controls: Single canonical prompt; CoT vs no-CoT.
- Ablations:
  1) N ∈ {3,5,10}.
  2) Paraphrase diversity (high vs low lexical change).
  3) Vote aggregator: simple majority vs confidence-weighted (if logprobs).
- Baselines: Canonical prompt; random paraphrase selection; average of 3 paraphrases without filtering.
- Metrics: Mean accuracy; std dev; worst-case drop; pairwise agreement; token cost per example.
- Expected outcomes: Variance reduced by 40–60%; worst-case drop reduced by ≥50%; small token overhead mitigated by combining with token compression from Exp1 [1][4][3].

Experiment 4: Black-box conformal prediction for calibrated abstention
- Hypothesis H4: Split-conformal set prediction or abstention yields ≥90% marginal coverage with ≤15% abstention on MMLU MC and TruthfulQA MC. Falsify if empirical coverage <90% or abstention >15% [P5][P2].
- Setup:
  - Models: Same as Exp1; collect either (a) token-level logprobs for choice tokens when available; or (b) verbalized confidence elicitation (“Probability answer is correct: X%”) with calibration mapping.
  - Datasets: MMLU MC (held-out calibration set 20%; evaluation 80%) [4]; TruthfulQA MC [3].
  - Methods:
    - Score function: negative margin between top-2 options (from logprobs) or learned monotone mapping from verbalized confidence to probability.
    - Use inductive conformal prediction with calibration split to determine thresholds for 1-set vs abstain (or prediction sets) [P5].
- Ablations:
  1) Score variants: logprob margin vs verbalized confidence.
  2) Calibration sizes: 5%, 10%, 20%.
  3) Temperature effects on scores.
- Baselines: Fixed-confidence thresholding without conformal correction; naive abstention by “I don’t know” detection.
- Metrics: Empirical coverage; average set size; abstention rate; Brier score/ECE if probabilities are available.
- Expected outcomes: 90–93% coverage with 10–15% abstention using logprob margins; 88–92% with verbalized confidence; improved reliability vs non-conformal baselines [P5][P2].

Experiment 5: Safety and bias auditing with instruction/system-prompt interventions
- Hypothesis H5: Safety-tuned system prompts reduce mean toxicity probability by ≥50% on RealToxicityPrompts, while Δ-bias on BBQ decreases by ≥25%, with ≤1.0 point accuracy loss on MMLU/TruthfulQA. Falsify if toxicity reduction <50% or accuracy drop >1.0 point [5][6][4][3].
- Setup:
  - Models: Same as Exp1.
  - Datasets: RealToxicityPrompts (use toxicity classifier such as Perspective API or open replicas; report mean P(toxic)) [5]; BBQ for bias (report Δ-bias and accuracy) [6]; MMLU and TruthfulQA for utility [4][3].
  - Methods:
    - System-prompt templates emphasizing harmlessness, neutrality, and instruction to abstain on unsafe content.
    - Refusal+redirect templates; style constraints limiting demeaning language.
- Ablations:
  1) Safety prompt variants: short vs long; refusal emphasis vs redirection.
  2) Temperature ∈ {0.0, 0.7}.
  3) With vs without token compression (to check interaction with safety).
- Baselines: Default system prompt; safety-off settings where available.
- Metrics: Mean toxicity probability; % toxic continuations; BBQ Δ-bias; accuracy on MMLU/TruthfulQA; token costs.
- Expected outcomes: 50–70% toxicity reduction; 25–40% Δ-bias reduction; ≤1 point average utility loss [5][6][4][3].

3) Timeline (6 months; bi-weekly checkpoints)

Month 1: Protocols, infra, and preregistration
- Week 1–2: Define task subsets; finalize metrics; preregister hypotheses, datasets, and analysis on OSF. Deliverable: Protocol v1.0 with acceptance criteria.
- Week 3–4: Build reproducible evaluation harness (Python 3.11, Hugging Face datasets, deterministic seeding; API wrappers; logging). Deliverable: CI-validated repo; HELM-style reporting templates [1].
- Go/no-go: If API access/logprobs unavailable, pivot conformal experiment to verbalized confidence variant [P5].

Month 2: Baseline replication and data splits
- Week 5–6: Replicate baseline accuracies on MMLU/TruthfulQA; measure token budgets. Deliverable: Baseline report ±95% CIs.
- Week 7–8: Implement paraphrase generation/filters; finalize calibration splits. Deliverable: Calibration datasets and paraphrase bank.

Month 3: Experiments 1–2
- Week 9–10: Run prompt compression sweeps; analyze retention vs compression. Checkpoint: r–accuracy curves; Pareto frontier.
- Week 11–12: Run self-consistency under budget. Deliverable: Budgeted accuracy gains; ablation plots.
- Go/no-go: If H1 or H2 clearly falsified, adjust compression heuristics or increase k modestly.

Month 4: Experiments 3–4
- Week 13–14: Paraphrase-robust ensembles; variance/worst-case analysis. Deliverable: Robustness report.
- Week 15–16: Conformal prediction; coverage/abstention tradeoffs. Deliverable: Calibration curves; coverage guarantees [P5][P2].
- Decision: If verbalized confidence underperforms, prioritize models with logprob APIs for final runs.

Month 5: Experiment 5 and integration
- Week 17–18: Safety/bias interventions; run toxicity and BBQ evaluations. Deliverable: Safety vs utility tradeoff analysis [5][6].
- Week 19–20: Integrated recipe combining compression + self-consistency + paraphrase ensembles + calibration. Deliverable: End-to-end pipeline; Pareto analysis.

Month 6: Writing, robustness checks, and release
- Week 21–22: Stress tests on extra subjects; sensitivity to temperature, k, r; reproducibility reruns with new API snapshots.
- Week 23–24: Paper draft, artifacts, and open-source release (dataset cards, scripts, configs). Deliverable: Submission-ready manuscript; replication package.
- Submission go/no-go: Proceed if at least three primary objectives met; otherwise reframe to negative results paper with preregistered deviations.

4) Resources

Compute
- GPUs: None required. CPU-only is sufficient for orchestration and analysis.
- API inference: 1–3 models; estimated 5–10M tokens total over 6 months (depends on breadth; throttle with subsets).
- Memory: 8–16 GB RAM sufficient for local processing.

Datasets and versions/splits
- MMLU (Hendrycks et al., v1; public test). Use official subject taxonomy; retain dev subsets for pilot [4].
- TruthfulQA (2021; MC and generation sets). Use standard scoring scripts [3].
- MT-Bench (2023; use categories with objective references; for judge-based scoring, fix the judge model version) [2].
- BBQ (2021; hand-built bias benchmark). Report Δ-bias and accuracy as per paper [6].
- RealToxicityPrompts (2020; ≥100k prompts). Use fixed 25k-sample subset and Perspective API or equivalent classifier for toxicity probability [5].
- Calibration splits: For conformal, reserve 10–20% of each dataset for calibration only (no tuning on eval) [P5].

Tooling and libraries
- Python 3.11, Hugging Face datasets ≥2.19, NumPy ≥1.26, pandas ≥2.2, SciPy ≥1.11, scikit-learn ≥1.4.
- OpenAI/Anthropic/Together/other API SDKs pinned to specific versions; log all model snapshot IDs and parameters.
- Experiment tracking: Weights & Biases or MLflow; YAML configs; deterministic random seeds.
- Toxicity scoring: Perspective API client (or Detoxify as a sensitivity analysis).
- Conformal utilities: lightweight custom code following split-conformal recipes [P5]; document score functions.

Estimated API budget
- Token planning per experiment:
  - Exp1: ~1.5M tokens (compression sweeps across subjects).
  - Exp2: ~2M tokens (k-sampling).
  - Exp3: ~1.5M tokens (N paraphrases).
  - Exp4: ~0.5–1M tokens (scoring + calibration).
  - Exp5: ~0.5M tokens (toxicity/bias + utility checks).
- Adjust by subsetting subjects (e.g., 10–20 subjects) to keep totals ≤5M tokens.

5) Risks and mitigations

- API variability across time
  - Probability: Medium; Impact: Medium–High.
  - Mitigation: Log model version/timestamp; rerun key results within 7 days; report confidence intervals and versioned sensitivity analysis [1][2].

- Lack of logprob access
  - Probability: Medium; Impact: Medium.
  - Mitigation: Use verbalized confidence with monotone calibration; switch to models providing logprobs; report both variants [P5].

- Prompt leakage/contamination concerns
  - Probability: Medium; Impact: Medium.
  - Mitigation: Use public benchmarks with established leakage checks; include negative controls; document any suspect items [1][4].

- Cost overrun due to token usage
  - Probability: Medium; Impact: Medium.
  - Mitigation: Early pilot with 5–10 subjects; enforce token budgets; reuse outputs where allowed; cache results; prefer shorter outputs.

- Measurement error in safety metrics
  - Probability: Medium; Impact: Medium.
  - Mitigation: Use multiple toxicity detectors (Perspective + alternative); compute confidence intervals; perform manual audit on a random subset [5].

- Overfitting to paraphrase bank
  - Probability: Medium; Impact: Medium.
  - Mitigation: Hold out paraphrase templates generated by a different model; release paraphrases to enable external replication.

6) Integrated recipe and scaling study

- Integration: The final recipe layers (a) compression to reduce tokens (Exp1), (b) self-consistency to recover/improve accuracy under a fixed budget (Exp2), (c) paraphrase ensembles to stabilize variance (Exp3), and (d) conformal abstention to guarantee coverage (Exp4). Safety prompts (Exp5) are orthogonal and can be applied to all stages.
- Scaling study:
  - Pareto analysis over (token cost, accuracy, coverage, toxicity). Map fronts for r ∈ [0.3,0.7], k ∈ [1,20], N ∈ [1,10], and abstention budget ∈ [5%,20%].
  - Sensitivity analysis: temperature, top_p, and per-model family robustness; report cross-model averages with HELM-like multi-metric sheets [1].
  - Cross-dataset generalization: Evaluate parameter settings chosen on MMLU on TruthfulQA/BBQ to test transfer.
- Expected consolidated outcome: A simple, training-free pipeline that provides (i) strong accuracy retention at 40–60% lower token cost, (ii) better reliability (lower paraphrase variance), and (iii) calibrated abstention with target coverage, while improving safety at marginal utility loss. If objectives are not met, negative results will be documented with preregistered hypotheses and ablations (still valuable as evaluation science) [1][2][3][4].

Why this is meaningful with zero compute
- All methods are training-free and rely on public APIs and open benchmarks; the intellectual contribution is rigorous evaluation design, token-efficiency methods, and calibration/robustness analysis, which align with community best practice (HELM) and widely studied techniques like self-consistency and conformal prediction [1][7][P5].

Citations
- [P1] Unveiling and Harnessing Hidden Attention Sinks: Enhancing Large Language Models without Training through Attention Calibration — http://arxiv.org/abs/2406.15765v1
- [P2] Kandinsky Conformal Prediction: Efficient Calibration of Image Segmentation Algorithms — http://arxiv.org/abs/2311.11837v1
- [P3] Uncertainty-Calibrated Test-Time Model Adaptation without Forgetting — http://arxiv.org/abs/2403.11491v2
- [P4] Calibrated Uncertainty Quantification for Operator Learning via Conformal Prediction — http://arxiv.org/abs/2402.01960v2
- [P5] Self-Calibrating Conformal Prediction — http://arxiv.org/abs/2402.07307v3
- [1] Holistic Evaluation of Language Models (HELM) — https://arxiv.org/abs/2211.09110
- [2] Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena — https://arxiv.org/abs/2306.05685
- [3] TruthfulQA: Measuring How Models Mimic Human Falsehoods — https://arxiv.org/abs/2109.07958
- [4] Measuring Massive Multitask Language Understanding (MMLU) — https://arxiv.org/pdf/2009.03300
- [5] RealToxicityPrompts — https://arxiv.org/pdf/2009.11462
- [6] BBQ: A Hand-Built Bias Benchmark for Question Answering — https://arxiv.org/abs/2110.08193
- [7] Self-Consistency Improves Chain-of-Thought Reasoning in LMs — https://arxiv.org/pdf/2203.11171
- [8] LLMLingua: Compressing Prompts for Accelerated Inference — https://arxiv.org/html/2310.05736