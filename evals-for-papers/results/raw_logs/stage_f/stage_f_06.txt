Below is a concise, ready-to-adapt set of double-blind compliance notes and checklists for a human-centered AI evaluation paper, covering manuscript, anonymized artifacts, and appendices. It aligns with common HCI/AI venue policies (e.g., CHI/HRI) and reproducibility expectations (e.g., NeurIPS checklist) [1][2][5].

Double-blind compliance notes (to include in your paper’s submission form or a brief “Compliance” footnote)
- Anonymization scope. The manuscript, figures, supplementary files, and appendices remove names, affiliations, acknowledgments, funding sources, and IRB protocol numbers; institution- or lab-identifying details are generalized (e.g., “a large research university,” “our institution’s IRB”) [1][5].  
- Self-citations. Prior work by the authors is cited in the third person without language implying identity or continuity (e.g., “Smith et al. (2022) showed …”), and we avoid citing non-anonymous preprints that would unblind the authors [1][5].  
- Ethics/IRB. We include a minimal ethics statement that does not reveal institutions or protocol IDs in the anonymized submission; full details will be provided upon acceptance if venue policy requires [5].  
- COI handling. Conflicts are declared in the submission system only (not in the PDF) to avoid identity leakage [4].  
- File- and metadata hygiene. We scrubbed document and image metadata and removed identifying watermarks, tracked changes, and author fields [1][5].  
- Public claims. The submission and artifacts avoid links to personal/lab pages or social media and avoid statements like “our GitHub,” which would break anonymity [1][5].

Anonymous artifact release plan (code, data, and materials during review)
- Hosting. We publish code and materials via an anonymous mirror that strips repository identity (e.g., an anonymous.4open.science mirror of a GitHub repo). The mirror disables issues and removes stars/forks that could deanonymize authors [3].  
- Reproducibility scope. We provide a minimal working example that reproduces the main quantitative tables/figures with fixed seeds, exact package versions, and a one-command run script (e.g., ./reproduce.sh) [2].  
- Environments. We include pinned dependencies (requirements.txt/lockfile), OS and hardware notes, and optionally a container spec (Dockerfile) to minimize environment drift [2].  
- Data handling. For human-subject data that cannot be shared, we include: (a) a synthetic or redacted dataset with the same schema, (b) a data dictionary, (c) unit tests validating pipeline behavior on synthetic data, and (d) scripts that can run without private data and still reproduce core analyses [2].  
- Privacy and safety. We verify that prompts, transcripts, and screenshots omit PII and platform/user handles. Any third-party platform screenshots are sanitized (no usernames, IDs, or URLs) [1][5].  
- Post-acceptance. We will de-anonymize the repository and replace the anonymous link with a permanent, citable URL after acceptance, consistent with venue policy [1][2][5].

Anonymized appendices: what to include and how to anonymize
- A1. Study protocol. Step-by-step session flow; participant instructions; counterbalancing; timing. Remove site/lab names and identifiable facility information [1][5].  
- A2. Materials and stimuli. Prompts, task scripts, UI screenshots, and rubrics. Redact watermarks, platform/user handles, or any visible personal info; provide alt-text for accessibility [1][5].  
- A3. Measures. Survey items, scoring keys, open-ended coding schemes. Include licensing/citation notes for standardized scales without naming your institution [1][5].  
- A4. Analysis plan. Exclusion criteria, preregistration summary (with anonymized link or blinded registration ID if allowed), statistical tests, power analysis, and error bars/CI conventions [2].  
- A5. Data processing. Preprocessing scripts, annotation guidelines, inter-rater reliability computation, and QA checks that run on synthetic/redacted data [2].  
- A6. Risk mitigation. Consent language (generic), debrief script, compensation details (ranges only), and adverse event handling—without naming IRBs or institutions [5].  
- A7. Reproducibility checklist mapping. A short table that maps what you provide to checklist items (code, data availability, compute details, hyperparameters, evaluation protocols) [2].

Pre-submission anonymization checklist (practical steps)
- PDF and media metadata. exiftool -all= file.pdf; remove author/organization fields in Word/LaTeX; export figures anew to strip metadata [1][5].  
- References. Third-person self-cites; no personal websites; avoid unpublished/self-hosted preprints that would deanonymize [1][5].  
- Links. Use neutral, unlisted links to the anonymous artifact mirror. Avoid link shorteners and analytics that could reveal identity [1][5].  
- Filenames. Avoid names like OurLabStudy-v3.pdf; use generic names (paper.pdf, materials.zip) [1].  
- Repository audit. Search the artifact for author names/emails/orgs (git log --format='%an <%ae>' | sort -u; ripgrep -nE '(@|.edu|.ac.|AuthorLastName)'); scrub or rebase as needed; ensure LICENSE and README lack identifiers [3].  
- COI and Acknowledgments. Leave both out of the PDF; enter COIs in the submission system; add acknowledgments only after acceptance [1][4][5].

Ready-to-paste “Compliance” block (example text)
- This submission complies with the venue’s double-blind policy. We removed all author-identifying information, including names, affiliations, acknowledgments, funding sources, and IRB identifiers. Prior work by the authors is cited in the third person. The ethics statement is included in generic form to maintain anonymity.  
- Anonymous artifacts: An anonymized repository provides code, scripts, and synthetic/redacted data sufficient to reproduce primary results, with pinned dependencies and a one-command run script. Human-subject data that cannot be shared are replaced by synthetic examples and a complete analysis pipeline. The repository will be de-anonymized and assigned a permanent URL upon acceptance.

Three concrete, falsifiable pre-submission “experiments” to validate compliance
1) Anonymity audit  
- Hypothesis: Fewer than 5% of blinded raters can infer any author identity or institution from the paper and artifacts.  
- Method: Recruit 10–15 colleagues unfamiliar with the project; provide the full anonymized package. Ask them to guess author names, affiliations, labs, and country; record accuracy.  
- Metric: Max correct-guess rate across identity categories; qualitative notes on leakage sources.  
- Pass criterion: ≤5% correct on any identity category; otherwise, patch the identified leakage and re-audit.

2) Reproducibility dry run  
- Hypothesis: A new machine reproduces the main table/figure within 2 hours following the README.  
- Method: Provide the anonymous artifact to a neutral lab member; prohibit use of private data; measure wall-clock time; record deviations.  
- Metric: Time-to-first-success; exact match on seeds or tolerance bands (e.g., ±0.1 accuracy, ±0.01 correlation); number of manual interventions.  
- Pass criterion: Reproduction under time budget with at most one manual intervention; results within predefined tolerance; otherwise improve environment pinning and docs.

3) Privacy leakage scan for materials  
- Hypothesis: No PII or institutional identifiers exist in appendices/materials.  
- Method: Run automated scans (regex + NER for person/org/location; image OCR for screenshots) and manual review.  
- Metric: Count of PII/identifiers; k-anonymity/re-id risk for any tabular release; log of fixes.  
- Pass criterion: Zero detected identifiers and acceptable re-id risk; otherwise redact/aggregate and document the change log.

Literature anchors (recommended)
- CHI Anonymization Policy (CHI 2024): clear guidance on what to remove, how to handle self-citations, and how to anonymize supplemental materials [1].  
- NeurIPS Paper Checklist: concrete reproducibility expectations for code/data/specification that you can map into your appendices and artifact README [2].

Citations
[1] CHI Anonymization Policy — https://chi2024.acm.org/submission-guides/chi-anonymization-policy/  
[2] NeurIPS Paper Checklist — https://neurips.cc/public/guides/PaperChecklist  
[3] Anonymous GitHub mirroring (anonymous.4open.science) — https://github.com/orgs/community/discussions/175306  
[4] ACM Peer Review Policy FAQ — https://www.acm.org/publications/policies/peer-review-faq  
[5] HRI 2025 Authorship and Anonymization — https://humanrobotinteraction.org/2025/anonymization

If you share the target venue, I can tailor the compliance notes to that policy’s exact wording and expectations.