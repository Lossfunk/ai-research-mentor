I'll create a comprehensive risk assessment and mitigation checklist for your climate-research AI paper. To provide **specific, actionable guidance based on your actual work**, please upload your paper PDF.

In the meantime, here's a complete framework:

---

## âš ï¸ REVIEWER-FACING RISKS & MITIGATION: Climate Research AI Paper

### ðŸŒ CRITICAL: Climate AI Papers Face Unique Scrutiny

Climate research AI papers encounter **exceptional reviewer skepticism** due to:
- **High-stakes claims**: Climate predictions affect policy, billions of dollars
- **Interdisciplinary gaps**: ML reviewers may lack climate expertise (and vice versa)
- **Reproducibility concerns**: Climate data is complex, models are computationally expensive
- **Overclaiming risks**: "AI solves climate change" headlines damage credibility
- **Domain validation**: ML metrics alone are insufficient; physical plausibility required
- **Ethical implications**: Misuse could delay climate action or spread misinformation

---

## ðŸŽ¯ SECTION 1: MAJOR REVIEWER-FACING RISKS

### Risk Matrix: Likelihood Ã— Impact

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    IMPACT                               â”‚
â”‚           Low      Medium      High      Critical       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  High    â”‚ R7      â”‚ R3, R5   â”‚ R1, R2  â”‚ R4          â”‚
â”‚  Medium  â”‚ R12     â”‚ R9, R10  â”‚ R6, R8  â”‚             â”‚
â”‚  Low     â”‚ R15     â”‚ R13, R14 â”‚ R11     â”‚             â”‚
â”‚                                                         â”‚
â”‚  LIKELIHOOD                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Priority: Address Critical Impact + High Likelihood first
```

---

### ðŸ”´ CRITICAL RISKS (Desk Reject Potential)

#### **RISK 1: Overclaiming Climate Impact**

**Description**: Claiming AI "solves" climate problems or making unrealistic impact statements.

**Why reviewers care**:
- Climate community is skeptical of AI hype
- Overclaiming damages credibility of entire field
- Reviewers have seen many overpromising papers

**Red flags**:
- âŒ "Our model will reduce global emissions by X%"
- âŒ "AI-driven climate solution"
- âŒ "Revolutionary approach to climate change"
- âŒ Extrapolating small improvements to global scale
- âŒ Ignoring deployment barriers (policy, infrastructure, cost)

**Example from reviews**:
> "The authors claim their model will 'significantly impact climate policy' but provide no evidence of real-world deployment or stakeholder engagement. This is speculative and overstated."

**Mitigation**: See Checklist Item #1

---

#### **RISK 2: Insufficient Domain Validation**

**Description**: Relying solely on ML metrics (MSE, RÂ²) without validating physical plausibility.

**Why reviewers care**:
- Climate models must obey physical laws (conservation of energy, mass)
- Statistical fit doesn't guarantee physical correctness
- Domain experts will reject physically implausible results

**Red flags**:
- âŒ Only reporting ML metrics (RMSE, MAE, RÂ²)
- âŒ No comparison to physics-based models
- âŒ Predictions violate known physical constraints
- âŒ No climate scientist co-authors or consultation
- âŒ Ignoring domain-specific evaluation metrics

**Example from reviews**:
> "The model achieves low RMSE but predicts negative precipitation in some regions, which is physically impossible. The authors don't discuss physical consistency."

**Mitigation**: See Checklist Item #2

---

#### **RISK 3: Data Leakage / Temporal Issues**

**Description**: Improper train/test splits that leak future information into training.

**Why reviewers care**:
- Climate data has strong temporal autocorrelation
- Spatial autocorrelation (nearby locations are similar)
- Random splits are invalid for time series
- This is a common mistake that invalidates results

**Red flags**:
- âŒ Random train/test split on time series data
- âŒ Testing on interpolated time points (not true forecasting)
- âŒ Using future data for feature engineering
- âŒ Spatial leakage (training and test locations overlap)
- âŒ Not accounting for autocorrelation in error estimates

**Example from reviews**:
> "The authors use random 80/20 split on time series data. This is inappropriate and leads to overly optimistic results. True forecasting requires temporal holdout."

**Mitigation**: See Checklist Item #3

---

#### **RISK 4: Missing Uncertainty Quantification**

**Description**: Providing point predictions without uncertainty estimates.

**Why reviewers care**:
- Climate predictions are inherently uncertain
- Decision-makers need uncertainty for risk assessment
- Overconfident predictions are dangerous for policy
- This is a fundamental requirement in climate science

**Red flags**:
- âŒ Only point predictions (no confidence intervals)
- âŒ No discussion of prediction uncertainty
- âŒ Ignoring model uncertainty, data uncertainty, scenario uncertainty
- âŒ Not calibrating probabilistic predictions
- âŒ No sensitivity analysis

**Example from reviews**:
> "Climate predictions without uncertainty quantification are useless for decision-making. The authors must provide confidence intervals and discuss sources of uncertainty."

**Mitigation**: See Checklist Item #4

---

### ðŸŸ  HIGH RISKS (Major Revision Likely)

#### **RISK 5: Inadequate Baseline Comparisons**

**Description**: Not comparing to established climate models or using weak baselines.

**Why reviewers care**:
- Climate community has decades of model development
- Must show improvement over existing methods
- Comparing only to naive baselines is insufficient

**Red flags**:
- âŒ Only comparing to linear regression or simple ML
- âŒ Not comparing to physics-based models (GCMs, RCMs)
- âŒ Not comparing to state-of-the-art ML climate models
- âŒ Cherry-picking weak baselines
- âŒ Unfair comparisons (different data, different metrics)

**Example from reviews**:
> "The authors compare only to linear regression. They must compare to established climate models (e.g., CMIP6 ensemble) and recent ML approaches (e.g., FourCastNet, GraphCast)."

**Mitigation**: See Checklist Item #5

---

#### **RISK 6: Reproducibility Issues**

**Description**: Insufficient detail to reproduce results; no code/data release.

**Why reviewers care**:
- Climate data is complex (preprocessing matters)
- Computational cost is high (need exact setup)
- Reproducibility crisis in ML
- Many venues now require code release

**Red flags**:
- âŒ Vague data description ("we used climate data")
- âŒ Missing preprocessing details
- âŒ No hyperparameter details
- âŒ No code release plan
- âŒ Proprietary data with no access path
- âŒ Missing random seeds, library versions

**Example from reviews**:
> "The authors don't specify which climate dataset they used (ERA5? CMIP6?), preprocessing steps, or hyperparameters. Results cannot be reproduced."

**Mitigation**: See Checklist Item #6

---

#### **RISK 7: Ignoring Climate-Specific Challenges**

**Description**: Treating climate prediction as generic time series forecasting.

**Why reviewers care**:
- Climate has unique challenges (non-stationarity, extremes, teleconnections)
- Generic ML approaches often fail on climate data
- Shows lack of domain understanding

**Red flags**:
- âŒ No discussion of non-stationarity (climate change trends)
- âŒ Ignoring extreme events (which matter most for impacts)
- âŒ Not handling missing data (common in climate observations)
- âŒ Ignoring spatial dependencies (teleconnections)
- âŒ Not addressing seasonal cycles properly

**Example from reviews**:
> "The authors apply standard LSTM without addressing non-stationarity due to climate change. The model will fail on future data outside the training distribution."

**Mitigation**: See Checklist Item #7

---

#### **RISK 8: Weak Evaluation Metrics**

**Description**: Using inappropriate or insufficient metrics for climate applications.

**Why reviewers care**:
- Different metrics matter for different applications
- Average metrics hide failures on extremes
- Climate community has established evaluation protocols

**Red flags**:
- âŒ Only reporting average metrics (RMSE, MAE)
- âŒ Not evaluating extreme events separately
- âŒ Ignoring spatial patterns (only point-wise metrics)
- âŒ Not using climate-specific metrics (e.g., skill scores)
- âŒ No stratification by season, region, event type

**Example from reviews**:
> "The authors report only global RMSE. They must evaluate extremes (heatwaves, droughts), spatial patterns, and seasonal performance separately."

**Mitigation**: See Checklist Item #8

---

### ðŸŸ¡ MEDIUM RISKS (Reviewer Flags, Addressable)

#### **RISK 9: Insufficient Related Work**

**Description**: Missing key climate AI papers or climate science literature.

**Why reviewers care**:
- Shows lack of domain knowledge
- May duplicate existing work
- Misses important context

**Red flags**:
- âŒ Only citing ML papers (no climate science)
- âŒ Missing recent climate AI work (2022-2024)
- âŒ Not citing established climate models (CMIP, ERA5)
- âŒ Ignoring relevant workshops (Climate Change AI)

**Mitigation**: See Checklist Item #9

---

#### **RISK 10: Unclear Practical Utility**

**Description**: Not explaining how results would be used in practice.

**Why reviewers care**:
- Climate research should inform action
- Reviewers want to see real-world relevance
- "Interesting ML" is not enough

**Red flags**:
- âŒ No discussion of use cases
- âŒ Not explaining who would use this (policymakers, farmers, etc.)
- âŒ Ignoring deployment constraints (latency, cost, interpretability)
- âŒ No stakeholder engagement

**Mitigation**: See Checklist Item #10

---

#### **RISK 11: Ethical Concerns Not Addressed**

**Description**: Not discussing potential misuse or ethical implications.

**Why reviewers care**:
- Climate AI can be misused (greenwashing, delaying action)
- Predictions affect vulnerable populations
- Ethical review increasingly required

**Red flags**:
- âŒ No ethics statement
- âŒ Not discussing potential harms
- âŒ Ignoring equity implications (who benefits?)
- âŒ No discussion of misuse (e.g., by fossil fuel industry)

**Mitigation**: See Checklist Item #11

---

#### **RISK 12: Computational Cost Not Reported**

**Description**: Not reporting training time, energy consumption, carbon footprint.

**Why reviewers care**:
- Climate AI should be sustainable
- Computational cost affects accessibility
- Carbon footprint is ironic for climate research

**Red flags**:
- âŒ No training time reported
- âŒ No hardware specifications
- âŒ No energy consumption or carbon footprint
- âŒ Ignoring efficiency vs. accuracy tradeoffs

**Mitigation**: See Checklist Item #12

---

### ðŸŸ¢ LOW RISKS (Minor Issues, Easy Fixes)

#### **RISK 13: Writing Quality Issues**

**Description**: Poor writing, unclear explanations, jargon.

**Mitigation**: Proofread, get feedback from non-experts, use clear language.

---

#### **RISK 14: Figure Quality**

**Description**: Low-resolution figures, unclear visualizations.

**Mitigation**: High-res figures (300+ DPI), clear labels, colorblind-friendly palettes.

---

#### **RISK 15: Missing Acknowledgments**

**Description**: Not acknowledging data sources, funding, collaborators.

**Mitigation**: Comprehensive acknowledgments section.

---

## âœ… SECTION 2: COMPREHENSIVE MITIGATION CHECKLIST

### Pre-Submission Checklist (4 Weeks Before Deadline)

---

### âœ… **CHECKLIST ITEM #1: Avoid Overclaiming**

**Risk mitigated**: RISK 1 (Critical)

**Actions**:

- [ ] **Remove hyperbolic language**
  - âŒ "Revolutionary," "game-changing," "solves climate change"
  - âœ… "Improves upon," "demonstrates potential," "contributes to"

- [ ] **Scope claims appropriately**
  - âŒ "Our model will reduce global emissions"
  - âœ… "Our model improves precipitation forecasts, which could inform water management"

- [ ] **Add limitations section** (0.5-1 page)
  - What your model does NOT do
  - Deployment barriers (policy, infrastructure, cost)
  - Generalization limits (geographic, temporal)
  - Uncertainty in impact estimates

- [ ] **Distinguish technical contribution from impact**
  - Technical: "We achieve 15% lower RMSE than baselines"
  - Impact: "Improved forecasts *could* help farmers plan irrigation (if deployed, if adopted, if...)"

- [ ] **Avoid extrapolation without evidence**
  - âŒ "Scaling this to all farms would save X tons of COâ‚‚"
  - âœ… "In our study region, improved forecasts reduced water use by Y%. Broader impact requires further study."

- [ ] **Get feedback from climate scientists**
  - Ask: "Do our claims sound reasonable to domain experts?"
  - Revise based on feedback

**Example revision**:

âŒ **Before**:
> "Our AI model will revolutionize climate prediction and significantly reduce global emissions by enabling better renewable energy planning."

âœ… **After**:
> "We present a machine learning model that improves short-term wind speed forecasts by 15% RMSE over operational baselines. More accurate forecasts could help grid operators better integrate wind energy, though deployment and impact assessment are left to future work."

---

### âœ… **CHECKLIST ITEM #2: Validate Physical Plausibility**

**Risk mitigated**: RISK 2 (Critical)

**Actions**:

- [ ] **Check physical constraints**
  - Energy conservation (radiation balance)
  - Mass conservation (water cycle)
  - Thermodynamic laws (temperature-pressure relationships)
  - Non-negativity (precipitation, humidity â‰¥ 0)
  - Bounded variables (humidity â‰¤ 100%)

- [ ] **Compare to physics-based models**
  - CMIP6 ensemble (climate projections)
  - ERA5 reanalysis (historical data)
  - Regional climate models (RCMs)
  - Weather forecast models (GFS, ECMWF)

- [ ] **Use domain-specific metrics**
  - Anomaly correlation coefficient (ACC)
  - Root mean square skill score (RMSSS)
  - Continuous ranked probability score (CRPS)
  - Extreme event detection (POD, FAR, CSI)

- [ ] **Visualize spatial patterns**
  - Maps showing predictions vs. observations
  - Difference maps (model - observation)
  - Climate scientists can spot implausible patterns

- [ ] **Consult domain experts**
  - Co-author with climate scientist (ideal)
  - Or: acknowledgment of expert consultation
  - Ask: "Do these results make physical sense?"

- [ ] **Add physical consistency checks to paper**
  - New subsection: "Physical Plausibility Analysis"
  - Show that predictions obey known constraints
  - Discuss any violations and why they occur

**Example addition to paper**:

```latex
\subsection{Physical Plausibility Analysis}

We verify that our model predictions satisfy key physical constraints:

\textbf{Energy conservation:} We compute the radiation budget 
(incoming solar - outgoing longwave - latent heat - sensible heat) 
for our predictions. The residual is <5 W/mÂ² (within observational 
uncertainty), indicating energy balance is preserved.

\textbf{Mass conservation:} We verify that precipitation - evaporation 
= change in atmospheric water vapor + runoff. Our model achieves 
closure within 8% (comparable to reanalysis products).

\textbf{Non-negativity:} All predicted precipitation and humidity 
values are non-negative. We use ReLU activation in the output layer 
to enforce this constraint.

\textbf{Comparison to physics-based models:} Figure X shows our 
predictions compared to CMIP6 ensemble mean. Spatial patterns are 
consistent (spatial correlation r=0.89), though our model slightly 
underestimates precipitation in tropical regions.
```

---

### âœ… **CHECKLIST ITEM #3: Proper Train/Test Splits**

**Risk mitigated**: RISK 3 (Critical)

**Actions**:

- [ ] **Use temporal holdout for time series**
  - Train: 1980-2010
  - Validation: 2011-2015
  - Test: 2016-2020
  - NO overlap, NO random shuffling

- [ ] **Use spatial holdout if applicable**
  - Train on some regions, test on others
  - Or: leave-one-out cross-validation by region
  - Ensures generalization to new locations

- [ ] **Avoid future information leakage**
  - Don't use future data for normalization (compute stats on train only)
  - Don't use future data for feature engineering
  - Don't use contemporaneous data from test set

- [ ] **Account for autocorrelation**
  - Use block bootstrap for confidence intervals
  - Or: effective sample size correction
  - Standard errors are too small if autocorrelation ignored

- [ ] **Document split clearly**
  - Table showing train/val/test periods
  - Map showing spatial splits (if applicable)
  - Justify split choice (e.g., "test on recent years to assess performance under climate change")

- [ ] **Test on true forecasting task**
  - If predicting T+1, train on (t-k, ..., t), predict t+1
  - Not: train on random subset of all time points

**Example table**:

```latex
\begin{table}[h]
\centering
\caption{Train/Validation/Test Split}
\begin{tabular}{lccc}
\toprule
\textbf{Split} & \textbf{Time Period} & \textbf{Samples} & \textbf{Purpose} \\
\midrule
Train & 1980-2010 & 11,323 days & Model training \\
Validation & 2011-2015 & 1,826 days & Hyperparameter tuning \\
Test & 2016-2020 & 1,826 days & Final evaluation \\
\bottomrule
\end{tabular}
\label{tab:split}
\end{table}

\textbf{Rationale:} We use temporal holdout to evaluate true forecasting 
performance. Testing on recent years (2016-2020) assesses generalization 
under climate change, as this period includes record-breaking temperatures 
and extreme events not seen in training data.
```

---

### âœ… **CHECKLIST ITEM #4: Uncertainty Quantification**

**Risk mitigated**: RISK 4 (Critical)

**Actions**:

- [ ] **Provide prediction intervals**
  - 95% confidence intervals for all predictions
  - Methods: quantile regression, conformal prediction, Bayesian NN, ensembles

- [ ] **Decompose uncertainty sources**
  - Model uncertainty (epistemic): different architectures, hyperparameters
  - Data uncertainty (aleatoric): measurement error, natural variability
  - Scenario uncertainty: different emission scenarios (RCP 2.6, 4.5, 8.5)

- [ ] **Calibrate probabilistic predictions**
  - Reliability diagrams (predicted probability vs. observed frequency)
  - Calibration metrics (ECE, Brier score)
  - Recalibrate if needed (temperature scaling, isotonic regression)

- [ ] **Sensitivity analysis**
  - How do predictions change with input perturbations?
  - Which inputs have largest impact? (SHAP, feature importance)
  - How sensitive to hyperparameters?

- [ ] **Ensemble methods**
  - Train multiple models (different seeds, architectures)
  - Report ensemble mean and spread
  - Ensemble spread indicates uncertainty

- [ ] **Visualize uncertainty**
  - Shaded regions for confidence intervals
  - Error bars on plots
  - Maps showing prediction uncertainty spatially

**Example addition to paper**:

```latex
\subsection{Uncertainty Quantification}

We quantify prediction uncertainty using an ensemble of 10 models 
trained with different random seeds. Figure X shows predictions with 
95% confidence intervals (shaded region = Â±2 standard deviations of 
ensemble).

\textbf{Calibration:} We assess calibration using reliability diagrams 
(Figure Y). Before calibration, our model is overconfident (predicted 
probabilities too extreme). We apply temperature scaling (T=1.5) to 
improve calibration, reducing ECE from 0.12 to 0.04.

\textbf{Uncertainty decomposition:} We decompose total uncertainty into:
\begin{itemize}
\item \textbf{Model uncertainty} (epistemic): 35\% of total variance. 
  Measured by ensemble spread.
\item \textbf{Data uncertainty} (aleatoric): 50\% of total variance. 
  Estimated from residuals.
\item \textbf{Scenario uncertainty}: 15\% of total variance. Difference 
  between RCP 4.5 and RCP 8.5 scenarios.
\end{itemize}

\textbf{Sensitivity analysis:} We perturb inputs by Â±10\% and measure 
output change. Temperature is most influential (Â±8\% output change), 
followed by humidity (Â±5\%). See Appendix C for full sensitivity analysis.
```

---

### âœ… **CHECKLIST ITEM #5: Strong Baseline Comparisons**

**Risk mitigated**: RISK 5 (High)

**Actions**:

- [ ] **Compare to physics-based models**
  - CMIP6 ensemble (for climate projections)
  - ERA5 reanalysis (for historical reconstruction)
  - Weather forecast models: GFS, ECMWF IFS (for short-term)
  - Regional models: WRF, RegCM (for downscaling)

- [ ] **Compare to state-of-the-art ML**
  - Recent climate AI papers (2022-2024)
  - FourCastNet (NVIDIA, weather forecasting)
  - GraphCast (DeepMind, weather forecasting)
  - ClimaX (Microsoft, foundation model)
  - Pangu-Weather (Huawei, weather forecasting)

- [ ] **Include simple baselines**
  - Persistence (tomorrow = today)
  - Climatology (long-term average)
  - Linear regression
  - ARIMA / seasonal ARIMA

- [ ] **Fair comparison**
  - Same data, same train/test split
  - Same evaluation metrics
  - Same computational budget (if possible)
  - Report statistical significance of differences

- [ ] **Ablation studies**
  - Remove components to show their contribution
  - E.g., "without attention," "without physics constraints"

- [ ] **Create comparison table**
  - All baselines in one table
  - Multiple metrics (RMSE, MAE, RÂ², skill score)
  - Statistical significance markers (*, **, ***)

**Example table**:

```latex
\begin{table}[h]
\centering
\caption{Comparison to Baselines (Temperature Prediction, 7-day lead)}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{RMSE (Â°C)} & \textbf{MAE (Â°C)} & \textbf{RÂ²} & \textbf{Skill Score} \\
\midrule
\multicolumn{5}{l}{\textit{Simple Baselines}} \\
Persistence & 2.45 & 1.89 & 0.62 & 0.00 \\
Climatology & 3.12 & 2.41 & 0.48 & -0.27 \\
Linear Regression & 2.18 & 1.67 & 0.71 & 0.11 \\
\midrule
\multicolumn{5}{l}{\textit{Physics-Based Models}} \\
CMIP6 Ensemble & 1.95 & 1.52 & 0.78 & 0.20 \\
ERA5 Reanalysis & 1.82 & 1.41 & 0.81 & 0.26 \\
\midrule
\multicolumn{5}{l}{\textit{ML Baselines}} \\
LSTM & 1.76 & 1.35 & 0.83 & 0.28 \\
Transformer & 1.68 & 1.29 & 0.85 & 0.31 \\
FourCastNet & 1.62 & 1.24 & 0.86 & 0.34 \\
GraphCast & 1.58 & 1.21 & 0.87 & 0.36 \\
\midrule
\textbf{Ours} & \textbf{1.52}*** & \textbf{1.16}*** & \textbf{0.88} & \textbf{0.38} \\
\bottomrule
\end{tabular}
\label{tab:baselines}
\end{table}

*** p < 0.001 (paired t-test vs. best baseline, GraphCast)

\textbf{Note:} Skill score is computed relative to persistence baseline. 
Our model achieves 38\% improvement over persistence, compared to 36\% 
for GraphCast (previous state-of-the-art).
```

---

### âœ… **CHECKLIST ITEM #6: Ensure Reproducibility**

**Risk mitigated**: RISK 6 (High)

**Actions**:

- [ ] **Data documentation**
  - Exact dataset name and version (e.g., "ERA5 hourly, 1979-2020")
  - Download link or access instructions
  - Preprocessing steps (regridding, normalization, filtering)
  - Train/val/test split details
  - Data statistics (mean, std, min, max)

- [ ] **Model documentation**
  - Architecture diagram
  - Exact hyperparameters (learning rate, batch size, layers, etc.)
  - Initialization method
  - Optimizer details (Adam, SGD, etc.)
  - Training procedure (epochs, early stopping, etc.)

- [ ] **Code release**
  - GitHub repository (public or anonymous for review)
  - README with installation instructions
  - Requirements file (requirements.txt, environment.yml)
  - Training script
  - Evaluation script
  - Pre-trained model weights (if possible)

- [ ] **Computational details**
  - Hardware (GPU model, RAM, CPU)
  - Training time (wall-clock hours)
  - Software versions (Python, PyTorch/TensorFlow, CUDA)
  - Random seeds (for reproducibility)

- [ ] **Reproducibility statement**
  - Dedicated section or appendix
  - Checklist of what's provided
  - Instructions to reproduce main results

**Example reproducibility statement**:

```latex
\section*{Reproducibility Statement}

We provide the following to ensure reproducibility:

\textbf{Data:}
\begin{itemize}
\item Dataset: ERA5 hourly reanalysis, 1979-2020
\item Download: https://cds.climate.copernicus.eu/
\item Preprocessing: See Appendix A for detailed steps
\item Train/val/test split: Table 2
\item Data statistics: Table 3
\end{itemize}

\textbf{Code:}
\begin{itemize}
\item Repository: https://github.com/yourlab/climate-ai (anonymous: https://anonymous.4open.science/r/climate-ai-XXXX)
\item README with installation and usage instructions
\item Training script: \texttt{train.py}
\item Evaluation script: \texttt{evaluate.py}
\item Pre-trained weights: \texttt{checkpoints/model.pth}
\end{itemize}

\textbf{Hyperparameters:}
\begin{itemize}
\item Architecture: Transformer with 12 layers, 768 hidden dim, 12 heads
\item Optimizer: AdamW, lr=1e-4, weight decay=0.01
\item Batch size: 32
\item Training: 100 epochs, early stopping (patience=10)
\item Random seed: 42
\item See Appendix B for complete hyperparameter table
\end{itemize}

\textbf{Computational Resources:}
\begin{itemize}
\item Hardware: 4Ã— NVIDIA A100 (40GB)
\item Training time: 48 hours
\item Software: Python 3.9, PyTorch 1.13, CUDA 11.7
\end{itemize}

\textbf{Reproducing Results:}
\begin{verbatim}
# Install dependencies
conda env create -f environment.yml
conda activate climate-ai

# Download data (requires CDS API key)
python scripts/download_data.py

# Train model
python train.py --config configs/default.yaml

# Evaluate
python evaluate.py --checkpoint checkpoints/model.pth
\end{verbatim}

Expected results: RMSE=1.52Â°C, MAE=1.16Â°C, RÂ²=0.88 (Table 4).
```

---

### âœ… **CHECKLIST ITEM #7: Address Climate-Specific Challenges**

**Risk mitigated**: RISK 7 (High)

**Actions**:

- [ ] **Non-stationarity**
  - Climate is changing (trends, not stationary)
  - Methods: detrending, adaptive models, domain adaptation
  - Evaluate on out-of-distribution data (future climate)

- [ ] **Extreme events**
  - Extremes matter most for impacts (heatwaves, droughts, floods)
  - Evaluate separately (e.g., 95th percentile events)
  - Use appropriate loss functions (quantile loss, focal loss)

- [ ] **Missing data**
  - Climate observations have gaps (sensor failures, etc.)
  - Methods: imputation, masking, robust training
  - Report performance with varying missingness

- [ ] **Spatial dependencies**
  - Teleconnections (El NiÃ±o affects global weather)
  - Methods: graph neural networks, attention, convolutions
  - Visualize learned spatial patterns

- [ ] **Seasonal cycles**
  - Strong seasonal patterns in climate
  - Methods: Fourier features, seasonal decomposition
  - Evaluate by season (winter, summer, etc.)

- [ ] **Multi-scale phenomena**
  - Climate has multiple timescales (diurnal, seasonal, decadal)
  - Methods: multi-resolution models, hierarchical models
  - Evaluate at different timescales

**Example additions to paper**:

```latex
\subsection{Handling Non-Stationarity}

Climate change introduces non-stationarity: the data distribution 
shifts over time. We address this in two ways:

\textbf{1. Detrending:} We remove linear trends from training data 
and add them back to predictions. This allows the model to focus on 
variability rather than trends.

\textbf{2. Domain adaptation:} We use adversarial training to make 
the model robust to distribution shift. We treat different decades 
as different domains and train the model to be domain-invariant.

\textbf{Evaluation on future climate:} To test generalization under 
climate change, we evaluate on 2016-2020 (not in training data). 
This period includes record-breaking temperatures. Our model maintains 
performance (RMSE=1.52Â°C vs. 1.48Â°C on validation set), while baselines 
degrade significantly (LSTM: 1.76Â°C â†’ 2.14Â°C).

\subsection{Extreme Event Evaluation}

We evaluate extreme events (>95th percentile) separately, as they 
have disproportionate impacts. Table X shows performance on extremes:

\begin{table}[h]
\centering
\caption{Performance on Extreme Events (>95th percentile)}
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{POD} & \textbf{FAR} & \textbf{CSI} \\
\midrule
Persistence & 0.42 & 0.58 & 0.28 \\
CMIP6 & 0.56 & 0.48 & 0.38 \\
GraphCast & 0.68 & 0.35 & 0.51 \\
\textbf{Ours} & \textbf{0.74} & \textbf{0.28} & \textbf{0.58} \\
\bottomrule
\end{tabular}
\end{table}

POD = Probability of Detection, FAR = False Alarm Rate, CSI = Critical Success Index

Our model achieves 74\% POD (detects 74\% of extreme events) with 
28\% FAR (28\% of predicted extremes are false alarms).
```

---

### âœ… **CHECKLIST ITEM #8: Comprehensive Evaluation Metrics**

**Risk mitigated**: RISK 8 (High)

**Actions**:

- [ ] **Multiple metrics**
  - Point metrics: RMSE, MAE, RÂ²
  - Distributional: CRPS, quantile score
  - Categorical: POD, FAR, CSI (for extremes)
  - Spatial: pattern correlation, spatial RMSE
  - Skill scores: relative to baseline

- [ ] **Stratified evaluation**
  - By season (DJF, MAM, JJA, SON)
  - By region (tropics, mid-latitudes, polar)
  - By event type (heatwave, drought, flood)
  - By lead time (1-day, 7-day, 30-day)

- [ ] **Climate-specific metrics**
  - Anomaly correlation coefficient (ACC)
  - Root mean square skill score (RMSSS)
  - Continuous ranked probability score (CRPS)
  - Brier skill score (BSS)

- [ ] **Visualizations**
  - Time series plots (predictions vs. observations)
  - Spatial maps (mean, bias, RMSE)
  - Taylor diagrams (correlation, std, RMSE in one plot)
  - Reliability diagrams (calibration)

- [ ] **Statistical significance**
  - Paired t-tests for metric differences
  - Bootstrap confidence intervals
  - Bonferroni correction for multiple comparisons

**Example comprehensive evaluation**:

```latex
\subsection{Evaluation Metrics}

We use multiple metrics to assess different aspects of performance:

\textbf{Point metrics:}
\begin{itemize}
\item RMSE: Root mean square error (penalizes large errors)
\item MAE: Mean absolute error (robust to outliers)
\item RÂ²: Coefficient of determination (variance explained)
\end{itemize}

\textbf{Distributional metrics:}
\begin{itemize}
\item CRPS: Continuous ranked probability score (for probabilistic predictions)
\item Quantile score: Accuracy of 5th, 50th, 95th percentile predictions
\end{itemize}

\textbf{Extreme event metrics:}
\begin{itemize}
\item POD: Probability of detection (sensitivity)
\item FAR: False alarm rate (1 - precision)
\item CSI: Critical success index (F1-like metric)
\end{itemize}

\textbf{Spatial metrics:}
\begin{itemize}
\item Pattern correlation: Spatial correlation between predicted and observed fields
\item Spatial RMSE: RMSE computed over spatial domain
\end{itemize}

\textbf{Skill scores:}
\begin{itemize}
\item RMSSS: RMSE skill score relative to persistence
\item BSS: Brier skill score for probabilistic predictions
\end{itemize}

\subsection{Stratified Evaluation}

Table X shows performance stratified by season and region:

\begin{table}[h]
\centering
\caption{RMSE (Â°C) by Season and Region}
\begin{tabular}{lcccc}
\toprule
\textbf{Region} & \textbf{DJF} & \textbf{MAM} & \textbf{JJA} & \textbf{SON} \\
\midrule
Tropics (30Â°S-30Â°N) & 1.21 & 1.18 & 1.15 & 1.19 \\
Mid-latitudes (30Â°-60Â°) & 1.68 & 1.52 & 1.45 & 1.61 \\
Polar (>60Â°) & 2.14 & 1.89 & 1.76 & 2.03 \\
\midrule
\textbf{Global} & \textbf{1.58} & \textbf{1.48} & \textbf{1.42} & \textbf{1.53} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key findings:}
\begin{itemize}
\item Best performance in summer (JJA), worst in winter (DJF)
\item Tropics easier to predict than polar regions (less variability)
\item Mid-latitudes show largest seasonal variation
\end{itemize}

Figure X shows Taylor diagram summarizing spatial performance. Our 
model (red star) has higher correlation (0.92) and lower RMSE (1.52Â°C) 
than all baselines.
```

---

### âœ… **CHECKLIST ITEM #9: Comprehensive Related Work**

**Risk mitigated**: RISK 9 (Medium)

**Actions**:

- [ ] **Climate science literature**
  - IPCC reports (AR6, 2021)
  - CMIP6 papers (climate model intercomparison)
  - Key climate modeling papers (GCMs, RCMs)
  - Domain-specific papers (precipitation, temperature, extremes)

- [ ] **Climate AI literature**
  - Recent papers (2022-2024)
  - Climate Change AI workshops (NeurIPS, ICML, ICLR)
  - Key papers: FourCastNet, GraphCast, ClimaX, Pangu-Weather
  - Downscaling papers (if applicable)
  - Extreme event prediction papers (if applicable)

- [ ] **ML methods literature**
  - Relevant architectures (Transformers, GNNs, CNNs)
  - Time series forecasting methods
  - Uncertainty quantification methods
  - Physics-informed ML (if applicable)

- [ ] **Organize related work**
  - By topic (climate modeling, ML for climate, specific task)
  - Highlight gaps your work addresses
  - Position your contribution clearly

**Example related work structure**:

```latex
\section{Related Work}

\subsection{Climate Modeling}

Climate prediction has a long history in atmospheric science. General 
Circulation Models (GCMs) simulate physical processes based on 
fundamental equations \citep{ipcc2021}. The CMIP6 project 
\citep{eyring2016cmip6} coordinates climate model intercomparison, 
providing ensemble projections for policy. However, GCMs are 
computationally expensive (weeks to months for century-scale simulations) 
and have limited spatial resolution (50-100 km).

Regional Climate Models (RCMs) \citep{giorgi2019rcm} provide higher 
resolution (10-50 km) by downscaling GCM outputs, but remain expensive. 
Statistical downscaling \citep{maraun2018downscaling} is faster but 
assumes stationarity, which is violated under climate change.

\subsection{Machine Learning for Climate}

Recent work applies ML to climate prediction, offering computational 
efficiency and data-driven pattern recognition.

\textbf{Weather forecasting:} FourCastNet \citep{pathak2022fourcastnet} 
uses Fourier neural operators for global weather prediction, achieving 
comparable accuracy to physics-based models at 1000Ã— speedup. GraphCast 
\citep{lam2022graphcast} uses graph neural networks and achieves 
state-of-the-art accuracy on medium-range forecasting. Pangu-Weather 
\citep{bi2022pangu} uses 3D transformers for global forecasting.

\textbf{Climate projection:} ClimaX \citep{nguyen2023climax} pre-trains 
a foundation model on climate data for various downstream tasks. 
\citet{watson2022climate} use neural operators for climate downscaling. 
\citet{rupe2022climate} apply physics-informed neural networks to 
climate modeling.

\textbf{Extreme events:} \citet{racah2017extremeweather} use deep 
learning for extreme weather detection. \citet{prabhat2021climate} 
develop ML methods for climate extremes. \citet{barnes2021heatwave} 
predict heatwaves using CNNs.

\subsection{Gaps and Our Contribution}

Existing work has limitations:
\begin{itemize}
\item Most focus on short-term weather (1-14 days), not long-term climate
\item Limited evaluation on extreme events (which matter most for impacts)
\item Insufficient uncertainty quantification
\item Lack of physical plausibility checks
\end{itemize}

Our work addresses these gaps by:
\begin{itemize}
\item Focusing on seasonal-to-decadal climate prediction
\item Comprehensive extreme event evaluation
\item Rigorous uncertainty quantification (ensembles, calibration)
\item Physical consistency checks (energy/mass conservation)
\end{itemize}
```

---

### âœ… **CHECKLIST ITEM #10: Clarify Practical Utility**

**Risk mitigated**: RISK 10 (Medium)

**Actions**:

- [ ] **Define use cases**
  - Who would use this? (farmers, grid operators, policymakers, etc.)
  - For what decisions? (irrigation, energy planning, adaptation, etc.)
  - What value does it provide? (cost savings, risk reduction, etc.)

- [ ] **Discuss deployment**
  - Computational requirements (can it run in real-time?)
  - Data requirements (what inputs are needed?)
  - Integration with existing systems
  - Interpretability for end-users

- [ ] **Stakeholder engagement**
  - Did you consult potential users?
  - What are their requirements?
  - How does your work meet (or not meet) those needs?

- [ ] **Limitations for deployment**
  - What barriers exist? (policy, infrastructure, cost, trust)
  - What additional work is needed before deployment?
  - Be honest about readiness level

**Example addition to paper**:

```latex
\section{Practical Applications and Deployment Considerations}

\subsection{Use Cases}

Our improved seasonal precipitation forecasts have several potential 
applications:

\textbf{1. Agricultural planning:} Farmers can use seasonal forecasts 
to decide crop selection, planting dates, and irrigation strategies. 
A 15\% improvement in forecast accuracy could reduce crop losses by 
an estimated 5-10\% \citep{hansen2011agriculture}.

\textbf{2. Water resource management:} Reservoir operators can optimize 
water allocation based on seasonal inflow forecasts. Improved forecasts 
could increase hydropower generation by 3-7\% while maintaining flood 
protection \citep{block2011water}.

\textbf{3. Disaster preparedness:} Early warning of drought or flood 
risk allows governments to pre-position resources and issue advisories. 
This could reduce disaster response costs by 20-30\% \citep{coughlan2000disaster}.

\subsection{Deployment Requirements}

Deploying our model in practice requires:

\textbf{Computational resources:} Inference takes 2 minutes on a single 
GPU for global forecast. This is fast enough for operational use (daily 
updates). Training requires 48 GPU-hours, which is expensive but only 
needed periodically (monthly retraining).

\textbf{Data requirements:} Our model requires ERA5 reanalysis data 
(freely available with 5-day latency) and SST observations (available 
in real-time). No proprietary data needed.

\textbf{Interpretability:} We provide attention maps showing which 
regions influence predictions (Figure X). This helps users understand 
and trust forecasts. We also provide uncertainty estimates (confidence 
intervals) for risk assessment.

\subsection{Stakeholder Engagement}

We consulted with:
\begin{itemize}
\item National Weather Service (NWS) forecasters
\item Agricultural extension agents in California
\item Water utility managers in the Southwest US
\end{itemize}

Key feedback:
\begin{itemize}
\item \textbf{Uncertainty is critical:} Users need confidence intervals, 
  not just point forecasts. (We now provide this.)
\item \textbf{Spatial resolution matters:} 50 km resolution is too coarse 
  for farm-level decisions. (Future work: downscaling to 10 km.)
\item \textbf{Trust requires validation:} Users want to see performance 
  on recent events they remember. (We provide case studies in Appendix.)
\end{itemize}

\subsection{Limitations for Deployment}

Our model is a research prototype, not a production system. Deployment 
would require:
\begin{itemize}
\item \textbf{Operational infrastructure:} Automated data ingestion, 
  model serving, monitoring
\item \textbf{Regulatory approval:} NWS has strict requirements for 
  operational forecasts
\item \textbf{User training:} End-users need training on interpreting 
  probabilistic forecasts
\item \textbf{Ongoing validation:} Performance must be monitored and 
  model retrained as climate changes
\end{itemize}

We estimate 1-2 years of additional development for operational deployment.
```

---

### âœ… **CHECKLIST ITEM #11: Address Ethical Concerns**

**Risk mitigated**: RISK 11 (Medium)

**Actions**:

- [ ] **Potential harms**
  - Misuse by fossil fuel industry (greenwashing, delaying action)
  - Overconfidence in predictions leading to poor decisions
  - Exacerbating inequalities (who has access to forecasts?)
  - Environmental cost of computation

- [ ] **Mitigation strategies**
  - Clear communication of limitations
  - Open-source release (democratize access)
  - Uncertainty quantification (avoid overconfidence)
  - Efficient models (reduce carbon footprint)

- [ ] **Equity considerations**
  - Who benefits from improved forecasts?
  - Who is excluded? (data-poor regions, marginalized communities)
  - How to ensure equitable access?

- [ ] **Dual-use concerns**
  - Could this be misused? (e.g., by climate deniers)
  - How to prevent misuse?

**Example ethics statement**:

```latex
\section*{Ethics Statement}

\subsection*{Potential Harms}

\textbf{1. Misuse for greenwashing:} Fossil fuel companies could 
misuse climate predictions to claim they are "managing climate risk" 
while continuing emissions. We mitigate this by clearly stating that 
improved predictions do not reduce the need for emissions reductions.

\textbf{2. Overconfidence in predictions:} Decision-makers may 
over-rely on our forecasts, ignoring uncertainty. We mitigate this 
by providing confidence intervals and emphasizing limitations.

\textbf{3. Exacerbating inequalities:} Improved forecasts may only 
benefit well-resourced actors (large farms, utilities) who can afford 
to act on them. Small-scale farmers in developing countries may lack 
access. We mitigate this by:
\begin{itemize}
\item Open-source release (no licensing fees)
\item Lightweight models (can run on modest hardware)
\item Collaboration with NGOs to disseminate forecasts
\end{itemize}

\textbf{4. Environmental cost:} Training our model consumed 48 GPU-hours, 
emitting an estimated 15 kg COâ‚‚ (using CodeCarbon \citep{codecarbon}). 
While small compared to climate impacts, we acknowledge the irony of 
carbon emissions for climate research. We mitigate this by:
\begin{itemize}
\item Using efficient architectures (Transformers, not larger models)
\item Training on renewable-powered cloud (Google Cloud, 60\% renewable)
\item Releasing pre-trained models (avoid redundant training)
\end{itemize}

\subsection*{Equity Considerations}

Our model is trained on global data but may perform better in 
data-rich regions (North America, Europe) than data-poor regions 
(Africa, parts of Asia). We evaluate performance by region (Table X) 
and find:
\begin{itemize}
\item North America: RMSE = 1.42Â°C
\item Europe: RMSE = 1.48Â°C
\item Africa: RMSE = 1.78Â°C (26\% worse)
\item Asia: RMSE = 1.65Â°C (16\% worse)
\end{itemize}

This disparity reflects data availability (more weather stations in 
developed countries). We recommend:
\begin{itemize}
\item Investing in observational infrastructure in underserved regions
\item Transfer learning from data-rich to data-poor regions
\item Collaborating with local institutions to validate and improve models
\end{itemize}

\subsection*{Dual-Use Concerns}

Climate predictions could be misused by climate deniers to cherry-pick 
forecast failures and undermine climate science. We mitigate this by:
\begin{itemize}
\item Transparent reporting of failures (Section 6.3)
\item Emphasizing that forecast errors don't invalidate climate change
\item Providing context: our model improves on baselines but isn't perfect
\end{itemize}

\subsection*{Broader Impacts}

We hope our work contributes to climate adaptation by improving 
seasonal forecasts. However, we emphasize that adaptation is not a 
substitute for mitigation. Improved forecasts help manage climate 
impacts but don't reduce the need for urgent emissions reductions.
```

---

### âœ… **CHECKLIST ITEM #12: Report Computational Cost**

**Risk mitigated**: RISK 12 (Medium)

**Actions**:

- [ ] **Training cost**
  - Wall-clock time (hours)
  - GPU-hours (number of GPUs Ã— hours)
  - Energy consumption (kWh)
  - Carbon footprint (kg COâ‚‚)
  - Cost (dollars, if using cloud)

- [ ] **Inference cost**
  - Latency (seconds per prediction)
  - Throughput (predictions per second)
  - Memory requirements (GB)

- [ ] **Efficiency analysis**
  - Compare to baselines (cost vs. accuracy tradeoff)
  - Ablation: which components are most expensive?
  - Efficiency improvements (quantization, pruning, distillation)

- [ ] **Tools**
  - CodeCarbon (https://codecarbon.io/) for carbon tracking
  - NVIDIA SMI for GPU monitoring
  - Cloud provider cost calculators

**Example addition to paper**:

```latex
\subsection{Computational Cost and Carbon Footprint}

We report computational cost and carbon footprint for transparency 
and to inform future work.

\textbf{Training:}
\begin{itemize}
\item Hardware: 4Ã— NVIDIA A100 (40GB)
\item Wall-clock time: 48 hours
\item GPU-hours: 192 GPU-hours
\item Energy consumption: 76.8 kWh (estimated at 400W per GPU)
\item Carbon footprint: 15.4 kg COâ‚‚ (using CodeCarbon, Google Cloud 
  us-west1, 0.2 kg COâ‚‚/kWh)
\item Cloud cost: \$576 (at \$3/GPU-hour)
\end{itemize}

\textbf{Inference:}
\begin{itemize}
\item Latency: 2.1 seconds per global forecast (single GPU)
\item Throughput: 0.48 forecasts/second
\item Memory: 12 GB GPU memory
\item Energy per forecast: 0.23 Wh
\item Carbon per forecast: 0.046 g COâ‚‚
\end{itemize}

\textbf{Comparison to baselines:}

\begin{table}[h]
\centering
\caption{Computational Cost vs. Accuracy Tradeoff}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{GPU-hours} & \textbf{COâ‚‚ (kg)} & \textbf{RMSE (Â°C)} & \textbf{Efficiency} \\
\midrule
Linear Regression & 0.1 & 0.02 & 2.18 & 1.0Ã— \\
LSTM & 24 & 4.8 & 1.76 & 1.2Ã— \\
Transformer & 96 & 19.2 & 1.68 & 1.3Ã— \\
FourCastNet & 384 & 76.8 & 1.62 & 1.3Ã— \\
GraphCast & 512 & 102.4 & 1.58 & 1.3Ã— \\
\textbf{Ours} & \textbf{192} & \textbf{38.4} & \textbf{1.52} & \textbf{1.4Ã—} \\
\bottomrule
\end{tabular}
\end{table}

Efficiency = (Baseline RMSE / Our RMSE) / (Our GPU-hours / Baseline GPU-hours)

Our model achieves the best accuracy with moderate computational cost. 
It is 2.7Ã— more efficient than GraphCast (similar accuracy, 2.7Ã— less 
compute).

\textbf{Carbon offset:} We offset our carbon footprint (15.4 kg COâ‚‚) 
through Stripe Climate (\$1.54 donation).

\textbf{Efficiency improvements:} We explored model compression:
\begin{itemize}
\item \textbf{Quantization (INT8):} 4Ã— smaller, 3Ã— faster inference, 
  +0.08Â°C RMSE
\item \textbf{Pruning (50\% sparsity):} 2Ã— smaller, 1.5Ã— faster, 
  +0.05Â°C RMSE
\item \textbf{Distillation (to smaller model):} 3Ã— smaller, 2Ã— faster, 
  +0.12Â°C RMSE
\end{itemize}

For deployment, we recommend quantized model (good accuracy-efficiency 
tradeoff).
```

---

## ðŸ“‹ SECTION 3: FINAL SUBMISSION PACKAGE CHECKLIST

### 2 Weeks Before Deadline

- [ ] **Paper content complete**
  - All sections written
  - All figures and tables finalized
  - All experiments complete
  - All baselines compared

- [ ] **Risk mitigation complete**
  - All 12 checklist items addressed
  - Limitations section written
  - Ethics statement written
  - Reproducibility statement written

- [ ] **Internal review**
  - Co-authors reviewed
  - Domain expert reviewed (climate scientist)
  - ML expert reviewed
  - All feedback incorporated

---

### 1 Week Before Deadline

- [ ] **Formatting**
  - Venue template applied
  - Page limit satisfied
  - References formatted correctly
  - Figures high resolution (300+ DPI)
  - Tables formatted consistently

- [ ] **Supplementary material**
  - Appendices complete
  - Additional experiments
  - Detailed hyperparameters
  - Extended related work
  - Proofs (if applicable)

- [ ] **Code release**
  - GitHub repository created
  - README complete
  - Installation tested
  - Main results reproducible
  - Pre-trained models uploaded (if applicable)

- [ ] **Data documentation**
  - Data sources documented
  - Preprocessing steps detailed
  - Train/val/test splits specified
  - Data statistics reported

---

### 3 Days Before Deadline

- [ ] **Proofreading**
  - Spell check
  - Grammar check
  - Notation consistency
  - Terminology consistency
  - Read aloud for flow

- [ ] **Figure/table check**
  - All figures referenced in text
  - All captions clear and self-contained
  - All axes labeled
  - All legends readable
  - Colorblind-friendly palettes

- [ ] **Citation check**
  - All claims cited
  - All citations in bibliography
  - No missing references
  - Citation format correct

- [ ] **Checklist verification**
  - Go through all 12 mitigation items
  - Verify each is addressed in paper
  - Note section/page numbers

---

### 1 Day Before Deadline

- [ ] **Final checks**
  - PDF renders correctly
  - All fonts embedded
  - File size within limit
  - Anonymization (if required)
  - Supplementary material attached

- [ ] **Submission system**
  - Account created
  - Abstract entered
  - Keywords entered
  - Conflicts of interest declared
  - All required forms completed

- [ ] **Backup**
  - Save final PDF
  - Save LaTeX source
  - Save all figures/tables
  - Save submission confirmation

---

### Submission Day

- [ ] **Upload**
  - Main PDF
  - Supplementary material
  - Code (if required)
  - Any additional files

- [ ] **Verify**
  - Preview submission
  - Check all files uploaded
  - Verify metadata correct

- [ ] **Submit**
  - Submit 24 hours before deadline (buffer for issues)
  - Download confirmation email
  - Save paper ID for tracking

- [ ] **Post-submission**
  - Notify co-authors
  - Prepare for rebuttal (if applicable)
  - Archive all materials

---

## ðŸ“¤ NEXT STEPS

**To get a personalized risk assessment and mitigation plan:**

**Upload your paper PDF**, and I will provide:

1. **Specific risk assessment**
   - Line-by-line analysis of your paper
   - Identify specific overclaiming statements
   - Flag missing baselines
   - Highlight reproducibility gaps

2. **Customized mitigation checklist**
   - Tailored to your specific methods
   - Your actual data and experiments
   - Your specific claims
   - Priority-ordered by risk level

3. **Reviewer simulation**
   - Anticipated reviewer concerns
   - Likely questions and criticisms
   - Suggested responses
   - Preemptive additions to paper

4. **Comparison to best practices**
   - How does your paper compare to top climate AI papers?
   - What are they doing that you're not?
   - What can you learn from them?

5. **Venue-specific guidance**
   - NeurIPS, ICML, ICLR requirements
   - Climate-specific venues (EGU, AGU)
   - Interdisciplinary venues (Nature Climate Change, etc.)

**What I need from you:**
- Paper PDF (current draft)
- Target venue (NeurIPS, ICML, Nature, etc.)
- Climate application (precipitation, temperature, extremes, etc.)
- Data used (ERA5, CMIP6, observations, etc.)
- Baselines compared (if any)

**Turnaround time**: 30-35 minutes for comprehensive analysis

**What specific climate problem does your AI paper address?** (Forecasting, downscaling, extreme events, attribution, impacts, etc. - this will help me tailor the risk assessment)