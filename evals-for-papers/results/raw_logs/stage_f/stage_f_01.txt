Below are strong, realistic venues for a 6‑month project on compressible LLMs (quantization, pruning/sparsity, distillation, KV‑cache compression), with fit, timelines, and key ethics/formatting constraints. I also propose concrete experiments and a brief simulated reviewer summary.

Top venues and their constraints (with fit for a 6‑month project)
- ACL Rolling Review (ARR) → ACL/EMNLP/NAACL
  - Fit: Rolling monthly deadlines; fast feedback makes it practical within 6 months. Strong for LLM compression work that targets NLP tasks and evaluation suites.
  - Constraints: ACL two‑column template; long papers typically 8 pages (excl. refs). Mandatory Limitations section; ethics review when flagged; authors should follow ARR ethics guidance. Code/data release strongly encouraged [6][7].
- NeurIPS (main conference + Efficient/Systems/Compression workshops)
  - Fit: Highly competitive; a 6‑month project can target workshops or a well-scoped main paper. Excellent visibility for efficiency/compression.
  - Constraints: NeurIPS style; strict Paper Checklist covering reproducibility (datasets, compute, code, metrics). Papers undergo ethics review when applicable [1].
- ICML
  - Fit: Core ML venue; compression and training/inference efficiency are in‑scope. Target if you can show clear algorithmic novelty or principled analysis.
  - Constraints: ICML template; main content length is typically limited (check current CFP). Ethics review policy in place; ensure limitations and broader impacts are discussed when relevant [2][3].
- ICLR
  - Fit: Good for method-centric compression and ablations, with open reviews. Useful if you can iterate based on community feedback on OpenReview.
  - Constraints: ICLR template; main text length typically around 9 pages; Limitations section required; code release expected for reproducibility [4].
- MLSys
  - Fit: Best if your contribution includes systems aspects (throughput/latency on specific hardware, KV‑cache and memory behavior, kernel/serving optimizations), beyond accuracy.
  - Constraints: See current CfP; artifact evaluation and reproducibility checklist strongly encouraged; reporting of hardware, inference stack, and profiling is expected [9][10].
- TMLR (Transactions on Machine Learning Research)
  - Fit: Rolling submissions; turnaround often compatible with 6 months if the work is focused. Good for solid, incremental or empirical synthesis papers in compression.
  - Constraints: Transparent reviews; ethics policy applies; reproducibility and artifact availability expected [5].

What to prioritize for venue compliance
- Mandatory sections: Limitations (ICLR/ACL), ethics/broader impacts where applicable; discuss data licenses and any downstream risks; quantify compute/energy when relevant [1][4][6][7][9][10].
- Reproducibility: Report seeds, model checkpoints, datasets, calibration data sizes, group sizes for quantization, kernel/runtime details, and hardware. Provide scripts and logs; prefer open weights if licensing allows [1][9][10].
- Evaluation breadth: Accuracy (task suites), perplexity, calibration data sensitivity, robustness, latency/throughput on target hardware; fairness/bias probes if compression might differentially affect subpopulations [6][7].

Concrete, falsifiable experiments (ready to run in ~6 months)
1) 4‑bit post‑training quantization (PTQ) vs QAT vs QLoRA recovery
- Hypothesis: For 7B–13B models, 4‑bit PTQ with careful calibration maintains ≤1.0 MMLU drop vs FP16; QLoRA fine‑tuning on 1–5% task‑specific data can recover most of the loss where PTQ fails.
- Setup: Llama‑2‑7B/13B and/or Mistral‑7B; PTQ baselines (e.g., GPTQ/AWQ‑style), QAT on a small proxy corpus, and QLoRA on task data. Vary calibration set size (500, 2k, 10k tokens) and group sizes (G=32/64).
- Metrics: MMLU, GSM8K, HellaSwag, ARC‑C; perplexity on WikiText2/C4; throughput/latency on A100 and a consumer GPU.
- Criterion: Success if PTQ ≤1.0 MMLU drop and QLoRA closes remaining gaps with ≤0.5 point deficit versus FP16.
- Rationale: Surveys document these approaches as dominant paradigms for efficient LLMs [P2]. QLoRA is widely used for accuracy recovery after quantization (e.g., Dettmers et al., 2023; see anchor below).

2) Does compression preferentially degrade parametric knowledge vs reasoning?
- Hypothesis: 4‑bit PTQ degrades factual recall more than short‑chain reasoning; small post‑quantization distillation mitigates knowledge loss more than it improves reasoning.
- Setup: Before/after compression evaluate open‑domain QA (NaturalQuestions, TriviaQA), closed‑book QA, TruthfulQA; contrast with reasoning tasks (GSM8K few-shot). Add a small KD stage from FP16 teacher to quantized student on a mixed QA corpus.
- Metrics: QA EM/F1; TruthfulQA score; GSM8K accuracy; delta vs FP16.
- Criterion: Significant (p<0.05) greater drop in QA than in GSM8K; KD improves QA more than reasoning.
- Rationale: Prior work finds compression can reduce parametric knowledge retention; your design directly probes that effect and mitigation via distillation [P5].

3) Combining sparsity and quantization for Pareto gains
- Hypothesis: 30–50% unstructured sparsity (one‑shot pruning) plus 4‑bit weight quantization achieves >1.5× throughput and >35% memory reduction over 4‑bit alone, with ≤1.5 MMLU drop.
- Setup: Apply SparseGPT‑style one‑shot pruning at various sparsity levels, then 4‑bit PTQ; measure accuracy and runtime on A100 and RTX 4090 with standard serving stack (TensorRT‑LLM or vLLM).
- Metrics: MMLU/ARC‑C; tokens/sec; peak memory; accuracy‑throughput Pareto front.
- Criterion: Meet the throughput/memory targets while staying within the accuracy budget.
- Rationale: Sparsity and quantization target complementary bottlenecks; surveys highlight their joint potential but also trade‑offs that need careful calibration [P2].

4) KV‑cache compression for long‑context inference
- Hypothesis: 8‑bit KV quantization with selective key sampling yields ≥1.3× speedup at 32k context with ≤0.5 perplexity increase and ≤1 point drop on long‑context QA/summarization.
- Setup: Long‑context tasks (LongBench subsets), vary KV precision (fp16/8‑bit/4‑bit) and cache eviction/sampling strategies; profile end‑to‑end latency and memory.
- Metrics: Throughput, latency p50/p95, VRAM usage; perplexity; task scores.
- Criterion: Hit the speedup target with accuracy within thresholds.
- Rationale: Long‑context workloads are increasingly common; compression must account for KV‑cache behavior (covered in efficiency/compression surveys [P2]).

Simulated brief reviewer summary (for a strong but not flawless submission)
- Summary: The paper proposes a calibrated 4‑bit PTQ pipeline with optional lightweight QLoRA recovery, and shows that combined sparsity+quantization improves the accuracy‑throughput Pareto frontier on 7B–13B models. It also disentangles effects on parametric knowledge vs reasoning and introduces a practical KV‑cache compression strategy for long‑context inference.
- Strengths: Thorough, multi‑axis evaluation (accuracy, latency, memory) on public models; careful ablations (calibration size, group size); strong systems profiling; open‑sourced artifacts and reproducibility checklist.
- Weaknesses: Limited theoretical justification for why sparsity and quantization compose well; fewer results on models >13B; fairness and robustness probes are relatively light.
- Significance: Solid empirical contribution that could influence best practices for deploying 7B–13B LLMs under tight memory/latency budgets; insights on knowledge vs reasoning degradation are useful for downstream application owners [P5].
- Clarity and reproducibility: Generally clear; includes Limitations and ethics considerations; code and logs provided; would benefit from more details on calibration data curation.
- Overall: 6 (weak accept). Confidence: 4/5.

Literature anchors (1–2 to get started)
- QLoRA: Efficient Finetuning of Quantized LLMs — https://arxiv.org/abs/2305.14314
- The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models — https://aclanthology.org/2023.findings-emnlp.349/ [P5]

Notes on venue selection strategy for a 6‑month horizon
- If you want fast feedback and flexible re‑targeting: submit to ARR within 2–3 months, then aim for ACL/EMNLP/NAACL. Alternatively, prepare a focused systems‑heavy version for MLSys and a workshop paper for NeurIPS/ICLR to build visibility [6][7][9].
- Ensure your draft includes Limitations, an ethics discussion (data licenses, potential harms), full compute/accounting, and a reproducibility checklist with code/scripts before submission [1][4][6][7][9][10].

Citations
- [P2] Efficient Compressing and Tuning Methods for Large Language Models: A Survey — https://dl.acm.org/doi/10.1145/3728636
- [P5] The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models — https://aclanthology.org/2023.findings-emnlp.349/
- [1] NeurIPS Paper Checklist — https://neurips.cc/public/guides/PaperChecklist
- [2] ICML Paper Guidelines — https://icml.cc/Conferences/2024/PaperGuidelines
- [3] ICML Publication Ethics — https://icml.cc/Conferences/2025/PublicationEthics
- [4] ICLR Author Guide — https://iclr.cc/Conferences/2026/AuthorGuide
- [5] TMLR Ethics Policy — https://jmlr.org/tmlr/ethics.html
- [6] ACL Rolling Review: Authors Guidelines — https://aclrollingreview.org/authors
- [7] ACL Policy on Publication Ethics — https://www.aclweb.org/adminwiki/index.php/ACL_Policy_on_Publication_Ethics
- [9] MLSys Call for Papers — https://mlsys.org/Conferences/2025/CallForPapers
- [10] Machine Learning Reproducibility Checklist (v2.0) — https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf

Sources
- Above venue policies and checklists; compression surveys and empirical studies on knowledge retention under compression [P2][P5]. QLoRA (Dettmers et al., 2023) as a practical baseline for finetuning quantized LLMs. Where page limits evolve annually, confirm the current year’s CFP before submission.