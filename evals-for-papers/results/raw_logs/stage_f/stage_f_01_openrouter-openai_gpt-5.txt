Here are strong, realistic venues for a 6‑month project on compressible LLMs, with what to expect for ethics and formatting. I’ve prioritized venues that regularly publish compression/efficiency work and that you can feasibly target on a 6‑month horizon.

Top venues and key constraints

1) NeurIPS (NeurIPS 2025)
- Why: Premier ML venue for quantization/pruning/distillation and training-time efficiency, with strong reproducibility norms and ethics review.
- Formatting: Use the official NeurIPS template and instructions; check the CFP for current page limits and any checklist requirements [1][2].
- Ethics: Explicit ethics guidance and dedicated ethics review process; authors should follow the NeurIPS Ethics Guidelines and address potential risks/harms [3].

2) ICML (ICML 2025)
- Why: Core ML venue for algorithmic and systems efficiency; compression papers commonly appear here.
- Formatting: Follow ICML Author Instructions and the official style/example paper; page limits and formatting are specified there [4][6].
- Ethics: Publication ethics and author responsibilities are documented in ICML’s ethics policy [5].

3) ICLR (ICLR 2025)
- Why: Strong for representation learning and practical LLM training/inference advances; OpenReview allows early feedback cycles.
- Formatting: Author Guide and provided template govern page limits and submission format [7].
- Ethics: Author Guide outlines policy; ensure clear discussion of risks and limitations in line with ICLR norms [7].

4) ACL ecosystem (ACL Rolling Review + ACL main) and COLM (Conference on Language Modeling)
- Why: If your compression is primarily for language models and you evaluate on NLP/LLM tasks, these are natural fits. ARR provides monthly cycles; COLM focuses squarely on language modeling.
- Formatting: Follow ARR author guidelines for cycles feeding into ACL venues; COLM has its own Author Guide and template [8][10].
- Ethics: ARR requires the Responsible NLP Research Checklist; you must include a completed checklist and address ethical/risk considerations [9].

5) MLSys (MLSys 2025)
- Why: Best fit if your work leans into systems-level efficiency (hardware-aware quantization, kernels, compilers, distributed training/inference).
- Formatting/Ethics: See the MLSys Call for Papers and posted dates for formatting and policy details [11].

6) TMLR (Transactions on Machine Learning Research; rolling journal with conference-like timelines)
- Why: Rolling submissions and rigorous reviews; good for a 6‑month horizon if you want continuous feedback rather than a single deadline.
- Formatting: See TMLR author guide for manuscript format and process [12].
- Ethics: TMLR ethics guidelines specify expectations around transparency, data/code, and potential societal impacts [13].

Optional journal path
- JMLR: Gold-standard ML journal; higher bar, often longer timelines. See author info and formatting guidance if you aim for a journal-first route [14].

Practical fit to a 6‑month project
- If your contribution is algorithmic and broadly ML: target NeurIPS/ICML/ICLR and prepare a thorough ablation and reproducibility package [1][4][7].
- If the work is NLP/LLM-centric with language tasks: ARR→ACL or COLM; budget time for the Responsible NLP Checklist and an ethics section [8][9][10].
- If the work is hardware/system-heavy: MLSys; emphasize kernels, throughput/latency, cost-per-token, and hardware diversity [11].
- If you want schedule flexibility: TMLR rolling review [12][13].

Tips to satisfy ethics and reproducibility expectations
- Always include: clear risk/impact discussion (e.g., bias amplification from compression, hallucination rates, robustness under distribution shift) and concrete mitigations [3][5][7][9][13].
- Provide a reproducibility package: code, config, scripts, seeds, and exact checkpoints; follow venue templates and checklists where applicable [1][4][7][9][12].

Simulated brief reviewer summary (for a hypothetical compressible LLMs paper)

- Summary: This paper proposes a mixed-precision, layer-wise adaptive quantization strategy combined with activation sparsification to compress 7B–70B LLMs with minimal accuracy loss. The method integrates a calibration pass using perplexity-driven sensitivity and a low-cost distillation step. Experiments report 2–3× memory reduction and 1.8–2.2× inference speedup on A100 and L4 GPUs across MMLU and GSM8K.

- Strengths:
  - Practicality: Targets standard LLMs, uses public benchmarks, and reports throughput/latency with batch-size scaling and KV‑cache effects.
  - Method clarity: Ablations on calibration set size, per-layer bitwidth, and interaction with KV‑cache quantization.
  - Reproducibility: Releases code, configs, and pretrained quantized checkpoints; evaluation includes multiple hardware backends.

- Weaknesses/Concerns:
  - Novelty: Combination of adaptive quantization and distillation is incremental vs. recent quantization and sparsification baselines; clearer positioning and stronger baselines (e.g., state-of-the-art 4‑bit quantizers) are needed.
  - Robustness: Limited evaluation on long-context and instruction‑following tasks; distribution shift and safety metrics (toxicity/harms) are lightly covered.
  - Generality: Results focus on decoder-only models; limited evidence on multilingual or encoder–decoder models.

- Questions for authors:
  - How sensitive is the method to calibration data domain and size? Does cross-domain calibration degrade performance?
  - What is the accuracy/speed trade‑off on consumer GPUs/CPUs? Any kernel-specific tuning required?
  - Can the approach be combined with structured sparsity (2:4) without regressions?

- Score: 6 (marginally above the acceptance threshold). Accept if authors strengthen baselines and robustness/safety evaluations.

Sources
[1] NeurIPS 2025 Call for Papers — https://neurips.cc/Conferences/2025/CallForPapers
[2] Formatting Instructions for NeurIPS 2025 — https://arxiv.org/html/2505.10292v1
[3] NeurIPS Ethics Guidelines — https://neurips.cc/public/EthicsGuidelines
[4] ICML 2025 Author Instructions — https://icml.cc/Conferences/2025/AuthorInstructions
[5] ICML 2025 Publication Ethics — https://icml.cc/Conferences/2025/PublicationEthics
[6] ICML 2025 Submission Guidelines (example paper/style) — https://media.icml.cc/Conferences/ICML2025/Styles/example_paper.pdf
[7] ICLR 2025 Author Guide — https://iclr.cc/Conferences/2025/AuthorGuide
[8] ACL Rolling Review — Authors Guidelines — https://aclrollingreview.org/authors
[9] Responsible NLP Research Checklist (ARR) — https://aclrollingreview.org/static/responsibleNLPresearch.pdf
[10] COLM 2025: Author Guide — https://colmweb.org/AuthorGuide.html
[11] MLSys 2025: Call for Papers — https://mlsys.org/Conferences/2025/CallForPapers
[12] TMLR: Guidelines for Authors — https://jmlr.csail.mit.edu/tmlr/author-guide.html
[13] TMLR Ethics Guidelines — https://jmlr.org/tmlr/ethics.html
[14] JMLR: Information for Authors — https://jmlr.org/author-info.html

Note: Exact page limits and checklist requirements can change year to year. Always confirm the latest details on the linked author/CfP pages before finalizing your submission.