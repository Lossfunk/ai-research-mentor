You can absolutely finish a solid thesis in 4 weeks by focusing on a compute‑light, evaluation/analysis project rather than inventing a new model. Below are three feasible topic directions, one recommended default plan with concrete experiments, and a week‑by‑week execution schedule.

Quick triage (answer in one line each if you can)
- Constraints: any required domain (NLP/CV), GPU access, and whether proprietary APIs are allowed?
- Interests: more into LLMs/RAG, robustness/evaluation, or measurement/reproducibility?
- Data: any institutional/IRB constraints? If none, we’ll use public QA/fact‑checking sets.

Three feasible topic directions (pick one)
1) RAG faithfulness and robustness under retrieval and chunking choices
- Why: Popular, measurable, and compute‑light; you can vary retrievers/chunking and evaluate groundedness/faithfulness with small public datasets. Surveys emphasize the need for rigorous RAG evaluation and transparent, explainable benchmarks [P2][P5].
- Core risk: over‑reliance on LLM‑as‑judge; mitigate with simple automatic metrics plus light human spot‑checks [P3][P6].

2) LLM hallucination robustness to prompt/temperature/seed changes
- Why: Hallucination remains a central problem; recent benchmarks/surveys highlight taxonomy and evaluation pitfalls, especially robustness to perturbations [P1][P6][P3].
- Core risk: defining “hallucination” ambiguously; use extrinsic, evidence‑grounded tasks and explicit criteria [P1][P6].

3) Benchmark contamination and leakage in LLM evaluation
- Why: Contamination can inflate reported performance; a focused audit on 1–2 benchmarks with clear heuristics is tractable in a month [P4]. 
- Core risk: distinguishing contamination from genuine generalization; predefine thresholds and multiple overlap measures [P4][P3].

Recommended default: Topic 1 (RAG faithfulness and robustness)
Rationale: Strong literature support and ready baselines; measurements are clear (faithfulness to provided evidence), and ablations are straightforward [P2][P5][P6].

Three concrete, falsifiable experiments (ready‑to‑run)
All three use a small public QA/fact‑checking set (e.g., FEVER or HotpotQA) and an open LLM (7–8B) to control cost; evaluate answer faithfulness to retrieved passages and citation correctness. Include 10–20% human spot‑checks to validate automatic metrics [P2][P3][P6].

1) Retriever and chunking trade‑offs on faithfulness
- Hypothesis: For a fixed context window, denser retrieval (e.g., Contriever) plus smaller chunks increases recall but can reduce answer faithfulness (more irrelevant snippets) compared to BM25 with larger chunks. 
- Variables: retriever ∈ {BM25, dense}, chunk size ∈ {200, 500, 1000 tokens}, top‑k ∈ {3, 5, 10}, with identical prompts.
- Metrics: Evidence recall/precision; citation correctness; answer‑support rate by source passages; simple n‑gram overlap and entailment checks; limited human verification [P2][P5][P6].
- Expected outcome: A Pareto‑like trade‑off between recall and faithfulness; your ablation should reveal which configurations best balance both under a fixed token budget [P2][P5].

2) Prompt and temperature robustness under fixed retrieval
- Hypothesis: Even with strong retrieval, higher temperature and prompt rewordings degrade faithfulness more than exactness/fluency, indicating decoupled robustness dimensions [P3][P6].
- Variables: temperature ∈ {0.0, 0.5, 0.8}; 3–5 semantically equivalent prompts; fixed retriever/chunking/top‑k from Experiment 1’s best configuration.
- Metrics: Same faithfulness metrics; additionally, variance across seeds/prompts; report confidence intervals [P3][P6].
- Expected outcome: Significant drop in evidence‑groundedness at higher temperatures and under prompt perturbations; quantify effect sizes to inform recommended default settings [P3][P6].

3) “Citations required” instruction and self‑checking
- Hypothesis: Requiring inline citations and adding a brief self‑check step reduces unsupported answers but at the cost of slightly lower exact‑match accuracy [P2][P6].
- Variables: instruction ∈ {no citation requirement, require exact citation spans}; self‑check ∈ {off, one‑step “verify with retrieved text”}; model fixed to best config from Experiment 1.
- Metrics: Citation correctness, groundedness, answer exactness, latency/token cost; human spot‑checks for borderline cases [P2][P6].
- Expected outcome: Measurable improvements in faithfulness with minor accuracy/cost trade‑offs; supports practical guidance for RAG deployments [P2][P6].

Ablations and controls
- Seed control and reporting: run 3–5 seeds and report mean ± CI [P3][P6].
- Small human audit: stratify by easy/hard questions; double‑annotate 50–100 predictions for inter‑annotator agreement [P6].
- Reproducibility: freeze package versions, log prompts/configs, release code/manifest [P6].
- LLM‑as‑judge caveat: if used, calibrate it against your human audit and report agreement before relying on it [P3][P6].

Week‑by‑week plan (fits in 4 weeks)
- Week 1: Scope + preregister
  - Finalize dataset and metrics; write a 1–2 page preregistration with hypotheses and analysis plan; set up retrievers and the base model; run a tiny smoke test [P6].
- Week 2: Core experiments
  - Run Experiment 1 grid; pick the best configuration; start Experiment 2 (prompt/temperature grid); begin human audit protocol [P2][P3][P6].
- Week 3: Finish runs + ablations
  - Finish Experiment 2; run Experiment 3; complete human audit; compute CIs and agreement; prepare figures/tables [P2][P6].
- Week 4: Write‑up + robustness checks
  - Add sensitivity analyses; document negative results; finalize code/artifacts; write discussion/limitations and concrete recommendations [P3][P6].

Deliverables your professor will appreciate
- Clear preregistered hypotheses, strong baselines, and rigorous ablations [P6].
- Transparent evaluation with both automatic metrics and a small, well‑documented human audit [P2][P3].
- Reproducible package (code, prompts, configs), plus a short practitioner takeaway table (recommended retriever/chunking/temperature settings).

If you prefer one of the other topics
- Hallucination robustness (Topic 2): Mirror Experiments 2–3 but without retrieval; evaluate where hallucinations occur under prompt/temperature/seed changes on tasks with verifiable references; use a recent hallucination benchmark or extrinsic QA/knowledge tasks [P1][P6].
- Contamination audit (Topic 3): Define contamination heuristics (n‑gram/fuzzy overlap, memorized Q–A pairs), apply to one benchmark, and measure performance changes when you filter likely‑contaminated items [P4][P3].

Want me to tailor this to your constraints? Tell me your compute/API access and whether you prefer RAG vs. general LLM robustness.

Citations
- [P1] HalluLens: LLM Hallucination Benchmark — https://arxiv.org/abs/2504.17550
- [P2] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey — https://arxiv.org/abs/2504.14891
- [P3] Toward Generalizable Evaluation in the LLM Era: A Survey Beyond Benchmarks — https://arxiv.org/abs/2504.18838
- [P4] The Emperor’s New Clothes in Benchmarking? A Rigorous Examination of Mitigation Strategies for LLM Benchmark Data Contamination — https://arxiv.org/abs/2503.16402
- [P5] Explainable Benchmark for Retrieval-Augmented Generation Systems — https://arxiv.org/abs/2407.11005
- [P6] A Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations — https://arxiv.org/abs/2407.04069