Short answer: Yes—you can do impactful AI research without leaning on heavy math by focusing on empirical, data-centric, human-in-the-loop, and evaluation/benchmark work. The key is to ask crisp questions, run careful experiments, and report transparently [G5], [G20], [G10]. Prompt and agent design can also be studied empirically (e.g., systematic prompt engineering) without deep theory [P5].

Math-light research tracks you can pursue
- Evaluation and benchmarking: Design targeted tests, failure analyses, and robust metrics; replicate and sanity-check claims [G5], [G20]. 
- Data-centric AI: Curate, document, and audit datasets; build tools and processes (datasheets, model cards) that improve reliability and fairness [G20].
- Prompt/agent design: Develop and evaluate prompting or tool-use patterns across tasks; focus on reproducible gains rather than ad hoc tricks [P5], [G5].
- Human-in-the-loop: Study annotation protocols, preference data, and user studies that improve model alignment and usability [G20].

Minimal math you’ll need
- Descriptive stats, uncertainty estimates (mean ± CI), basic experimental design (train/test splits, multiple seeds), and preregistration of your analysis plan [G1], [G20]. You can keep this lightweight while maintaining rigor.

Three concrete, falsifiable experiments you can run
1) Prompting patterns generalize or not?
- Hypothesis: Structured prompting (e.g., role + constraints + stepwise checklist) consistently outperforms plain “do X” prompts on reasoning and extraction tasks.
- Method: Choose 3–4 tasks (e.g., GSM8K subset for math reasoning, SQuAD subset for extraction, a small classification set). Compare 4 prompt variants: plain, chain-of-thought, role+constraints, and self-check with a verification step. Evaluate across 3 seeds/temperatures; hold model constant.
- Metrics: Exact match/F1 for extraction, accuracy for classification, and pass@1 for reasoning; report mean ± 95% CI; preregister evaluation and stopping criteria [G20]. 
- Expected outcome: Either structured prompts yield stable improvements across at least 3 tasks, or gains are narrow/sensitive—both are publishable with careful analysis [P5], [G5].

2) Behavioral testing of an LLM and targeted fixes
- Hypothesis: A CheckList-style battery targeting negation, quantifiers, and coreference reveals repeatable failure modes; a small synthetic augmentation addressing these cases reduces error rates on those capabilities.
- Method: Build a minimal CheckList for 2 phenomena (e.g., negation, number agreement). Run baseline model; then augment a small training/tuning set (or a retrieval bank) with counterexamples targeting failures; re-run tests.
- Metrics: Capability-level accuracy before/after; track unintended regressions on unrelated phenomena. Use multiple seeds and report uncertainty [G20].
- Expected outcome: Targeted augmentation reduces specific failure rates without regressing unrelated ones—quantified and reproducible.

3) Do dataset and model documentation actually improve downstream reliability?
- Hypothesis: Creating a Datasheet for Datasets and a Model Card for a small benchmark/model reduces misuse and catches issues earlier (e.g., label noise, leakage), improving reproducibility and reviewer satisfaction on a blinded checklist.
- Method: Pick a small public dataset (e.g., sentiment or news classification). Produce a rigorous datasheet and model card. Ask 5–10 external testers (or labmates) to run a simple baseline with and without your documentation package; record time-to-first-run, number of reproducibility issues, and errors caught.
- Metrics: Time-to-reproduce, number of corrections made prior to submission, and rate of issues found in review. Predefine a scoring rubric [G20].
- Expected outcome: The documented version improves reproducibility and reduces preventable errors; if no difference, document why and refine the template.

A practical 4–6 week plan
- Week 1: Choose one track and preregister your plan: hypotheses, datasets, metrics, seed protocol, and decision criteria [G20], [G10]. 
- Weeks 2–3: Implement baselines; build minimal evaluation harness; pilot error analysis [G5].
- Weeks 4–5: Run the main study with multiple seeds; conduct ablations (e.g., remove one prompt component or one documentation section to test necessity) [G5], [G20].
- Week 6: Write up results with clear claims, limitations, and a reproducibility checklist; release code/data and documentation [G10], [G20].

Why this works if you dislike math
- These paths emphasize experimental design, analysis discipline, and tool-building. You’ll still use some stats, but the value comes from careful framing, measurement, and transparency—not advanced theory [G5], [G20]. Prompt and agent work is especially accessible and benefits from systematic evaluation [P5].

Literature anchors (titles with links)
- Beyond Accuracy: Behavioral Testing of NLP models with CheckList — http://arxiv.org/abs/2005.04118
- Datasheets for Datasets — http://arxiv.org/abs/1803.09010

Citations
- [P5] Intro to AI Series: Introduction to LLM Prompt Engineering — https://www.youtube.com/watch?v=6ohxR_qlEdA
- [G1] Best Practices Research: A Methodological Guide for the Perplexed — https://www.researchgate.net/publication/31052323_Best_Practices_Research_A_Methodological_Guide_for_the_Perplexed
- [G5] Schulman, An Opinionated Guide to ML Research — http://joschu.net/blog/opinionated-guide-ml-research.html
- [G10] Manifesto for Doing Good Science — https://letters.lossfunk.com/p/manifesto-for-doing-good-science
- [G20] On best practices/reproducibility in ML research — https://arxiv.org/abs/2304.05585

Additional literature anchors
- Model Cards for Model Reporting — http://arxiv.org/abs/1810.03993
- Training language models to follow instructions with human feedback — http://arxiv.org/abs/2203.02155

Key takeaway: You don’t need advanced math to contribute meaningfully. Anchor on well-scoped, falsifiable questions, run clean experiments with transparent reporting, and focus on evaluation, data quality, and human factors.