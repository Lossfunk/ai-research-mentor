You have one month, so pick a small, well-scoped question with clear baselines and ship 2–3 tightly controlled experiments. Below are three “thesis-ready” topics that are feasible on a single GPU or API access, each with concrete, falsifiable experiments and a 4‑week plan.

Pick one of these topics
A) Efficient adaptation of language models (LoRA/QLoRA) for text classification
- Why: Strong baselines exist; easy to implement; clear compute/memory tradeoffs to measure. LoRA adapts only low‑rank matrices and often matches full fine‑tuning with far fewer trainable parameters [Hu et al., 2022](https://arxiv.org/abs/2106.09685). QLoRA enables 4‑bit finetuning with minimal quality loss and large memory savings [Dettmers et al., 2023](https://arxiv.org/abs/2305.14314).
- Data: SST‑2, AG News (small, fast), or Emotion. Models: BERT-base or a 7B open LLM via QLoRA if you have ≥24 GB VRAM; otherwise BERT/DistilBERT.
- Deliverable: Empirical comparison of accuracy, stability (across seeds), and memory/throughput across methods.

B) Prompting and decoding for reasoning and truthfulness in small LLMs
- Why: Prompting/decoding changes can move metrics meaningfully with minimal code. Chain-of-thought (CoT) and self-consistency are standard interventions [Wei et al., 2022](https://arxiv.org/abs/2201.11903); [Wang et al., 2023](https://arxiv.org/abs/2203.11171).
- Data: 100–200‑item subsets of GSM8K (math), BBH categories, and TruthfulQA (MC).
- Deliverable: Clean evaluation of prompt/decoding effects with statistical testing.

C) Vision robustness on CIFAR‑10/CIFAR‑10‑C via simple training changes
- Why: Fast to run; strong baselines; robustness and calibration can improve with simple augmentations and schedules [Hendrycks & Dietterich, 2019](https://arxiv.org/abs/1903.12261); [Cubuk et al., 2020](https://arxiv.org/abs/1909.13719).
- Data: CIFAR‑10 and CIFAR‑10‑C. Model: ResNet‑18.
- Deliverable: Robustness and calibration gains from augmentations/regularization with rigorous evaluation.

At least three concrete, falsifiable experiments (choose one track and run 2–3 of these)
Track A (LoRA/QLoRA)
1) LoRA vs full fine‑tune parity
- Hypothesis: On SST‑2, LoRA (rank r=8) achieves accuracy within 1.0 percentage point of full fine‑tuning under identical training budgets.
- Setup: BERT‑base; train/val/test splits; same optimizer, epochs, batch size; 5 seeds. Metrics: accuracy (mean ± std), wall‑clock, peak VRAM. Reject if gap >1.0 pp or variance explodes.

2) QLoRA memory–quality tradeoff
- Hypothesis: 4‑bit QLoRA matches 16‑bit LoRA within 1.0 pp accuracy while reducing peak VRAM by ≥2× on AG News.
- Setup: Same model/data; measure peak VRAM and throughput. 5 seeds. Reject if accuracy drop >1.0 pp or VRAM <2× reduction.

3) Sample‑efficiency: prompt‑tuning vs LoRA
- Hypothesis: With 100 labeled examples per class (SST‑2), LoRA outperforms prompt‑tuning by ≥3.0 pp; the gap shrinks to ≤1.0 pp at ≥5k examples.
- Setup: Curate small/large subsets; same compute, 5 seeds. Reject if thresholds not met.

Track B (Prompting/decoding)
1) Self‑consistency improves math accuracy
- Hypothesis: On a 100‑problem GSM8K subset, self‑consistency (5 samples, majority vote) improves accuracy by ≥5.0 pp over greedy decoding using the same model and prompt.
- Setup: Fix temperature/top‑p; 5 seeds. Paired test across problems. Reject if <5.0 pp or not significant. Literature anchors: CoT and self‑consistency [Wei et al., 2022]; [Wang et al., 2023].

2) CoT vs direct answer on BBH
- Hypothesis: “Let’s think step by step” increases accuracy by ≥3.0 pp on 200 BBH items versus a direct-answer prompt, holding decoding fixed.
- Setup: Balance categories; 5 seeds; paired test. Reject if <3.0 pp gain.

3) Truthfulness guardrails
- Hypothesis: A concise “avoid myths; cite facts” system prompt increases TruthfulQA MC accuracy by ≥5.0 pp on 100 items vs neutral prompt, same decoding.
- Setup: Random subset; 5 seeds; paired test. Reject if <5.0 pp.

Track C (Vision robustness)
1) RandAugment improves corruption robustness
- Hypothesis: RandAugment improves CIFAR‑10‑C mean accuracy by ≥3.0 pp over flip+crop, at similar clean accuracy (±0.5 pp).
- Setup: ResNet‑18; equal epochs/compute; 5 seeds. Metrics: clean test accuracy; CIFAR‑10‑C mean across severities; report mean ± std. Reject if thresholds not met. Anchor: RandAugment [Cubuk et al., 2020]; CIFAR‑10‑C [Hendrycks & Dietterich, 2019].

2) MixUp improves calibration without hurting accuracy
- Hypothesis: MixUp reduces ECE by ≥20% relative to baseline while keeping accuracy within 0.5 pp.
- Setup: Same as above; compute ECE with 15 bins; 5 seeds. Reject if ECE not reduced or accuracy drops.

3) Cosine LR with warmup stabilizes training
- Hypothesis: Cosine decay with warmup reduces across‑seed std of test accuracy by ≥30% vs StepLR at matched epochs/compute.
- Setup: Same optimizer/epochs; 5 seeds. Reject if variance reduction <30%.

Four‑week execution plan (fits any track)
- Week 1: Reproduce a published baseline. Lock data splits, code, and evaluation. Write Methods and Dataset sections as you go.
- Week 2: Run Experiment 1 across 5 seeds. Log configs, measure compute/VRAM, and draft Results for E1.
- Week 3: Run Experiment 2; add ablations if time. Draft Results + Discussion (threats to validity).
- Week 4: Run Experiment 3 (or deeper analysis). Finalize figures/tables, write Abstract/Intro/Related Work, and polish.

Thesis skeleton (keep it focused)
- Title + Abstract (≤200 words: problem, method, key results with numbers).
- Introduction (motivation; 1–2 literature anchors; your contributions in bullet points).
- Background/Related Work (short; define methods you compare to).
- Methods (models, prompts/augments, training details).
- Experiments (datasets; metrics; protocol; statistical tests).
- Results (tables/figures; mean ± std; significance; compute/VRAM).
- Discussion (limits; failure cases; implications).
- Conclusion (what you showed; what’s next).
- Appendix (configs, prompts, hyperparameters, seeds, license/data notes).
- Artifact: GitHub repo with scripts/notebooks and a requirements file.

What I need to tailor this in 1–2 messages
- Compute (API only, laptop CPU, single GPU?), preferred domain (NLP vs vision), and dataset constraints. I’ll return an exact config (hyperparameters, commands, and templates for figures/tables) for your chosen track.

Literature anchors
- Hu et al., 2022. LoRA: Low‑Rank Adaptation of Large Language Models — https://arxiv.org/abs/2106.09685
- Wei et al., 2022. Chain‑of‑Thought Prompting Elicits Reasoning in Large Language Models — https://arxiv.org/abs/2201.11903

Additional sources
- Dettmers et al., 2023. QLoRA: Efficient Finetuning of Quantized LLMs — https://arxiv.org/abs/2305.14314
- Wang et al., 2023. Self‑Consistency Improves Chain of Thought Reasoning in LLMs — https://arxiv.org/abs/2203.11171
- Hendrycks & Dietterich, 2019. Benchmarking Neural Network Robustness to Common Corruptions and Perturbations — https://arxiv.org/abs/1903.12261
- Cubuk et al., 2020. RandAugment — https://arxiv.org/abs/1909.13719