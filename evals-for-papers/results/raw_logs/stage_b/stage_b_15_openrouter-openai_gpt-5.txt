Short answer: The space is active but not saturated. Many “audits” stop at basic adverse-impact checks to meet regulations (e.g., NYC Local Law 144) rather than testing ranking fairness, robustness, explanation quality, or job-related validity. You can carve out novelty if you (a) go beyond four-fifths selection-rate ratios to causal/counterfactual and ranking analyses, (b) evaluate end-to-end hiring funnels and intersectional groups, and (c) scope the work within legal/ethical guardrails (Title VII/EEOC, NIST AI RMF, coordinated disclosure) [1][2][3].

How to check novelty quickly (and credibly)
- Map the regulatory baseline: Read EEOC’s 2023 technical assistance on assessing adverse impact for AI in employment and NYC AEDT (Local Law 144) FAQs; list exactly what most compliance audits measure (selection rates by group, 4/5ths rule, documentation) [1][2]. If your plan only replicates these, it’s unlikely to be novel.
- Scan the literature/meta-reviews: Skim at least one current survey on algorithmic hiring; note which modalities (resume screening, video interviews, games, ads) and which fairness metrics (rate-based vs ranking/calibration/validity) are common vs missing [4].
- Identify under-audited failure modes:
  - Ranking fairness (who gets top shortlist slots), calibration (do scores mean the same thing across groups), and intersectional impacts are thin in public audits compared to simple selection rates [1][4].
  - Delivery and platform effects (e.g., ad targeting skew) can create bias even if the model is “neutral” [5].
  - Regulatory cases highlight age/sex discrimination risk, including real enforcement (iTutorGroup settlement), suggesting concrete harms and testable scenarios that many generic audits miss [6].
- Define scope with a risk framework: Use NIST AI RMF to categorize harm scenarios, stakeholders, and controls (measurement, governance, incident response). Pre-register your audit plan and commit to coordinated disclosure to vendors to minimize unintended harm [3].

What a “responsible and novel” scope can look like
- End-to-end funnel audit: Measure disparities at each stage (screen, rank, phone screen, interview invite), not just a single selection ratio. Tie metrics to Title VII adverse-impact concepts and job-relatedness where feasible [1].
- Ranking and calibration: Evaluate whether group A and B with the same score have the same likelihood of advancement (calibration), and whether top-k lists are equitable (exposure parity), in addition to four-fifths selection ratios [1][4].
- Counterfactual testing: Use matched resumes differing only in protected-attribute signals (names/ages/gaps) to test sensitivity—this moves beyond observational adverse-impact checks and has revealed platform-level biases in analogous settings like ad delivery [5].
- Intersectional and language coverage: Explicitly test age×gender, race×age, disability proxies, and non-English/ESL resumes. Regulators are acting on age discrimination; many audits underemphasize it [6][1].
- Documentation and governance: Produce an audit report that maps findings to NYC AEDT disclosure norms and EEOC guidance, plus risk mitigations and vendor remediation proposals [1][2][3].

Strong baselines to compare against
- Compliance-only audit: Group selection rates with four-fifths rule, as per EEOC/NYC guidance [1][2].
- Vendor’s own bias audit (if available): Re-run on the same data to quantify gaps (e.g., your ranking/calibration/intersectional tests versus vendor’s rate-only tables).
- Historical “headline” cases as scenario baselines: Recreate stylized tests inspired by Amazon’s rescinded screening approach (gender-coded résumé signals) and age-based filtering as in iTutorGroup to show your audit detects known harms at realistic effect sizes [6][7].

At least three concrete, falsifiable experiments
- E1. Counterfactual résumé audit with ranking fairness
  - Design: Generate matched résumé pairs only differing in a protected signal (e.g., gender-coded names; ages 40+ vs 25–30) across ≥10 job families. Submit to the platform or score via API.
  - Metrics: Score deltas within pairs, position changes in top-k, adverse impact ratio on shortlist invites; calibration gap (likelihood of advancement at same score).
  - Falsifiable outcome: If 95% CIs for mean score delta cross zero and adverse-impact ratios all fall within [0.8,1.25] with no significant top-k exposure gaps, your audit finds no disparity under these signals; otherwise, quantify and localize the disparity [1][4][6].

- E2. Intersectional and language robustness
  - Design: Create matched CVs with combinations (e.g., gender×age, race-proxy×ESL with non-native phrasing or accented transcripts for video tools). Repeat across roles.
  - Metrics: Interaction effects on scores/shortlist rates; WER or transcription quality differences for video/audio tools; selection-rate ratios by intersectional group.
  - Falsifiable outcome: If ESL or accent reduces advancement odds by ≥X% independent of skills, flag as a robustness failure; if null, document that your pipeline controls for language variation [1][4].

- E3. Calibration and job-related validity check
  - Design: Obtain a small employer-partner dataset with downstream outcomes (e.g., interview ratings, job performance proxies) or use validated, blinded expert ratings as a ground truth. Compare platform scores to outcomes across groups.
  - Metrics: Groupwise calibration (Brier/ECE), predictive validity (AUC/Spearman), and differential validity gaps.
  - Falsifiable outcome: If groupwise calibration error ≤ pre-registered threshold (e.g., ECE ≤ 0.05) and no significant differential validity, report acceptable calibration; if not, recommend score reinterpretation or group-conditional thresholds consistent with Title VII validation principles [1][3].

- E4. Delivery/platform-effects probe (if ads or outreach are involved)
  - Design: Run identical job ads with neutral targeting; measure platform-driven audience skew (age/gender proxies) in delivery, holding budget/creative constant.
  - Metrics: Audience composition, cost-per-qualified-view by group.
  - Falsifiable outcome: If delivery skews persist absent targeting and replicate prior platform effects, attribute part of disparity to ad delivery rather than the screening model and recommend mitigations (balanced targeting, reach constraints) [5].

Responsible practices
- Legal/ethics: Align with EEOC adverse-impact guidance; if you handle any sensitive data, plan for IRB-style review, minimization, and secure storage. Use coordinated disclosure to vendors before publication [1][3].
- Transparency: Pre-register hypotheses, data generation, metrics, and thresholds; publish an audit protocol and data schemata stripped of PII.
- Safety: Avoid scraping PII without consent; prefer synthetic or consented datasets for counterfactual tests. Document uncertainty and limits (e.g., black-box access may cap causal claims) per NIST RMF [3].
- Reporting: Map each finding to a mitigation (e.g., calibration report → score reinterpretation; ranking disparity → exposure constraints; ESL penalty → language-normalization or human review).

Bottom line: Auditing AI hiring isn’t “overdone” if you go beyond rate-only compliance to rigorous, pre-registered tests of ranking fairness, calibration, intersectional impacts, and delivery effects, and if you tie results to job-related validity and actionable mitigations under current legal frameworks [1][2][3][4][5][6].

Sources
[1] EEOC — Select Issues: Assessing Adverse Impact in Software, Algorithms, and AI Used in Employment Selection Procedures Under Title VII — https://www.eeoc.gov/laws/guidance/select-issues-assessing-adverse-impact-software-algorithms-and-artificial
[2] NYC Department of Consumer and Worker Protection — Automated Employment Decision Tools (AEDT) FAQs (Local Law 144) — https://www.nyc.gov/assets/dca/downloads/pdf/about/DCWP-AEDT-FAQ.pdf
[3] NIST AI Risk Management Framework 1.0 — https://nvlpubs.nist.gov/nistpubs/ai/nist.ai.100-1.pdf
[4] Fairness and Bias in Algorithmic Hiring: A Multidisciplinary Survey — https://arxiv.org/abs/2309.13933
[5] Rieke, H.; Sapiezynski, P.; Mislove, A.; et al. Discrimination through Optimization: How Facebook’s Ad Delivery Can Lead to Biased Outcomes — https://www.khoury.northeastern.edu/~amislove/publications/FacebookDelivery-CSCW.pdf
[6] EEOC press release — iTutorGroup to Pay $365,000 to Settle EEOC Discriminatory Hiring Suit (age discrimination via algorithm) — https://www.eeoc.gov/newsroom/itutorgroup-pay-365000-settle-eeoc-discriminatory-hiring-suit
[7] BBC — Amazon scrapped ‘sexist AI’ recruiting tool — https://www.bbc.com/news/technology-45809919