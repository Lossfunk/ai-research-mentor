**Intuition**: The AI hiring audit space is indeed crowded, but most work focuses on static fairness metrics. Your novelty will likely come from either: (1) examining temporal dynamics or deployment realities that prior work ignores, or (2) targeting underexplored failure modes like procedural fairness or partial compliance scenarios [P1][P2].

**Why this is principled**: Rather than avoiding a "saturated" area, effective researchers find specific angles that existing work overlooks. The key is systematic novelty assessment followed by aggressive scope refinement to find tractable, high-impact questions [G1][G16].

## Strategic Questions

1. **What's your access level?** Can you get real hiring data from companies, or are you limited to public datasets? This dramatically affects what questions are feasible.

2. **Which specific platforms?** Are you targeting major vendors (HireVue, Pymetrics, etc.) or internal corporate systems? Different platforms have different audit surfaces.

3. **What's your timeline and resources?** A 3-month project needs a much narrower scope than a PhD thesis.

## Responsible Novelty Checking Protocol

**Phase 0 (Next 2 weeks):**

1. **Systematic literature mapping**: Create a spreadsheet of 30+ recent hiring audit papers. For each, note: dataset used, fairness metrics evaluated, platform type, key findings, and identified limitations. Look for patterns in what's missing.

2. **Platform audit surface analysis**: Document what aspects of hiring platforms are audit-able: resume parsing, video interview analysis, game-based assessments, etc. Most papers focus on 1-2 components—find the gaps.

3. **Stakeholder interview sprint**: Talk to 3-5 people: HR tech vendor, corporate HR manager, job seeker who's experienced AI screening, and a labor lawyer. Ask each: "What fairness concerns aren't being addressed by current research?"

## Three Concrete Experiments to Test Novelty

**Experiment 1: Temporal Fairness Drift**
*Objective*: Test if hiring platform fairness degrades over time due to concept drift or feedback loops. Hypothesis: Fairness metrics will show significant degradation over 6-12 months due to changing job markets and candidate pools.
*Setup*: Partner with a company using AI hiring tools to collect longitudinal data. Track fairness metrics (demographic parity, equalized odds) monthly.
*Metrics*: Rate of fairness metric change, correlation with hiring volume/seasonality.
*Expected*: 15-30% degradation in fairness metrics over 6 months, with faster drift during high-volume hiring periods.
*Interpretation*: Would reveal need for continuous auditing rather than one-time fairness assessments.
*Follow-up*: Develop adaptive fairness monitoring systems.

**Experiment 2: Multi-Stage Fairness Cascades**
*Objective*: Examine how fairness compounds across hiring pipeline stages (resume screen → video interview → assessment games). Hypothesis: Early-stage bias amplifies through the pipeline, but different protected attributes are affected at different stages.
*Setup*: Audit a platform that uses multiple AI components. Measure fairness at each stage independently and in combination.
*Metrics*: Fairness metric progression, attribution analysis of which stages contribute most to final bias.
*Expected*: Later stages show 2-3x higher bias than early stages for some attributes, but not others.
*Interpretation*: Would show need for stage-specific fairness interventions rather than pipeline-wide fixes.
*Follow-up*: Design stage-aware fairness constraints.

**Experiment 3: Procedural vs. Distributive Fairness Gap**
*Objective*: Test whether platforms optimized for distributive fairness (equal outcomes) create procedural unfairness (unfair process). Hypothesis: Current platforms sacrifice transparent, explainable decisions to achieve statistical fairness.
*Setup*: Compare platforms on both procedural fairness metrics (transparency, contestability, consistency) and distributive metrics across demographic groups.
*Metrics*: Correlation between distributive fairness scores and procedural fairness measures, user trust surveys.
*Expected*: Strong negative correlation (-0.6 or higher) between distributive fairness optimization and procedural fairness.
*Interpretation*: Would highlight need to balance process and outcome fairness, not just optimize for statistical parity.
*Follow-up*: Develop multi-objective optimization frameworks.

## Key Literature Anchors

Start with **Procedural Fairness in Machine Learning** [P1]—it identifies the gap between process and outcome fairness that most hiring audits ignore. Then examine **Fair Machine Learning Under Partial Compliance** [P2] for insights on real-world deployment challenges.

**Next Actions (This Week):**
1. Complete the literature mapping spreadsheet (8-10 hours)
2. Reach out to 2 potential industry contacts for data access conversations
3. Draft a one-page problem statement focusing on the most underexplored angle you identify

What access to hiring platforms or data do you currently have? This will determine which of these experiments is most feasible for your situation.