Great instinct to sanity-check novelty and scope before you commit. Here’s a fast, responsible way to do it that balances rigor, feasibility, and ethics.

A. Quick novelty scan (1–2 weeks)
- Map the space before you pick a target:
  - Inventory what regulators and vendors already publish. NYC’s Local Law 144 requires independent bias audits for Automated Employment Decision Tools and public summaries; scrape those summaries and vendor docs into a simple matrix (tool type, tasks, protected attributes, metrics, time covered, sample size, audit method) [P6]. 
  - Read at least one recent technical audit/automation effort to see what’s “standard practice” (e.g., pipelines for bias detection and reporting) [P1].
  - Organize your notes using an auditability framework: claims (validity, utility), evidence, and verification routes (black-box vs access), so you can see where prior work is thin (e.g., robustness, replicability, temporal drift, documentation) [G25].
- Decide your differentiator up front:
  - Choose one narrow axis (e.g., resume screeners only; or public LL144 audit summaries vs actual outcomes; or robustness to formatting changes) and commit to a small number of crisp, falsifiable tests [G12]. 
  - Prefer tractable problems where you can get feedback fast and credibly show value, even if the scope is smaller [G8].

B. Bound the scope responsibly
- Ethical/legal guardrails:
  - If you plan black-box audits (e.g., submitting synthetic applications), pre-register your protocol, model harms/benefits, and get IRB if applicable; minimize deception and avoid interfering with real candidates [G5]. 
  - Use public LL144 postings and sandbox datasets where possible, and respect platform terms of use; the Singapore BAC Big Data/AI guidance is a useful reference for risk analysis and transparency expectations [G24].
- Reproducibility:
  - Pre-register hypotheses and analysis plans; define primary outcomes and power targets; share your code/audit harness and a synthetic dataset for replication [G5], [G12].
- Feasibility check:
  - Verify you can run end-to-end once: input procurement (data or sandbox), test harness, metric computation, reporting. Kill any component that doesn’t fit your timeline/resources [G8].

C. Three concrete, falsifiable experiments
1) Black-box disparate impact test of resume screeners
- Hypothesis: For a fixed job family and period, the model’s impact ratios for sex and race/ethnicity meet or exceed the common 80% rule threshold that appears in many LL144 bias audit reports. 
- Setup: Construct matched synthetic resumes varying only protected-attribute proxies (name, pronouns, demographic-coded signals) while holding qualifications constant. Submit to the platform or a realistic reproduction.
- Metrics: Selection rate per group; impact ratio (minority rate / majority rate); confidence intervals via bootstrap; preregistered subgroup list. 
- Expected outcome: Either impact ratios ≥ 0.8 (consistent with compliance claims) or violations localized to specific roles/time windows; either way, your contribution is a transparent, replicable harness and effect-size estimates [P6], [P1], [G12], [G25].

2) Robustness and auditability stress test
- Hypothesis: Rankings are stable under content-preserving changes (formatting, minor style edits, identical content in different file types) and under removal of non-job-relevant signals.
- Setup: Take a base set of resumes and generate controlled variants (PDF vs DOCX; reordered sections; formatting-only changes; removal of demographic hints). Evaluate rank stability and score drift.
- Metrics: Spearman rank correlation; absolute score deltas; fraction of candidates that cross decision thresholds due to non-substantive changes.
- Expected outcome: Quantified robustness profile with clear pass/fail thresholds; if instability exists, you identify the specific transformations that break auditability and replicability [G25], [G12].

3) Public LL144 summary reproducibility audit
- Hypothesis: For a sample of platforms/employers with posted LL144 bias audit summaries, the reported metrics are reproducible given their stated methodology and a similarly stratified sample.
- Setup: Select N posted summaries; extract their declared methodology, time windows, sample sizes, and protected attributes. Recreate the methodology on a matched synthetic or partner dataset, or contact vendors for de-identified evaluation splits if feasible.
- Metrics: Agreement between reported impact ratios and your recomputed metrics (tolerance bands); completeness of required fields; transparency scorecard vs LL144 guidance.
- Expected outcome: A transparency/reproducibility map highlighting where summaries are sufficient versus under-specified, with concrete recommendations for standardized reporting [P6], [G25], [G5].

(Optional) 4) Temporal drift of fairness metrics
- Hypothesis: Impact ratios and selection rates drift across months/quarters, even if a snapshot passes audit.
- Setup: Repeat Experiment 1 monthly on the same job family.
- Metrics: Time-series of impact ratios; drift detection statistics; change-point analysis.
- Expected outcome: Evidence for or against stability; guidance on audit frequency [G25], [G12].

D. How to judge novelty quickly
- If you can show any of the following, you likely have a novel, useful slice:
  - A replicable black-box harness that links LL144-style public reporting to independently measured outcomes [P6], [P1].
  - Robustness findings (content-preserving perturbations) that existing audits don’t test explicitly [G25].
  - Transparent, preregistered methodology with power analysis and open materials—surprisingly rare in vendor audits [G5].
- If your initial scan shows your exact slice is already done, pivot to:
  - A different tool type (e.g., interview/video analysis vs resume screening), 
  - A different dimension (temporal drift, intersectionality, documentation quality),
  - Or a meta-audit: reproducibility and transparency of public audit summaries.

E. Minimal execution plan
- Week 1: Literature/legal scan and matrix; pick one narrow differentiator [P6], [P1], [G25].
- Week 2: Draft/preregister protocols; pilot data and power analysis [G5].
- Weeks 3–4: Run Experiment 1 + a small robustness slice; write up a short, data-first report with clear figures and limitations [G12].

Two anchor readings to start
- What we learned while automating bias detection in AI hiring (for pipeline ideas and pitfalls) [P1].
- NYC DCWP’s Automated Employment Decision Tools page (to align with LL144 obligations and public summary requirements) [P6].

Citations
• [P1] What we learned while automating bias detection in AI hiring ... - arXiv — https://arxiv.org/html/2501.10371v1
• [P6] Automated Employment Decision Tools (AEDT) - DCWP - NYC.gov — https://www.nyc.gov/site/dca/about/automated-employment-decision-tools.page
• [G5] Best Practices Research: A Methodological Guide for the Perplexed — https://www.researchgate.net/publication/31052323_Best_Practices_Research_A_Methodological_Guide_for_the_Perplexed
• [G8] Principles of effective research — http://michaelnielsen.org/blog/principles-of-effective-research/
• [G12] An Opinionated Guide to ML Research — http://joschu.net/blog/opinionated-guide-ml-research.html
• [G24] Ethical Use of Big Data and AI (BAC-BDAI report) — https://file.go.gov.sg/bac-bdai-report-2025.pdf
• [G25] Assessing the Auditability of AI-integrating Systems — https://arxiv.org/html/2411.08906v1

If you share which platform type and access level you have (black-box vs partner data), I can help tailor the scope and preregistration template.