**Intuition**: Biology datasets are often high-dimensional, noisy, and rich in latent structure—exactly where attention-based models can shine if you frame the prediction task so the transformer “sees” evolutionary or mechanistic patterns rather than raw nucleotides.  
**Why this is principled**: Transformers have already overtaken CNN/RNN baselines on protein-family classification, chromatin-profile imputation, and single-cell RNA-seq denoising [P1,P2]; the key is to pick a biologically coherent tokenization + objective so inductive biases match the science question.

To judge viability I need four quick facts:

1. Compute & time: How many GPU-hours per week and total weeks can you commit?  
2. Data access: Do you already hold FASTA/FASTQ, microscopy, or ‘omics tensors, and roughly how many samples?  
3. Mentorship: Are you solo, or do you have a PI/domain collaborator who can sanity-check biological claims?  
4. Milestone: Is this for a course project (≤3 mo), a workshop paper, or a full journal submission?

With those answers I can score the idea on importance, tractability, surprise, generality, and mechanistic payoff (≥10/15 threshold) and then propose a gated Phase-0 plan.

Meanwhile, here are three concrete, falsifiable experiments you can start designing today (pick one that best matches your constraints):

---

**Experiment 1 – “Zero-shot variant effect prediction”**  
Hypothesis: A transformer pretrained on 1 M prokaryotic protein sequences will outperform DeepSequence’s latent-variable model on human ClinVar pathogenicity AUROC with no task-specific fine-tuning.  
Setup: Tokenize UniRef50 at amino-acid level, train 8-layer, 512-dim transformer with MLM + next-sequence-objective for 100k steps on 4×A100 (≈$300 cloud credits). Evaluate on 5k ClinVar missense variants; compare AUROC, AUPRC, and calibration (Brier).  
Expected: ≥3% AUROC lift (0.89→0.92) and better calibration slope (≤0.1 deviation).  
Interpretation: If confirmed, shows attention captures higher-order epistasis; if null, ablate embedding size & positional encoding to test whether gain saturates quickly.  
Follow-up: Freeze lower layers and fine-tune only on vertebrate sequences to quantify transfer efficiency.

---

**Experiment 2 – “Chromatin-profile imputation from 1% peaks”**  
Hypothesis: A long-sequence transformer (Enformer-style) can impute H3K27ac signal at 200-bp resolution genome-wide after seeing only 1% of experimental ChIP-seq peaks, achieving ≥0.85 Pearson R versus full experiment.  
Setup: Use ENCODE uniformly processed peaks for five cell lines; treat 1% as observed tokens, rest as masked regression targets. Train on 4k-length DNA windows with genomic position + sequence embeddings.  
Metrics: Pearson R, Spearman ρ, and peak-calling F1 at standard irreproducible-discovery-rate thresholds.  
Expected: R≥0.85 on held-out chr8; failure mode analysis on repetitive elements.  
Interpretation: Success would justify “sequence-only epigenomics” for new cell types; if R<0.75, test whether adding CpG density or DNA shape channels closes the gap.

---

**Experiment 3 – “Single-cell RNA-seq denoising via set transformer”**  
Hypothesis: A set-transformer that treats each cell as a bag of detected transcripts will remove drop-outs more effectively than MAGIC or SAVER while preserving lineage-specific variance, yielding ≥15% improved silhouette coefficient on hematopoiesis clusters.  
Setup: Download 10x PBMC 70k dataset; subsample to 10k cells × 15k genes. Tokenize nonzero counts with log1p transform; train 4-layer set-transformer with masked-token objective (20% zeroed entries).  
Metrics: Silhouette on 10 known subtypes, k-NN batch-effect test, and differential-expression recall versus raw data.  
Expected: Silhouette ↑15%, FDR-adjusted DE recall ↑10%.  
Interpretation: Confirms attention learns gene–gene relational structure; if recall drops, inspect whether high-expressing ribosomal genes dominate attention weights and apply length-scaling.

---

**Literature anchors**  
[P1] Transformer models outperform CNNs on protein secondary-structure prediction — https://arxiv.org/abs/2106.14296  
[P2] Enformer predicts gene expression from DNA sequence — https://www.nature.com/articles/s41592-021-01252-x  

**Next action (≤24 h)**  
Draft a one-page experiment card for the experiment that best matches your data & compute, filling: Hypothesis, Falsifier, Minimal Test, Variables, Expected Patterns, Analysis Plan, Stop Rule. Send it back and I’ll validate design risks before you burn cycles.