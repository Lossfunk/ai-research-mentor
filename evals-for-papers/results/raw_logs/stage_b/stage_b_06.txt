Short answer: yes, it’s viable—provided you define a concrete prediction task, control batch effects and leakage, and benchmark against strong non-transformer baselines. Multiple recent works show that transformers and multimodal models can (a) predict gene expression from histopathology and (b) improve downstream clinical tasks when fusing images with molecular features [P2][P3][P7]. New datasets aligning slides and transcriptomics further lower the barrier to experimentation [P6], and spatial transcriptomics modeling is an active area where transformer-like and diffusion architectures are being explored [P1][P8].

Practical guidance to make it work
- Nail the task and alignment: Specify supervised targets (e.g., per-gene expression, molecular subtype, survival) and whether you have paired data (e.g., slide–RNA) vs weak alignment (cohort-level). Paired slide–gene expression enables direct supervision and clearer evaluation [P2][P3][P6].  
- Guard against leakage and batch effects: Split by patient/institution; reserve an out-of-cohort test set (different hospital/scanner/assay) to assess generalization [P7]. Use stain normalization and include cohort/batch covariates; report results stratified by site.  
- Baselines to beat: Elastic net or random forest on hand-crafted features; CNNs or co-expression CNNs for image→gene expression [P3].  
- Metrics that matter: For image→gene expression, per-gene Pearson r/MSE and gene-set-level concordance (e.g., pathway-level correlation). For classification, AUROC/AUPRC; for prognosis, C-index. Evaluate cross-cohort drop (ID vs OOD).  
- Pretraining and fusion: Start with ImageNet- or pathology-pretrained ViTs; compare early vs late fusion when combining slides and transcriptomic embeddings [P7].  
- Interpretability and biological plausibility: Relate salient regions/genes to pathways (e.g., enrichment on attention-selected genes/tiles); avoid causal claims—these are correlational unless backed by interventional evidence.  
- Reporting: Include data/model cards; describe tissue types, assay kits, scanners, and preprocessing; release splits and code for reproducibility.

Three concrete, falsifiable experiments
1) Slide-to-gene expression prediction with ViT vs CNN baselines  
Hypothesis: A ViT trained on H&E tiles predicts a subset of genes better than a matched CNN and regression baselines, with robust performance on a held-out institution.  
Setup: Use a slide–RNA paired dataset (e.g., STimage-1K4M or similar cohorts) and tile-level ViT with patient-level splits; compare to a ResNet/CNN and to co-expression CNNs [P3]; optionally include ImageNet pretraining.  
Metrics: Per-gene Pearson r and MSE on ID and OOD cohorts; fraction of genes with r>0.2; pathway-level correlation (GSEA on predicted vs observed).  
Expected outcome: ViT outperforms CNNs on a subset of genes; performance drops OOD but remains above baselines for robust genes [P2][P3][P6].

2) Multimodal fusion for clinical endpoints  
Hypothesis: Fusing slide-derived embeddings with measured (or predicted) transcriptomic features improves subtype classification or survival prediction vs single-modality models.  
Setup: Build late-fusion models combining ViT features with RNA embeddings; compare to image-only and RNA-only models. Evaluate cross-cohort generalization (train on Site A, test on Site B).  
Metrics: AUROC/AUPRC (subtype), C-index (survival), and OOD delta; ablations for fusion strategy (early vs late) and calibration.  
Expected outcome: Fusion improves ID and OOD performance over single modalities, consistent with reported gains from combining pathology and transcriptomics [P7].

3) Spatial transcriptomics completion from histology with transformer backbones  
Hypothesis: Incorporating transformer features into a spatial transcriptomics completion pipeline improves spatial gene prediction versus non-transformer backbones.  
Setup: On spatial transcriptomics datasets, predict spatially resolved expression from H&E images. Compare transformer-based encoders versus CNN encoders within a completion framework; add spatial priors (e.g., neighborhood attention).  
Metrics: Gene-wise spatial Pearson r, spot-level MSE, spatial autocorrelation (Moran’s I) preservation; generalization across tissue types.  
Expected outcome: Transformer encoders yield higher spatial correlation and better structure preservation than CNN counterparts, aligning with recent progress in ST completion models [P1][P8].

Anchor papers to start with
- Generating crossmodal gene expression from cancer histopathology improves multimodal AI predictions — http://arxiv.org/abs/2502.00568v3 [P7]  
- STimage-1K4M: A histopathology image-gene expression dataset for spatial transcriptomics — http://arxiv.org/abs/2406.06393v2 [P6]

If you share the specific modalities (e.g., slides + bulk RNA vs single-cell + images) and tasks you have in mind, I can tailor architectures, baselines, and evaluation plans.

Citations
- [P1] Latent Gene Diffusion for Spatial Transcriptomics Completion — http://arxiv.org/abs/2509.01864v1
- [P2] Breast Cancer Histopathology Image based Gene Expression Prediction using Spatial Transcriptomics data and Deep Learning — http://arxiv.org/abs/2303.09987v1
- [P3] Transcriptome-wide prediction of prostate cancer gene expression from histopathology images using co-expression based convolutional neural networks — http://arxiv.org/abs/2104.09310v1
- [P4] Leveraging Vision-Language Embeddings for Zero-Shot Learning in Histopathology Images — http://arxiv.org/abs/2503.10731v1
- [P5] Histopathology Image Report Generation by Vision Language Model with Multimodal In-Context Learning — http://arxiv.org/abs/2506.17645v1
- [P6] STimage-1K4M: A histopathology image-gene expression dataset for spatial transcriptomics — http://arxiv.org/abs/2406.06393v2
- [P7] Generating crossmodal gene expression from cancer histopathology improves multimodal AI predictions — http://arxiv.org/abs/2502.00568v3
- [P8] Spatial Transcriptomics Analysis of Spatially Dense Gene Expression Prediction — http://arxiv.org/abs/2503.01347v2