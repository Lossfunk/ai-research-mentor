Short answer: Yes, it’s viable—and an active area already—if you scope it to a concrete biological modality and task. Transformers are being used for single‑cell foundation models (scGPT, Geneformer), sequence-to-function modeling (Enformer), protein language modeling (ESM), and multi‑omics integration (Pathformer and related work) [1][2][3][4][5][6]. Novelty will come from the specific biological question, how you fuse modalities, and whether you show robust gains over strong baselines on out‑of‑distribution cohorts.

Where this is already working (evidence)
- Single‑cell “foundation” models (pretrained on millions of cells) transfer to downstream tasks like cell type annotation and perturbation response prediction [1][2].
- Sequence→function models (e.g., Enformer) use long‑context transformers to predict gene expression from DNA sequence by capturing long‑range interactions [3].
- Protein LMs (ESM) learn rich structure/function representations that transfer to many tasks [4].
- Transformer architectures for multi‑omics integration have been shown to outperform or match alternatives on disease prediction and cross‑modal inference, especially when they encode pathway structure or cross‑attention between modalities [5][6][7].

What could be novel
- Multi‑omics alignment with explicit biological structure: pathway‑ or network‑informed attention, then show better OOD generalization and interpretability vs generic fusion [5].
- Cross‑modal prediction at single‑cell resolution (e.g., RNA→ATAC or RNA→protein) with uncertainty estimates and calibration across labs/technologies [1][6].
- Data‑efficient transfer: demonstrate strong few‑shot adaptation of a pretrained transformer (scGPT/Geneformer) to rare cell types or small cohorts vs training from scratch [1][2].
- Robustness across platforms/cohorts: show stable performance from one sequencing chemistry/lab to another without re‑tuning (or with minimal adapters) [1][6].

Three concrete, falsifiable experiments
- E1. Multi‑omics fusion vs strong baselines
  - Task: Disease classification or prognosis from matched RNA+ATAC (or RNA+proteomics).
  - Models: (a) Late‑concatenation MLP baseline; (b) Transformer with cross‑attention; (c) Pathway‑informed transformer (Pathformer‑style) [5].
  - Metrics: AUROC/AUPRC, calibration (ECE), and OOD performance across cohorts.
  - Criterion: (c) improves OOD AUROC by ≥0.03 vs (a) and ≥0.02 vs (b) at matched parameter count; else no fusion advantage. Supported by prior multi‑omics transformer literature [5][7].

- E2. Pretraining helps low‑data regimes
  - Task: Cell type or perturbation response prediction on a small labeled dataset.
  - Models: (a) From‑scratch transformer; (b) Fine‑tuned scGPT or Geneformer frozen encoder + small head [1][2].
  - Metrics: Accuracy/F1, label efficiency (accuracy vs number of labeled cells).
  - Criterion: (b) matches (a) with ≤25% of labels and retains ≥95% of accuracy at full data; if not, no practical pretraining benefit. Prior single‑cell foundation models motivate this test [1][2].

- E3. Cross‑modal prediction and calibration
  - Task: Predict ATAC peak accessibility or surface protein levels from RNA at single‑cell level.
  - Models: Transformer encoder on RNA with cross‑modal decoder vs a non‑transformer baseline; compare to prior transformer multimodal prediction results [6].
  - Metrics: Pearson/Spearman, R2, CRPS for probabilistic predictions; calibration across labs.
  - Criterion: Transformer improves mean R2 by ≥0.05 and maintains 90% prediction‑interval coverage within 90% ± 5% across labs; else benefit not established. Prior work shows feasibility of transformer multimodal prediction [6].

Practical next steps
- Pick a concrete scope first: single‑cell multi‑omics (RNA+ATAC/protein) vs bulk multi‑omics vs sequence→expression. Start with public datasets and a single task.
- Reuse pretrained models when possible: scGPT or Geneformer for cell‑level tasks; Enformer for sequence→expression; ESM for protein‑related tasks [1][2][3][4].
- Plan for batch effects and OOD testing: split by lab/technology to make claims credible; report calibration and uncertainty.
- Keep interpretability in mind: pathway‑informed attention or post‑hoc enrichment analyses to avoid black‑box claims [5].

If you share a bit more about your specific datasets (single‑cell vs bulk; which modalities; target task), I can tailor a minimal viable plan and baselines.

Sources
[1] scGPT: toward building a foundation model for single-cell multi-omics using generative AI — https://www.nature.com/articles/s41592-024-02201-0
[2] Geneformer — pretrained transformer for gene expression — https://geneformer.readthedocs.io/en/latest/
[3] Enformer: Effective gene expression prediction from sequence by integrating long-range interactions — https://storrs.io/enformer/
[4] ESM (FAIR): Pretrained Protein Language Models — https://github.com/facebookresearch/esm
[5] Pathformer: a biological pathway informed transformer for disease multi-omics integration — https://academic.oup.com/bioinformatics/article/40/5/btae316/7671099
[6] Single-Cell Multimodal Prediction via Transformers — http://arxiv.org/pdf/2303.00233v3.pdf
[7] A Transformer-Based Deep Learning Model Integrating Multiomic Data for Cancer Diagnosis and Prognosis — https://pmc.ncbi.nlm.nih.gov/articles/PMC11652962/