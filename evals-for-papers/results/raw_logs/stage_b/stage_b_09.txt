Yes—there is still room for novelty, but only if you deliberately step away from the “fine-tune a multilingual LLM and report perplexity” template that now dominates the small literature [P2][P5]. The clearest gaps are (1) speaker-validated evaluation protocols, (2) generative *data augmentation* that respects cultural protocol, and (3) small-model distillation that keeps inference on-device for rural/remote communities [P4].

**Strategic questions to shape your angle**  
1. Which revitalization bottleneck matters most to the community you work with—curriculum content, conversational practice, or archival transcription—and what evidence do you have that a generative model is the cheapest way to close that gap?  
2. How will you quantify “cultural fidelity” beyond BLEU—e.g., taboo-word avoidance, clan-specific register, or story-line coherence judged by elders?  
3. What compute and data storage constraints actually apply on the ground (bandwidth, offline phones, GPU hours) and how will you ablate model size vs. quality under those hard ceilings?

**Three concrete, falsifiable experiments** (14-day Phase 0 deliverables)

**E1. Speaker-in-the-loop preference distillation**  
Hypothesis: A 1.1 B-parameter model distilled with 500 community-rated generations beats a 7 B-parameter baseline on *human* acceptability despite 3× lower perplexity.  
Setup: Fine-tune two Llama-derived models on 2 k aligned sentence pairs in Māori; collect 20 elder preference ratings per generated sentence (N=1 k).  
Metrics: Acceptability %, cultural-safety violation count, inference latency on Pixel 4a.  
Expected: ≥15 % acceptability lift for small model; ≥90 % latency under 300 ms.  
Interpretation: If acceptability ≥ parity, publish distillation recipe; if not, trace failures to register or lexical gaps.

**E2. Controlled story generation for language nests**  
Hypothesis: Prompting with morphological slot templates reduces code-switching (English tokens) by ≥40 % versus vanilla few-shot prompting.  
Setup: Use 100 bilingual Chickasaw–English seed stories; generate 500 new stories with/without template control; count English tokens and elder-rated code-switch severity.  
Metrics: % English tokens, mean code-switch severity Likert score.  
Expected: Template condition ≤5 % English tokens; control ≥12 %.  
Follow-up: Vary template granularity (word vs. clause level) to find saturation point.

**E3. Synthetic data augmentation vs. human-curated only**  
Hypothesis: Adding 2 k synthetic sentences (filtered by community vote) improves downstream ASR word-error-rate by ≥10 % relative over human-curated 2 k baseline.  
Setup: Re-train the 4.8 % WER Chickasaw ASR pipeline [P1] with three data regimes: human-only, synthetic-only, mixed; keep utterance length and speaker distribution constant.  
Metrics: WER %, speaker acceptance %, training time.  
Stop rule: If synthetic data raises WER by >2 % absolute, halt and publish negative result.

**Pitfalls to flag now**  
- **Cultural extraction**: Publishing a high-perplexity model without community co-authorship or data-sharing agreements can breach Indigenous data sovereignty [P4].  
- **Evaluation inflation**: Automatic metrics like BLEU correlate weakly with elder judgments; omitting human evaluation is now a desk-reject at *AmericasNLP* [P2].  
- **Lexical leakage**: Generating ceremonial or gender-restricted words can cause real harm; build block-lists *with* speakers, not for them.  
- **Compute promise mismatch**: A 13 B model that needs A100-hours is unusable in villages with one 3G tower; always report energy & hardware cost per 1 k inferences.

**Next actions (this week)**  
1. Draft the experiment card for E1 (include falsifier: “small-model acceptability ≤ baseline”).  
2. Secure IRB/community research agreement that gives veto power over any generated release.  
3. Run a 1-hour latency test on the cheapest phone your target learners actually own; log baseline milliseconds—this number gates every later modeling choice.

Citations  
[P1] Indigenous Language Revitalization through AI and Community — https://computing.mit.edu/wp-content/uploads/2024/06/Indigenous-Language-Revitalization.pdf  
[P2] Harnessing NLP for Indigenous Language Education — https://aclanthology.org/2025.americasnlp-1.14.pdf  
[P4] Harnessing the Power of Artificial Intelligence to Vitalize Endangered Indigenous Languages — https://arxiv.org/html/2407.12620  
[P5] Can small language models revitalize Indigenous languages? — https://www.brookings.edu/articles/can-small-language-models-revitalize-indigenous-languages/