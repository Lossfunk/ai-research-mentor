Short answer
Assess novelty by defining a precise wildfire-containment task and checking whether your safety constraints, coordination assumptions, and simulator fidelity go beyond prior safe-RL work (e.g., non-Markovian safety constraints, probabilistic risk bounds, and multi-objective safety-performance trade-offs) [P1][P6][P7]. Assess feasibility with a staged plan: formalize a constrained MDP, start in sim with validated fire-spread dynamics, establish strong non-learning baselines, quantify safety/performance trade-offs, and progress to hardware-in-the-loop and controlled field trials only after meeting safety thresholds [P2][P3][P4].

How to assess novelty
- Problem framing: Specify the exact containment task (e.g., line-building, water/retardant drop scheduling, plume mapping for evacuations), state/action spaces, and constraints (no-fly human zones, heat exposure budgets, minimum separation, battery and payload). Novelty often comes from accurate safety modeling and decision coupling, not just using RL.
- Safety modeling beyond standard C-MDPs:
  - Non-Markovian constraints (e.g., cumulative thermal dose, time-in-smoke, or sequence-dependent actions) are under-explored in robotics; methods that learn and enforce history-dependent constraints are recent [P1]. If your constraints are temporal/sequence-based and you enforce them during learning, that’s a plausible novelty axis [P1].
  - Risk-bounded policies (chance constraints) with explicit violation probabilities (e.g., ≤1%)—if you integrate probabilistic safety with task performance in an aerial-fire context, it can be novel [P6].
  - Conservative, risk-sensitive return shaping (distributional RL) to avoid tail-risk behaviors may differentiate your approach if you evaluate risk metrics, not just expected reward [P4].
  - Multi-objective safety-performance optimization with explicit Pareto analysis (containment time vs. violation rate) is a current safe-RL thread; applying it to wildfire aerial robotics with realistic constraints can be novel [P7].
- Action feasibility: If the aircraft must satisfy tight dynamics/actuation limits or flight envelopes, “action mapping” to constrained control spaces during learning can be a distinctive contribution for aerial firefighting [P2].
- Evaluation setting: Using validated fire-spread simulators (with wind/fuel variability), non-stationarity (changing wind or spot fires), communication dropouts, and partial observability will strengthen novelty if you quantify robustness and safety under these conditions. Safe exploration schemes that limit violations during learning further increase novelty [P3].

How to assess feasibility (stage-gated)
- Stage 0: Formal model
  - Define a constrained MDP or POMDP with safety costs and thresholds (e.g., chance constraints or non-Markovian constraints) [P1][P6].
  - Select safety metrics: violation rate, worst-case/quantile cost (e.g., CVaR), and area burned/containment time [P4][P7].
- Stage 1: Simulator fidelity
  - Start with a computationally cheap spread model for rapid iteration; later validate against higher-fidelity models and wind/fuel maps. Randomize wind, fuel moisture, ignition points to test non-stationarity and generalization [P4].
- Stage 2: Baselines and sample efficiency
  - Implement rule-based/MPC baselines and safety filters; compare learning curves, sample needs, and asymptotic performance. Constrained environments often reduce sample efficiency; plan for curriculum or demonstration-augmented training [P2][P3].
- Stage 3: Safety during learning
  - Use shields/filters or conservative objectives to bound violations during training; report exploration-phase safety separately from deployment-phase safety [P3][P4][P7].
- Stage 4: Robustness and ablations
  - Stress-test against wind gusts, degraded sensing, comms dropouts; run ablations on safety components (e.g., remove constraint learner) to quantify their contribution [P1][P4].
- Stage 5: Hardware-in-the-loop (HITL)
  - Close the sim-to-real gap with HITL, latency-in-the-loop testing, and action-mapping to enforce real vehicle limits [P2].
- Stage 6: Field readiness
  - Proceed only if violation probability and risk-sensitive metrics meet pre-registered thresholds with margins (e.g., 95% CI). Document procedures for fail-safes, remote pilot oversight, and geofencing.

What to measure (core metrics)
- Performance: containment time, area burned, resource usage (water/retardant, battery), mission completion rate.
- Safety: probability of entering prohibited zones, cumulative thermal exposure, minimum separation, near-miss count, CVaR of safety cost [P4][P6].
- Learning/compute: episodes to reach threshold performance, wall-clock training time, on-board inference latency [P2].
- Robustness: performance under wind/fuel shifts, sensor noise, GPS denial, comms loss [P4][P7].

Baselines to include
- Heuristics: myopic hotspot targeting; perimeter-following; sector coverage with no-fly zones.
- Model-based: MPC with fire-spread predictor and hand-tuned safety penalties.
- Safe-RL: constrained policy optimization or Lagrangian baselines; risk-sensitive (distributional) RL; multi-objective policy optimization [P4][P6][P7].
- Action-feasible control: RL with action-mapping vs. naive bounded actions [P2].

Three concrete, falsifiable experiments
1) Risk-bounded vs. unconstrained RL for containment
- Hypothesis: A probabilistic-constraint RL agent will maintain violation probability ≤1% without significantly increasing area burned compared to unconstrained RL [P6].
- Variables: Algorithm (chance-constrained RL vs. unconstrained SAC/PPO), violation threshold (0.5%, 1%, 5%), wind variability level.
- Metrics: Safety violation rate (with 95% CI), area burned, containment time, CVaR of safety cost [P4][P6].
- Expected outcome: Constrained agent achieves target violation rate with at most X% increase in area burned; unconstrained agent violates more often.

2) Non-Markovian safety constraints for thermal exposure
- Hypothesis: Enforcing learned non-Markovian constraints (e.g., cumulative heat dose over a time window) reduces exposure violations compared to Markovian penalties at similar task performance [P1].
- Variables: Constraint type (non-Markovian learned vs. Markovian penalty), exposure window length, wind non-stationarity level.
- Metrics: Exposure-violation rate, area burned, policy instability across non-stationary episodes.
- Expected outcome: Non-Markovian approach yields lower exposure violations without degrading containment time [P1].

3) Action mapping for flight-envelope feasibility
- Hypothesis: Explicit action mapping to feasible control sets improves safety and sample efficiency vs. naive clipped actions in a high-wind regime [P2].
- Variables: Control interface (action mapping vs. clipping), wind magnitude distribution, training seed.
- Metrics: Rate of dynamic-feasibility violations (in sim proxy), learning curves (episodes to threshold), containment time.
- Expected outcome: Action mapping reduces feasibility violations and accelerates learning [P2].

4) Safety-performance Pareto analysis under non-stationarity
- Hypothesis: Multi-objective policy optimization produces a better Pareto front (dominates penalty-tuned baselines) under wind shifts [P7].
- Variables: Optimization method (multi-objective vs. Lagrangian penalty), wind-shift magnitude, communication drop probability.
- Metrics: Pareto front (violation rate vs. containment time), hypervolume.
- Expected outcome: Multi-objective method dominates in hypervolume under distribution shift [P7].

5) Conservative distributional RL for tail-risk reduction
- Hypothesis: Conservative distributional RL reduces severe-outlier events (95th–99th percentile area burned) versus expectation-based safe RL at comparable mean performance [P4].
- Variables: Algorithm (conservative distributional vs. Lagrangian), risk level (quantiles).
- Metrics: Upper-quantile area burned, CVaR of safety cost, mean containment time.
- Expected outcome: Lower tail-risk metrics for distributional RL [P4].

Quick feasibility checklist
- Constraints codified and measurable (Markovian vs. non-Markovian)? [P1][P6]
- Baselines implemented and pre-registered? [P7]
- Training-time safety plan (shields/filters) in place? [P3][P4]
- Robustness plan (domain randomization + non-stationarity tests)? [P4]
- Action-feasibility mapping and latency budget validated? [P2]
- Stage gates for HITL and field tests with quantitative thresholds? [P6][P7]

Literature anchors (starting points)
- Safe Reinforcement Learning with Learned Non-Markovian Safety Constraints — http://arxiv.org/abs/2405.03005v1 [P1]
- Probabilistic Constraint for Safety-Critical Reinforcement Learning — http://arxiv.org/abs/2306.17279v2 [P6]

Citations
- [P1] Safe Reinforcement Learning with Learned Non-Markovian Safety Constraints — http://arxiv.org/abs/2405.03005v1
- [P2] Action Mapping for Reinforcement Learning in Continuous Environments with Constraints — http://arxiv.org/abs/2412.04327v1
- [P3] A Safe Exploration Strategy for Model-free Task Adaptation in Safety-constrained Grid Environments — http://arxiv.org/abs/2408.00997v1
- [P4] Conservative Distributional Reinforcement Learning with Safety Constraints — http://arxiv.org/abs/2201.07286v2
- [P5] Neurosymbolic Meta-Reinforcement Lookahead Learning Achieves Safe Self-Driving in Non-Stationary Environments — http://arxiv.org/abs/2309.02328v1
- [P6] Probabilistic Constraint for Safety-Critical Reinforcement Learning — http://arxiv.org/abs/2306.17279v2
- [P7] Safety Optimized Reinforcement Learning via Multi-Objective Policy Optimization — http://arxiv.org/abs/2402.15197v1

Key takeaways
- Anchor novelty in safety modeling (non-Markovian constraints, chance constraints, Pareto safety-performance) and realistic evaluation (non-stationarity, feasibility-aware control).
- Gate feasibility with a disciplined, safety-first pipeline from sim to HITL to field, backed by strong baselines and risk-sensitive metrics.