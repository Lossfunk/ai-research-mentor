Stage: A–B (problem framing and viability)

Quick intake so I can tailor this: 
- Compute/time: What GPUs and flight hardware do you have, and how many hours/week can you invest?
- Current assets: Do you already have a wildfire simulator or flight testbed? Single drone or multi-UAV?
- Mentorship/collab: Any wildfire scientists or forestry partners, or access to prescribed-burn sites?
- Targets/timeline: Research venue or demo date you’re aiming for?
- Biggest bottleneck right now: simulator fidelity, safety constraints, or coordination algorithms?

Strategic questions to scope novelty and feasibility
- Novelty: Which dimension are you improving—objective (containment vs. monitoring), safety guarantees (hard constraints), partial observability/delays, multi-agent coordination, or sim-to-real? Picking one yields a crisp contribution claim. [G11]
- Safety: Will you use constrained RL, runtime shielding, or human-in-the-loop safety envelopes? Your choice determines baselines and evaluation. [P1][P3][P4]
- Reality gap: What latencies and sensing gaps will you model (wind shifts, smoke occlusion, comm delays)? This affects whether POMDPs and delay-robust methods are essential. [P2][P3][P6]
- Evidence plan: What operational tests and critical-event scenarios will you pass before field trials? [P5]

How to assess novelty
- Map prior art by task: monitoring (front tracking), suppression (retardant drops), coordination (search/cover), and safety (constraint satisfaction). Identify gaps along one axis: e.g., “delay-robust, safety-constrained MARL for multi-UAV containment with runtime shields.” Safety-constrained RL and shielding under delay are current, active threads—position against them. [P1][P3]
- Define competitive baselines: (a) rule-based line-holding/coverage control; (b) MPC with safety buffer; (c) constrained RL (e.g., Lagrangian/CPO-style) and shielded RL. [P1][P7]
- Articulate your hard constraints/costs (e.g., no-fly envelopes, fuel, thermal limits), and how you enforce them (constrained RL vs. runtime safety layer). [P1][P7]

How to assess feasibility
- Model partial observability and delays (sensor/comm/actuation). Choose algorithms robust to delays and POMDP structure or add safety shields. [P2][P3][P6]
- Start with operational test design: critical fire-behavior scenarios (wind shift, spot fires, failed comms) and pass/fail criteria for safety and containment. [P5]
- Plan human oversight via dynamic safety envelopes for early deployments (clear escalation/override logic). [P4]

Three falsifiable experiments (compact, actionable)
1) Safety-constrained RL vs. Shielded RL under observation delays
- Hypothesis: Under random sensor/comm delays, a constrained RL policy with explicit cost budgets reduces safety violations vs. vanilla RL, and matches or exceeds a runtime-shielded RL policy on containment efficacy. [P1][P3]
- Setup: Multi-UAV containment in a wildfire sim with injected delays (e.g., 0–500 ms, dropouts). Train: (i) unconstrained PPO, (ii) constrained RL with cost budgets, (iii) PPO + runtime shield. 
- Metrics: Safety violations per hour, burned area growth rate, time-to-containment, fuel used. Success: (ii) <50% violations of (i) with ≥95% of (iii)’s efficacy.
- Interpretation: If constrained RL underperforms shielded RL on safety, prefer shields; if it matches safety and beats on efficiency, your novelty is validated.
- Follow-ups: Vary delay distributions; test worst-case bursts; add communication topology changes. [P1][P3]

2) Human-in-the-loop safety envelope for rare events
- Hypothesis: A dynamic safety envelope with human escalation across critical events cuts violation rate without materially degrading containment time (≤5% slowdown). [P4][P5][P6]
- Setup: Integrate an envelope that tightens constraints when forecasts predict spread/ember spotting; human operator can pause/revector teams. Stress-test with synthetic wind shifts and sensor occlusions.
- Metrics: Critical-event violation rate, near-miss counts, operator interventions/hour, containment time delta.
- Interpretation: If envelopes reduce violations >60% with small time penalty, you have a practical deployment path; if penalties are large, envelope design needs tuning.
- Variations: Different escalation thresholds; compare to always-on conservative policy. [P4][P5][P6]

3) Delay-robust policy learning
- Hypothesis: Explicitly modeling random observation delays in model-based RL yields better safety/efficacy trade-offs than training without delay modeling. [P2][P3]
- Setup: Train model-based RL with delay-aware dynamics (state augmentation or learned delay model) vs. delay-agnostic baseline. Inject real-world latency traces if available.
- Metrics: Regret under delay, safety-cost budget adherence, containment metrics across delay regimes.
- Interpretation: Gains that persist across delay distributions argue this is a primary contributor; if gains vanish with shields, your novelty may need to shift to shield-policy co-design.
- Follow-ups: Combine with constrained RL costs; ablate observation vs. actuation delays. [P2][P3]

Phase 0 (≤14 days) gating
- Deliverables: (1) Prediction log (≥14 entries) and one reproduced safety-vs-efficacy curve from a safety RL paper, matched within 10% across ≥3 seeds. [P1][G22] (2) One experiment card + one negative result with a post-mortem. If either gate is missed, stay in Phase 0 and iterate.
- Scoreboard: Calibration, reproduction fidelity, ablation clarity, writing cadence (1 page/week).

Intuition
- For safety-critical flight, your edge comes from handling what breaks naive RL: constraints, delays, and partial observability; operational tests make this legible. Safety mechanisms (constraints, shields, oversight) change the feasible set more than reward shaping.

Why this is principled
- Constrained RL and runtime shielding are established safety levers; delays make both policy learning and safety filtering harder, so addressing them is scientifically meaningful. Human-in-the-loop safety envelopes and operational testing are standard for de-risking real deployments before trials. [P1][P3][P4][P5][P6][G11]

Next actions (1–3 days)
- Pick your primary novelty axis (constraints vs. delay robustness vs. envelope design) and draft an experiment card.
- Stand up a simple sim with injected delay/occlusion and implement two baselines: constrained RL and shielded RL.
- Write your operational-test checklist (critical scenarios + pass/fail thresholds).

Citations
- [P1] Safety through feedback in Constrained RL — http://arxiv.org/abs/2406.19626v3
- [P2] Model-Based Reinforcement Learning under Random Observation Delays — http://arxiv.org/abs/2509.20869v1
- [P3] Safety Shielding under Delayed Observation — http://arxiv.org/abs/2307.02164v1
- [P4] Oversight of Unsafe Systems via Dynamic Safety Envelopes — http://arxiv.org/abs/1811.09246v1
- [P5] Closed-Loop Policies for Operational Tests of Safety-Critical Systems — http://arxiv.org/abs/1707.08234v3
- [P6] Human-in-the-Loop Synthesis for Partially Observable Markov Decision Processes — http://arxiv.org/abs/1802.09810v1
- [G11] Opinionated ML research guide — http://joschu.net/blog/opinionated-guide-ml-research.html

Literature anchors
- Distributed Wildfire Surveillance with Autonomous Aircraft using Deep RL (2018) — http://arxiv.org/abs/1810.04244v1
- Safety through feedback in Constrained RL (2024) — http://arxiv.org/abs/2406.19626v3