**Novelty check (score 11/15)**  
- Importance 3: 10× storage/bandwidth savings unlock on-device & edge deployment.  
- Tractability 2: You can get signal in ~3 wks on 1–2 B-param models with existing codebases [P1, P3].  
- Surprise 2: Sparse + quantized LLMs already hit 90 % compression; the twist is *one model* that lets users dial *any* compression at inference time—still rarely demonstrated.  
- Generality 3: If it works, every LLM owner benefits.  
- Mechanistic payoff 1: We lack a crisp “why” for which weights stay important across compression levels.

**Key open questions your work should answer**  
1. Can a *single* checkpoint be trained such that dynamic sparsity/quantization schedules preserve <2 % perplexity loss at 8-bit, 4-bit, 2-bit, 90 % sparse?  
2. Does joint sparsity+quantization outperform either alone at extreme ratios?  
3. Are importance rankings *stable* across downstream tasks, or does dial-a-compression need task-specific masks?

**Three concrete, falsifiable experiments**  
*(expand each into an experiment card before you start)*

**E1 “Dial-a-CAST”**  
Hypothesis: End-to-end differentiable sparsity-aware training [P1] with *multiple* target sparsity heads (90 %, 95 %, 98 %) yields one checkpoint whose 98 % sparse sub-network keeps WikiText-103 perplexity within 1.1× of dense baseline.  
Setup: Fine-tune 1.3 B param pre-trained model for 5 k steps with CAST loss, branching heads for each sparsity. Evaluate each head’s perplexity, zero-shot QA, & summarization ROUGE.  
Metrics: Δ-perplexity vs dense, inference latency on A100, memory footprint.  
Expected: 98 % head hits <2 % loss; 90 % head matches dense.  
Follow-up: Swap sparsity head at *inference* time without re-loading weights; measure cold-start latency.

**E2 “Joint Null-Space 2-bit”**  
Hypothesis: Applying null-space optimization [P3] *after* 90 % N:M sparsity lets you push weight precision to 2-bit with <1 % additional loss.  
Setup: Sparsify Llama-2-7B with 2:4 structured sparsity → run NullQuant calibration on 200 C4 sequences → evaluate MMLU, HellaSwag.  
Metrics: MMLU accuracy drop vs 4-bit sparse, calibration ECE.  
Expected: Joint 2-bit+sparse stays within 0.5 % of 4-bit sparse.  
Follow-up: Ablate order: quantize-then-sparse vs sparse-then-quantize.

**E3 “Task-stability of importance masks”**  
Hypothesis: Magnitude-based importance masks computed on pre-training data transfer to downstream tasks with <3 % accuracy loss up to 95 % sparsity.  
Setup: Derive 95 % sparse mask from C4 → apply to fine-tuned model on GSM8K & HumanEval → compare with task-specific 95 % mask.  
Metrics: GSM8K solve-rate, HumanEval pass@1, mask overlap coefficient.  
Expected: Overlap ≥0.8 and accuracy drop ≤3 %.  
Follow-up: If overlap low, train a *meta-mask* predictor that conditions on task embedding.

**Next 3-day action pack**  
1. Clone CAST repo, reproduce 90 % sparse 1.3 B model on WikiText (baseline checkpoint).  
2. Draft experiment cards for E1–E3; get stop-rules & compute budget signed off by advisor.  
3. Build 1-page prediction log template (date, claim, Brier before/after) and log your expected Δ-perplexity for E1.

**Citations**  
[P1] CAST: Continuous and Differentiable Semi-Structured Sparsity-Aware Training for Large Language Models — http://arxiv.org/abs/2509.25996v1  
[P3] Boost Post-Training Quantization via Null Space Optimization for Large Language Models — http://arxiv.org/abs/2506.11044v3