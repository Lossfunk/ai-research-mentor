Stage: A → B (sharpening the idea and scoping first experiments)

Quick intake to tailor the plan
- Compute/time: What GPUs/TPUs and weekly hours do you have for experiments?
- Scope: Are you focusing on post-hoc compression (quantize/prune/distill) or training-for-compressibility from scratch/fine-tuning?
- Mentorship/collab: Do you have a lab/partner for shared baselines/eval?
- Target/Timeline: Which venue or demo timeline are you aiming for?
- Biggest bottleneck now: datasets/eval harness, baselines, or method design?

What could be novel (pick 1–2 to lead with)
- Compression-aware training objectives: Explicitly optimize a rate–distortion objective that trades size/latency against task utility using differentiable surrogates for quantization and sparsity during training. The novelty is moving from post-hoc compression to Pareto-frontier training. Prior compression work motivates composing quantization, pruning, and distillation but largely applies them after training [P2][P1]. Framing this as a rate–distortion problem gives a principled curve to target [P8].
- Intrinsically low-dimensional LLMs: Constrain updates to low-rank subspaces (e.g., LoRA rank scheduling) or minimize a proxy for intrinsic dimension so the final model admits superior low-bit quantization and high sparsity without accuracy loss. This goes beyond “parameter-efficient fine-tuning” to “compression-efficient fine-tuning.”
- Budgeted distillation curricula: Distil under an explicit bitrate/latency budget (e.g., quantized student with sparse structure) and adapt the teacher signal by block or layer as the budget tightens. Blockwise KD literature suggests structure-aware targets can help [P3].
- Compressibility certificates: Define simple pre-quantization indicators (e.g., weight spectral decay, activation range entropy) that predict post-quantization loss, enabling early stopping or adaptive constraints.

Three falsifiable Phase-0 experiments (≤14 days)
1) Baseline rate–distortion frontier
- Hypothesis: Your method shifts the model’s rate–distortion curve outward versus strong post-hoc baselines (higher accuracy at the same size/latency). 
- Setup: Take an open LLM (e.g., LLaMA-2-7B or Mistral 7B). Produce families of models via 8/6/4-bit post-training quantization and structured/unstructured pruning; add a distilled student at the same budgets. 
- Metrics: Accuracy on MMLU and GSM8K; perplexity on WikiText; latency and peak memory on single A100; footprint (bytes). Success: ≥1–2 pt MMLU or ≥2 pt GSM8K improvement at matched size/latency. 
- Interpretation: If curves overlap, your gains aren’t beyond existing methods; if consistently better across budgets, that’s evidence of novelty. Follow-ups: test out-of-domain generalization (TruthfulQA/ARC). Motivation: compare combined pruning/quant/ KD frontiers as in classic compression stacks [P1][P2] and analyze as rate–distortion [P8].

2) Train-for-compressibility vs post-hoc
- Hypothesis: Injecting differentiable quantization noise and L0/L1 sparsity penalties during fine-tuning yields better low-bit/post-prune performance than compressing after standard fine-tuning. 
- Setup: Fine-tune on a modest mixture (e.g., OpenOrca or Dolly) with fake quantization and sparsity; match training tokens with a standard fine-tune. Then quantize/prune both to identical budgets. 
- Metrics: Same as above plus calibration (ECE). Success: ≥20% relative reduction in accuracy drop at 4-bit or 50% sparsity. 
- Interpretation: If gains appear only at certain layers, ablate layer-wise penalties; consider blockwise distillation to reinforce fragile blocks [P3].

3) Low-rank schedule for intrinsic compressibility
- Hypothesis: A decreasing LoRA-rank schedule (or subspace annealing) improves final compressibility at equal task performance vs fixed-rank adapters. 
- Setup: Fine-tune identical models with fixed ranks r∈{8,16} vs a schedule r:32→4 across training. Then apply identical 4-bit quantization. 
- Metrics: Task scores, post-quantization degradation, weight spectral norm/trace norm as proxies. Success: scheduled training yields smaller degradation at 4-bit with equal pre-quant accuracy. 
- Interpretation: If only some modules benefit (e.g., attention vs MLP), adapt schedules per module.

Next actions (Phase 0 gates)
- Reproduce: Implement the baseline frontier (quantization/pruning/distillation) and log a rate–distortion curve on one model and two tasks (deliverable). 
- Prediction log: 14 daily predictions with confidence about which knobs shift the curve (deliverable; track Calibration and Reproduction fidelity). 
- Draft one experiment card (above) and run at least one ablation or negative result with a short post-mortem before expanding.

Intuition
Compression works best when the model’s representations are already low-variance and robust to quantization/pruning noise; shaping these properties during training should move the Pareto frontier, not just a single operating point.

Why this is principled
Classical results show combining pruning, quantization, and distillation yields complementary gains [P1][P2]. Casting evaluation as a rate–distortion problem turns “compressible LLMs” into a measurable objective and supports ablations on quantization noise models [P8]. Structure-aware distillation (e.g., blockwise) motivates curricula aligned with the architecture [P3].

Strategic questions
- Which budget matters most to your use case: model size, VRAM, or latency? That choice will drive the rate term and the target frontier.
- Do you want novelty in objective design (rate term/proxies), training schedule (rank/sparsity annealing), or evaluation methodology (new, multi-axis frontier and certificates)?

Citations
- [P1] PQK: Model Compression via Pruning, Quantization, and Knowledge Distillation — http://arxiv.org/abs/2106.14681v1
- [P2] Model compression via distillation and quantization — http://arxiv.org/abs/1802.05668v1
- [P3] Parallel Blockwise Knowledge Distillation for Deep Neural Network Compression — http://arxiv.org/abs/2012.03096v1
- [P8] Universally Quantized Neural Compression — http://arxiv.org/abs/2006.09952v2

Literature anchors (LLM-focused)
- QLoRA: Efficient Finetuning of Quantized LLMs — https://arxiv.org/abs/2305.14314
- SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot — https://arxiv.org/abs/2301.00774