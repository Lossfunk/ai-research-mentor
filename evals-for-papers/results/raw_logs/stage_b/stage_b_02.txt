Short answer
“Compressible LLMs” = models intentionally trained so they retain accuracy, calibration, and behavioral properties under aggressive post-training or in-training compression. The novelty is not “another compression trick,” but training objectives/architectures that make compressibility a first-class property (predictably good at low bitwidths, high sparsity, low-rank, or small KV caches), with hardware-aware evaluation and crisp baselines against strong post-hoc compression methods like simple pruning/quantization/factorization [P4][P1][P3]. Methodologically, define the problem precisely, pick clear baselines, and run ablations that isolate where the gains come from [G1][G6][G14][G2].

Where the novelty can be
- Train-time compressibility objectives: Optimize for low effective rank, structured sparsity, weight sharing, and quantization robustness during pretraining or finetuning, rather than relying only on post-hoc compression. Prior work shows post-hoc dense-sparse factorization and low-rank prune+factorize are effective; making this a training-time property is the novel step [P1][P3].  
- Compression-robust activation/KV paths: Add quantization/clipping/noise to activations and KV-cache during training so long-context decoding stays stable at low-precision KV or small caches. Most compression works focus on weights; robust KV/activations is underexplored in the LLM regime (conjecture; motivate and test carefully) [G6][G1].  
- Hardware-/energy-aware co-design: Optimize for specific targets (e.g., 4-bit weights + 8-bit activations on consumer GPUs/edge) and report throughput/latency/energy alongside accuracy, not just size [P4][G2].  
- Reliability under compression: Characterize calibration, robustness to input perturbations, and safety behavior shifts as compression increases; these are usually underreported and can differentiate “compressible” models from merely “small” ones [G2][G1].  
- Scaling laws for compressibility: Measure how compressibility (quality at k bits or s% sparsity) scales with model size and data; propose a “compressibility exponent” predicting loss under a given budget (novel framing; validate empirically) [G6][G14].

Baselines you should beat or match at a given budget
- Post-hoc pruning + simple fine-tuning (e.g., “Just CHOP” style simple pipelines) [P4].  
- Low-rank factorization and prune+factorize baselines [P3].  
- Post-hoc dense–sparse factorization methods (e.g., DSFormer) [P1].  
- Quantization baselines at 8b/4b with standard calibration (you can cite your implementation, but ensure strong baselines) [G2].

Core evaluation protocol
- Quality: Pretraining loss/perplexity and standard downstream evals (zero-/few-shot tasks appropriate for your domain).  
- Resource metrics: Model size and working set (weights + KV), peak memory, tokens/s, latency p50/p95, and energy per 1k tokens on target hardware [P4].  
- Robustness: Accuracy under noise/perturbations, calibration (ECE/Brier), and safety behavior drift when compressed vs. uncompressed [G2].  
- Stability: Degradation curves across bitwidths (16→8→4→3→2), sparsity (0→90%), and KV precision/size; report area-under-curve as a compressibility score [G6].  
- Reproducibility: Release exact recipes, seeds, calibration data, and ablations [G14][G2].

Three concrete, falsifiable experiments
1) Quantization-aware training (QAT) vs. post-hoc quantization  
- Hypothesis: Adding fake quantization to weights and activations during training yields smaller accuracy drop at 4-bit weights + 8-bit activations versus post-hoc quantization.  
- Setup: Train the same model with and without QAT; evaluate at 16b, 8b, 4b.  
- Metrics: Δaccuracy/Δperplexity at each bitwidth, tokens/s, energy per 1k tokens.  
- Success criteria: At 4b weights the QAT model shows ≥30% smaller accuracy drop compared to post-hoc quantization, with equal or better throughput.  
- Rationale/baseline: Compare to simple post-hoc pipelines (e.g., CHOP-like) [P4] and factorization baselines [P3][P1]. Use clear controls/ablations per best-practice guidance [G2][G14].

2) Train-time low-rank+sparsity regularization vs. post-hoc factorization  
- Hypothesis: Regularizing for low effective rank and structured sparsity during training yields better quality–size tradeoffs than applying DSFormer or prune-and-factorize after the fact.  
- Setup: Add nuclear-norm proxy or low-rank adapters plus N:M sparsity constraints in attention/MLP weights during pretraining/finetuning; compare against post-hoc DSFormer and prune+factorize with equal final parameter counts.  
- Metrics: Task accuracy/perplexity at fixed parameter budget; compression ratio; latency.  
- Success criteria: At matched size/sparsity, train-time-regularized model outperforms DSFormer/prune+factorize by ≥X points on a held-out eval suite.  
- Rationale: DSFormer and low-rank prune+factorize indicate substantial redundancy post-hoc; pushing these structures into training is the novelty to test [P1][P3]. Ensure ablations isolate rank-only vs sparsity-only vs combined [G1][G14].

3) KV-cache compression-aware training for long-context decoding  
- Hypothesis: Injecting quantization/noise into KV tensors during training reduces quality loss when limiting KV precision (e.g., 8→4→2 bits) or cache size (chunked eviction) at inference.  
- Setup: During training, randomly quantize KV to target bitwidths and simulate cache eviction; at inference, evaluate long-context generation with compressed KV on target hardware.  
- Metrics: Long-context task scores, perplexity drift across context length, latency/memory, and robustness to KV eviction.  
- Success criteria: At 4-bit KV with 50% cache reduction, <20% of the performance drop seen by the model trained without KV perturbations.  
- Rationale: Most compression focuses on weights; making the attention path compressible is a differentiator. Frame and report results with hardware-aware metrics per best practices [G6][G2].

Optional extensions (pick ≥1 for a compelling paper)
- Compressibility scaling study: Train 3–4 sizes with the same recipe; fit accuracy vs. model size vs. bits; report “compressibility exponent” and compare to post-hoc baselines [G6].  
- Calibration and safety drift under compression: Measure ECE and standardized safety prompts before/after compression; quantify drift to show whether “compressible” models preserve behavior better than post-hoc compressed ones [G2].  
- Hardware–model co-design: Show Pareto curves (accuracy vs. latency/energy) for CPU vs consumer GPU, highlighting where your training-time constraints change the frontier compared to simple pipelines [P4].

Reporting checklist
- State contributions crisply (what’s new vs. known compression methods), list strong baselines, and include ablations/controls and exact recipes to enable replication [G1][G14][G2].  
- Report both model quality and system metrics; avoid cherry-picking bitwidths or tasks; include full degradation curves [G6].  
- Release code and configuration for quantizers, sparsifiers, and KV compression to allow verification [G2].

Anchoring literature
- DSFormer shows dense–sparse factorization for text transformers; use as a strong post-hoc baseline [P1].  
- Low-Rank Prune-and-Factorize and simple compression pipelines like “Just CHOP” provide competitive baselines; beating them at equal budgets strengthens the novelty claim [P3][P4].

Citations
- [P1] DSFormer: Effective Compression of Text-Transformers by Dense-Sparse Weight Factorization — http://arxiv.org/abs/2312.13211v1  
- [P3] Low-Rank Prune-And-Factorize for Language Model Compression — http://arxiv.org/abs/2306.14152v1  
- [P4] Just CHOP: Embarrassingly Simple LLM Compression — http://arxiv.org/abs/2305.14864v3  
- [G1] Opinionated Guide ML Research — http://joschu.net/blog/opinionated-guide-ml-research.html  
- [G2] Best Practices: Research Methodological Guide — https://www.researchgate.net/publication/31052323_Best_Practices_Research_A_Methodological_Guide_for_the_Perplexed  
- [G6] Framing Machine Learning Research — https://trendspider.com/learning-center/framing-machine-learning-research/  
- [G14] Highly Opinionated Advice on How to Write ML Papers — https://www.alignmentforum.org/posts/Xt8tMtwfsLo2jRCEj/highly-opinionated-advice-on-how-to-write-ml-papers

Key takeaways
- Make compressibility a training target, not just a post-hoc trick.  
- Compare against strong, simple baselines and report full quality–resource tradeoffs.  
- Validate on weights, activations, and KV paths, and include calibration/safety and hardware metrics for a convincing, end-to-end story.