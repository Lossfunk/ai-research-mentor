Short answer: Using GANs in poetry is not new for generation, but “GANs for poetry analysis” (meter/rhyme/authorship/structure) is underexplored compared to supervised or contrastive approaches—so there is room for novelty if you (a) define a concrete analysis task, and (b) show that an adversarial objective measurably improves it over strong non‑GAN baselines [2][3][4][5]. Most prior work uses GANs to generate poems; analysis tasks like scansion and authorship have relied on feature‑based or neural classifiers without adversarial training [2][4][5].

What could be novel
- Discriminator-as-analyst: Train a GAN where the discriminator learns to distinguish true poetic structures (e.g., meter/rhyme graphs, scansion sequences) from generated ones, and then repurpose that discriminator as a high-accuracy, well-calibrated analyzer. Show gains over supervised scansion/rhyme classifiers [4][5].
- Adversarial data augmentation for rare forms: Use a generator to synthesize scarce meters or rhyme schemes (e.g., anapestic, terza rima) and demonstrate improved analysis accuracy on low-resource forms versus non‑adversarial augmentation [2].
- Cycle-consistent style perturbations for causal analysis: Use a CycleGAN‑style setup to translate poems between meters or rhyme densities while preserving semantics; quantify which structural features drive classifier decisions by counterfactual evaluation (does changing only meter flip the prediction?) [3].
- Latent, structure-aware GANs: Generate and discriminate over structured representations (scansion tags, rhyme graphs, syllable stress sequences) rather than raw text, which avoids discrete-token GAN pitfalls and aligns with analysis objectives [4][5].

At least three concrete, falsifiable experiments
- E1. Scansion with adversarially trained discriminator vs supervised baselines
  - Task: English meter tagging (per-syllable stress and foot labels).
  - Models: (a) BiLSTM/Transformer tagger (supervised), (b) same tagger with an adversarial discriminator trained against a structure generator (WGAN or RL-based token generator), evaluating the discriminator as the analyst.
  - Metrics: Token-level F1/accuracy, per-line exact match, calibration (ECE).
  - Falsifiable outcome: Discriminator-as-analyst improves token F1 by ≥2–3 points over the supervised tagger on held-out poets/meters; if not, adversarial training does not add value for scansion [4][3].

- E2. Rare-form augmentation
  - Task: Classify meter or rhyme scheme with few-shot data for rare forms.
  - Setup: Train with and without adversarially generated synthetic examples matched to target form statistics.
  - Metrics: Accuracy/F1 on rare-form test sets; overfitting checks via lexical diversity and form conformity.
  - Falsifiable outcome: Augmented training yields ≥5-point F1 gain on rare forms without hurting common-form accuracy; otherwise, GAN augmentation is not useful [2].

- E3. Counterfactual meter transfer for causal probing
  - Task: Authorship or period classification.
  - Setup: Translate poems between meters (iambic↔trochaic) using a cycle-consistent adversarial model operating on scansion/rhyme representations; test whether classifier decisions flip when only meter is changed.
  - Metrics: Counterfactual flip rate, preservation of content semantics (BLEU/semantic similarity), meter conformity score.
  - Falsifiable outcome: Classifier accuracy decreases or flips under meter-only counterfactuals, indicating causal reliance on meter; if not, meter is not a primary driver of the classifier.

- E4. Structured vs token-level adversarial training ablation (optional)
  - Compare GANs on scansion/rhyme structures vs token-level GANs (SeqGAN-style) for the same analysis tasks.
  - Falsifiable outcome: Structure-level GANs are more stable (lower mode collapse rate) and yield better downstream analysis accuracy; otherwise, token-level methods suffice [3].

Practical scope and baselines
- Start with scansion/rhyme analysis: well-studied, labeled datasets exist in prior scansion work; build simple supervised baselines first [4][5].
- Avoid pure text GANs initially: GANs over discrete tokens are unstable; if you need them, use RL-based generators (SeqGAN) or continuous relaxations, and compare to non‑adversarial contrastive/self-supervised baselines [3].
- Report decision-relevant metrics: accuracy/F1, calibration, and robustness to poet/out-of-domain shifts.

Bottom line
- Not original for generation; potentially original for analysis if you frame GANs as a way to learn structure-aware discriminators, create rare-form counterfactuals, and demonstrate measurable, statistically sound gains over strong supervised/contrastive baselines on scansion, rhyme, or authorship tasks [2][3][4][5].

Sources
[1] Machine-Learning-Tokyo/Poetry-GAN (example of GANs used for poem generation) — https://github.com/Machine-Learning-Tokyo/Poetry-GAN
[2] Creative GANs for generating poems, lyrics, and metaphors — https://arxiv.org/abs/1909.09534
[3] SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient — https://arxiv.org/abs/1609.05473
[4] A Comparison of Feature-Based and Neural Scansion of Poetry (RANLP 2017) — https://www.acl-bg.org/proceedings/2017/RANLP%202017/pdf/RANLP003.pdf
[5] Understanding poetry using natural language processing tools — https://academic.oup.com/dsh/article/39/2/500/7602425