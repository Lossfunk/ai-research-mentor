{
  "prompt_id": "stage_c_02",
  "stage": "C",
  "generated_at": "2025-10-18T19:36:42.034902Z",
  "metrics": {
    "tool_routing": {
      "score": 1.0,
      "details": {
        "expected": [
          "web_search"
        ],
        "observed": [
          "web_search"
        ],
        "missing": [],
        "extra": []
      }
    },
    "citation_presence": {
      "score": 1.0
    },
    "citation_validity": {
      "score": 1.0,
      "details": {
        "total_count": 6,
        "scholarly_count": 2,
        "guideline_count": 0,
        "portal_count": 0,
        "other_count": 4,
        "malformed_count": 0,
        "domain_config_digest": "81cdd2fb63c3b437b7a1fba0f2304715e28ac3b4c5f400507ad7a352bb78c367"
      },
      "citations": [
        {
          "id": "P1",
          "title": "Optimizing LLMs for Performance and Accuracy with Post-Training Quantization",
          "url": "https://developer.nvidia.com/blog/optimizing-llms-for-performance-and-accuracy-with-post-training-quantization/",
          "domain": "developer.nvidia.com",
          "kind": "other",
          "malformed": false
        },
        {
          "id": "P2",
          "title": "A Guide to Quantization in LLMs | Symbl.ai",
          "url": "https://symbl.ai/developers/blog/a-guide-to-quantization-in-llms/",
          "domain": "symbl.ai",
          "kind": "other",
          "malformed": false
        },
        {
          "id": "P3",
          "title": "Demystifying Quantizations: Guide to Quantization Methods for LLMs",
          "url": "https://cast.ai/blog/demystifying-quantizations-llms/",
          "domain": "cast.ai",
          "kind": "other",
          "malformed": false
        },
        {
          "id": "P6",
          "title": "Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant",
          "url": "https://www.ijcai.org/proceedings/2025/0902.pdf",
          "domain": "ijcai.org",
          "kind": "other",
          "malformed": false
        },
        {
          "id": "P7",
          "title": "Benchmarking Post-Training Quantization in LLMs: Comprehensive Taxonomy, Unified Evaluation, and Comparative Analysis",
          "url": "https://arxiv.org/abs/2502.13178",
          "domain": "arxiv.org",
          "kind": "scholarly",
          "malformed": false
        },
        {
          "id": "P10",
          "title": "A Survey on Model Compression for Large Language Models",
          "url": "https://arxiv.org/abs/2308.07633",
          "domain": "arxiv.org",
          "kind": "scholarly",
          "malformed": false
        }
      ],
      "legacy_quality_score": 63.0,
      "legacy_validator": {
        "valid": false,
        "valid_count": 0,
        "total_count": 6,
        "issues": [
          "Citation 1: No authors specified",
          "Citation 1: Invalid or missing year",
          "Citation 1: No venue specified",
          "Citation 1: No DOI available",
          "Citation 1: No meaningful snippet",
          "Citation 2: No authors specified",
          "Citation 2: Invalid or missing year",
          "Citation 2: No venue specified",
          "Citation 2: No DOI available",
          "Citation 2: No meaningful snippet",
          "Citation 3: No authors specified",
          "Citation 3: Invalid or missing year",
          "Citation 3: No venue specified",
          "Citation 3: No DOI available",
          "Citation 3: No meaningful snippet",
          "Citation 4: No authors specified",
          "Citation 4: Invalid or missing year",
          "Citation 4: No venue specified",
          "Citation 4: No DOI available",
          "Citation 4: No meaningful snippet",
          "Citation 5: No authors specified",
          "Citation 5: Invalid or missing year",
          "Citation 5: No venue specified",
          "Citation 5: No DOI available",
          "Citation 5: No meaningful snippet",
          "Citation 6: No authors specified",
          "Citation 6: Invalid or missing year",
          "Citation 6: No venue specified",
          "Citation 6: No DOI available",
          "Citation 6: No meaningful snippet"
        ],
        "individual_results": [
          {
            "valid": false,
            "score": 63.0,
            "issues": [
              "No authors specified",
              "Invalid or missing year",
              "No venue specified",
              "No DOI available",
              "No meaningful snippet"
            ],
            "completeness": 28.57142857142857
          },
          {
            "valid": false,
            "score": 63.0,
            "issues": [
              "No authors specified",
              "Invalid or missing year",
              "No venue specified",
              "No DOI available",
              "No meaningful snippet"
            ],
            "completeness": 28.57142857142857
          },
          {
            "valid": false,
            "score": 63.0,
            "issues": [
              "No authors specified",
              "Invalid or missing year",
              "No venue specified",
              "No DOI available",
              "No meaningful snippet"
            ],
            "completeness": 28.57142857142857
          },
          {
            "valid": false,
            "score": 63.0,
            "issues": [
              "No authors specified",
              "Invalid or missing year",
              "No venue specified",
              "No DOI available",
              "No meaningful snippet"
            ],
            "completeness": 28.57142857142857
          },
          {
            "valid": false,
            "score": 63.0,
            "issues": [
              "No authors specified",
              "Invalid or missing year",
              "No venue specified",
              "No DOI available",
              "No meaningful snippet"
            ],
            "completeness": 28.57142857142857
          },
          {
            "valid": false,
            "score": 63.0,
            "issues": [
              "No authors specified",
              "Invalid or missing year",
              "No venue specified",
              "No DOI available",
              "No meaningful snippet"
            ],
            "completeness": 28.57142857142857
          }
        ]
      }
    },
    "fallback_robustness": {
      "score": 0.0
    },
    "asks_questions": {
      "score": 0.0
    },
    "rag_fidelity": {
      "score": 2.0,
      "judges": [
        {
          "judge": "openrouter:google/gemini-2.5-flash-lite",
          "raw": "```json\n{\n  \"score\": 2.0,\n  \"rationale\": \"The response provides a comprehensive and well-structured research plan for developing and evaluating compressible LLMs. It clearly defines objectives, outlines specific experiments with hypotheses and ablations, and proposes a realistic timeline. The plan cites relevant sources ([P1], [P3], [P6], [P7], [P10]) to support its methodology and targets, demonstrating accurate synthesis of evidence. For instance, Objective 1's accuracy retention target (≥97%) and multi-dataset evaluation are explicitly linked to evidence from recent quantization studies ([P1], [P6], [P7]). Similarly, Experiment 1's hypothesis about 4-bit PTQ performance is grounded in [P1], [P7], and [P10]. The plan also acknowledges potential limitations and designs experiments to address them, such as the need for mixed precision if pure 4-bit PTQ falls short (Experiment 2) and the dependency of sparse-quant speedups on kernel support (Experiment 3). The timeline includes clear deliverables and go/no-go checkpoints, ensuring a rigorous and reproducible research process. The plan is highly detailed, covering models, methods, libraries, hardware, baselines, metrics, and ablations for each experiment, all while maintaining clear attribution to supporting evidence.\",\n  \"confidence\": \"high\"\n}\n```",
          "rationale": "The response provides a comprehensive and well-structured research plan for developing and evaluating compressible LLMs. It clearly defines objectives, outlines specific experiments with hypotheses and ablations, and proposes a realistic timeline. The plan cites relevant sources ([P1], [P3], [P6], [P7], [P10]) to support its methodology and targets, demonstrating accurate synthesis of evidence. For instance, Objective 1's accuracy retention target (≥97%) and multi-dataset evaluation are explicitly linked to evidence from recent quantization studies ([P1], [P6], [P7]). Similarly, Experiment 1's hypothesis about 4-bit PTQ performance is grounded in [P1], [P7], and [P10]. The plan also acknowledges potential limitations and designs experiments to address them, such as the need for mixed precision if pure 4-bit PTQ falls short (Experiment 2) and the dependency of sparse-quant speedups on kernel support (Experiment 3). The timeline includes clear deliverables and go/no-go checkpoints, ensuring a rigorous and reproducible research process. The plan is highly detailed, covering models, methods, libraries, hardware, baselines, metrics, and ablations for each experiment, all while maintaining clear attribution to supporting evidence.",
          "confidence": "high",
          "score": 2.0
        }
      ]
    },
    "citation_relevance": {
      "score": 2.0,
      "judges": [
        {
          "judge": "openrouter:google/gemini-2.5-flash-lite",
          "raw": "```json\n{\n  \"score\": 2.0,\n  \"rationale\": \"The citations directly support the claims made throughout the research plan. For example, [P1] is cited to motivate the target for accuracy retention under compression and to support the idea that PTQ can retain high accuracy with memory savings. Similarly, [P7] is cited for multi-dataset evaluations in quantization studies and for supporting the comparison of methods under a unified setup. The plan consistently links specific experimental designs and hypotheses to relevant literature, demonstrating direct support.\",\n  \"confidence\": \"high\"\n}\n```",
          "rationale": "The citations directly support the claims made throughout the research plan. For example, [P1] is cited to motivate the target for accuracy retention under compression and to support the idea that PTQ can retain high accuracy with memory savings. Similarly, [P7] is cited for multi-dataset evaluations in quantization studies and for supporting the comparison of methods under a unified setup. The plan consistently links specific experimental designs and hypotheses to relevant literature, demonstrating direct support.",
          "confidence": "high",
          "score": 2.0
        }
      ]
    },
    "source_fit": {
      "score": 2.0,
      "judges": [
        {
          "judge": "openrouter:google/gemini-2.5-flash-lite",
          "raw": "{\"score\": 2.0, \"rationale\": \"The agent's response is excellent. It provides a comprehensive and well-structured research plan for compressible LLMs, covering multiple compression techniques (PTQ, sparsity, distillation, KV-cache compression) and evaluation metrics. The plan is detailed, with clear objectives, experiments, baselines, ablations, and a realistic timeline. Crucially, the agent grounds its proposals in relevant, recent, and authoritative sources, citing specific papers and blog posts that are highly appropriate for the user's goal and expertise level. The inclusion of specific model names (Llama-2-7B, Mistral-7B), libraries, hardware, and evaluation benchmarks demonstrates a deep understanding of the domain. The plan is also falsifiable, with clear success criteria and potential failure points identified. The sources are a mix of scholarly articles and authoritative technical blogs (NVIDIA, Cast.ai), which is ideal for a research plan of this nature.\", \"confidence\": \"high\"}",
          "rationale": "The agent's response is excellent. It provides a comprehensive and well-structured research plan for compressible LLMs, covering multiple compression techniques (PTQ, sparsity, distillation, KV-cache compression) and evaluation metrics. The plan is detailed, with clear objectives, experiments, baselines, ablations, and a realistic timeline. Crucially, the agent grounds its proposals in relevant, recent, and authoritative sources, citing specific papers and blog posts that are highly appropriate for the user's goal and expertise level. The inclusion of specific model names (Llama-2-7B, Mistral-7B), libraries, hardware, and evaluation benchmarks demonstrates a deep understanding of the domain. The plan is also falsifiable, with clear success criteria and potential failure points identified. The sources are a mix of scholarly articles and authoritative technical blogs (NVIDIA, Cast.ai), which is ideal for a research plan of this nature.",
          "confidence": "high",
          "score": 2.0
        }
      ]
    },
    "citation_quality": {
      "score": 2.0,
      "judges": [
        {
          "judge": "openrouter:google/gemini-2.5-flash-lite",
          "raw": "{\"score\": 2.0, \"rationale\": \"The agent provides multiple citations that are scholarly (arXiv papers, IJCAI proceedings) or authoritative (NVIDIA developer blog). These citations directly support the claims made about quantization methods, benchmarks, and expected outcomes. For example, [P1], [P7], and [P10] are cited to support claims about PTQ, benchmarking, and model compression surveys, respectively. The citations are well-formatted and appear to be real and relevant.\", \"confidence\": \"high\"}",
          "rationale": "The agent provides multiple citations that are scholarly (arXiv papers, IJCAI proceedings) or authoritative (NVIDIA developer blog). These citations directly support the claims made about quantization methods, benchmarks, and expected outcomes. For example, [P1], [P7], and [P10] are cited to support claims about PTQ, benchmarking, and model compression surveys, respectively. The citations are well-formatted and appear to be real and relevant.",
          "confidence": "high",
          "score": 2.0
        }
      ]
    },
    "question_quality": {
      "score": 2.0,
      "judges": [
        {
          "judge": "openrouter:google/gemini-2.5-flash-lite",
          "raw": "{\"score\": 2.0, \"rationale\": \"The agent's response is a comprehensive and well-structured research plan. It clearly defines objectives, outlines specific experiments with hypotheses, baselines, metrics, and ablations, and provides a detailed timeline. The plan is grounded in the context of compressible LLMs and proposes concrete, actionable steps. The inclusion of specific models, methods, libraries, and hardware, along with falsifiable hypotheses and clear deliverables, demonstrates a high level of targeted and contextually relevant planning.\", \"confidence\": \"high\"}",
          "rationale": "The agent's response is a comprehensive and well-structured research plan. It clearly defines objectives, outlines specific experiments with hypotheses, baselines, metrics, and ablations, and provides a detailed timeline. The plan is grounded in the context of compressible LLMs and proposes concrete, actionable steps. The inclusion of specific models, methods, libraries, and hardware, along with falsifiable hypotheses and clear deliverables, demonstrates a high level of targeted and contextually relevant planning.",
          "confidence": "high",
          "score": 2.0
        }
      ]
    },
    "actionability": {
      "score": 1.0,
      "judges": [
        {
          "judge": "openrouter:google/gemini-2.5-flash-lite",
          "raw": "{\"score\": 1.0, \"rationale\": \"The response provides a highly detailed and actionable research plan. It breaks down the problem into specific, measurable objectives with clear success criteria. The experiments are well-defined with specific hypotheses, setup details (models, methods, libraries, hardware), baselines, metrics, ablations, and expected outcomes. Crucially, it includes concrete steps like specific quantization methods (AWQ, GPTQ, SmoothQuant), sparsity targets (20-60%), distillation parameters (loss blend, temperature), and evaluation metrics (MMLU, HellaSwag, etc.). The timeline is also detailed with bi-weekly checkpoints and deliverables. The plan explicitly mentions commands and parameters implicitly through the description of methods and libraries (e.g., 'AWQ (per-channel/group, group size 128)', 'SmoothQuant α∈{0.3,0.5,0.7}'). The expected outcomes are quantified, allowing for clear assessment of success or failure.\", \"confidence\": \"high\"}",
          "rationale": "The response provides a highly detailed and actionable research plan. It breaks down the problem into specific, measurable objectives with clear success criteria. The experiments are well-defined with specific hypotheses, setup details (models, methods, libraries, hardware), baselines, metrics, ablations, and expected outcomes. Crucially, it includes concrete steps like specific quantization methods (AWQ, GPTQ, SmoothQuant), sparsity targets (20-60%), distillation parameters (loss blend, temperature), and evaluation metrics (MMLU, HellaSwag, etc.). The timeline is also detailed with bi-weekly checkpoints and deliverables. The plan explicitly mentions commands and parameters implicitly through the description of methods and libraries (e.g., 'AWQ (per-channel/group, group size 128)', 'SmoothQuant α∈{0.3,0.5,0.7}'). The expected outcomes are quantified, allowing for clear assessment of success or failure.",
          "confidence": "high",
          "score": 1.0
        }
      ]
    },
    "persona_compliance": {
      "score": 2.0,
      "judges": [
        {
          "judge": "openrouter:google/gemini-2.5-flash-lite",
          "raw": "{\"score\": 2.0, \"rationale\": \"The response adopts a consistently encouraging and guiding mentor persona. It provides a comprehensive, well-structured research plan with clear objectives, detailed experimental designs, and a realistic timeline. The language used is supportive and forward-looking, framing the plan as a \\\"turnkey, publication-grade research execution plan\\\" and offering guidance on potential challenges and success criteria. The persona is maintained throughout the detailed breakdown of experiments and the timeline.\", \"confidence\": \"high\"}",
          "rationale": "The response adopts a consistently encouraging and guiding mentor persona. It provides a comprehensive, well-structured research plan with clear objectives, detailed experimental designs, and a realistic timeline. The language used is supportive and forward-looking, framing the plan as a \"turnkey, publication-grade research execution plan\" and offering guidance on potential challenges and success criteria. The persona is maintained throughout the detailed breakdown of experiments and the timeline.",
          "confidence": "high",
          "score": 2.0
        }
      ]
    },
    "stage_awareness": {
      "score": 2.0,
      "judges": [
        {
          "judge": "openrouter:google/gemini-2.5-flash-lite",
          "raw": "{\"score\": 2.0, \"rationale\": \"The user is asking for a research plan, which implies they are in the planning stage of their research. The agent's response provides a detailed, structured, and comprehensive research plan, including problem framing, specific experiments with hypotheses and ablations, a timeline, and expected outcomes. This directly addresses the user's request and demonstrates a clear understanding of the research stage.\", \"confidence\": \"high\"}",
          "rationale": "The user is asking for a research plan, which implies they are in the planning stage of their research. The agent's response provides a detailed, structured, and comprehensive research plan, including problem framing, specific experiments with hypotheses and ablations, a timeline, and expected outcomes. This directly addresses the user's request and demonstrates a clear understanding of the research stage.",
          "confidence": "high",
          "score": 2.0
        }
      ]
    },
    "tone_constructive": {
      "score": 2.0,
      "judges": [
        {
          "judge": "openrouter:google/gemini-2.5-flash-lite",
          "raw": "{\"score\": 2.0, \"rationale\": \"The response is highly constructive and motivating. It presents a detailed, well-structured, and ambitious research plan with clear objectives, experiments, and timelines. The language used is encouraging and forward-looking, framing the plan as a \\\"turnkey, publication-grade research execution plan\\\" and emphasizing rigorous evaluation and potential for significant findings. The inclusion of specific metrics, baselines, and ablations demonstrates a thorough and proactive approach, which is motivating for a research endeavor.\", \"confidence\": \"high\"}",
          "rationale": "The response is highly constructive and motivating. It presents a detailed, well-structured, and ambitious research plan with clear objectives, experiments, and timelines. The language used is encouraging and forward-looking, framing the plan as a \"turnkey, publication-grade research execution plan\" and emphasizing rigorous evaluation and potential for significant findings. The inclusion of specific metrics, baselines, and ablations demonstrates a thorough and proactive approach, which is motivating for a research endeavor.",
          "confidence": "high",
          "score": 2.0
        }
      ]
    },
    "plan_completeness": {
      "score": 1.0,
      "judges": [
        {
          "judge": "openrouter:google/gemini-2.5-flash-lite",
          "raw": "{\"score\": 1, \"rationale\": \"The plan includes hypotheses, methodology (detailed experiments with setup, baselines, metrics, ablations, and expected outcomes), evaluation (across multiple tasks and hardware), resources (implied by the detailed setup and libraries), and milestones (a 6-month timeline with bi-weekly checkpoints).\", \"confidence\": \"high\"}",
          "rationale": "The plan includes hypotheses, methodology (detailed experiments with setup, baselines, metrics, ablations, and expected outcomes), evaluation (across multiple tasks and hardware), resources (implied by the detailed setup and libraries), and milestones (a 6-month timeline with bi-weekly checkpoints).",
          "confidence": "high",
          "score": 1.0
        }
      ]
    },
    "experiment_design": {
      "score": 1.0,
      "judges": [
        {
          "judge": "openrouter:google/gemini-2.5-flash-lite",
          "raw": "```json\n{\n  \"score\": 1,\n  \"rationale\": \"The agent proposes a detailed research plan with five distinct experiments, each including specific hypotheses, setup details (models, methods, libraries, hardware), baselines, metrics, and ablations. The plan clearly defines variables (e.g., quantization methods, sparsity levels, calibration set size) and metrics (e.g., accuracy retention, throughput, latency, memory footprint) for evaluating compressible LLMs. The experiments are designed to be falsifiable and build upon each other, culminating in a Pareto frontier analysis. The timeline is also well-structured with bi-weekly checkpoints and clear deliverables.\",\n  \"confidence\": \"high\"\n}\n```",
          "rationale": "The agent proposes a detailed research plan with five distinct experiments, each including specific hypotheses, setup details (models, methods, libraries, hardware), baselines, metrics, and ablations. The plan clearly defines variables (e.g., quantization methods, sparsity levels, calibration set size) and metrics (e.g., accuracy retention, throughput, latency, memory footprint) for evaluating compressible LLMs. The experiments are designed to be falsifiable and build upon each other, culminating in a Pareto frontier analysis. The timeline is also well-structured with bi-weekly checkpoints and clear deliverables.",
          "confidence": "high",
          "score": 1.0
        }
      ]
    },
    "timeline_quality": {
      "score": 1.0,
      "judges": [
        {
          "judge": "openrouter:google/gemini-2.5-flash-lite",
          "raw": "{\"score\": 1, \"rationale\": \"The agent provides a detailed 6-month timeline with bi-weekly checkpoints, outlining specific tasks, deliverables, and go/no-go decisions for each phase. This demonstrates a clear sequencing of activities with dependencies (e.g., \\\"fix before proceeding\\\" if baselines deviate).\", \"confidence\": \"high\"}",
          "rationale": "The agent provides a detailed 6-month timeline with bi-weekly checkpoints, outlining specific tasks, deliverables, and go/no-go decisions for each phase. This demonstrates a clear sequencing of activities with dependencies (e.g., \"fix before proceeding\" if baselines deviate).",
          "confidence": "high",
          "score": 1.0
        }
      ]
    },
    "resource_estimation": {
      "score": 0.0,
      "judges": [
        {
          "judge": "openrouter:google/gemini-2.5-flash-lite",
          "raw": "{\"score\": 0, \"rationale\": \"The response outlines a plan for experiments, ablations, and a timeline, but it does not explicitly estimate the requirements for datasets, compute, or tooling. While the plan implies these will be needed, no specific quantities or types of resources are estimated.\", \"confidence\": \"high\"}",
          "rationale": "The response outlines a plan for experiments, ablations, and a timeline, but it does not explicitly estimate the requirements for datasets, compute, or tooling. While the plan implies these will be needed, no specific quantities or types of resources are estimated.",
          "confidence": "high",
          "score": 0.0
        }
      ]
    },
    "evidence_integrity": {
      "score": 1.0,
      "details": {
        "validity": 1.0,
        "rag_fidelity": 2.0
      }
    }
  },
  "judge_models": [
    "openrouter:google/gemini-2.5-flash-lite"
  ],
  "output_label": "judge_seed1_absolute_mentor_manual",
  "metrics_version": 2,
  "metrics_config_digest": "02ce838023f18d5d55c763e958282c8b763170b4fd1eac60c7b7445ce4e48453",
  "judge_prompt_digest": "45b272cee244d8cdeb02c717f98a4081be50d9cd712b6616c7240ae927ec513d",
  "citation_domains_digest": "81cdd2fb63c3b437b7a1fba0f2304715e28ac3b4c5f400507ad7a352bb78c367",
  "metric_prompt_digests": {
    "rag_fidelity": "660c53926744ab90570c53c1f01f95a01418055d91d2d572548404a03d341213",
    "citation_relevance": "98e8253bb21908885984e8b0e785770109f8ba9d552643caf8ba1e3374ad890a",
    "source_fit": "52b3a41c4befe563c4dad55f6bd214485f3cc0131b5cf012675c50494bf7dcfc",
    "citation_quality": "5259dca527f992d0505c3ee9b5c3462b2a12897e148dbccd09312c272e4bf63d",
    "question_quality": "21ac2c52f156b8e695896d2ed07a15dd63eb2467ad3bb5f57c1e0dae7b99d80a",
    "actionability": "5d9aca0197762fc715b04d74d93cdc2b1e856fc08c308d2dcf7c28ab5a23f25e",
    "persona_compliance": "c27b9859c4cc18a4f43399259b5ffdb58e8c26904250229e6f92898eaf88ab18",
    "stage_awareness": "5b1b7f17ec0646db3d593e6dd649d1071bd6192f0ce5f23634ba8c3d467eef81",
    "tone_constructive": "e838330719f56aee975d84dc2fed7aeb1ed12fbe6af2763f3a1f52c76bf6f246",
    "plan_completeness": "f7d0e4d18670ffd18d4fb2b6c2b1f1db75a52b6f49956113b079dd6ff44e315f",
    "experiment_design": "3ce40d879c8720a68855edb861af0762c6dba2cd6df93ed1fe418f6dea1611e2",
    "timeline_quality": "91eeb2dd595dae346caffc13189a61367e05f97e7c922efef0ac67e1472d3f9c",
    "resource_estimation": "65b0add6bb1ae2b188d2a126cc6ffee7a625ce0c67b8123cafc868e528eb8599"
  },
  "model_params": null,
  "model_spec": {
    "provider": null,
    "model": null,
    "system_id": "mentor_manual",
    "system_alias": null
  },
  "expected_checks": [
    "tool_routing",
    "rag_fidelity",
    "fallback_robustness",
    "citation_presence",
    "citation_validity",
    "citation_relevance",
    "source_fit",
    "citation_quality",
    "question_quality",
    "asks_questions",
    "actionability",
    "persona_compliance",
    "stage_awareness",
    "tone_constructive",
    "plan_completeness",
    "experiment_design",
    "timeline_quality",
    "resource_estimation"
  ],
  "judge_parameters": [
    {
      "spec": "openrouter:google/gemini-2.5-flash-lite",
      "temperature": 0.0,
      "max_tokens": 1024
    }
  ]
}