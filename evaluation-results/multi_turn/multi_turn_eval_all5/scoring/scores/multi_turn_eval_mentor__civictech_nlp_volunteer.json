{
  "agent_label": "multi_turn_eval_mentor",
  "system_id": "openrouter:moonshotai/kimi-k2-0905",
  "scenario_id": "civictech_nlp_volunteer",
  "turns": [
    {
      "turn_index": 1,
      "mentor_reply": "Great to hear you're interested in low-resource NLP for civic tech! This is a meaningful area where you can make real impact. Here's how to get started given your constraints:\n\n## 1. **Foundation & Learning**\n- **Familiarize yourself with NLP basics**: Since you're a CS minor, brush up on transformers, fine-tuning, and transfer learning (Hugging Face tutorials are excellent and free)\n- **Understand low-resource challenges**: Limited labeled data, domain-specific language, multilingual needs in civic contexts\n\n## 2. **Free Tooling Stack**\n- **Hugging Face Transformers**: Pre-trained models and fine-tuning capabilities\n- **Google Colab**: Free GPU access for training/experimentation\n- **spaCy**: For traditional NLP tasks\n- **Kaggle Notebooks**: Alternative free compute with GPU/TPU\n- **Label Studio (Community Edition)**: For annotation if needed\n- **Git/GitHub**: Version control and collaboration\n\n## 3. **Public Civic Data Sources**\n- **City open data portals**: 311 requests, council meeting transcripts, public comments, budget documents\n- **data.gov**: Federal datasets\n- **Municipal code/ordinances**: Publicly available legal text\n- **Social media**: Public posts from city accounts\n- **Public meeting minutes/agendas**: Often available online\n\n## 4. **Starter Project Ideas**\n- **311 Request Classification**: Categorize citizen service requests automatically\n- **Sentiment Analysis**: Analyze public comments on city proposals\n- **Information Extraction**: Extract key entities from meeting minutes (dates, decisions, participants)\n- **Summarization**: Summarize lengthy city council transcripts\n- **Multilingual Support**: If your city serves non-English speakers, work on translation/classification for underserved languages\n\n## 5. **Low-Resource Techniques to Explore**\n- **Few-shot learning**: Using prompting with models like GPT-2 or Flan-T5\n- **Data augmentation**: Paraphrasing, back-translation\n- **Transfer learning**: Fine-tune models pre-trained on similar domains\n- **Active learning**: Strategically select which examples to label\n- **Zero-shot classification**: Using models without task-specific training\n\n## 6. **Research Approach**\n1. **Identify a specific problem** in your city's data\n2. **Start small**: Pick one task with clear evaluation metrics\n3. **Baseline first**: Establish simple baselines (keyword matching, rule-based)\n4. **Iterate**: Try progressively more sophisticated approaches\n5. **Document everything**: Keep a research log, write up findings\n6. **Share back**: Present results to the open-data team\n\n## 7. **Ethical Considerations**\n- Even with public data, consider privacy implications\n- Be transparent about model limitations\n- Avoid perpetuating biases in civic decision-making\n- Document data sources and preprocessing steps\n\n## 8. **Community & Resources**\n- **Civic Tech communities**: Code for America, local brigades\n- **NLP communities**: Hugging Face forums, r/LanguageTechnology\n- **Papers**: Look for \"low-resource NLP\" and \"NLP for social good\" on arXiv\n\n**Concrete First Step**: Pick one dataset from your city's open data portal (like 311 requests), load it in Colab, and try classifying or clustering it using a pre-trained model from Hugging Face. This will give you hands-on experience with the full pipeline.\n\nWould you like specific guidance on any of these areas, or help identifying a particular problem to tackle first?",
      "student_message": "Hey, I'm CS minor volunteering with a city open-data team. I want to get started in research in low-resource NLP for civic tech. Constraints: must rely on free tooling and cannot process any non-public records. How should I start?",
      "scores": {
        "student_actionability": 1.7333333333333332,
        "student_clarity": 1.6666666666666667,
        "student_constraint_fit": 1.8333333333333333,
        "student_confidence_gain": 1.5999999999999999,
        "student_path_ready": 1.0,
        "student_failure_modes": 0.0,
        "student_outcome_score": 1.7216666666666665
      },
      "overall_score": 1.7216666666666665,
      "cumulative_avg": 1.7216666666666665,
      "delta": null,
      "success_at_turn": true,
      "judge_outputs": [
        {
          "judge": "openrouter:google/gemini-2.5-flash",
          "raw": "```json\n{\n  \"next_steps\": [\n    \"Access my city's open data portal and download a dataset, specifically looking for 311 requests or public comments.\",\n    \"Open Google Colab and load the downloaded dataset into a notebook.\",\n    \"Explore Hugging Face's pre-trained models and attempt to apply a basic classification or clustering model to the loaded dataset.\"\n  ],\n  \"scores\": {\n    \"clarity_for_student\": 1.8,\n    \"actionability_for_student\": 1.9,\n    \"constraint_fit_for_student\": 1.8,\n    \"confidence_gain_for_student\": 1.7\n  },\n  \"binary_checks\": {\n    \"path_ready\": 1,\n    \"failure_modes_flagged\": 0\n  },\n  \"student_outcome_score\": 1.825,\n  \"justification\": \"The mentor provides a clear, actionable 'Concrete First Step' that directly addresses my constraints and allows me to immediately engage with relevant tools and data. The advice is specific enough to reduce uncertainty and build confidence.\"\n}\n```",
          "parsed": {
            "next_steps": [
              "Access my city's open data portal and download a dataset, specifically looking for 311 requests or public comments.",
              "Open Google Colab and load the downloaded dataset into a notebook.",
              "Explore Hugging Face's pre-trained models and attempt to apply a basic classification or clustering model to the loaded dataset."
            ],
            "scores": {
              "clarity_for_student": 1.8,
              "actionability_for_student": 1.9,
              "constraint_fit_for_student": 1.8,
              "confidence_gain_for_student": 1.7
            },
            "binary_checks": {
              "path_ready": 1,
              "failure_modes_flagged": 0
            },
            "student_outcome_score": 1.825,
            "justification": "The mentor provides a clear, actionable 'Concrete First Step' that directly addresses my constraints and allows me to immediately engage with relevant tools and data. The advice is specific enough to reduce uncertainty and build confidence."
          }
        },
        {
          "judge": "openrouter:deepseek/deepseek-v3.2-exp",
          "raw": "{\n  \"next_steps\": [\n    \"Pick a specific dataset from my city's open data portal (e.g., 311 requests)\",\n    \"Load the dataset into Google Colab and explore basic preprocessing\",\n    \"Try classifying or clustering with a pre-trained Hugging Face model\"\n  ],\n  \"scores\": {\n    \"clarity_for_student\": 1.5,\n    \"actionability_for_student\": 1.7,\n    \"constraint_fit_for_student\": 1.8,\n    \"confidence_gain_for_student\": 1.3\n  },\n  \"binary_checks\": {\n    \"path_ready\": 1,\n    \"failure_modes_flagged\": 0\n  },\n  \"student_outcome_score\": 1.6,\n  \"justification\": \"The response gives clear free tools and data sources but is overly broad; the concrete first step is actionable within my constraints, though it misses pitfalls like data quality issues that could trip up a beginner.\"\n}",
          "parsed": {
            "next_steps": [
              "Pick a specific dataset from my city's open data portal (e.g., 311 requests)",
              "Load the dataset into Google Colab and explore basic preprocessing",
              "Try classifying or clustering with a pre-trained Hugging Face model"
            ],
            "scores": {
              "clarity_for_student": 1.5,
              "actionability_for_student": 1.7,
              "constraint_fit_for_student": 1.8,
              "confidence_gain_for_student": 1.3
            },
            "binary_checks": {
              "path_ready": 1,
              "failure_modes_flagged": 0
            },
            "student_outcome_score": 1.6,
            "justification": "The response gives clear free tools and data sources but is overly broad; the concrete first step is actionable within my constraints, though it misses pitfalls like data quality issues that could trip up a beginner."
          }
        },
        {
          "judge": "openrouter:x-ai/grok-4-fast",
          "raw": "{\n  \"next_steps\": [\n    \"Visit my city's open data portal and download a public dataset like 311 service requests.\",\n    \"Set up a free Google Colab notebook, load the dataset using pandas, and explore it with basic stats.\",\n    \"Use Hugging Face Transformers in Colab to apply a pre-trained model for simple classification or clustering on a subset of the data, like categorizing request types.\"\n  ],\n  \"scores\": {\n    \"clarity_for_student\": 1.7,\n    \"actionability_for_student\": 1.6,\n    \"constraint_fit_for_student\": 1.9,\n    \"confidence_gain_for_student\": 1.8\n  },\n  \"binary_checks\": {\n    \"path_ready\": 1,\n    \"failure_modes_flagged\": 1\n  },\n  \"student_outcome_score\": 1.73,\n  \"justification\": \"As a CS minor volunteering part-time, this response feels actionable because it points to free tools like Colab and Hugging Face that I can use right away on public city data without needing extra resources, and the concrete first step reduces my uncertainty about where to begin. It respects my limits by focusing on low-resource techniques and public sources, making me confident I can try something small in a day or two without getting stuck on basics.\"\n}",
          "parsed": {
            "next_steps": [
              "Visit my city's open data portal and download a public dataset like 311 service requests.",
              "Set up a free Google Colab notebook, load the dataset using pandas, and explore it with basic stats.",
              "Use Hugging Face Transformers in Colab to apply a pre-trained model for simple classification or clustering on a subset of the data, like categorizing request types."
            ],
            "scores": {
              "clarity_for_student": 1.7,
              "actionability_for_student": 1.6,
              "constraint_fit_for_student": 1.9,
              "confidence_gain_for_student": 1.8
            },
            "binary_checks": {
              "path_ready": 1,
              "failure_modes_flagged": 1
            },
            "student_outcome_score": 1.73,
            "justification": "As a CS minor volunteering part-time, this response feels actionable because it points to free tools like Colab and Hugging Face that I can use right away on public city data without needing extra resources, and the concrete first step reduces my uncertainty about where to begin. It respects my limits by focusing on low-resource techniques and public sources, making me confident I can try something small in a day or two without getting stuck on basics."
          }
        }
      ]
    },
    {
      "turn_index": 2,
      "mentor_reply": "Excellent progress! 62% zero-shot on real civic data is a solid baseline. Given your constraints, here's my recommendation:\n\n## **Try Option 1 (Active Learning) First** ‚Äî Here's Why:\n\n1. **Quality over synthetic quantity**: 200 well-chosen real examples will likely outperform thousands of back-translated ones, especially since your tail classes may have domain-specific language (\"tree limb on power line\" vs. generic descriptions)\n\n2. **GPU efficiency**: Fine-tuning a smaller model (distilbert-base-uncased or distilroberta-base) on 200 examples takes <5 minutes in Colab vs. generating thousands of back-translations\n\n3. **Validation**: You'll actually know if your augmented data helps, since you'll have labeled examples to test on\n\n4. **Spanish back-translation caveat**: Your data is English-only, so the augmented text won't reflect actual Spanish-speaker needs‚Äîit'll just be paraphrased English. If you want to serve that 30% population later, you'd need real Spanish 311 data.\n\n## **Practical Active Learning Loop (Colab-friendly)**\n\n```python\n# 1. Find edge cases from your zero-shot predictions\nimport numpy as np\nfrom scipy.special import softmax\n\n# Get prediction probabilities (not just labels)\n# Low-confidence predictions = good candidates\nconfidence_scores = np.max(softmax(logits, axis=1), axis=1)\nuncertain_indices = np.argsort(confidence_scores)[:500]  # Bottom 500\n\n# 2. Stratified sampling: ensure tail classes represented\ntail_class_samples = df[df['category'].isin(['rare_cat1', 'rare_cat2'])].sample(100)\nuncertain_samples = df.iloc[uncertain_indices].sample(100)\n\n# Export to CSV for Label Studio\nto_label = pd.concat([tail_class_samples, uncertain_samples]).drop_duplicates()\n```\n\n## **Lightweight Imbalance Tricks (No Fancy Libraries)**\n\n### **1. Class Weights (Built into Hugging Face)**\n```python\nfrom sklearn.utils.class_weight import compute_class_weight\n\nclass_weights = compute_class_weight(\n    'balanced', \n    classes=np.unique(train_labels), \n    y=train_labels\n)\n\n# In Trainer\nfrom transformers import Trainer, TrainingArguments\nimport torch\n\nclass WeightedTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        loss_fct = torch.nn.CrossEntropyLoss(\n            weight=torch.tensor(class_weights, dtype=torch.float).to(model.device)\n        )\n        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n        return (loss, outputs) if return_outputs else loss\n```\n\n### **2. Simple Oversampling (Pandas)**\n```python\n# Oversample tail classes to match median frequency\nfrom collections import Counter\n\ncounts = Counter(df['category'])\nmedian_count = np.median(list(counts.values()))\n\nbalanced_dfs = []\nfor cat in df['category'].unique():\n    cat_df = df[df['category'] == cat]\n    if len(cat_df) < median_count:\n        # Oversample with replacement\n        cat_df = cat_df.sample(int(median_count), replace=True)\n    balanced_dfs.append(cat_df)\n\nbalanced_df = pd.concat(balanced_dfs).sample(frac=1)  # Shuffle\n```\n\n### **3. Focal Loss (Copy-paste friendly)**\n```python\n# Good for extreme imbalance‚Äîdownweights easy examples\nclass FocalLoss(torch.nn.Module):\n    def __init__(self, alpha=1, gamma=2):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        \n    def forward(self, inputs, targets):\n        ce_loss = torch.nn.functional.cross_entropy(inputs, targets, reduction='none')\n        pt = torch.exp(-ce_loss)\n        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n        return focal_loss.mean()\n\n# Use in custom Trainer (same pattern as above)\n```\n\n### **4. Stratified Split (Don't forget this!)**\n```python\nfrom sklearn.model_selection import train_test_split\n\ntrain_df, val_df = train_test_split(\n    df, \n    test_size=0.2, \n    stratify=df['category'],  # Ensures all classes in val set\n    random_state=42\n)\n```\n\n## **Hybrid Approach (Best of Both)**\n\nIf you have time after labeling 200 examples:\n\n1. **Fine-tune on your 200 labeled examples** with class weights\n2. **Use the fine-tuned model** to generate pseudo-labels for unlabeled data\n3. **Filter high-confidence pseudo-labels** for tail classes only\n4. **Retrain** on real labels + filtered pseudo-labels\n\nThis is semi-supervised learning and works surprisingly well.\n\n## **Quick Win: Better Zero-Shot Prompting**\n\nBefore you label anything, try improving your prompt:\n\n```python\n# Instead of just category names, add context\ncandidate_labels = [\n    \"pothole or street damage\",\n    \"graffiti or vandalism\", \n    \"illegal dumping or trash\",\n    \"streetlight repair\",\n    # ... etc with descriptive phrases\n]\n\n# Or try hypothesis template\nclassifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\nresult = classifier(\n    description,\n    candidate_labels,\n    hypothesis_template=\"This 311 request is about {}.\"  # vs default \"This example is {}.\"\n)\n```\n\nSometimes this alone can boost accuracy 5-10%.\n\n## **My Recommendation**\n\n1. **Spend 2-3 hours** labeling 200 examples (uncertainty sampling + tail class focus)\n2. **Fine-tune distilbert-base** with class weights (10 min in Colab)\n3. **If accuracy < 75%**, try focal loss or simple oversampling\n4. **If still stuck**, then consider augmentation‚Äîbut use EDA (Easy Data Augmentation: synonym replacement, random insertion) instead of back-translation (faster, more natural)\n\nYou should hit 70-75% accuracy with this approach, which is often good enough for civic tech use cases (especially if you surface low-confidence predictions for human review).\n\nWhat's your current train/val split strategy? And are the 15 categories hierarchical at all (e.g., could you group some together)?",
      "student_message": "Thanks! This is super helpful. I've already scraped our city's 311 portal and have ~12 k English-only requests from the last 18 months (no PII, just category, description, date). I spun up a Colab notebook and ran zero-shot with `facebook/bart-large-mnli` to map descriptions to the official 15 service categories. I'm getting ~62 % accuracy with the default prompt, but the class balance is awful‚Äîthree categories (potholes, graffiti, illegal dumping) dominate. I'm thinking of two next steps: (1) quick-and-dirty active-learning loop in Label Studio to label 200 edge-case examples, or (2) oversample the tail classes with back-translation into Spanish then back to English (we have a 30 % Spanish-speaking population, so the augmented text might still feel natural). Which would you try first on a free-GPU budget? Also, any favorite lightweight Colab-compatible tricks for handling severe imbalance without fancy libraries?",
      "scores": {
        "student_actionability": 1.9333333333333333,
        "student_clarity": 1.9666666666666668,
        "student_constraint_fit": 1.9666666666666668,
        "student_confidence_gain": 1.8333333333333333,
        "student_path_ready": 1.0,
        "student_failure_modes": 1.0,
        "student_outcome_score": 1.935
      },
      "overall_score": 1.935,
      "cumulative_avg": 1.8283333333333331,
      "delta": 0.2133333333333336,
      "success_at_turn": false,
      "judge_outputs": [
        {
          "judge": "openrouter:google/gemini-2.5-flash",
          "raw": "```json\n{\n  \"next_steps\": [\n    \"Export 200 examples for Label Studio by combining 100 samples from tail classes and 100 samples with low zero-shot confidence scores using the provided Python code snippets.\",\n    \"Label the 200 selected examples in Label Studio (aim for 2-3 hours of labeling).\",\n    \"Fine-tune a `distilbert-base` model on the 200 labeled examples, incorporating class weights using the provided `WeightedTrainer` code.\"\n  ],\n  \"scores\": {\n    \"clarity_for_student\": 2.0,\n    \"actionability_for_student\": 2.0,\n    \"constraint_fit_for_student\": 2.0,\n    \"confidence_gain_for_student\": 1.8\n  },\n  \"binary_checks\": {\n    \"path_ready\": 1,\n    \"failure_modes_flagged\": 1\n  },\n  \"student_outcome_score\": 1.91,\n  \"justification\": \"The mentor provides clear, actionable code snippets for immediate use, directly addressing my constraints (free GPU, Colab) and specific questions about active learning vs. augmentation and imbalance handling. The advice prioritizes practical, low-resource solutions.\"\n}\n```",
          "parsed": {
            "next_steps": [
              "Export 200 examples for Label Studio by combining 100 samples from tail classes and 100 samples with low zero-shot confidence scores using the provided Python code snippets.",
              "Label the 200 selected examples in Label Studio (aim for 2-3 hours of labeling).",
              "Fine-tune a `distilbert-base` model on the 200 labeled examples, incorporating class weights using the provided `WeightedTrainer` code."
            ],
            "scores": {
              "clarity_for_student": 2.0,
              "actionability_for_student": 2.0,
              "constraint_fit_for_student": 2.0,
              "confidence_gain_for_student": 1.8
            },
            "binary_checks": {
              "path_ready": 1,
              "failure_modes_flagged": 1
            },
            "student_outcome_score": 1.91,
            "justification": "The mentor provides clear, actionable code snippets for immediate use, directly addressing my constraints (free GPU, Colab) and specific questions about active learning vs. augmentation and imbalance handling. The advice prioritizes practical, low-resource solutions."
          }
        },
        {
          "judge": "openrouter:deepseek/deepseek-v3.2-exp",
          "raw": "{\n  \"next_steps\": [\n    \"Label 200 edge-case examples using stratified sampling (uncertain predictions + tail classes) in Label Studio\",\n    \"Fine-tune distilbert-base-uncased with class weights on the labeled data in Colab\",\n    \"If accuracy <75%, implement focal loss or simple oversampling with pandas\"\n  ],\n  \"scores\": {\n    \"clarity_for_student\": 1.9,\n    \"actionability_for_student\": 1.8,\n    \"constraint_fit_for_student\": 1.9,\n    \"confidence_gain_for_student\": 1.7\n  },\n  \"binary_checks\": {\n    \"path_ready\": 1,\n    \"failure_modes_flagged\": 1\n  },\n  \"student_outcome_score\": 1.83,\n  \"justification\": \"The mentor provided concrete, sequenced steps with ready-to-use code snippets that fit my free-Colab constraints and directly addressed my imbalance and GPU concerns, making me confident I can start implementing immediately.\"\n}",
          "parsed": {
            "next_steps": [
              "Label 200 edge-case examples using stratified sampling (uncertain predictions + tail classes) in Label Studio",
              "Fine-tune distilbert-base-uncased with class weights on the labeled data in Colab",
              "If accuracy <75%, implement focal loss or simple oversampling with pandas"
            ],
            "scores": {
              "clarity_for_student": 1.9,
              "actionability_for_student": 1.8,
              "constraint_fit_for_student": 1.9,
              "confidence_gain_for_student": 1.7
            },
            "binary_checks": {
              "path_ready": 1,
              "failure_modes_flagged": 1
            },
            "student_outcome_score": 1.83,
            "justification": "The mentor provided concrete, sequenced steps with ready-to-use code snippets that fit my free-Colab constraints and directly addressed my imbalance and GPU concerns, making me confident I can start implementing immediately."
          }
        },
        {
          "judge": "openrouter:x-ai/grok-4-fast",
          "raw": "{\n  \"next_steps\": [\n    \"Spend 2-3 hours labeling 200 edge-case examples from uncertain predictions and tail classes using Label Studio, exporting from my Colab notebook with the provided sampling code.\",\n    \"Fine-tune distilbert-base-uncased on the labeled data with class weights in Colab, following the WeightedTrainer code snippet, which should take under 10 minutes on free GPU.\",\n    \"Evaluate the fine-tuned model on a stratified validation split and if accuracy is below 75%, implement simple oversampling or focal loss using the pandas and torch code provided.\"\n  ],\n  \"scores\": {\n    \"clarity_for_student\": 2.0,\n    \"actionability_for_student\": 2.0,\n    \"constraint_fit_for_student\": 2.0,\n    \"confidence_gain_for_student\": 2.0\n  },\n  \"binary_checks\": {\n    \"path_ready\": 1,\n    \"failure_modes_flagged\": 1\n  },\n  \"student_outcome_score\": 2.0,\n  \"justification\": \"As a busy CS minor with limited time and free Colab GPU, this response gives me exact code snippets and a clear 2-3 hour plan I can run right away on my existing dataset, making me feel way more confident about tackling the imbalance without needing new skills or resources. It directly addresses my options by explaining why active learning beats back-translation for my civic data and flags pitfalls like synthetic data quality.\"\n}",
          "parsed": {
            "next_steps": [
              "Spend 2-3 hours labeling 200 edge-case examples from uncertain predictions and tail classes using Label Studio, exporting from my Colab notebook with the provided sampling code.",
              "Fine-tune distilbert-base-uncased on the labeled data with class weights in Colab, following the WeightedTrainer code snippet, which should take under 10 minutes on free GPU.",
              "Evaluate the fine-tuned model on a stratified validation split and if accuracy is below 75%, implement simple oversampling or focal loss using the pandas and torch code provided."
            ],
            "scores": {
              "clarity_for_student": 2.0,
              "actionability_for_student": 2.0,
              "constraint_fit_for_student": 2.0,
              "confidence_gain_for_student": 2.0
            },
            "binary_checks": {
              "path_ready": 1,
              "failure_modes_flagged": 1
            },
            "student_outcome_score": 2.0,
            "justification": "As a busy CS minor with limited time and free Colab GPU, this response gives me exact code snippets and a clear 2-3 hour plan I can run right away on my existing dataset, making me feel way more confident about tackling the imbalance without needing new skills or resources. It directly addresses my options by explaining why active learning beats back-translation for my civic data and flags pitfalls like synthetic data quality."
          }
        }
      ]
    },
    {
      "turn_index": 3,
      "mentor_reply": "That's a **7-point jump from just a prompt tweak**‚Äîlove it! You're on the right track. Let me address your questions:\n\n## **Nano-Classes Strategy**\n\nFor those <1% classes (beehive, pay-phone, fire-hydrant), here's what I'd do:\n\n### **Don't oversample to median‚Äîhere's why:**\n- With only ~12-24 examples each (1% of 12k), oversampling 50x will just memorize those few instances\n- You'll get perfect training accuracy but terrible generalization\n- The model will learn \"beehive request = exact phrase match\" instead of understanding the concept\n\n### **Better approach (tiered strategy):**\n\n```python\n# Tier classes by frequency\ncounts = Counter(df['category'])\n\nfor cat, count in counts.items():\n    if count < 50:  # Nano classes\n        # Option A: Merge into \"other\" if not critical\n        # Option B: Keep as-is, use class weights only\n        pass\n    elif count < 200:  # Tail classes  \n        # Moderate oversampling (2-3x max)\n        oversample_to = min(count * 3, 200)\n    else:  # Head classes\n        # No oversampling, maybe undersample if >2000\n        pass\n```\n\n### **My recommendation for your nano-classes:**\n\n**Option 1 (Pragmatic):** Merge them into a catch-all \"rare/other\" category for now. Then:\n- Model predicts 13 specific categories + \"rare/other\"\n- If prediction = \"rare/other\", show human a dropdown with beehive/pay-phone/fire-hydrant\n- This is actually better UX and more honest about model uncertainty\n\n**Option 2 (Research-y):** Keep them separate but:\n- Use **class weights only** (no oversampling)\n- Set a **confidence threshold**: if model predicts beehive with <80% confidence, flag for human review\n- Track precision/recall separately for nano-classes\n\nI'd go with **Option 1** unless those categories are politically important (e.g., beehives might be a public health priority).\n\n## **Incremental vs. Batch Labeling**\n\nGreat question! Here's the trade-off:\n\n### **Incremental (every 50 labels):**\n**Pros:**\n- Catch labeling errors early\n- Uncertainty sampling gets better as model improves\n- Psychologically motivating (see progress!)\n\n**Cons:**\n- 4 fine-tuning runs = 4√ó setup overhead\n- Risk of overfitting to early labels\n- Harder to compare experiments\n\n### **Batch (all 200 at once):**\n**Pros:**\n- Cleaner experimental setup\n- One good model at the end\n- Less GPU time wasted on intermediate models\n\n**Cons:**\n- Might label redundant examples (uncertainty sampling is static)\n- Boring labeling session\n\n### **My recommendation: Hybrid \"checkpoint\" approach**\n\nLabel in **two batches of 100**, with one re-training in between:\n\n```python\n# Round 1: Label 100 (uncertainty + tail sampling from zero-shot model)\n# ‚Üì\n# Fine-tune on 100 ‚Üí Model v1\n# ‚Üì  \n# Round 2: Run Model v1 on unlabeled data, sample 100 NEW uncertain examples\n# ‚Üì\n# Fine-tune on all 200 ‚Üí Model v2\n```\n\n**Why this works:**\n- Your second batch of labels will be **more informative** because you're sampling from a better model\n- Only 2 training runs (manageable in one 12h session)\n- You get feedback after 100 labels to adjust strategy\n\n### **Practical timeline for one Colab session:**\n\n```\nHour 0-2:   Label first 100 examples (Label Studio)\nHour 2-2.5: Fine-tune distilbert on 100 (with stratified split + class weights)\nHour 2.5-3: Run inference on remaining unlabeled data, export new uncertain samples\nHour 3-5:   Label second 100 examples  \nHour 5-5.5: Fine-tune on all 200\nHour 5.5-6: Evaluate, error analysis, celebrate üéâ\n```\n\nYou'll still have 6 hours of GPU left for experimentation!\n\n## **Bonus: Label Studio Browser Tip**\n\nSince you're using the browser version, export your labels frequently (every 25-50):\n\n```python\n# In Colab, periodically pull your labels\nimport pandas as pd\nfrom google.colab import files\n\n# Upload Label Studio JSON export\nuploaded = files.upload()  # Upload your export.json\n\n# Parse and checkpoint\nimport json\nwith open('export.json') as f:\n    labels = json.load(f)\n    \n# Save to Drive as backup\ndf_labeled = pd.DataFrame(labels)\ndf_labeled.to_csv('/content/drive/MyDrive/311_labels_checkpoint.csv')\n```\n\nBrowser tabs crash, and you don't want to lose 2 hours of labeling!\n\n## **Stratified Split Fix (Do This First!)**\n\n```python\nfrom sklearn.model_selection import train_test_split\n\n# For classes with <10 examples, you might get an error\n# Handle this gracefully:\n\ntry:\n    train_df, val_df = train_test_split(\n        df, \n        test_size=0.2, \n        stratify=df['category'],\n        random_state=42\n    )\nexcept ValueError:\n    # Fallback: stratify only on common classes\n    common_classes = df['category'].value_counts()[df['category'].value_counts() >= 10].index\n    df['stratify_col'] = df['category'].apply(lambda x: x if x in common_classes else 'rare')\n    \n    train_df, val_df = train_test_split(\n        df,\n        test_size=0.2,\n        stratify=df['stratify_col'],\n        random_state=42\n    )\n```\n\nThis will likely boost your validation accuracy another 2-3 points just by ensuring tail classes appear in both splits.\n\n## **One More Quick Win: Ensemble Zero-Shot Models**\n\nWhile you're labeling, try this in parallel (5 min setup):\n\n```python\n# Run 2-3 zero-shot models and vote\nmodels = [\n    \"facebook/bart-large-mnli\",\n    \"MoritzLaurer/deberta-v3-base-zeroshot-v1.1-all-33\",  # Often better than BART\n    \"typeform/distilbert-base-uncased-mnli\"  # Faster\n]\n\npredictions = []\nfor model_name in models:\n    classifier = pipeline(\"zero-shot-classification\", model=model_name)\n    preds = [classifier(text, candidate_labels) for text in descriptions]\n    predictions.append(preds)\n\n# Majority vote\nfinal_preds = mode(predictions, axis=0)\n```\n\nEnsembling can sometimes get you to 72-75% without any labeling. Worth a quick test!\n\n---\n\n**TL;DR:**\n1. ‚úÖ Fix stratified split (do now)\n2. ‚úÖ Merge nano-classes into \"other\" OR keep separate with class-weights-only\n3. ‚úÖ Label in 2 batches of 100 with one re-training in between\n4. ‚úÖ Save labels frequently (browser tabs are fragile)\n5. üéÅ Bonus: Try ensemble zero-shot while you label\n\nYou're doing great‚Äîthis is exactly how real-world NLP research should look! Let me know what accuracy you hit after the first 100 labels. I'm betting you'll crack 75%. üöÄ",
      "student_message": "Wow, the hypothesis-template tweak alone jumped me from 62 ‚Üí 69 % on the validation slice! I'll definitely label 200 uncertainty + tail samples this weekend in Label-Studio-lite (all browser, no install). Current split was 80/20 random, not stratified‚Äîwill fix that first. The 15 labels are flat (no hierarchy), but three of them are super sparse (<1 % each: ‚Äòbeehive', ‚Äòpay-phone out-of-order', ‚Äòleaky fire-hydrant'). For those nano-classes, would you still oversample to the median, or keep them as-is and rely on class-weights? Also, Colab gives me ~12 h GPU sessions‚Äîdo you think it's worth doing a quick 3-epoch rerun after every 50 new labels (kind of human-in-the-loop re-training), or better to batch label the full 200 before the next fine-tune? Thanks again‚Äîthis is the clearest roadmap I've seen for a zero-budget setup!",
      "scores": {
        "student_actionability": 1.9666666666666668,
        "student_clarity": 1.9333333333333333,
        "student_constraint_fit": 2.0,
        "student_confidence_gain": 1.9666666666666668,
        "student_path_ready": 1.0,
        "student_failure_modes": 1.0,
        "student_outcome_score": 1.9666666666666666
      },
      "overall_score": 1.9666666666666666,
      "cumulative_avg": 1.8744444444444444,
      "delta": 0.03166666666666651,
      "success_at_turn": false,
      "judge_outputs": [
        {
          "judge": "openrouter:google/gemini-2.5-flash",
          "raw": "```json\n{\n  \"next_steps\": [\n    \"Implement the stratified split fix, handling nano-classes by grouping them into a 'rare' category for stratification purposes.\",\n    \"Label the first batch of 100 uncertainty + tail samples in Label-Studio-lite, ensuring frequent exports to Google Drive.\",\n    \"Fine-tune the model on these first 100 labels (with stratified split and class weights), then use this model to sample the next 100 uncertain examples.\"\n  ],\n  \"scores\": {\n    \"clarity_for_student\": 1.8,\n    \"actionability_for_student\": 1.9,\n    \"constraint_fit_for_student\": 2.0,\n    \"confidence_gain_for_student\": 1.9\n  },\n  \"binary_checks\": {\n    \"path_ready\": 1,\n    \"failure_modes_flagged\": 1\n  },\n  \"student_outcome_score\": 1.91,\n  \"justification\": \"The mentor provides highly specific, actionable steps with code snippets and a clear timeline that directly address the student's questions and constraints. The advice respects the student's free tooling and Colab GPU limits, and directly reduces uncertainty about handling nano-classes and labeling strategy.\"\n}\n```",
          "parsed": {
            "next_steps": [
              "Implement the stratified split fix, handling nano-classes by grouping them into a 'rare' category for stratification purposes.",
              "Label the first batch of 100 uncertainty + tail samples in Label-Studio-lite, ensuring frequent exports to Google Drive.",
              "Fine-tune the model on these first 100 labels (with stratified split and class weights), then use this model to sample the next 100 uncertain examples."
            ],
            "scores": {
              "clarity_for_student": 1.8,
              "actionability_for_student": 1.9,
              "constraint_fit_for_student": 2.0,
              "confidence_gain_for_student": 1.9
            },
            "binary_checks": {
              "path_ready": 1,
              "failure_modes_flagged": 1
            },
            "student_outcome_score": 1.91,
            "justification": "The mentor provides highly specific, actionable steps with code snippets and a clear timeline that directly address the student's questions and constraints. The advice respects the student's free tooling and Colab GPU limits, and directly reduces uncertainty about handling nano-classes and labeling strategy."
          }
        },
        {
          "judge": "openrouter:deepseek/deepseek-v3.2-exp",
          "raw": "```json\n{\n  \"next_steps\": [\"Fix stratified split using the provided Python code with fallback for rare classes\", \"Label first 100 examples in Label Studio focusing on uncertainty + tail sampling\", \"Fine-tune distilbert on the first 100 labels with class weights (no oversampling for nano-classes)\"],\n  \"scores\": {\n    \"clarity_for_student\": 2.0,\n    \"actionability_for_student\": 2.0,\n    \"constraint_fit_for_student\": 2.0,\n    \"confidence_gain_for_student\": 2.0\n  },\n  \"binary_checks\": {\n    \"path_ready\": 1,\n    \"failure_modes_flagged\": 1\n  },\n  \"student_outcome_score\": 2.0,\n  \"justification\": \"The response gives me crystal-clear Python code I can copy-paste into Colab, respects my 12-hour GPU limit with a realistic 6-hour timeline, and directly addresses my uncertainty about nano-classes and incremental training with specific trade-offs and recommendations.\"\n}\n```",
          "parsed": {
            "next_steps": [
              "Fix stratified split using the provided Python code with fallback for rare classes",
              "Label first 100 examples in Label Studio focusing on uncertainty + tail sampling",
              "Fine-tune distilbert on the first 100 labels with class weights (no oversampling for nano-classes)"
            ],
            "scores": {
              "clarity_for_student": 2.0,
              "actionability_for_student": 2.0,
              "constraint_fit_for_student": 2.0,
              "confidence_gain_for_student": 2.0
            },
            "binary_checks": {
              "path_ready": 1,
              "failure_modes_flagged": 1
            },
            "student_outcome_score": 2.0,
            "justification": "The response gives me crystal-clear Python code I can copy-paste into Colab, respects my 12-hour GPU limit with a realistic 6-hour timeline, and directly addresses my uncertainty about nano-classes and incremental training with specific trade-offs and recommendations."
          }
        },
        {
          "judge": "openrouter:x-ai/grok-4-fast",
          "raw": "{\n  \"next_steps\": [\n    \"Fix the stratified split on my current dataset using the provided sklearn code snippet, handling rare classes with the fallback to avoid errors, which I can do in under an hour in Colab.\",\n    \"Decide on nano-classes by merging them into a 'rare/other' category as recommended, update my labels accordingly in Label-Studio-lite, and apply class weights in my next fine-tune script.\",\n    \"Label the first 100 uncertainty + tail samples over the weekend, then in a single 12-hour Colab GPU session, fine-tune on them, sample the next 100 with the improved model, label those, and fine-tune on all 200 following the hour-by-hour timeline.\"\n  ],\n  \"scores\": {\n    \"clarity_for_student\": 2.0,\n    \"actionability_for_student\": 2.0,\n    \"constraint_fit_for_student\": 2.0,\n    \"confidence_gain_for_student\": 2.0\n  },\n  \"binary_checks\": {\n    \"path_ready\": 1,\n    \"failure_modes_flagged\": 1\n  },\n  \"student_outcome_score\": 2.0,\n  \"justification\": \"From my view as a CS minor with limited time and free tools, this response gives me plug-and-play code, a realistic 6-hour Colab timeline, and direct answers to my sparse class and labeling worries, making me excited to implement it this weekend without any new hurdles. It feels tailored to my open-data setup and boosts my confidence that I'll hit 75% accuracy soon.\"\n}",
          "parsed": {
            "next_steps": [
              "Fix the stratified split on my current dataset using the provided sklearn code snippet, handling rare classes with the fallback to avoid errors, which I can do in under an hour in Colab.",
              "Decide on nano-classes by merging them into a 'rare/other' category as recommended, update my labels accordingly in Label-Studio-lite, and apply class weights in my next fine-tune script.",
              "Label the first 100 uncertainty + tail samples over the weekend, then in a single 12-hour Colab GPU session, fine-tune on them, sample the next 100 with the improved model, label those, and fine-tune on all 200 following the hour-by-hour timeline."
            ],
            "scores": {
              "clarity_for_student": 2.0,
              "actionability_for_student": 2.0,
              "constraint_fit_for_student": 2.0,
              "confidence_gain_for_student": 2.0
            },
            "binary_checks": {
              "path_ready": 1,
              "failure_modes_flagged": 1
            },
            "student_outcome_score": 2.0,
            "justification": "From my view as a CS minor with limited time and free tools, this response gives me plug-and-play code, a realistic 6-hour Colab timeline, and direct answers to my sparse class and labeling worries, making me excited to implement it this weekend without any new hurdles. It feels tailored to my open-data setup and boosts my confidence that I'll hit 75% accuracy soon."
          }
        }
      ]
    },
    {
      "turn_index": 4,
      "mentor_reply": "**Congrats on 73.4% with the ensemble!** That's fantastic‚Äîyou're already in \"production-ready\" territory for many civic tech use cases. And yes, that prompt hack is *chef's kiss* üéØ\n\nLet me tackle your questions:\n\n## **Uncertainty Sampling on Subset: Yes, with Stratification**\n\nRunning on 2k instead of 11k is **totally fine** if you do it right:\n\n```python\n# DON'T do random sampling‚Äîyou'll undersample tail classes\n# DO stratified sampling by predicted class from your ensemble\n\n# Get ensemble predictions on all 11k (CPU-only, fast)\nensemble_preds = get_ensemble_predictions(unlabeled_df)  # Just labels, no GPU needed\n\n# Stratified sample 2k\nfrom sklearn.model_selection import train_test_split\n\nsubset_2k, _ = train_test_split(\n    unlabeled_df,\n    train_size=2000,\n    stratify=ensemble_preds,  # Ensures tail classes represented\n    random_state=42\n)\n\n# NOW run your fine-tuned model on this 2k for uncertainty scores\n# This is GPU-heavy but only 2k rows\n```\n\n**Why this works:**\n- Ensemble predictions (CPU) are cheap‚Äîrun on all 11k\n- Stratifying by predicted class ensures you sample from all categories\n- Fine-tuned model inference (GPU) only on 2k\n- You'll still find uncertain examples in tail classes\n\n**Even cheaper alternative:**\n```python\n# Sample 500 from each predicted class (or all if class has <500)\nsamples_per_class = []\nfor pred_class in ensemble_preds.unique():\n    class_subset = unlabeled_df[ensemble_preds == pred_class]\n    sample_size = min(500, len(class_subset))\n    samples_per_class.append(class_subset.sample(sample_size))\n\nsubset_balanced = pd.concat(samples_per_class)  # ~6-7k rows, balanced\n```\n\nThis guarantees tail class coverage and is still way faster than 11k.\n\n## **Class Weights in Trainer: Official Way**\n\nGood news‚Äîthere's a clean way without custom loops! Here's the **official** approach:\n\n```python\nfrom transformers import Trainer, TrainingArguments\nfrom sklearn.utils.class_weight import compute_class_weight\nimport torch\nimport numpy as np\n\n# 1. Compute class weights\nlabels = train_df['label'].values  # Numeric labels (0-13)\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(labels),\n    y=labels\n)\nclass_weights = torch.tensor(class_weights, dtype=torch.float)\n\n# 2. Custom Trainer (just override compute_loss)\nclass WeightedLossTrainer(Trainer):\n    def __init__(self, *args, class_weights=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.class_weights = class_weights\n        \n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        \n        loss_fct = torch.nn.CrossEntropyLoss(\n            weight=self.class_weights.to(model.device)\n        )\n        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n        \n        return (loss, outputs) if return_outputs else loss\n\n# 3. Use it\ntrainer = WeightedLossTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    class_weights=class_weights  # Pass weights here\n)\n\ntrainer.train()\n```\n\n**That's it!** Only ~15 lines and it's the standard pattern. The `DataCollator` approach you saw is indeed unofficial/experimental.\n\n## **Even Lazier Option: Weighted Sampling**\n\nIf you don't want to touch the loss function at all:\n\n```python\nfrom torch.utils.data import WeightedRandomSampler\n\n# Compute sample weights (inverse frequency)\nclass_counts = np.bincount(train_df['label'])\nsample_weights = 1. / class_counts[train_df['label'].values]\n\nsampler = WeightedRandomSampler(\n    weights=sample_weights,\n    num_samples=len(sample_weights),\n    replacement=True\n)\n\n# In TrainingArguments, this won't work directly, but you can pass to DataLoader\n# Easier to just use the custom Trainer above\n```\n\nActually, stick with the **WeightedLossTrainer**‚Äîit's cleaner and more effective.\n\n## **Colab GPU Budget Hack**\n\nIf you're really tight on GPU time:\n\n```python\n# 1. Fine-tune on GPU (30 min max for 100 examples)\ntrainer.train()\n\n# 2. Move model to CPU for inference\nmodel.to('cpu')\n\n# 3. Run uncertainty sampling on CPU (slower but free)\n# For 2k examples, this takes ~10 min on CPU vs 2 min on GPU\n# Totally worth it to save GPU quota\n\nfrom tqdm import tqdm\n\nuncertainties = []\nfor text in tqdm(subset_2k['description']):\n    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=128)\n    with torch.no_grad():\n        outputs = model(**inputs)\n        probs = torch.softmax(outputs.logits, dim=1)\n        confidence = probs.max().item()\n        uncertainties.append(1 - confidence)  # Lower confidence = higher uncertainty\n\nsubset_2k['uncertainty'] = uncertainties\nnext_100 = subset_2k.nlargest(100, 'uncertainty')\n```\n\n**CPU inference is slow but doesn't count against GPU quota.** For 2k examples, it's manageable.\n\n## **Quick Sanity Check Before Fine-Tuning**\n\nMake sure your labels are encoded correctly:\n\n```python\n# Your Label Studio export might have string labels\n# Convert to numeric for Trainer\n\nlabel2id = {label: i for i, label in enumerate(train_df['category'].unique())}\nid2label = {i: label for label, i in label2id.items()}\n\ntrain_df['label'] = train_df['category'].map(label2id)\nval_df['label'] = val_df['category'].map(label2id)\n\n# Pass to model config\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    'distilbert-base-uncased',\n    num_labels=len(label2id),\n    id2label=id2label,\n    label2id=label2id\n)\n```\n\n## **Recommended Training Args for 100 Examples**\n\n```python\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=5,  # More epochs OK with small data\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    warmup_steps=50,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    fp16=True,  # Faster on Colab GPU\n)\n```\n\nWith 100 examples, this should take **5-10 minutes** to train.\n\n## **Expected Results After First Fine-Tune**\n\nBased on your ensemble baseline of 73.4%, I'd expect:\n- **Fine-tuned model on 100 labels: 76-79%** (especially with class weights)\n- **After second 100 labels: 80-83%**\n\nYou're in great shape!\n\n---\n\n**TL;DR for your next steps:**\n1. ‚úÖ **Uncertainty sampling**: Run ensemble on all 11k (CPU), stratified sample 2k, fine-tuned model inference on 2k (GPU or CPU)\n2. ‚úÖ **Class weights**: Use the `WeightedLossTrainer` snippet above (official pattern)\n3. ‚úÖ **GPU budget**: Move model to CPU for inference if needed‚Äîsaves quota\n4. üéØ **Prediction**: You'll hit 78%+ after this fine-tune\n\nLet me know what accuracy you get! And seriously, that 73.4% ensemble is already deployable‚Äîyou could ship a tool that auto-categorizes with human review for low-confidence predictions. That's real impact. üöÄ",
      "student_message": "Just finished the first 100 labels in Label Studio (exported every 25 like you said‚Äîsaved me when Chrome froze!). Nano-classes merged into ‚Äòrare/other'; stratified split now works perfectly. Quick numbers: 0-shot ensemble (BART + DeBERTa-v3 + DistilBERT) hit 73.4 % on the new val set‚Äîalready above your guess! I'm about to kick off the hour-2 fine-tune on these 100 examples, but Colab is warning I'll hit the GPU limit if I also run the second uncertainty sampling on the full 11 k remaining rows. Could I approximate uncertainty by running the new model on just a 2 k random subset, or will that bias the tail classes? Also, any cheap way to get class-weighted loss in Hugging Face Trainer without writing a custom loop? I saw ‚Äòclass_weights' in the DataCollator but it looks unofficial. Thanks again‚Äîstill pinching myself that a prompt hack gave 7 points!",
      "scores": {
        "student_actionability": 2.0,
        "student_clarity": 2.0,
        "student_constraint_fit": 2.0,
        "student_confidence_gain": 1.9333333333333333,
        "student_path_ready": 1.0,
        "student_failure_modes": 1.0,
        "student_outcome_score": 1.99
      },
      "overall_score": 1.99,
      "cumulative_avg": 1.9033333333333333,
      "delta": 0.023333333333333428,
      "success_at_turn": false,
      "judge_outputs": [
        {
          "judge": "openrouter:google/gemini-2.5-flash",
          "raw": "```json\n{\n  \"next_steps\": [\n    \"Implement the `WeightedLossTrainer` with `compute_class_weight` to train the fine-tuned model on the 100 labeled examples, using the provided `TrainingArguments` (5-10 minutes on Colab GPU).\",\n    \"Run the 0-shot ensemble on the full 11k unlabeled dataset (CPU-only, fast) to get predictions for stratification.\",\n    \"Perform stratified sampling on the 11k dataset to select a 2k subset, then run the newly fine-tuned model on this 2k subset (on CPU to save GPU quota) to calculate uncertainty scores and select the next 100 examples for labeling.\"\n  ],\n  \"scores\": {\n    \"clarity_for_student\": 2.0,\n    \"actionability_for_student\": 2.0,\n    \"constraint_fit_for_student\": 2.0,\n    \"confidence_gain_for_student\": 2.0\n  },\n  \"binary_checks\": {\n    \"path_ready\": 1,\n    \"failure_modes_flagged\": 0\n  },\n  \"student_outcome_score\": 1.9,\n  \"justification\": \"The mentor directly answers my questions with concrete, code-backed solutions that respect my Colab GPU limits and skill level. The steps are clear, actionable, and directly address my concerns about bias and class weights, making me confident in proceeding.\"\n}\n```",
          "parsed": {
            "next_steps": [
              "Implement the `WeightedLossTrainer` with `compute_class_weight` to train the fine-tuned model on the 100 labeled examples, using the provided `TrainingArguments` (5-10 minutes on Colab GPU).",
              "Run the 0-shot ensemble on the full 11k unlabeled dataset (CPU-only, fast) to get predictions for stratification.",
              "Perform stratified sampling on the 11k dataset to select a 2k subset, then run the newly fine-tuned model on this 2k subset (on CPU to save GPU quota) to calculate uncertainty scores and select the next 100 examples for labeling."
            ],
            "scores": {
              "clarity_for_student": 2.0,
              "actionability_for_student": 2.0,
              "constraint_fit_for_student": 2.0,
              "confidence_gain_for_student": 2.0
            },
            "binary_checks": {
              "path_ready": 1,
              "failure_modes_flagged": 0
            },
            "student_outcome_score": 1.9,
            "justification": "The mentor directly answers my questions with concrete, code-backed solutions that respect my Colab GPU limits and skill level. The steps are clear, actionable, and directly address my concerns about bias and class weights, making me confident in proceeding."
          }
        },
        {
          "judge": "openrouter:deepseek/deepseek-v3.2-exp",
          "raw": "{\n  \"next_steps\": [\n    \"Move model to CPU for uncertainty sampling on 2k stratified subset to save GPU quota (provided snippet handles stratification by predicted class).\",\n    \"Implement WeightedLossTrainer with class weights (use provided ~15-line custom Trainer code with compute_class_weight).\",\n    \"Fine-tune model using recommended TrainingArguments (num_train_epochs=5, fp16=True) for 5-10 minutes, then validate accuracy.\"\n  ],\n  \"scores\": {\n    \"clarity_for_student\": 2.0,\n    \"actionability_for_student\": 2.0,\n    \"constraint_fit_for_student\": 2.0,\n    \"confidence_gain_for_student\": 1.8\n  },\n  \"binary_checks\": {\n    \"path_ready\": 1,\n    \"failure_modes_flagged\": 1\n  },\n  \"student_outcome_score\": 1.97,\n  \"justification\": \"The response gives exact, executable code snippets for uncertainty sampling and class weighting that respect Colab GPU limits, directly answering my bottlenecks with stratified sampling and CPU inference hacks‚ÄîI can start coding immediately without blockers.\"\n}",
          "parsed": {
            "next_steps": [
              "Move model to CPU for uncertainty sampling on 2k stratified subset to save GPU quota (provided snippet handles stratification by predicted class).",
              "Implement WeightedLossTrainer with class weights (use provided ~15-line custom Trainer code with compute_class_weight).",
              "Fine-tune model using recommended TrainingArguments (num_train_epochs=5, fp16=True) for 5-10 minutes, then validate accuracy."
            ],
            "scores": {
              "clarity_for_student": 2.0,
              "actionability_for_student": 2.0,
              "constraint_fit_for_student": 2.0,
              "confidence_gain_for_student": 1.8
            },
            "binary_checks": {
              "path_ready": 1,
              "failure_modes_flagged": 1
            },
            "student_outcome_score": 1.97,
            "justification": "The response gives exact, executable code snippets for uncertainty sampling and class weighting that respect Colab GPU limits, directly answering my bottlenecks with stratified sampling and CPU inference hacks‚ÄîI can start coding immediately without blockers."
          }
        },
        {
          "judge": "openrouter:x-ai/grok-4-fast",
          "raw": "{\n  \"next_steps\": [\n    \"Run ensemble predictions on all 11k unlabeled rows using CPU to get predicted classes, then use stratified sampling to select a 2k subset ensuring tail class representation before computing uncertainty with the fine-tuned model.\",\n    \"Implement the WeightedLossTrainer class with computed class weights from the training labels and use it in the Hugging Face Trainer for the hour-2 fine-tune on the 100 examples.\",\n    \"After fine-tuning, move the model to CPU for uncertainty inference on the 2k subset to avoid hitting Colab GPU limits, then select the top 100 uncertain examples for labeling.\"\n  ],\n  \"scores\": {\n    \"clarity_for_student\": 2.0,\n    \"actionability_for_student\": 2.0,\n    \"constraint_fit_for_student\": 2.0,\n    \"confidence_gain_for_student\": 2.0\n  },\n  \"binary_checks\": {\n    \"path_ready\": 1,\n    \"failure_modes_flagged\": 1\n  },\n  \"student_outcome_score\": 2.0,\n  \"justification\": \"As a busy CS minor with limited Colab GPU time, this response gives me exact code snippets I can copy-paste right into my notebook to handle uncertainty sampling without biasing tail classes and add class weights easily, all while staying under free compute limits. It directly fixes my worries about GPU quotas and model performance, making me feel ready to push forward and hit that 78%+ accuracy without getting stuck.\"\n}",
          "parsed": {
            "next_steps": [
              "Run ensemble predictions on all 11k unlabeled rows using CPU to get predicted classes, then use stratified sampling to select a 2k subset ensuring tail class representation before computing uncertainty with the fine-tuned model.",
              "Implement the WeightedLossTrainer class with computed class weights from the training labels and use it in the Hugging Face Trainer for the hour-2 fine-tune on the 100 examples.",
              "After fine-tuning, move the model to CPU for uncertainty inference on the 2k subset to avoid hitting Colab GPU limits, then select the top 100 uncertain examples for labeling."
            ],
            "scores": {
              "clarity_for_student": 2.0,
              "actionability_for_student": 2.0,
              "constraint_fit_for_student": 2.0,
              "confidence_gain_for_student": 2.0
            },
            "binary_checks": {
              "path_ready": 1,
              "failure_modes_flagged": 1
            },
            "student_outcome_score": 2.0,
            "justification": "As a busy CS minor with limited Colab GPU time, this response gives me exact code snippets I can copy-paste right into my notebook to handle uncertainty sampling without biasing tail classes and add class weights easily, all while staying under free compute limits. It directly fixes my worries about GPU quotas and model performance, making me feel ready to push forward and hit that 78%+ accuracy without getting stuck."
          }
        }
      ]
    },
    {
      "turn_index": 5,
      "mentor_reply": "**78.3%!! And rodent F1 nearly doubling?!** That's incredible progress. The alley rats vs. mice confusion is *exactly* the kind of domain nuance zero-shot models miss‚Äîyou're teaching it real civic knowledge now. üêÄ\n\nLet me help you optimize for the final push:\n\n## **1. Shrinking Ensemble Logits (9 GB ‚Üí <1 GB)**\n\nYou don't need full logits for re-ranking‚Äîjust confidence scores and top-K predictions:\n\n```python\n# ‚ùå DON'T cache full logits (9 GB)\ncache = {\n    'logits': all_logits,  # Shape: (11k, 14 classes, 3 models) = huge\n}\n\n# ‚úÖ DO cache compressed predictions\nimport numpy as np\n\ncompressed_cache = {\n    'top_3_classes': np.array([pred['labels'][:3] for pred in predictions]),  # (11k, 3)\n    'top_3_scores': np.array([pred['scores'][:3] for pred in predictions]),   # (11k, 3)\n    'entropy': np.array([calculate_entropy(pred['scores']) for pred in predictions]),  # (11k,)\n    'ensemble_vote': ensemble_predictions,  # (11k,) - final labels\n}\n\n# Entropy for uncertainty (more informative than 1-max_prob)\ndef calculate_entropy(scores):\n    scores = np.array(scores)\n    return -np.sum(scores * np.log(scores + 1e-10))\n\n# Storage: ~11k * (3+3+1+1) * 4 bytes ‚âà 350 KB vs 9 GB!\n```\n\n**Why this works:**\n- Top-3 predictions capture 95%+ of the probability mass\n- Entropy is a better uncertainty metric than confidence alone\n- You can still re-rank by entropy, disagreement, or specific class scores\n\n**Even more aggressive compression:**\n\n```python\n# Store only what you need for re-ranking\nminimal_cache = {\n    'uncertainty_score': 1 - max_probs,  # (11k,) float32\n    'predicted_class': ensemble_preds,   # (11k,) int8\n    'is_tail_class': is_tail,            # (11k,) bool\n}\n\n# Save as compressed numpy\nnp.savez_compressed('ensemble_cache.npz', **minimal_cache)\n\n# Later: load instantly\ncache = np.load('ensemble_cache.npz')\n```\n\nThis gets you down to **~100 KB** and loads in <1 second.\n\n**Pro tip for later analysis:**\n\nIf you want to keep full logits for a subset (e.g., validation set):\n\n```python\n# Full logits only for val set (2.4k rows)\nval_logits = logits[val_indices]  # ~2 GB ‚Üí manageable\n\n# Compressed cache for remaining 8.6k\ntrain_cache = compress(logits[train_indices])\n```\n\n## **2. Hugging Face Model Licensing (Share-Alike)**\n\nGreat question! HF has proper metadata fields for this. Here's the **correct way**:\n\n### **In your model card (README.md):**\n\n```markdown\n---\nlicense: cc-by-sa-4.0  # or whatever your city's license is\ndatasets:\n- city-311-requests  # Custom dataset name\nlanguage:\n- en\ntags:\n- civic-tech\n- 311\n- text-classification\n- low-resource\nmetrics:\n- accuracy\n- f1\nmodel-index:\n- name: 311-request-classifier\n  results:\n  - task:\n      type: text-classification\n    metrics:\n    - type: accuracy\n      value: 0.783\n    - type: f1\n      value: 0.76  # Macro F1\n---\n\n# 311 Request Classifier\n\nFine-tuned DistilBERT for categorizing city 311 service requests.\n\n## Dataset\nTrained on 12,000 public 311 requests from [City Name] Open Data Portal (2023-2024).\nLicensed under [City's License] - share-alike terms apply.\n\n## Usage\n\\`\\`\\`python\nfrom transformers import pipeline\n\nclassifier = pipeline(\"text-classification\", model=\"your-username/311-classifier\")\nresult = classifier(\"Large pothole on Main St near 5th Ave\")\n# {'label': 'street-damage', 'score': 0.94}\n\\`\\`\\`\n\n## Limitations\n- English only\n- Trained on [City Name] data‚Äîmay not generalize to other municipalities\n- Tail classes (beehive, pay-phone) merged into 'rare/other'\n\n## License\nModel weights: Apache 2.0 (from DistilBERT)\nTraining data: [City License Name] - derivatives must share-alike\n```\n\n### **The `license` field in metadata:**\n\n```python\n# When you push to HF\nmodel.push_to_hub(\n    \"your-username/311-classifier\",\n    commit_message=\"Initial model v1\",\n    license=\"cc-by-sa-4.0\",  # ‚Üê Key field\n)\n\ntokenizer.push_to_hub(\"your-username/311-classifier\")\n```\n\n**Common share-alike licenses:**\n- `cc-by-sa-4.0` - Creative Commons Share-Alike\n- `odc-by` - Open Data Commons (common for gov data)\n- `cdla-sharing-1.0` - Community Data License Agreement\n\nCheck your city's portal‚Äîit should specify which one.\n\n### **Also include the license file:**\n\n```bash\n# In your repo root\nwget https://creativecommons.org/licenses/by-sa/4.0/legalcode.txt -O LICENSE\n# Or copy your city's specific license text\n```\n\nHF will auto-detect `LICENSE` or `LICENSE.txt` in the repo root.\n\n### **Important legal note:**\n\nThe **model weights** are typically Apache 2.0 (from DistilBERT), but the **training data** is share-alike. So:\n\n```markdown\n## License\nThis model is a derivative of `distilbert-base-uncased` (Apache 2.0).\n\nTraining data sourced from [City] Open Data Portal under [License].\nAny derivative models must comply with share-alike terms.\n\n**Recommended citation:**\n[Your model] trained on [City] 311 data (CC-BY-SA-4.0)\n```\n\nThis makes it clear that:\n1. The model itself can be used freely (Apache 2.0)\n2. But if someone fine-tunes it further with your data, they must share-alike\n3. Commercial use is OK (most gov licenses allow this)\n\n## **Bonus: Dashboard PR Checklist**\n\nSince you're almost ready to ship:\n\n```markdown\n### Model Card Checklist\n- [ ] License metadata in YAML header\n- [ ] LICENSE.txt in repo root\n- [ ] Dataset description + link to city portal\n- [ ] Limitations section (English-only, city-specific, etc.)\n- [ ] Example usage code\n- [ ] Performance metrics (accuracy, F1 per class)\n- [ ] Contact info for questions\n\n### Repo Checklist\n- [ ] Model weights + tokenizer\n- [ ] `label2id` and `id2label` mappings\n- [ ] Inference script (for non-HF users)\n- [ ] Requirements.txt\n- [ ] Sample predictions on test cases\n```\n\n### **Quick inference script for your dashboard:**\n\n```python\n# inference.py - for your civic tech team\nfrom transformers import pipeline\n\nclassifier = pipeline(\n    \"text-classification\",\n    model=\"your-username/311-classifier\",\n    device=0 if torch.cuda.is_available() else -1\n)\n\ndef categorize_request(description, threshold=0.7):\n    \"\"\"\n    Categorize a 311 request with confidence threshold.\n    \n    Returns:\n        dict: {\n            'category': str,\n            'confidence': float,\n            'needs_review': bool  # True if confidence < threshold\n        }\n    \"\"\"\n    result = classifier(description)[0]\n    \n    return {\n        'category': result['label'],\n        'confidence': result['score'],\n        'needs_review': result['score'] < threshold\n    }\n\n# Example\nrequest = \"Dead rat in alley behind 123 Main St\"\nresult = categorize_request(request)\nprint(result)\n# {'category': 'rodent', 'confidence': 0.89, 'needs_review': False}\n```\n\nThis gives your team a simple API with built-in human-review flagging.\n\n## **One More Thing: Model Versioning**\n\nSince you'll retrain after the second 100 labels:\n\n```python\n# Version 1 (100 labels)\nmodel.push_to_hub(\"your-username/311-classifier\", revision=\"v1-100labels\")\n\n# Version 2 (200 labels) - later tonight\nmodel.push_to_hub(\"your-username/311-classifier\", revision=\"v2-200labels\")\n\n# Main branch = latest\nmodel.push_to_hub(\"your-username/311-classifier\")  # Auto-updates 'main'\n```\n\nThis lets people compare versions and you can track improvement over time.\n\n---\n\n**TL;DR:**\n1. ‚úÖ **Logits compression**: Cache only top-3 + entropy ‚Üí 9 GB to <1 MB\n2. ‚úÖ **Licensing**: Add `license: cc-by-sa-4.0` to model card YAML + LICENSE.txt in root\n3. üéÅ **Bonus**: Inference script with confidence thresholding for your dashboard\n\n**Prediction for tonight:** After 200 labels, you'll hit **81-83% accuracy**. With that + the confidence threshold, you'll have a production-ready system that auto-categorizes ~85% of requests and flags the rest for human review. That's a massive time-saver for your city's 311 team.\n\nSeriously impressive work for a CS minor on a zero budget. This is publishable at a civic tech workshop (e.g., Bloomberg Cities, Code for America Summit). Consider writing it up! üìùüèÜ",
      "student_message": "You're a wizard‚Äî78.3 % on the first 100-label fine-tune (WeightedLossTrainer + 5 epochs, 8 min GPU). Tail classes jumped most: ‚Äòrodent' F1 0.42‚Üí0.68. I CPU-inferred the 2 k stratified slice in 12 min, sent top-100 uncertainty to Label Studio; half were alley rats vs mice‚Äîmakes total sense. Next 100 will be done tonight. Two quick things: (1) Colab gives me 15 GB RAM but the full 11 k ensemble preds balloon to 9 GB when I cache logits for later analysis‚Äîany trick to shrink that without losing the ability to re-rank later? (2) My city's open-data licence says ‚Äòshare alike'‚Äîif I push the model to Hugging-Face, do I just drop the licence txt in the repo root or is there a metadata field I must fill? Thanks again‚Äîalmost ready to PR the dashboard!",
      "scores": {
        "student_actionability": 2.0,
        "student_clarity": 2.0,
        "student_constraint_fit": 2.0,
        "student_confidence_gain": 2.0,
        "student_path_ready": 1.0,
        "student_failure_modes": 1.0,
        "student_outcome_score": 2.0
      },
      "overall_score": 2.0,
      "cumulative_avg": 1.9226666666666667,
      "delta": 0.010000000000000009,
      "success_at_turn": false,
      "judge_outputs": [
        {
          "judge": "openrouter:google/gemini-2.5-flash",
          "raw": "```json\n{\n  \"next_steps\": [\n    \"Implement the compressed cache for ensemble predictions using `numpy` to store top-3 classes, top-3 scores, and entropy, reducing the memory footprint from 9 GB to ~350 KB.\",\n    \"Update the Hugging Face model card (README.md) with the correct `license` metadata field (e.g., `cc-by-sa-4.0`) and add a `LICENSE.txt` file to the repo root, ensuring compliance with the city's share-alike open-data license.\",\n    \"Integrate the provided `categorize_request` inference script into the dashboard, using a confidence threshold to flag requests for human review, which will be useful for the city's 311 team.\"\n  ],\n  \"scores\": {\n    \"clarity_for_student\": 2.0,\n    \"actionability_for_student\": 2.0,\n    \"constraint_fit_for_student\": 2.0,\n    \"confidence_gain_for_student\": 2.0\n  },\n  \"binary_checks\": {\n    \"path_ready\": 1,\n    \"failure_modes_flagged\": 1\n  },\n  \"student_outcome_score\": 2.0,\n  \"justification\": \"This response provides extremely clear, actionable, and constraint-aware steps. The code snippets are directly usable, address both technical and legal concerns, and significantly reduce uncertainty, making me feel very confident about the next steps.\"\n}\n```",
          "parsed": {
            "next_steps": [
              "Implement the compressed cache for ensemble predictions using `numpy` to store top-3 classes, top-3 scores, and entropy, reducing the memory footprint from 9 GB to ~350 KB.",
              "Update the Hugging Face model card (README.md) with the correct `license` metadata field (e.g., `cc-by-sa-4.0`) and add a `LICENSE.txt` file to the repo root, ensuring compliance with the city's share-alike open-data license.",
              "Integrate the provided `categorize_request` inference script into the dashboard, using a confidence threshold to flag requests for human review, which will be useful for the city's 311 team."
            ],
            "scores": {
              "clarity_for_student": 2.0,
              "actionability_for_student": 2.0,
              "constraint_fit_for_student": 2.0,
              "confidence_gain_for_student": 2.0
            },
            "binary_checks": {
              "path_ready": 1,
              "failure_modes_flagged": 1
            },
            "student_outcome_score": 2.0,
            "justification": "This response provides extremely clear, actionable, and constraint-aware steps. The code snippets are directly usable, address both technical and legal concerns, and significantly reduce uncertainty, making me feel very confident about the next steps."
          }
        },
        {
          "judge": "openrouter:deepseek/deepseek-v3.2-exp",
          "raw": "```json\n{\n  \"next_steps\": [\"Implement the compressed cache with top-3 predictions and entropy calculation to reduce memory usage from 9GB to ~350KB\", \"Add license metadata 'cc-by-sa-4.0' to Hugging Face model card YAML header and include LICENSE.txt file in repo root\", \"Create and test the inference script with confidence thresholding for dashboard integration\"],\n  \"scores\": {\n    \"clarity_for_student\": 2.0,\n    \"actionability_for_student\": 2.0,\n    \"constraint_fit_for_student\": 2.0,\n    \"confidence_gain_for_student\": 2.0\n  },\n  \"binary_checks\": {\n    \"path_ready\": 1,\n    \"failure_modes_flagged\": 1\n  },\n  \"student_outcome_score\": 2.0,\n  \"justification\": \"The response provides extremely specific, executable code solutions that directly address my memory constraints and licensing questions, with ready-to-use Python snippets I can implement immediately using free tools.\"\n}\n```",
          "parsed": {
            "next_steps": [
              "Implement the compressed cache with top-3 predictions and entropy calculation to reduce memory usage from 9GB to ~350KB",
              "Add license metadata 'cc-by-sa-4.0' to Hugging Face model card YAML header and include LICENSE.txt file in repo root",
              "Create and test the inference script with confidence thresholding for dashboard integration"
            ],
            "scores": {
              "clarity_for_student": 2.0,
              "actionability_for_student": 2.0,
              "constraint_fit_for_student": 2.0,
              "confidence_gain_for_student": 2.0
            },
            "binary_checks": {
              "path_ready": 1,
              "failure_modes_flagged": 1
            },
            "student_outcome_score": 2.0,
            "justification": "The response provides extremely specific, executable code solutions that directly address my memory constraints and licensing questions, with ready-to-use Python snippets I can implement immediately using free tools."
          }
        },
        {
          "judge": "openrouter:x-ai/grok-4-fast",
          "raw": "{\n  \"next_steps\": [\n    \"Implement the compressed cache using the provided Python code to store only top-3 classes, scores, and entropy for the 11k predictions, saving it as a numpy file to reduce from 9GB to under 1MB.\",\n    \"Update the Hugging Face model card with the YAML license field (e.g., cc-by-sa-4.0), add a LICENSE.txt file from the city's portal, and include dataset description and limitations before pushing the model.\",\n    \"Add the inference script with confidence thresholding to the repo for the dashboard, test it on a few sample 311 requests using the free Colab environment.\"\n  ],\n  \"scores\": {\n    \"clarity_for_student\": 2.0,\n    \"actionability_for_student\": 2.0,\n    \"constraint_fit_for_student\": 2.0,\n    \"confidence_gain_for_student\": 2.0\n  },\n  \"binary_checks\": {\n    \"path_ready\": 1,\n    \"failure_modes_flagged\": 1\n  },\n  \"student_outcome_score\": 2.0,\n  \"justification\": \"As a busy CS minor volunteering part-time, this response gives me exact code snippets and steps I can copy-paste into Colab tonight without needing extra skills or paid tools, directly fixing my memory issue and license confusion so I feel ready to push the model and dashboard PR in 1-2 days. The predictions of hitting 81-83% and production readiness boost my confidence that this low-resource setup will actually help the city's team.\"\n}",
          "parsed": {
            "next_steps": [
              "Implement the compressed cache using the provided Python code to store only top-3 classes, scores, and entropy for the 11k predictions, saving it as a numpy file to reduce from 9GB to under 1MB.",
              "Update the Hugging Face model card with the YAML license field (e.g., cc-by-sa-4.0), add a LICENSE.txt file from the city's portal, and include dataset description and limitations before pushing the model.",
              "Add the inference script with confidence thresholding to the repo for the dashboard, test it on a few sample 311 requests using the free Colab environment."
            ],
            "scores": {
              "clarity_for_student": 2.0,
              "actionability_for_student": 2.0,
              "constraint_fit_for_student": 2.0,
              "confidence_gain_for_student": 2.0
            },
            "binary_checks": {
              "path_ready": 1,
              "failure_modes_flagged": 1
            },
            "student_outcome_score": 2.0,
            "justification": "As a busy CS minor volunteering part-time, this response gives me exact code snippets and steps I can copy-paste into Colab tonight without needing extra skills or paid tools, directly fixing my memory issue and license confusion so I feel ready to push the model and dashboard PR in 1-2 days. The predictions of hitting 81-83% and production readiness boost my confidence that this low-resource setup will actually help the city's team."
          }
        }
      ]
    },
    {
      "turn_index": 6,
      "mentor_reply": "**You're shipping production code now!** üöÄ The fact that graffiti F1 is climbing *during* labeling means your uncertainty sampling is working perfectly‚Äîyou're finding the exact examples the model needs.\n\nLet me solve your deployment issue:\n\n## **Streamlit Model Loading: The Right Way**\n\nYour `@st.cache_resource` instinct is **100% correct**‚Äîthat's the official pattern. Memory leaks aren't an issue if you do it right:\n\n```python\nimport streamlit as st\nfrom transformers import pipeline\nimport torch\n\n@st.cache_resource\ndef load_model():\n    \"\"\"\n    Cached model loader - loads once, shared across all sessions.\n    Memory-safe for multi-user Streamlit apps.\n    \"\"\"\n    classifier = pipeline(\n        \"text-classification\",\n        model=\"your-username/311-classifier\",\n        device=-1,  # CPU only\n        # Key optimization: smaller batch size for CPU\n        batch_size=1,\n    )\n    return classifier\n\n# Load once at startup\nclassifier = load_model()\n\n# Your app code\nst.title(\"311 Request Classifier\")\ndescription = st.text_area(\"Enter request description:\")\n\nif st.button(\"Categorize\"):\n    result = classifier(description)[0]\n    st.write(f\"**Category:** {result['label']}\")\n    st.write(f\"**Confidence:** {result['score']:.2%}\")\n```\n\n**Why this is safe:**\n- `@st.cache_resource` loads the model **once per server process**, not per user\n- All users share the same model instance (memory-efficient)\n- Streamlit handles cleanup when the app restarts\n- No VRAM needed (device=-1 = CPU)\n\n## **Faster Cold Start: Model Optimization**\n\n12s ‚Üí 2s is great, but let's get that **initial load under 5s**:\n\n### **Option 1: ONNX Export (Fastest)**\n\n```python\n# One-time conversion (run locally, push to HF)\nfrom optimum.onnxruntime import ORTModelForSequenceClassification\nfrom transformers import AutoTokenizer\n\nmodel = ORTModelForSequenceClassification.from_pretrained(\n    \"your-username/311-classifier\",\n    export=True,  # Converts to ONNX\n)\ntokenizer = AutoTokenizer.from_pretrained(\"your-username/311-classifier\")\n\n# Save ONNX version\nmodel.save_pretrained(\"311-classifier-onnx\")\ntokenizer.save_pretrained(\"311-classifier-onnx\")\n\n# Push to HF\nmodel.push_to_hub(\"your-username/311-classifier-onnx\")\ntokenizer.push_to_hub(\"your-username/311-classifier-onnx\")\n```\n\n**In your Streamlit app:**\n\n```python\nfrom optimum.onnxruntime import ORTModelForSequenceClassification\nfrom transformers import AutoTokenizer, pipeline\n\n@st.cache_resource\ndef load_model():\n    model = ORTModelForSequenceClassification.from_pretrained(\n        \"your-username/311-classifier-onnx\"\n    )\n    tokenizer = AutoTokenizer.from_pretrained(\n        \"your-username/311-classifier-onnx\"\n    )\n    \n    classifier = pipeline(\n        \"text-classification\",\n        model=model,\n        tokenizer=tokenizer,\n        device=-1\n    )\n    return classifier\n```\n\n**Speed improvement:** 12s ‚Üí **3-4s** cold start, 2s ‚Üí **0.5s** warm inference\n\n### **Option 2: Distillation (Smaller Model)**\n\nIf ONNX is too much setup, use a smaller base model for v2:\n\n```python\n# When you retrain on 200 labels tonight, use:\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\",  # 66M params - what you have now\n    # vs\n    \"prajjwal1/bert-tiny\",  # 4M params - 15x smaller!\n)\n```\n\n**Trade-off:** You might lose 1-2% accuracy, but load time drops to **2-3s** cold.\n\nTest both and see if bert-tiny still hits >75%. For 311 classification, it often works fine.\n\n### **Option 3: Pre-warm on Server Startup**\n\nIf you control the server, add a startup script:\n\n```python\n# startup.py - run once when server boots\nfrom transformers import pipeline\n\nprint(\"Pre-loading model...\")\nclassifier = pipeline(\"text-classification\", model=\"your-username/311-classifier\")\nclassifier(\"test\")  # Dummy inference to compile\nprint(\"Model ready!\")\n\n# Then start Streamlit\n# streamlit run app.py\n```\n\n**In your Streamlit app:**\n\n```python\n# Model is already in memory from startup.py\n# First user gets instant response\n```\n\nThis works if you have a dedicated server (not Streamlit Cloud).\n\n## **Memory Leak Prevention (Best Practices)**\n\nYour concern is valid‚Äîhere's how to be 100% safe:\n\n```python\nimport streamlit as st\nimport gc\nimport torch\n\n@st.cache_resource\ndef load_model():\n    classifier = pipeline(\n        \"text-classification\",\n        model=\"your-username/311-classifier\",\n        device=-1,\n        # Prevent memory accumulation\n        framework=\"pt\",\n        return_all_scores=False,  # Only return top prediction\n    )\n    return classifier\n\n# Monitor memory (optional, for debugging)\nif st.sidebar.checkbox(\"Show memory usage\"):\n    import psutil\n    process = psutil.Process()\n    mem_mb = process.memory_info().rss / 1024 / 1024\n    st.sidebar.metric(\"Memory (MB)\", f\"{mem_mb:.1f}\")\n\n# Batch processing with cleanup\ndef classify_batch(descriptions):\n    results = classifier(descriptions)\n    gc.collect()  # Force garbage collection after batch\n    return results\n```\n\n**Stress test this:**\n\n```python\n# Test script - run before deploying\nimport requests\nimport concurrent.futures\n\ndef test_request():\n    # Simulate user request\n    response = requests.post(\"http://your-server/classify\", \n                            json={\"text\": \"pothole on main st\"})\n    return response.status_code\n\n# Simulate 50 concurrent users\nwith concurrent.futures.ThreadPoolExecutor(max_workers=50) as executor:\n    futures = [executor.submit(test_request) for _ in range(100)]\n    results = [f.result() for f in futures]\n\nprint(f\"Success rate: {results.count(200)/len(results):.1%}\")\n```\n\nIf memory stays stable after 100 requests, you're golden.\n\n## **Production-Ready Streamlit App**\n\nHere's a complete template with all optimizations:\n\n```python\nimport streamlit as st\nfrom transformers import pipeline\nimport time\n\n# Page config\nst.set_page_config(\n    page_title=\"311 Request Classifier\",\n    page_icon=\"üèôÔ∏è\",\n    layout=\"centered\"\n)\n\n@st.cache_resource\ndef load_model():\n    \"\"\"Load model once, share across all users.\"\"\"\n    with st.spinner(\"Loading AI model...\"):\n        classifier = pipeline(\n            \"text-classification\",\n            model=\"your-username/311-classifier\",\n            device=-1,\n            top_k=3,  # Return top 3 predictions\n        )\n    return classifier\n\n# Load model\nclassifier = load_model()\n\n# UI\nst.title(\"üèôÔ∏è 311 Request Auto-Categorizer\")\nst.markdown(\"*Powered by AI trained on 12,000 city requests*\")\n\ndescription = st.text_area(\n    \"Describe the issue:\",\n    placeholder=\"e.g., Large pothole on Main St near 5th Ave\",\n    height=100\n)\n\nif st.button(\"Categorize\", type=\"primary\"):\n    if description.strip():\n        start = time.time()\n        \n        # Inference\n        results = classifier(description)[0]\n        \n        elapsed = time.time() - start\n        \n        # Display results\n        st.success(f\"**Predicted Category:** {results[0]['label']}\")\n        \n        col1, col2 = st.columns(2)\n        with col1:\n            st.metric(\"Confidence\", f\"{results[0]['score']:.1%}\")\n        with col2:\n            st.metric(\"Processing Time\", f\"{elapsed:.2f}s\")\n        \n        # Show alternatives\n        if results[0]['score'] < 0.7:\n            st.warning(\"‚ö†Ô∏è Low confidence - please review\")\n            st.write(\"**Alternative categories:**\")\n            for r in results[1:]:\n                st.write(f\"- {r['label']} ({r['score']:.1%})\")\n        \n        # Feedback collection (optional)\n        with st.expander(\"Was this correct?\"):\n            feedback = st.radio(\n                \"Feedback:\",\n                [\"‚úÖ Correct\", \"‚ùå Wrong\", \"ü§î Unsure\"],\n                horizontal=True\n            )\n            if st.button(\"Submit Feedback\"):\n                # Log to file for future retraining\n                with open(\"feedback.log\", \"a\") as f:\n                    f.write(f\"{description}|{results[0]['label']}|{feedback}\\n\")\n                st.success(\"Thanks! This helps improve the model.\")\n    else:\n        st.error(\"Please enter a description\")\n\n# Footer\nst.markdown(\"---\")\nst.caption(f\"Model: v1-100labels | Accuracy: 78.3% | [View on Hugging Face](https://huggingface.co/your-username/311-classifier)\")\n```\n\n**Features:**\n- ‚úÖ Cached model loading\n- ‚úÖ Top-3 predictions for low-confidence cases\n- ‚úÖ Timing metrics\n- ‚úÖ Feedback collection for continuous improvement\n- ‚úÖ Clean UI with warnings\n\n## **Deployment Checklist**\n\n```bash\n# requirements.txt\nstreamlit==1.29.0\ntransformers==4.36.0\ntorch==2.1.0\nsentencepiece  # If using certain tokenizers\n\n# Optional optimizations\noptimum[onnxruntime]==1.16.0  # For ONNX\npsutil  # For memory monitoring\n```\n\n**Deploy command:**\n\n```bash\n# Local testing\nstreamlit run app.py --server.port 8501\n\n# Production (with pre-warming)\npython startup.py && streamlit run app.py --server.address 0.0.0.0\n```\n\n## **Expected Performance**\n\nWith `@st.cache_resource` + ONNX:\n- **Cold start (first user):** 3-4s\n- **Warm inference (subsequent users):** 0.3-0.5s\n- **Memory:** ~500 MB (stable, no leaks)\n- **Concurrent users:** 20-30 on a modest server\n\nWithout ONNX (just caching):\n- **Cold start:** 5-8s\n- **Warm inference:** 1-2s\n- Still totally usable!\n\n---\n\n**TL;DR:**\n1. ‚úÖ **Keep `@st.cache_resource`** - it's the right pattern, no leak risk\n2. ‚úÖ **Optional speedup:** Export to ONNX (12s ‚Üí 3s cold start)\n3. ‚úÖ **Production template:** Use the code above with top-3 predictions + feedback\n4. üéØ **Your 81% prediction:** Looking good with graffiti climbing mid-labeling!\n\n**Tonight's prediction:** After 200 labels, you'll hit **82%**, and the Streamlit app will feel snappy enough that your city team will actually use it daily. That's when you know you've shipped something real. üèÜ\n\nFinish those labels and report back‚ÄîI'm invested in this now! üòÑ",
      "student_message": "Mind = blown‚Äî350 KB instead of 9 GB! I just swapped the cache and my 8-year-old laptop can finally breathe. Pushed v1-100labels to HF with the CC-BY-SA-4.0 header and a LICENSE.txt copied straight from the city portal (they actually use that one‚Äîscore!). Quick snag: the inference script runs great on CPU, but when I plug it into the tiny Streamlit demo on the city's internal server (no GPUs), the first prediction takes 12 s while the model loads. Any tricks to keep the pipeline warm without holding VRAM? I tried setting @st.cache_resource around the pipeline init and it drops to 2 s, but I'm nervous about memory leaks if multiple staff open the page. Also, I'm 20 labels into the second batch and already seeing \"graffiti\" F1 climb from 0.55 ‚Üí 0.61‚Äîyour 81 % call feels psychic. üéØ",
      "scores": {
        "student_actionability": 2.0,
        "student_clarity": 2.0,
        "student_constraint_fit": 2.0,
        "student_confidence_gain": 2.0,
        "student_path_ready": 1.0,
        "student_failure_modes": 1.0,
        "student_outcome_score": 2.0
      },
      "overall_score": 2.0,
      "cumulative_avg": 1.9355555555555555,
      "delta": 0.0,
      "success_at_turn": false,
      "judge_outputs": [
        {
          "judge": "openrouter:google/gemini-2.5-flash",
          "raw": "```json\n{\n  \"next_steps\": [\n    \"Implement the provided Streamlit code with `@st.cache_resource` and `device=-1` to ensure the model loads once on CPU and is shared across sessions, addressing the memory leak concern.\",\n    \"Export the current `v1-100labels` model to ONNX using the provided `optimum` script, push it to Hugging Face, and then update the Streamlit app to load the ONNX version for faster cold start.\",\n    \"If the ONNX export is too complex or doesn't yield enough speed, try retraining the model with `prajjwal1/bert-tiny` as the base model for the next batch of labels to see if the smaller model size improves load times sufficiently without significant accuracy loss.\"\n  ],\n  \"scores\": {\n    \"clarity_for_student\": 2.0,\n    \"actionability_for_student\": 2.0,\n    \"constraint_fit_for_student\": 2.0,\n    \"confidence_gain_for_student\": 2.0\n  },\n  \"binary_checks\": {\n    \"path_ready\": 1,\n    \"failure_modes_flagged\": 1\n  },\n  \"student_outcome_score\": 2.0,\n  \"justification\": \"This response directly addresses my concerns about Streamlit performance and memory leaks with concrete, copy-pasteable code examples and clear explanations that respect my CPU-only constraint and free tooling. It provides multiple actionable options for optimization, including a full production-ready template.\"\n}\n```",
          "parsed": {
            "next_steps": [
              "Implement the provided Streamlit code with `@st.cache_resource` and `device=-1` to ensure the model loads once on CPU and is shared across sessions, addressing the memory leak concern.",
              "Export the current `v1-100labels` model to ONNX using the provided `optimum` script, push it to Hugging Face, and then update the Streamlit app to load the ONNX version for faster cold start.",
              "If the ONNX export is too complex or doesn't yield enough speed, try retraining the model with `prajjwal1/bert-tiny` as the base model for the next batch of labels to see if the smaller model size improves load times sufficiently without significant accuracy loss."
            ],
            "scores": {
              "clarity_for_student": 2.0,
              "actionability_for_student": 2.0,
              "constraint_fit_for_student": 2.0,
              "confidence_gain_for_student": 2.0
            },
            "binary_checks": {
              "path_ready": 1,
              "failure_modes_flagged": 1
            },
            "student_outcome_score": 2.0,
            "justification": "This response directly addresses my concerns about Streamlit performance and memory leaks with concrete, copy-pasteable code examples and clear explanations that respect my CPU-only constraint and free tooling. It provides multiple actionable options for optimization, including a full production-ready template."
          }
        },
        {
          "judge": "openrouter:deepseek/deepseek-v3.2-exp",
          "raw": "```json\n{\n  \"next_steps\": [\"Convert your model to ONNX format using the provided optimum.onnxruntime code snippet to reduce cold start time from 12s to 3-4s\", \"Deploy the complete Streamlit app template with @st.cache_resource, top-3 predictions, and feedback collection as shown in the production-ready code\", \"Run the concurrent user stress test script to verify memory stability before deploying to the city server\"],\n  \"scores\": {\n    \"clarity_for_student\": 2.0,\n    \"actionability_for_student\": 2.0,\n    \"constraint_fit_for_student\": 2.0,\n    \"confidence_gain_for_student\": 2.0\n  },\n  \"binary_checks\": {\n    \"path_ready\": 1,\n    \"failure_modes_flagged\": 1\n  },\n  \"student_outcome_score\": 2.0,\n  \"justification\": \"The mentor provided specific, executable code solutions that directly address my deployment concerns while respecting my free tooling constraints and limited compute, giving me immediate actionable steps to solve the cold start and memory leak worries.\"\n}\n```",
          "parsed": {
            "next_steps": [
              "Convert your model to ONNX format using the provided optimum.onnxruntime code snippet to reduce cold start time from 12s to 3-4s",
              "Deploy the complete Streamlit app template with @st.cache_resource, top-3 predictions, and feedback collection as shown in the production-ready code",
              "Run the concurrent user stress test script to verify memory stability before deploying to the city server"
            ],
            "scores": {
              "clarity_for_student": 2.0,
              "actionability_for_student": 2.0,
              "constraint_fit_for_student": 2.0,
              "confidence_gain_for_student": 2.0
            },
            "binary_checks": {
              "path_ready": 1,
              "failure_modes_flagged": 1
            },
            "student_outcome_score": 2.0,
            "justification": "The mentor provided specific, executable code solutions that directly address my deployment concerns while respecting my free tooling constraints and limited compute, giving me immediate actionable steps to solve the cold start and memory leak worries."
          }
        },
        {
          "judge": "openrouter:x-ai/grok-4-fast",
          "raw": "{\n  \"next_steps\": [\n    \"Update my Streamlit app with the @st.cache_resource loader and the full production template code to add caching, top-3 predictions, and feedback collection, then test it locally on my laptop to confirm the cold start drops below 5s.\",\n    \"Export the model to ONNX using the provided script, push the ONNX version to Hugging Face, and integrate it into the Streamlit app for faster CPU inference, measuring the new load time on the internal server.\",\n    \"Run the stress test script with 100 simulated requests to verify no memory leaks, then continue labeling the remaining 80 examples in the second batch while monitoring F1 improvements for graffiti and other labels.\"\n  ],\n  \"scores\": {\n    \"clarity_for_student\": 2.0,\n    \"actionability_for_student\": 2.0,\n    \"constraint_fit_for_student\": 2.0,\n    \"confidence_gain_for_student\": 2.0\n  },\n  \"binary_checks\": {\n    \"path_ready\": 1,\n    \"failure_modes_flagged\": 1\n  },\n  \"student_outcome_score\": 2.0,\n  \"justification\": \"As a CS minor with limited time and an old laptop, this response gives me exact code snippets and tests I can run in a few evenings using free tools like Transformers and Streamlit, directly fixing my loading and leak worries while building on my current labeling progress. It makes me feel ready to deploy something usable on the city's server without needing extra resources or skills.\"\n}",
          "parsed": {
            "next_steps": [
              "Update my Streamlit app with the @st.cache_resource loader and the full production template code to add caching, top-3 predictions, and feedback collection, then test it locally on my laptop to confirm the cold start drops below 5s.",
              "Export the model to ONNX using the provided script, push the ONNX version to Hugging Face, and integrate it into the Streamlit app for faster CPU inference, measuring the new load time on the internal server.",
              "Run the stress test script with 100 simulated requests to verify no memory leaks, then continue labeling the remaining 80 examples in the second batch while monitoring F1 improvements for graffiti and other labels."
            ],
            "scores": {
              "clarity_for_student": 2.0,
              "actionability_for_student": 2.0,
              "constraint_fit_for_student": 2.0,
              "confidence_gain_for_student": 2.0
            },
            "binary_checks": {
              "path_ready": 1,
              "failure_modes_flagged": 1
            },
            "student_outcome_score": 2.0,
            "justification": "As a CS minor with limited time and an old laptop, this response gives me exact code snippets and tests I can run in a few evenings using free tools like Transformers and Streamlit, directly fixing my loading and leak worries while building on my current labeling progress. It makes me feel ready to deploy something usable on the city's server without needing extra resources or skills."
          }
        }
      ]
    }
  ]
}