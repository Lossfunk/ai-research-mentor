Given late-night, limited hours, the winning strategy is to pick one tiny, evaluatable question you can answer in 2–4 sessions, lock a baseline, change one thing, and write up the result the same week. Aim for experiments that run in ≤30–60 minutes on a subset so you can iterate.

Scope principles (keeps you on track)
- One dataset, one metric, one lever: choose a public dataset, a single primary metric (e.g., accuracy or MAE), and one intervention (prompt wording, decoding setting, simple augmentation).
- Time-boxed runs: use 100–200-item subsets or small models so a full run fits in a single late-night block.
- Baseline first, then one change: reproduce a simple baseline, freeze configs, then change exactly one ingredient.
- Reproducibility from day one: fix seeds, record exact configs/versions, and save results in a notebook/README each session.
- Deliverables-first: target a minimal artifact per week (one clean notebook + 10-sentence summary).

A 4-week late-night plan (2–4 sessions/week, 45–90 minutes each)
- Week 1: Pick a dataset and reproduce a baseline on a tiny subset. Write your evaluation script and document splits, metric, and seeds.
- Week 2: Run your first intervention across 3–5 seeds. Collect numbers and examples; note failure cases.
- Week 3: Add one ablation (e.g., tweak prompt or augmentation strength) and a robustness check (category-wise results or alternate metric).
- Week 4: Write the short report (motivation, hypothesis, setup, results with mean ± std and a simple significance check, caveats) and publish the notebook/repo.

Three concrete, falsifiable experiments you can run late at night
1) Truthfulness via guardrail prompting (LLM; 100-item subset)
- Hypothesis: A brief system prompt (“answer carefully; avoid myths; cite facts if unsure”) increases TruthfulQA multiple-choice accuracy by at least 5 percentage points versus a neutral prompt under identical decoding.
- Setup: Sample 100 TruthfulQA MC questions (stratified across categories). Same model and decoding (temperature/top-p); vary only system prompt. Run 5 seeds (sampling seeds).
- Metrics: MC accuracy mean ± std; paired t-test across questions (p < 0.05). Reject if <5 pp or not significant.
- Anchor: TruthfulQA measures whether models mimic common falsehoods; prompting is a standard, low-code intervention [TruthfulQA, 2022].

2) Decoding to reduce toxic degeneration (200 prompts)
- Hypothesis: Lowering temperature (0.2 vs 0.8) at fixed top-p reduces mean toxicity score by at least 10% on RealToxicityPrompts, with ≤5% change in continuation length (to avoid trivial truncation).
- Setup: Randomly sample 200 prompts. Generate continuations with both temperatures, same max tokens. Score with a standard toxicity classifier (e.g., Detoxify) or dataset guidance.
- Metrics: Mean toxicity and continuation length (mean ± std); paired test across prompts. Reject if toxicity drop <10% or length change >5%.
- Anchor: RealToxicityPrompts is designed to quantify toxic degeneration under innocuous prompts [Gehman et al., 2020].

3) Data cleaning improves a small text classifier (AG News or Emotion)
- Hypothesis: Deduplication plus removal of top-k likely label errors improves test accuracy by at least 1.0 percentage point over a baseline DistilBERT fine-tune at fixed compute.
- Setup: Baseline: fine-tune DistilBERT. Cleaning: remove exact/near-duplicates; identify likely label errors via cross-validated confidence or simple disagreement heuristics; drop top-k; retrain. 5 seeds.
- Metrics: Test accuracy and calibration (ECE, 15 bins), mean ± std; paired test across seeds. Reject if accuracy gain <1.0 pp or calibration worsens materially.
- Note: Label issues are common and materially affect results; small, principled cleaning often helps (see Northcutt et al., 2021 for context if you want a third reference).

Tooling that keeps nights short
- Compute: Google Colab/Kaggle (free GPUs) or a local machine for small models. Cap runtime to 30–60 minutes per experiment on a subset; scale after you see a signal.
- Libraries: Hugging Face Datasets/Transformers for quick text experiments; torchvision for CIFAR-10 if you prefer vision; Detoxify for toxicity scoring.
- Tracking: Save a JSON/YAML config per run (model, prompt, decoding, seed, dataset slice); log results to a simple CSV so you can make a table quickly.

What to hand in each week
- A notebook or script that reruns end-to-end on your subset.
- A short README: problem, hypothesis, setup, metric, results (mean ± std, n seeds), a figure/table, costs (runtime, hardware), and 2–3 caveats.

If you tell me your model access (API vs open-source), preferred domain (LLMs vs small classifiers vs vision), and how many nights you can commit, I’ll return a one-page plan with exact prompts/configs, code snippets, and runtime budgets.

Literature anchors
- TruthfulQA: Measuring How Models Mimic Human Falsehoods — https://aclanthology.org/2022.acl-long.229/
- RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models — https://aclanthology.org/2020.findings-emnlp.301/

Sources
- Lin et al., 2022. TruthfulQA: Measuring How Models Mimic Human Falsehoods — https://aclanthology.org/2022.acl-long.229/
- Gehman et al., 2020. RealToxicityPrompts — https://aclanthology.org/2020.findings-emnlp.301/