Great—you can contribute meaningfully with 2 hours/week by focusing on small, well-scoped tasks that accumulate into public artifacts (repos, short notes, or bug reports). Evidence-based guidance for low-time contributors emphasizes: start with narrow replications or evals, write down concrete questions/gaps, and share short write-ups; this keeps scope tractable and impact visible [P1]. Newcomer overviews map the technical AI safety landscape and can help you pick a lane (evals, interpretability, robustness, alignment techniques) aligned with your skills and compute budget [P4]. Practical starter lists also suggest “simple things” like replications, audits/evals, red teaming, and data curation—ideal for short weekly sessions [P6].

A 2-hour weekly template
- 60 min: Run/extend a tiny experiment or eval (see below).
- 30 min: Log results, plots, and decisions in a public notebook or repo (minimum viable documentation).
- 30 min: Write a brief note (150–300 words) with what changed your mind, remaining uncertainties, and next step. Post monthly digests (EA/Alignment Forum) to get feedback [P1].

Choose one focus area
- Safety Evals & Red Teaming: Small, repeatable tests of jailbreaks/refusals, policy compliance, or prompt-injection robustness [P6].
- Replication & Auditing: Reproduce a figure/metric from a paper/blog; test on a slightly different dataset or setting [P1].
- Lightweight Interpretability: Run and document simple probes/activation patching on small models; prioritize clarity over breadth [P4].
- Data Curation: Build tiny “safety-relevant” evaluation sets (paraphrased harmful prompts, policy-edge cases) with careful documentation [P6].

Three concrete, falsifiable experiments (2–6 sessions each)
1) Micro-jailbreak robustness across prompt-guards
- Hypothesis: A deterministic “safety preface” reduces jailbreak success rates for an open small model (e.g., 1–8B) by at least 10 percentage points on a 50-prompt set.
- Setup: Select 50 known jailbreak prompts (public lists or your curated set). Compare base prompting vs. base+concise safety preface. Fixed decoding (e.g., greedy).
- Variables: Guard prompt presence (on/off), temperature, paraphrase level.
- Metrics: Jailbreak success rate (binary policy violation), refusal rate on clearly disallowed prompts, helpfulness on benign controls.
- Expected outcome: Some reduction from guard prompts but incomplete robustness; paraphrasing partially restores attacks [P6].
- Deliverable: CSV of prompts/labels, script, 1-page note with error bars and failure examples.

2) Refusal generalization under paraphrase
- Hypothesis: For a safety-tuned model, refusal consistency drops by >15% when harmful prompts are paraphrased with benign-sounding intent markers (e.g., “for research/fiction”).
- Setup: Create 40 harmful prompts; produce 2 paraphrases each (benign veneer, obfuscated request). Include 40 benign controls.
- Variables: Prompt variant type; model (two models if feasible).
- Metrics: Refusal accuracy (harmful), false refusal rate (benign), calibration plot vs. paraphrase type.
- Expected outcome: Noticeable degradation in refusal reliability under paraphrase; baseline differences across models [P6][P4].

3) Minimal interpretability audit: Feature steering on safety-related tokens
- Hypothesis: For a small decoder model, steering activations associated with “disallowed-topic” tokens nudges refusals upward on edge-case prompts without degrading benign helpfulness beyond 5%.
- Setup: Identify a small set of safety-related tokens; collect 200 prompts (100 harmful/100 benign). Use a simple feature-steering method (e.g., scaled residual edits at a single layer).
- Variables: Steering strength; layer index.
- Metrics: Change in refusal rate (harmful), change in helpfulness (benign), KL divergence from baseline outputs.
- Expected outcome (conjecture): Weak positive shift in refusals with modest helpfulness cost; clear trade-off curve you can visualize [P4].

Minimum viable research hygiene (optimized for 2 h/week)
- Timebox each micro-question to 2–4 sessions; write down your “decision to stop” criteria upfront (avoid sunk-cost drift) [P1].
- Predefine metrics and pass/fail thresholds; keep seeds/models fixed unless you preregister a new fork (even a few bullet points suffice) [P1].
- Release code, prompts, and an environment.txt; include a reproducibility checklist and 1-minute “how to run” section.
- Report negatives: if something didn’t work, say why and what you’d try next. Public negative results are valuable, especially in safety [P1].

Where to share and get feedback
- Post short updates on EA/Alignment Forum; “newcomer guides” and starter packs list communities and reading groups to plug into for light-touch mentorship [P2][P4].
- Participate in periodic community sprints (e.g., alignment jams/hackathons) to batch your limited time into one-off pushes, then maintain in 2-hour weekly increments [P5].

Picking your first target in one week
- Session 1: Choose one lane and define a single binary metric (e.g., jailbreak success rate on 50 prompts). Fork a minimal eval script.
- Session 2: Run baseline vs. one intervention (guard prompt or paraphrase); publish the smallest possible artifact (CSV + 1-page note).
- Month-end: One polished post summarizing results and next questions. Ask for 1–2 reviews; incorporate feedback into v2 [P1].

If you share your background (e.g., coding/ML experience, compute access), I can tailor the experiments and tools to your constraints.

Citations
- [P1] Getting started independently in AI Safety — https://forum.effectivealtruism.org/posts/naJ9cJfHMTJ9CACvD/getting-started-independently-in-ai-safety
- [P2] AI safety starter pack — https://forum.effectivealtruism.org/posts/pbiGHk6AjRxdBPoD8/ai-safety-starter-pack
- [P4] A newcomer’s guide to the technical AI safety field — https://www.alignmentforum.org/posts/5rsa37pBjo4Cf9fkE/a-newcomer-s-guide-to-the-technical-ai-safety-field
- [P5] AI Safety Research Outside the Hubs: A Guide for … — https://www.youtube.com/watch?v=ABlf18gs7P0
- [P6] What are some simple things I can do to contribute to AI safety? — https://aisafety.info/questions/6474/What-are-some-simple-things-I-can-do-to-contribute-to-AI-safety