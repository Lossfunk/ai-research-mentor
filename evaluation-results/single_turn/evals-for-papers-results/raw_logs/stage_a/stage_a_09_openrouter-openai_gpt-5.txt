Both tracks are valuable. Pick the one that lets you run tiny, falsifiable experiments in your weekly schedule and publish a clean, reproducible artifact quickly. Use this to decide:

Quick decision heuristics
- Prefer writing, policy, and careful evaluation? Start with AI ethics. You can do impactful measurement and documentation work with public datasets and small models. Use model/dataset cards to scope and report responsibly [1].
- Prefer systems, control, and simulators? Start with robotics safety. You can work entirely in simulation (no hardware) using safety-focused RL environments and measure constraint violations vs reward [2].

A 2-week taste-test (so you can choose with evidence)
- Week 1 (ethics): Reproduce a tiny truthfulness/toxicity baseline on a 100–200 item subset and add one low-code intervention (prompt or decoding). Document with a model card-style README (intended use, limitations, metrics) [1].
- Week 2 (robotics safety): Run a safe-RL baseline in a simulator (e.g., Safety Gym PointGoal) and compare “unconstrained PPO” vs “PPO with a cost penalty.” Track reward, constraint violations, and seeds [2].
Whichever week felt clearer, faster, and more motivating is your starter focus.

Three concrete, falsifiable starter experiments (each fits a few late nights)
1) Ethics: Guardrail prompting for truthfulness (100-item subset)
- Hypothesis: A short system prompt (“answer carefully; avoid myths; cite facts if unsure”) increases multiple-choice truthfulness accuracy by ≥5 percentage points vs a neutral system prompt under identical decoding.
- Setup: Use a public truthfulness/belief benchmark (e.g., TruthfulQA MC). Sample 100 items across categories. Fix model and decoding; vary only system prompt. Run 5 seeds (sampling seeds).
- Metrics: Accuracy mean ± std; paired t-test across questions (p < 0.05).
- Rejection: Gain <5 pp or not significant.

2) Ethics: Documentation checklists improve reporting quality
- Hypothesis: Using a model card template (fields: intended use, limitations, safety considerations, metrics, evaluation data) reduces missing critical reporting fields to ≤10% across your own experiments and 5 comparable public baselines you re-run, compared with their original READMEs.
- Setup: Define 8–10 checklist items from Model Cards. For each experiment (yours + baselines), mark present/absent and time-to-reproduce.
- Metrics: Fraction of missing fields; median minutes-to-reproduce. Report differences with 95% CIs.
- Rejection: Missing-field rate not reduced to ≤10% or reproduction time not improved.
- Rationale: Model Cards provide a practical, widely adopted template for responsible reporting [1].

3) Robotics safety: Cost-penalty vs unconstrained baseline in simulation
- Hypothesis: Adding a cost penalty to PPO reduces average constraint violations by ≥50% on Safety Gym PointGoal1 at a reward drop ≤10% vs unconstrained PPO.
- Setup: Safety Gym “PointGoal1” with hazards (fixed seed splits). Train PPO with and without a per-step cost penalty (sweep λ ∈ {0.01, 0.1, 1.0}). 5 seeds each.
- Metrics: Episode return; average cost/violations per episode; learning curves. Use a paired test across seeds.
- Rejection: Violations not reduced by ≥50% or reward drops >10%.
- Rationale: Safety Gym supplies standardized environments with measurable safety costs for safe RL comparisons [2].

How to scope responsibly (applies to both)
- One dataset/env, one primary metric, one lever per experiment.
- Small first runs (≤30–60 min on subsets or short training) to validate your pipeline; scale only after you see a signal.
- Reproducibility: fix seeds, pin versions, log configs; publish a minimal notebook + README with intended use, limitations, and safety notes [1].
- Report uncertainty: mean ± std over 3–5 seeds; do a simple significance test; include failure cases.

Pick this if you’re still unsure
- If you enjoy writing and careful measurement more than debugging simulators: start with AI ethics.
- If you like interactive environments, control trade-offs, and learning curves: start with robotics safety.

If you share your compute setup (Colab/local GPU/API only) and preference (text vs simulators), I’ll give you a one-page, ready-to-run plan (code snippets, configs, and runtime budgets) for the experiment you choose.

Citations
[1] Model Cards for Model Reporting — https://arxiv.org/abs/1810.03993  
[2] OpenAI Safety Gym — https://openai.com/index/safety-gym/