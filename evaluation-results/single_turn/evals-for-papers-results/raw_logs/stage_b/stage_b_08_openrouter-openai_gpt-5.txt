Short answer: Viable and partially original. Using LLMs directly on heterogeneous citizen-science air-quality logs (free-text notes, custom fields, inconsistent units) remains underexplored compared to (a) sensor-correction/validation work on low-cost networks like PurpleAir and (b) conventional ML over curated time series. Novelty is strongest if you show that LLMs measurably improve schema harmonization, metadata extraction, event triage, and decision-ready uncertainty without degrading scientific validity—especially when tied to reference monitors and published correction methods [1][2][3][4][6][7][8].

What’s risky
- Data validity and drift: Low-cost PM2.5 sensors need site-specific corrections (humidity, season, hardware version); naïve use leads to biased estimates. EPA-style PurpleAir corrections exist but vary by region and conditions [1][2][3]. Many low-cost networks show inconsistent quality and require QA/QC (outlier removal, indoor/outdoor classification) [4].
- Sampling bias and equity: Citizen-sensor coverage is uneven; conclusions can systematically miss or misrepresent disadvantaged areas without explicit bias analysis [10].
- LLM reliability: LLMs can hallucinate fields, misinterpret free text, and generate overconfident narratives. For public-health messaging or advisories, you need strict guardrails, citation, and human-in-the-loop validation [11].
- Governance/compliance: If outputs inform alerts or interventions, document provenance, uncertainty, and adherence to agency guidance (e.g., EPA correction use cases) to avoid misuse or over-trust [1][4].

Where originality can be
- Schema and metadata normalization at scale: Use LLMs to map disparate community logs (sensor model, location, indoor/outdoor, units, humidity notes) into a clean schema; quantify gains over rule-based/ML baselines in precision/recall and downstream AQ metrics [7][8].
- LLM-augmented corrections: Combine standard PM2.5 corrections with LLM-extracted context (e.g., indoor flag, device notes, event tags) to reduce MAE vs colocated reference monitors; most corrections currently ignore rich user metadata [1][2][3][4].
- Event triage and summarization: Align spikes in time series with citizen narratives (odors, smoke) and produce auditable summaries with calibrated uncertainty; few works connect free-text reports to sensor spikes with validation [6][9].
- QA/QC copilot: LLM proposes flags for anomalies (sensor stuck values, humidity artifacts), but final decisions are data-driven (reference checks, statistical tests). This pairs LLM usability with established QA/QC science [4][11].

At least three concrete, falsifiable experiments
- E1. Schema matching and metadata extraction
  - Task: Map heterogeneous logs into a canonical schema; extract fields (units, indoor/outdoor, sensor model, location) from free text.
  - Baselines: Heuristics + regex; supervised classifiers.
  - Metrics: Field-level precision/recall/F1 on a hand-labeled test set; downstream impact on PM2.5 MAE after applying corrections that rely on extracted fields.
  - Falsifiable outcome: LLM-based pipeline improves metadata F1 by ≥10 points and reduces colocated PM2.5 MAE by ≥10% vs baselines; if not, LLMs add little beyond rules [7][8].

- E2. Correction-aware accuracy vs reference monitors
  - Setup: Colocate citizen sensors with regulatory-grade monitors across seasons; compare raw, EPA-style corrected, and “LLM-context–aware corrected” PM2.5 (e.g., excluding indoor segments, humidity-aware flags).
  - Metrics: MAE/RMSE, bias, coverage of 90% prediction intervals.
  - Falsifiable outcome: Context-aware pipeline outperforms standard corrections by statistically significant MAE reduction without undercoverage; otherwise, added complexity isn’t justified [1][2][3][4].

- E3. Event detection and narrative alignment
  - Setup: Detect pollution events from time series; use LLM to summarize/label co-occurring community reports (smoke, fireworks, industrial odor).
  - Ground truth: Official wildfire/fireworks timestamps or incident logs.
  - Metrics: Event precision/recall, lead time vs baselines, factuality rate of summaries (human audit with rubric).
  - Falsifiable outcome: LLM-assisted pipeline improves event F1 or lead time by ≥10% while keeping ≥95% factuality on audits; if not, prefer simpler detectors [6][9][11].

- E4. Equity and robustness audit (recommended)
  - Setup: Evaluate accuracy and coverage across neighborhoods stratified by socioeconomic indicators and sensor density.
  - Metrics: Error disparity (MAE ratios), calibration coverage across groups.
  - Falsifiable outcome: Disparities ≤20% and nominal coverage across strata; else, require targeted sampling/regularization [10].

Risk mitigations to build in from day one
- Proven corrections + co-location tests before any downstream modeling [1][2][3][4].
- LLMs as assistive, not authoritative: enforce deterministic transformations where possible; require citations in summaries; add uncertainty and “do-not-use-for-health-decisions” flags unless validated [11].
- Data governance: Store original logs, version transformations, document prompts/checkpoints, and publish an audit trail.
- Open benchmarks: Release labeled schema-matching and co-location datasets to support reproducibility and comparability [6][7][8].

Key anchors to read first
- An evaluation of the U.S. EPA’s correction equation for PurpleAir PM2.5 sensors (Atmospheric Measurement Techniques, 2023) — shows when and how corrections work and where they fail [1].
- LLMs Automatically Generate Code to Data Clean (KDD 2024) — evidence that LLMs can materially reduce data-wrangling time and errors, with measurable quality improvements [7].
- Observed data quality concerns involving low-cost air sensors (2021) — practical pitfalls and QA/QC patterns to integrate into your pipeline [4].
- OpenAQ platform — context on aggregating heterogeneous community air-quality data and metadata conventions [6].

Bottom line
- Not saturated and potentially original if you make LLMs demonstrably improve schema harmonization, correction-aware accuracy, and event triage with rigorous validation against reference monitors and transparent uncertainty. The main risk is scientific validity—mitigate it with co-location studies, established corrections, and strict human-in-the-loop QA/QC [1][2][3][4][11].

Citations
[1] An evaluation of the U.S. EPA’s correction equation for PurpleAir PM2.5 — https://amt.copernicus.org/articles/16/1311/2023/  
[2] Correction and Accuracy of PurpleAir PM2.5 — https://pmc.ncbi.nlm.nih.gov/articles/PMC9784900/  
[3] Calibration of PurpleAir low-cost particulate matter sensors — https://amt.copernicus.org/articles/17/6735/2024/  
[4] Observed data quality concerns involving low-cost air sensors — https://pmc.ncbi.nlm.nih.gov/articles/PMC8318136/  
[5] EPA’s Air Sensor Toolbox (citizen science) — https://www.ecos.org/wp-content/uploads/2016/03/air-sensors-poster.pdf  
[6] OpenAQ (open community AQ data platform) — https://openaq.org/  
[7] LLMs Automatically Generate Code to Data Clean (KDD 2024) — https://kdd2024.kdd.org/wp-content/uploads/2024/07/paper_16.pdf  
[8] Schema Matching with Large Language Models (VLDB workshop 2024) — https://vldb.org/workshops/2024/proceedings/TaDA/TaDA.8.pdf  
[9] EnviroExam: Benchmarking Environmental Science Knowledge of LLMs — https://arxiv.org/html/2405.11265v1  
[10] The Role of Citizen Science in Promoting Health Equity — https://pmc.ncbi.nlm.nih.gov/articles/PMC9034747/  
[11] The Clinicians’ Guide to Large Language Models: A General Introduction — https://pmc.ncbi.nlm.nih.gov/articles/PMC11815294/