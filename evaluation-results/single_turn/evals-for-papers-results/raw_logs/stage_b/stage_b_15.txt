Stage: A–B (clarifying the idea and sizing a feasible first study)

Quick intake so I can tailor scope:
- Compute/time: What’s your weekly time budget for the next 8–10 weeks and any compute or funds for data collection (e.g., resume generation, paid human raters)?
- Context: Are you solo or part of a lab/clinic with IRB access and legal support?
- Target: Which venue/timeline are you aiming for (e.g., FAccT’25, AIES, a policy report), and what’s your biggest bottleneck right now?
- Access: Do you have any collaborator access to a specific platform or will this be a black-box audit only?

How to check novelty and scope responsibly
- Do a 90‑minute prior‑art sweep anchored by two references, then snowball: start from (i) EEOC’s guidance on adverse impact testing (four‑fifths rule and cautions) [P1], and (ii) recent surveys of algorithmic hiring audits [P2][P3]. Scan for your specific combo of: platform type (resume/ATS, interview scoring, assessments), occupation, jurisdiction, protected classes, and audit method (paired counterfactuals, causal tests, construct validity). Document overlaps in a “novelty matrix” (rows = papers/reports, columns = your choices). If every cell matching your combo is filled, pivot one axis (new occupation, language, or method) [P2].
- Pre‑register scope and metrics: explicitly define outcomes (rank, invite rate), fairness tests (adverse impact ratios, equalized odds), and validity checks (construct and external validity) before collecting data. Use four‑fifths as a screening threshold, not a legal conclusion, and include confidence intervals and sample size rationale [P1][P6].
- Bound risk and legality: avoid scraping that violates ToS; obtain IRB or ethics review if interacting with real applicants; separate “capability” audits from “compliance” claims unless you have full system context. Recent rules in some U.S. jurisdictions require structured bias audits and disclosures—align your reporting template accordingly [P10][P9]. A cross‑functional audit checklist helps structure consent, logging, and reporting [P6], and legal scholarship underscores the ethical and legal stakes of hiring audits [P5].
- Choose a minimal publishable slice: one job family in one jurisdiction on one platform, with one primary fairness test and one validity test. Use an audit framework that separates data, model, and outcome auditing, and call out confounders and construct validity limits explicitly [P4][P7].

Three concrete, falsifiable experiments
1) Counterfactual resume-pair audit for gender-coded names in STEM
- Objective/hypothesis: Holding qualifications constant, male-coded names receive higher rankings than female-coded names for entry-level software roles on Platform X. Expected direction: male-coded > female-coded rank. Falsifier: no systematic pairwise advantage (win rate ≤52%).
- Setup: Generate 500 matched resume pairs (only first name differs), vary university tier and programming stack; submit to Platform X via public interface. Controls: identical skills/experience; randomize order.
- Metrics: Pairwise win rate; mean rank delta; adverse impact ratio if a pass/fail threshold exists, with 95% CIs and power analysis targeting ±5% precision [P1][P2]. Success criterion: win rate ≥58% with CI excluding 50%.
- Interpretation: If effect holds, report magnitude and heterogeneity (by sub-discipline). If null, conclude constraints or platform robustness and try senior roles as follow‑up.
- Variations: Add race-coded names; test non-STEM roles; perturb resume formatting to check robustness [P2][P4].

2) Construct validity of video interview “communication” scores vs. human raters and accent
- Objective/hypothesis: Platform Y’s “communication” score correlates moderately (r ≥0.5) with expert rater scores but penalizes non‑US accents controlling for content. Falsifier: r <0.3 and no accent effect.
- Setup: 80 scripted answers read verbatim by speakers across accents (US, Indian, Nigerian, British), identical content and lighting; upload to Platform Y. Obtain blinded human ratings from trained raters.
- Metrics: Correlation with human ratings; ANCOVA for accent effect; standardized mean differences; calibration plots [P4][P7]. Success: significant accent effect (|d|≥0.4) after controlling for human ratings.
- Interpretation: If accent penalty appears, attribute to acoustic or prosodic features; propose mitigation tests (ASR front‑end swap, noise normalization).
- Variations: Use high‑quality TTS to isolate acoustic features; test different prompts.

3) Stability/manipulability audit of ATS ranking
- Objective/hypothesis: Small non‑substantive wording changes induce ≥10% rank shifts, indicating low stability and potential gaming risk. Falsifier: median shift <3%.
- Setup: Take 300 real‑looking resumes; generate three paraphrases preserving content (LLM paraphrases, bullet vs. paragraph, reordered sections). Submit each version to Platform Z for the same posting.
- Metrics: Kendall’s tau between versions; absolute rank change distribution; sensitivity to section order [P2][P7]. Success: median absolute rank change ≥10 percentile points across paraphrases.
- Interpretation: If unstable, recommend platform‑side smoothing or employer guidance; if stable, report as a positive robustness property.
- Variations: Test keyword‑stuffing injections and formatting changes (PDF vs. DOCX).

Phase 0 (≤14 days) with gates
- Deliverables: (1) A novelty matrix with at least 20 sources and a one‑page preregistration of hypotheses/metrics tied to [P1][P6]; (2) One mini‑reproduction (e.g., a paired‑resume audit from prior work) with ≤10% metric gap and an experiment card plus one ablation or negative result.
- Scoreboard to track: Reproduction fidelity, Ablation clarity, and Writing cadence (one-page journal/week).

Strategic questions to sharpen scope
- Which platform type and job family are you most likely to access (resume vs. interview vs. assessments)?
- Which jurisdiction matters for your audience (e.g., California, NYC), and do you want a “capability” or “compliance” framing? [P10][P9]
- Do you have IRB or institutional backing for human subjects or platform interactions? [P5][P6]

Intuition
- Most “overdone” topics still have untapped intersections (method × population × platform). By scoping to one crisp hypothesis with strong validity checks, you create a clean empirical contribution even in a crowded space.

Why this is principled
- Anchoring on EEOC adverse‑impact and validity standards avoids overclaiming and aligns with legal/ethical expectations [P1][P5]. Using structured audit frameworks and checklists reduces confounding and improves reproducibility and reporting quality [P4][P6][P7].

Next actions (1–3 days)
- Build the novelty matrix (seed from [P1], [P2], [P7]) and pick one narrow slice.
- Draft a one‑page prereg (hypotheses, metrics, sample size) and an experiment card for Experiment 1 [P1][P6].
- Email your IRB/ethics office with a short protocol outline and confirm platform ToS boundaries [P5][P6].

Citations
- [P1] EEOC: Select Issues—Adverse Impact in AI Hiring — https://www.robinsonbradshaw.com/assets/htmldocuments/Select%20Issues%20Assessing%20Adverse%20Impact%20in%20Software%20Algorithms%20and%20Artificial%20Intelligence%20Used%20in%20Employment%20Selection%20Procedures%20Under%20Title%20VII%20of%201964.pdf
- [P2] Fairness and Bias in Algorithmic Hiring: a Multidisciplinary Survey — https://arxiv.org/pdf/2309.13933
- [P3] Fairness in AI-Driven Recruitment: Challenges, Metrics, Methods — https://arxiv.org/html/2405.19699v3
- [P4] Systematizing Audit in Algorithmic Recruitment — https://mdpi-res.com/d_attachment/jintelligence/jintelligence-09-00046/article_deploy/jintelligence-09-00046.pdf
- [P5] An Auditing Imperative for Automated Hiring Systems — https://jolt.law.harvard.edu/assets/articlePDFs/v34/5.-Ajunwa-An-Auditing-Imperative-for-Automated-Hiring-Systems.pdf
- [P6] AI Auditing Checklist — https://www.edpb.europa.eu/system/files/2024-06/ai-auditing_checklist-for-ai-auditing-scores_edpb-spe-programme_en.pdf
- [P7] Systematizing Audit in Algorithmic Recruitment (PMC) — https://pmc.ncbi.nlm.nih.gov/articles/PMC8482073/
- [P9] Navigating Changing U.S. AI Hiring Laws (overview) — https://stemsearchgroup.com/navigating-changing-u-s-ai-hiring-laws-what-employers-need-to-know-as-of-oct-2025/
- [P10] The AI Hiring Audit: New California Rules (overview) — https://www.amsinform.com/pre-employment-checks/the-ai-hiring-audit-what-octobers-new-california-rules-mean-for-every-employer/