Stage: A – Pre-idea sharpening

Short answer on originality and risk
- Originality: Moderate, with room to be high depending on the specific angle. LLM-powered air-quality chat/insight systems and domain agents exist (e.g., VayuBuddy; AirGPT; Emission-GPT), but tightly coupling LLMs to citizen logs for quality control, sensor calibration, or event reconciliation with regulatory/satellite data is still emerging and underexplored [P8][P7][P2]. Positioning around data quality, calibration, or human-in-the-loop validation could be novel enough for a strong workshop/short paper if executed rigorously [P9][P4].
- Risk: Medium. Main risks are data quality/coverage bias in citizen datasets, LLM hallucinations and causal misattribution, and evaluation without ground truth; all are manageable with retrieval grounding, gold labels, and clear falsifiers [P9][P6][P7].

Strategic questions (answering these will change the plan)
1) What data will you actually have? Free-text diaries, PurpleAir streams, photos, or forum posts—and which ground-truth anchors (EPA AirNow, regulatory PM2.5, AOD, wildfire event timelines)? [P4]  
2) What contribution type do you want: (a) data quality/QA pipeline, (b) calibration/fusion (citizen + regulatory/satellite), or (c) public-facing explanations with safety guarantees? Each faces different baselines [P4][P7].  
3) Compute and timeline: can you fine-tune domain LLMs or do you need RAG + prompt-only? What’s your 8–12 week milestone? [P7]  
4) Ethical guardrails: how will you address privacy, consent, and harmful advice (e.g., exposure recommendations) in a deployed system? [P9]

Recommendations (pick one primary contribution to reduce risk)
- If you want lower risk, aim for LLM-assisted data quality control: use LLMs to parse citizen notes and flag sensor anomalies/events; benchmark against non-LLM baselines. This sits squarely in recognized gaps around citizen-science data quality [P9].  
- If you want higher originality, target LLM-guided calibration/fusion: extract context from logs (e.g., indoor/outdoor switches, maintenance, source proximity) to improve calibration against regulatory monitors beyond standard RH/temperature methods [P4].  
- If you want societal impact, do retrieval-grounded explanations with safety rails, explicitly demonstrating reduced hallucinations and improved comprehension versus rule-based summaries [P7][P8].

Three concrete, falsifiable experiments
1) LLM-assisted quality control of citizen logs  
- Objective & hypothesis: LLMs that read free-text logs + sensor time series will better detect data issues (misplacement, indoor episodes, device malfunction) than rule-based or ML baselines.  
- Setup: Collect paired logs and PM2.5 from citizen sensors (e.g., PurpleAir) over 3–6 months; label 1–2k windows with expert QA tags. Models: (a) few-shot RAG-LLM reading a short time window + log text; (b) gradient-boosted trees on numeric features; (c) rules.  
- Metrics: Precision/recall/F1 for issue classes; AUROC for anomaly detection; cost-weighted error emphasizing false “safe” flags.  
- Interpretation: Gains show LLM value in unstructured-to-structured inference; parity or worse means stick to traditional QA pipelines.  
- Follow-ups: Calibrate with temperature/RH and season; stress-test wildfire periods; error taxonomy to guide prompt refinements [P9][P8].

2) LLM-guided calibration via context extraction  
- Objective & hypothesis: Extracting context (e.g., “box fan filter added,” “construction nearby,” “sensor moved indoors”) from logs improves calibration to regulatory PM2.5 beyond standard meteorological corrections.  
- Setup: Pair citizen sensors within 2–5 km of EPA stations; train standard calibration (RF/GBM with RH, temp) vs. augmented model with LLM-derived categorical/context features and text embeddings.  
- Metrics: MAE/RMSE vs. EPA; Bland–Altman agreement; subgroup error under exceptional events (wildfires).  
- Interpretation: Significant error reductions—especially under atypical conditions—support the approach; no gain implies log-context adds little signal.  
- Follow-ups: Causal sensitivity checks to avoid spurious attributions; ablate each context feature; external validation in another city [P4][P1][P6].

3) Retrieval-grounded public explanations with safety checks  
- Objective & hypothesis: A RAG-LLM tied to local sensor + regulatory data can produce more accurate, comprehensible, and trusted daily air-quality summaries than templates, while keeping hallucinations below a preset threshold.  
- Setup: Build a small RAG over recent AQI/sensor stats; compare (a) templated NLG, (b) generic LLM, (c) RAG-LLM with tool-use to pull current values. Run user study (n≈60) for comprehension/trust; concurrently score factual accuracy against ground truth.  
- Metrics: Exact-match rate for key numbers, faithfulness (human + automated), comprehension quiz scores, and unsafe advice incidents.  
- Interpretation: If RAG-LLM improves comprehension (+>10%) without increasing factual errors, it’s a win; hallucination rate >2% triggers stricter retrieval/tooling or abstention policies.  
- Follow-ups: Add uncertainty statements; counterfactuals (“If AQI rises by 50, what changes?”); test wildfire scenarios where LLMs often fail [P7][P8][P10].

Risk notes to address explicitly
- Causal claims: treat LLM “explanations” as hypotheses; validate with causal or quasi-experimental checks where possible [P6].  
- Data quality/coverage: citizen networks are spatially biased; report demographics/coverage and conduct robustness across neighborhoods [P9].  
- Hallucinations/safety: retrieval-grounding + tool-use + abstention policies reduce risk in scientific domains [P7].  
- Evaluation: use regulatory monitors and clearly defined event datasets for hard grounding [P4].

Phase 0 (≤14 days) gate
- Deliverables: (1) Prediction log (≥14 entries) and one reproduced baseline (e.g., a simple PurpleAir→EPA calibration) within ≤10% relative error; (2) One experiment card (from above) plus a negative result or ablation with post-mortem.  
- If gates aren’t met, iterate before expanding scope.

Intuition
- Citizen logs encode context (moves, sources, maintenance) that numerical streams miss; LLMs can extract this weak but useful signal to improve QA, calibration, and communication. Retrieval grounding lets LLMs answer “why/what now?” without making up facts.

Why this is principled
- Prior work shows methodological need around gap-filling/calibration of citizen sensing and the perils of domain LLM hallucinations—so coupling LLMs with hard data and strong evaluation is the right path [P4][P7]. Reviews of citizen science + ML emphasize data quality, engagement, and governance as core risks, which your design and metrics can target directly [P9].

Quick intake to tailor next steps
- What data will you have in the first 4–8 weeks (logs, sensors, cities)?  
- Compute and time budget per week?  
- Target venue/timeline (e.g., ML4H/AI4Earth workshop vs. HCI venue)?  
- Biggest bottleneck right now (data access, labeling, evaluation, or deployment)?

Literature anchors
- VayuBuddy: an LLM-Powered Chatbot to Democratize Air Quality Insights — https://arxiv.org/html/2411.12760v1 [P8]  
- A novel spatiotemporal prediction approach using mobile sensors, ML, and citizen science — https://www.nature.com/articles/s41612-024-00859-z [P4]

Citations
- [P1] Using low-cost sensors and citizen science to examine air quality — https://researchoutreach.org/articles/using-low-cost-sensors-and-citizen-science-to-examine-air-quality/  
- [P2] Emission-GPT: A domain-specific language model agent for atmospheric emissions — https://arxiv.org/html/2510.02359v1  
- [P3] How citizen science can amplify community-led climate action — https://www.weforum.org/stories/2025/09/ai-citizen-science-community-climate-action/  
- [P4] A novel spatiotemporal prediction approach to fill air pollution data gaps using mobile sensors, machine learning and citizen science techniques — https://www.nature.com/articles/s41612-024-00859-z  
- [P5] New directions in mapping the Earth's surface with citizen science and generative AI — https://pmc.ncbi.nlm.nih.gov/articles/PMC11879609/  
- [P6] Are Large Language Models Capable of Causal Reasoning for Sensing Data? — https://dl.acm.org/doi/10.1145/3662006.3662064  
- [P7] AirGPT: convergence of conversational AI with atmospheric science — https://www.nature.com/articles/s41612-025-01070-4  
- [P8] VayuBuddy: an LLM-Powered Chatbot to Democratize Air Quality Insights — https://arxiv.org/html/2411.12760v1  
- [P9] The Partnership of Citizen Science and Machine Learning — https://mdpi-res.com/d_attachment/sustainability/sustainability-13-08087/article_deploy/sustainability-13-08087-v2.pdf  
- [P10] LLMs in computational social science — https://link.springer.com/article/10.1007/s13278-025-01428-9