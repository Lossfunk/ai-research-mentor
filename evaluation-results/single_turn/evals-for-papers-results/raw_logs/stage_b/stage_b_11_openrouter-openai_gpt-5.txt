Short answer: Feasible, and there’s room for novelty—but you’ll need to (1) position your RL contribution against strong non-RL baselines (coverage planners, MPC, rule-based suppression), (2) handle partial observability, safety, and comms constraints typical of wildfire ops, and (3) show sim-to-real robustness. Existing work covers UAV wildfire tracking and multi‑agent RL concepts, but robust suppression/containment policies under real constraints remain underexplored, especially with formal safety guarantees and hardware-validated performance [1][5][3][4].

How to assess novelty
- Map the task precisely: tracking/mapping vs allocation vs suppression.
  - Tracking/mapping with UAVs via RL exists; a recent paper tackles wildfire perimeter tracking with UAVs, indicating active but not saturated space [1]. Novelty requires harder settings (e.g., wind-shift OOD, sparse comms, decentralized policies) or better safety/robustness.
  - Coordinated suppression/containment (e.g., dispatching water/retardant drops, ignition for backburn in sim) with multi-drone RL has early precedents (distributed DRL firefighting robots), but practical, constraints-aware policies with guarantees are rare—still a novelty opportunity if you compare to strong planners [5][3].
- Raise the bar on baselines and evaluations:
  - Include non-RL baselines: greedy coverage, receding-horizon MPC using a fire-spread model, multi-armed bandit resource allocation. If RL doesn’t beat these under realistic constraints, the contribution is weak.
- Add safety and governance: Safety shields or verified constraints (no-fly zones, min altitude, geofencing) and decentralized control under comms loss are practical gaps in many RL papers—addressing them is novel and increases feasibility [5].

How to assess feasibility early
- Simulation realism: Use a spread model (e.g., Rothermel-like dynamics) and randomize wind, slope, fuel to prevent overfitting; quantify sim-to-real gap by validating spread rates against data where possible. If you can’t model suppression physics, limit scope to tracking/resource triage and state that explicitly.
- Architecture: Let RL decide high-level actions (where to go, which sector to service); keep low-level flight on tested autopilots (PX4/ArduPilot) to reduce risk.
- Multi-agent scaling: Prefer decentralized policies with limited comms; test with packet loss and latency.
- Safety: Add constraint layers (control barrier functions, geofencing) or a “shielded RL” approach; measure violation rates.
- Compute/data: Use off-policy or model-based RL for sample efficiency; parallelize sim to achieve millions of steps/day.

At least three concrete, falsifiable experiments
- E1. Perimeter tracking and mapping under wind shift
  - Setup: Simulate evolving wildfires with stochastic wind; train a decentralized RL policy for K drones to maintain up-to-date perimeter maps.
  - Baselines: Greedy frontier/coverage planner; MPC over a spread model.
  - Metrics: Time-averaged IoU of mapped vs true perimeter, worst-case latency to detect new spread, energy per drone.
  - Falsifiable outcome: RL achieves ≥5% higher mean IoU and ≥20% lower worst-case detection latency vs best baseline across ≥3 unseen wind regimes; otherwise, prefer planners. Recent UAV wildfire tracking work supports the task framing [1].

- E2. Multi-drone suppression allocation with constraints
  - Setup: Simulated water/retardant drops with refill cycles, no-fly zones, and comms outages. Train multi-agent RL to assign drones to sectors and schedule drops.
  - Baselines: Heuristic priority queues (spread rate × asset value), receding-horizon integer programming.
  - Metrics: Final burned area, number/duration of safety violations (airspace, minimum separation), mission completion time.
  - Falsifiable outcome: RL reduces burned area by ≥10% without increasing violation rates compared to best baseline; if not, heuristics suffice. Prior distributed DRL firefighting with aerial robots provides precedent for the setup [5]; multi-agent RL scaffolding is established in the literature [3][4].

- E3. Sim-to-real robustness with hardware-in-the-loop
  - Setup: Run the trained high-level policy against a PX4 SITL/HITL stack; then field-test on small UAVs with a surrogate “fire” (moving heat/LED targets) and smoke obscurants.
  - Stressors: GPS multipath, 10–20% packet loss, wind gusts.
  - Metrics: Task success rate, constraint-violation rate (geofence, separation), and performance drop from pure sim to HITL and field.
  - Falsifiable outcome: ≤15% degradation from sim to HITL, ≤25% to field, while keeping violations below predefined thresholds; otherwise, apply domain randomization or safety shields and retest. Prior wildfire RL/robotics work highlights the need for robust decentralized policies [5][1][3].

Common pitfalls (and how to avoid them)
- Reward hacking/shortcuts: Use interpretable metrics (burned area, perimeter coverage) and hold-out OOD conditions to catch gaming.
- Non-stationarity in multi-agent RL: Centralized training with decentralized execution and curriculum over team sizes helps [3][4].
- Overclaiming “suppression” without physics: If suppression effects are simplistic, constrain claims to “allocation/planning in a stylized spread model.”
- Ignoring comms/safety: Always test with lossy comms and enforce hard constraints; report violation statistics, not just rewards.

Go/no-go novelty checklist
- Does your RL beat strong MPC/heuristic baselines on mission-relevant metrics under OOD winds/fuels?
- Are safety violations at or below baseline while meeting compute/energy limits on target hardware?
- Do results hold with decentralized, lossy-comms execution and in HITL/field trials?

If you can answer yes with statistically robust tests, you likely have a novel and feasible contribution.

Sources
[1] A Reinforcement Learning Approach for Wildfire Tracking with UAV — https://arxiv.org/abs/2407.05473
[3] Training UAV Teams with Multi-Agent Reinforcement Learning (thesis) — https://open.clemson.edu/cgi/viewcontent.cgi?article=5399&context=all_theses
[4] A multi-agent deep reinforcement learning approach — https://www.sciencedirect.com/science/article/pii/S0925231225018831
[5] Distributed Deep Reinforcement Learning for Fighting Forest Fires with a Network of Aerial Robots (IROS 2018) — https://web.stanford.edu/~schwager/MyPapers/HaksarSchwagerIROS18DRLFirefighting.pdf