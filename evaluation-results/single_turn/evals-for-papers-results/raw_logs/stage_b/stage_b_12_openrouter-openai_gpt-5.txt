Short answer: Partially novel. “LLM tutoring” isn’t new, but making confidence calibration and abstention a first‑class design goal for community‑college contexts (nontraditional learners, developmental math/writing, resource constraints) is still underexplored. You can make a clear contribution if you (a) quantify calibration quality and deferral-to-human policies, (b) show improved learning outcomes or reduced harmful help vs strong baselines, and (c) document guardrails that reduce hallucination‑driven harm without throttling useful help [1][2][3].

Where novelty likely lies
- Confidence- and abstention-aware tutoring: Teach the tutor to say “I’m not sure” and route to hints, retrieval, or a human; evaluate with ECE/Brier and deferral coverage. Most LLM‑tutor work focuses on correctness/engagement, not calibrated confidence or abstain policies [1][2].  
- Guardrails that improve outcomes: Empirical evidence shows unguarded generative AI can harm learning; demonstrating that calibration + verification + retrieval reduces wrong-but-confident guidance is valuable [3].  
- CC‑specific deployment/equity: Show effects for developmental courses and subgroups (Pell-eligible, first‑gen, ESL), where evidence for LLM tutors is sparse; prior work is mostly K‑12 or selective universities [8].

What to use as baselines
- Non-LLM tutoring baselines
  - Established ITS: ALEKS, ASSISTments or comparable systems with RCT/meta‑analytic evidence in college math; report learning gains/time‑on‑task [6][7].  
  - Human tutoring center (if feasible): measure attendance-adjusted outcomes.
- LLM baselines
  - Uncalibrated LLM tutor: same model and pedagogical script, but no confidence estimation/abstention, to isolate the impact of calibration.  
  - Retrieval-augmented and solver-verified LLM: Ground on vetted materials and verify math/code steps; this is a strong safety baseline [1].  
  - Popular deployed AI tutor (e.g., Khanmigo‑style configuration) if accessible; large-scale RCT evidence exists for general efficacy, not calibration per se [8].

At least three concrete, falsifiable experiments
- E1. Calibration ablation at the step level
  - Design: Randomize students at the session level to Calibrated vs Uncalibrated LLM tutor. Same prompts/content; calibrated tutor produces probability/confidence and abstains or downgrades help below a threshold.  
  - Metrics: Step correctness, hallucination rate in explanations (expert rubric), ECE/Brier, help‑abuse rates, time‑to‑mastery.  
  - Falsifiable outcome: With matched accuracy, calibrated tutor reduces high‑confidence errors by ≥30% and lowers hallucination‑tainted hints by ≥25%; otherwise, calibration isn’t adding safety [3][4][5].

- E2. Adaptive deferral with conformal policies
  - Design: Add a conformal/coverage‑controlled deferral rule that routes uncertain turns to (a) retrieval of vetted materials, (b) worked‑example hints, or (c) human TA.  
  - Metrics: Coverage at nominal levels, deferral rate, learning gains on isomorphic post‑items, student trust/overtrust survey.  
  - Falsifiable outcome: 90% nominal coverage yields ≥88–92% empirical coverage with equal or better learning gains vs non‑deferring LLM; if not, tune or drop deferral [3][4].

- E3. Course‑level randomized evaluation in community college sections
  - Design: Cluster‑randomize sections into (A) ITS‑only (e.g., ALEKS/ASSISTments), (B) Uncalibrated LLM tutor, (C) Calibrated + guarded LLM tutor (with retrieval/verification/deferral).  
  - Outcomes: Common assessments, pass/DFW rates, persistence, subgroup effects (ESL/Pell).  
  - Falsifiable outcome: Group C improves standardized post‑test by ≥0.15–0.25 SD over A and reduces DFW by ≥3–5 pp without widening subgroup gaps; if not, the added complexity may not justify deployment cost [6][7][8].

- E4. Robustness and pedagogy audit (recommended)
  - Design: Expert panel rates pedagogical soundness (scaffolding, misconceptions) and factuality across topics; stress‑test OOD problems.  
  - Metrics: Factuality, pedagogical quality, and calibration under distribution shift.  
  - Falsifiable outcome: Calibrated tutor maintains calibration (ECE ≤ 0.05) and pedagogy scores under OOD; otherwise, require domain adaptation.

Feasibility and risks to plan for
- Feasibility: Off‑the‑shelf calibration methods (self‑reflection, temperature scaling, selective prediction) and abstention policies exist for LLMs; integrate with retrieval and solver checks for math/code [4][5].  
- Risks: Hallucinations and overconfident wrong explanations can harm learning; calibration and guardrails are specifically meant to counter this [3]. FERPA/privacy and accessibility (screen readers, language level) must be addressed in CC deployments.

Reporting to make the case
- Always report both learning outcomes and safety: ECE/Brier, high‑confidence error rate, deferral coverage, hallucination audits, plus student outcomes.  
- Compare cost‑effectiveness: instructor time saved, TA escalation load, and compute cost per learning gain.

Bottom line: Novelty comes from making confidence calibration, abstention, and guardrails measurable levers that improve learning and reduce harm in community‑college settings. Benchmark against an ITS baseline, an uncalibrated LLM tutor, and a retrieval/verification‑enhanced LLM. If you can show better learning gains with fewer confident errors and well‑calibrated deferrals, that’s a strong contribution.

Sources
[1] Combining Large Language Models with Tutoring System Intelligence — https://dl.acm.org/doi/10.1145/3706468.3706516  
[2] Training LLM-based Tutors to Improve Student Learning Outcomes — https://arxiv.org/abs/2503.06424  
[3] Generative AI Without Guardrails Can Harm Learning — https://hamsabastani.github.io/education_llm.pdf  
[4] Calibrating the Confidence of Large Language Models by Eliciting Self-Reflection (EMNLP 2024) — https://aclanthology.org/2024.emnlp-main.173.pdf  
[5] Large Language Models Must Be Taught to Know What They Don’t Know — https://arxiv.org/html/2406.08391v1  
[6] Meta-Analysis of the Effectiveness of Intelligent Tutoring Systems on College Students’ Academic Learning — https://www.researchgate.net/publication/263918046_Meta-Analysis_of_the_Effectiveness_of_Intelligent_Tutoring_Systems_on_College_Students'_Academic_Learning  
[7] A Meta-Analysis of the Effectiveness of ALEKS on Learning — https://www.researchgate.net/publication/329172128_A_meta-analysis_of_the_effectiveness_of_ALEKS_on_learning  
[8] University of Toronto randomized controlled trial shows positive effects of Khan Academy — https://blog.khanacademy.org/university-of-toronto-randomized-controlled-trial-of-11k-students-demonstrates-a-meaningful-positive-effect-of-khan-academy-on-student-learning/