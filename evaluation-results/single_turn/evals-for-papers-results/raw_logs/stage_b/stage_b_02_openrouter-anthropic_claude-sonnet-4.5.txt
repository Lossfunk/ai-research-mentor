## Novelty Assessment

**LLM compression is a very active, crowded field.** The core techniques are well-established:

- **Quantization** (reducing precision, e.g., 8-bit, 4-bit, even 1-bit)
- **Pruning** (removing weights, attention heads, or layers)
- **Distillation** (training smaller models to mimic larger ones)
- **Low-rank factorization** (decomposing weight matrices)

Recent work includes:
- [A Survey on Model Compression for Large Language Models](https://arxiv.org/abs/2308.07633) (comprehensive 2023 review)
- [Apple's "Compressing LLMs: The Truth is Rarely Pure and Never Simple"](https://machinelearning.apple.com/research/compressing-llms) (practical insights)
- [EvoPress: Accurate Dynamic Model Compression via Evolutionary Search](https://arxiv.org/html/2410.14649v2) (2024, adaptive compression)
- [Training LLMs over Neurally Compressed Text](https://arxiv.org/html/2404.03626v2) (2024, compression-aware training)

### Where Novelty Might Lie

To carve out novelty, you need to be **specific** about what "compressible" means:

1. **Dynamic/adaptive compression**: Compression that adjusts per-input, per-layer, or per-token (some work exists, e.g., EvoPress)
2. **Compression-aware pretraining**: Training LLMs from scratch to be inherently compressible (emerging area)
3. **Task-specific compression**: Compress differently for different downstream tasks
4. **Lossless or near-lossless compression**: Maintaining performance while achieving extreme compression ratios
5. **Hardware co-design**: Compression schemes optimized for specific accelerators
6. **Compositional compression**: Combining multiple techniques in novel ways with theoretical guarantees
7. **Compression for specific modalities**: E.g., multimodal LLMs, code LLMs, domain-specific models

## Concrete Experiments to Run

Here are **three falsifiable experimental tracks**:

### **Experiment 1: Compression-Performance Tradeoff Characterization**
**Hypothesis**: Your compression method achieves better accuracy at a given compression ratio than existing baselines.

**Protocol**:
- **Baselines**: Compare against GPTQ (quantization), SparseGPT (pruning), and a distillation baseline (e.g., DistilBERT-style)
- **Models**: Test on at least two scales (e.g., 1B and 7B parameter models like Llama-2 or Pythia)
- **Benchmarks**: Evaluate on standard LLM benchmarks:
  - **MMLU** (Massive Multitask Language Understanding)
  - **HellaSwag** (commonsense reasoning)
  - **HumanEval** (code generation, if relevant)
  - **Perplexity** on held-out text (e.g., WikiText-103)
- **Compression ratios**: Test at 2×, 4×, 8×, 16× compression
- **Metrics**: Plot accuracy vs. compression ratio; measure inference latency and memory footprint

**Success criterion**: Your method should dominate baselines on the Pareto frontier (better accuracy at same compression, or same accuracy at higher compression).

---

### **Experiment 2: Generalization Across Tasks**
**Hypothesis**: Compressed models retain task-specific performance better than generic compression.

**Protocol**:
- **Setup**: Compress a pretrained LLM, then fine-tune on downstream tasks
- **Tasks**: Choose 3–5 diverse tasks from GLUE, SuperGLUE, or domain-specific benchmarks (e.g., BioASQ for biomedical, CodeXGLUE for code)
- **Conditions**:
  - Compress before fine-tuning (static)
  - Compress after fine-tuning (task-aware)
  - Compress during fine-tuning (joint optimization)
- **Metrics**: Task accuracy, F1, or BLEU; compare to uncompressed fine-tuned baseline

**Success criterion**: Task-aware or joint compression should outperform static compression by ≥2% on at least 2/3 tasks.

---

### **Experiment 3: Ablation on Compression Components**
**Hypothesis**: Specific design choices in your compression method are critical to performance.

**Protocol**:
- **Identify 3–4 key design choices** in your method (e.g., layer-wise vs. global compression, learned vs. fixed thresholds, retraining budget)
- **Ablate each component**: Remove or replace it with a simpler alternative
- **Measure**: Accuracy and compression ratio for each ablation
- **Example ablations**:
  - Remove learned compression parameters → use fixed heuristics
  - Remove layer-wise adaptation → use uniform compression
  - Remove retraining → compress without fine-tuning

**Success criterion**: Each component should contribute ≥1% accuracy improvement or ≥10% better compression ratio.

---

## Additional Recommended Tests

- **Robustness**: Test on out-of-distribution data (e.g., compress on English, test on multilingual or domain-shifted text)
- **Efficiency**: Measure wall-clock inference time and memory usage on real hardware (GPU/CPU)
- **Scaling laws**: Does your method's advantage grow or shrink with model size?
- **Compression speed**: How long does the compression process itself take?

---

## Literature Anchors

1. **[A Survey on Model Compression for Large Language Models](https://arxiv.org/abs/2308.07633)** (Zhu et al., 2023) — Comprehensive taxonomy of compression methods
2. **[Compressing LLMs: The Truth is Rarely Pure and Never Simple](https://machinelearning.apple.com/research/compressing-llms)** (Apple ML Research, 2024) — Practical insights on real-world compression challenges

---

## Bottom Line

**Novelty is narrow but achievable.** You must clearly define what makes your compression approach different (e.g., adaptive, training-aware, task-specific) and demonstrate measurable advantages over strong baselines. Focus on **concrete metrics** (accuracy, latency, memory) across **multiple benchmarks** and **compression ratios** to make a convincing case.

---

## Sources
- [A Survey on Model Compression for Large Language Models](https://arxiv.org/abs/2308.07633) — arXiv, 2023
- [Compressing LLMs: The Truth is Rarely Pure and Never Simple](https://machinelearning.apple.com/research/compressing-llms) — Apple ML Research, 2024
- [EvoPress: Accurate Dynamic Model Compression via Evolutionary Search](https://arxiv.org/html/2410.14649v2) — arXiv, 2024
- [Training LLMs over Neurally Compressed Text](https://arxiv.org/html/2404.03626v2) — arXiv, 2024
- [Awesome-LLM-Compression GitHub Repository](https://github.com/HuangOwen/Awesome-LLM-Compression) — Curated list of compression papers