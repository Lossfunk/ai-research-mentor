## Novelty and Scope Assessment

**This is moderately novel with HIGH social impact, but you're right to be cautious—the field is active and growing.** AI hiring audits exist (academic research, regulatory efforts, journalism), but **rigorous, comprehensive audits with actionable recommendations are still relatively rare.** The key is finding a **specific angle** (demographic group, platform type, methodology, or intervention) that hasn't been thoroughly explored.

Let me help you assess novelty responsibly and scope this effectively.

---

## What Exists: AI Hiring Audits and Fairness Research

### **Current State of the Field**

#### **1. Academic Research on AI Hiring Bias**

**Existing work** (ACTIVE):

**Bias detection studies**:
- [Discrimination in Online Ad Delivery](https://arxiv.org/abs/1301.6822) (Sweeney, arXiv 2013) — Early work on algorithmic discrimination
- [Gender Shades](http://gendershades.org/) (Buolamwini & Gebru, 2018) — Facial recognition bias (relevant for video interviews)
- [Bias in Resume Screening](https://arxiv.org/abs/1906.09208) (Raghavan et al., arXiv 2019) — Automated resume screening
- [Auditing AI Hiring Tools](https://dl.acm.org/doi/10.1145/3351095.3372828) (Köchling & Wehner, CHI 2020) — Survey of bias in hiring AI
- [Algorithmic Hiring Discrimination](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3631137) (Hoffman et al., SSRN 2020) — Legal analysis

**Platform-specific audits**:
- [HireVue audit](https://www.eeoc.gov/newsroom/eeoc-releases-new-resource-artificial-intelligence-and-title-vii) (EEOC, 2023) — Video interview platform
- [LinkedIn job recommendations](https://arxiv.org/abs/2203.11672) (Sapiezynski et al., arXiv 2022) — Gender bias in job ads
- [Indeed resume screening](https://www.technologyreview.com/2021/07/07/1027916/ai-hiring-tools-biased-unproven-resume-scanners/) (MIT Tech Review, 2021) — Investigative journalism

**Methodology papers**:
- [Auditing Algorithms](https://dl.acm.org/doi/10.1145/3278721.3278723) (Sandvig et al., ICA 2014) — Audit study methods
- [Fairness Definitions Explained](https://fairware.cs.umass.edu/papers/Verma.pdf) (Verma & Rubin, 2018) — Fairness metrics
- [Causal Fairness Analysis](https://arxiv.org/abs/1706.02744) (Kusner et al., NeurIPS 2017) — Causal approaches to fairness

**Status**: ACTIVE field; 50-100 papers per year on algorithmic fairness in hiring

---

#### **2. Regulatory and Legal Landscape**

**U.S. regulations** (EMERGING):
- **EEOC guidance** (2023): [AI and Title VII](https://www.eeoc.gov/laws/guidance/select-issues-assessing-adverse-impact-software-algorithms-and-artificial) — Adverse impact analysis
- **NYC Local Law 144** (2023): Requires bias audits for automated employment decision tools (AEDT)
  - Annual audits by independent auditors
  - Disclosure of audit results
  - Notification to candidates
- **Illinois AI Video Interview Act** (2020): Consent, disclosure requirements for video interviews
- **California AB 1651** (proposed): Transparency in AI hiring

**EU regulations**:
- **AI Act** (2024): High-risk classification for hiring AI, conformity assessments required
- **GDPR** (2018): Right to explanation, data protection

**Status**: Rapidly evolving; compliance audits will be required

---

#### **3. Industry Audits and Certifications**

**Existing audit providers**:
- [O'Neil Risk Consulting & Algorithmic Auditing (ORCAA)](https://orcaarisk.com/) — Cathy O'Neil's firm
- [ForHumanity](https://forhumanity.center/) — Independent audit certification
- [Holistic AI](https://www.holisticai.com/) — AI governance and auditing
- [BABL AI](https://www.bablai.com/) — Bias testing for hiring platforms

**Platform self-audits**:
- HireVue, Pymetrics, Modern Hire publish fairness reports (varying quality)
- Often lack independent verification
- May use favorable metrics or limited demographic groups

**Status**: Growing industry; quality varies widely

---

#### **4. Investigative Journalism and Advocacy**

**Notable investigations**:
- [Amazon scraps secret AI recruiting tool](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G) (Reuters, 2018) — Gender bias in resume screening
- [AI hiring tools are biased](https://www.technologyreview.com/2021/07/07/1027916/ai-hiring-tools-biased-unproven-resume-scanners/) (MIT Tech Review, 2021)
- [HireVue's AI faces scrutiny](https://www.washingtonpost.com/technology/2019/11/06/hirevue-facial-analysis/) (Washington Post, 2019)

**Advocacy organizations**:
- [Algorithmic Justice League](https://www.ajl.org/) — Buolamwini's organization
- [AI Now Institute](https://ainowinstitute.org/) — Research and advocacy
- [Data & Society](https://datasociety.net/) — Research on tech and society

**Status**: Active; ongoing investigations

---

## Where Novelty Lies (and Doesn't)

### **NOT Novel (Well-Covered Areas)**:

1. **"AI hiring tools can be biased"** — Well-established; dozens of papers
2. **"Resume screening discriminates by race/gender"** — Documented extensively
3. **"Facial analysis in video interviews is problematic"** — Known issue (HireVue removed facial analysis in 2021)
4. **"General fairness metrics for hiring"** — Standard metrics exist (demographic parity, equalized odds, etc.)

### **POTENTIALLY Novel (Underexplored Areas)**:

#### **Option 1: Intersectional Bias Audits**

**Idea**: Audit for bias at intersections of protected characteristics (e.g., Black women, older disabled workers, LGBTQ+ immigrants).

**Why novel**:
- Most audits examine single dimensions (race OR gender, not race AND gender)
- Intersectional discrimination is documented but underaudited in AI systems
- Requires larger sample sizes and more sophisticated analysis

**Example research question**: "Do AI resume screening tools exhibit intersectional bias against Black women that is not captured by separate race and gender audits?"

**Relevant work**:
- [Intersectional Fairness](https://arxiv.org/abs/1807.08362) (Foulds et al., arXiv 2018) — Theoretical framework
- [Intersectionality in AI](https://dl.acm.org/doi/10.1145/3351095.3372861) (Scheuerman et al., CHI 2020) — General discussion
- **Gap**: Few empirical audits of hiring platforms

---

#### **Option 2: Longitudinal Impact Studies**

**Idea**: Study long-term effects of AI hiring on workforce diversity, not just immediate hiring decisions.

**Why novel**:
- Most audits are snapshots (does tool discriminate now?)
- Longitudinal effects underexplored: Do biased tools compound over time? Do they affect retention, promotion?
- Requires access to company data over months/years

**Example research question**: "Do companies using AI resume screening show declining workforce diversity over 2-3 years compared to companies using traditional hiring?"

**Relevant work**:
- [Long-term Fairness](https://arxiv.org/abs/1711.05144) (Liu et al., NeurIPS 2018) — Theoretical framework
- **Gap**: Empirical longitudinal studies of hiring platforms

---

#### **Option 3: Audits of Emerging Platforms/Technologies**

**Idea**: Audit newer platforms or technologies that haven't been studied yet.

**Why novel**:
- Most research focuses on established platforms (HireVue, LinkedIn, Indeed)
- New platforms emerge constantly (AI-powered applicant tracking systems, skills assessments, chatbot screeners)
- LLM-based hiring tools (GPT-4 for resume screening, interview analysis) are very new

**Example research question**: "Do LLM-based resume screening tools (GPT-4, Claude) exhibit different bias patterns than traditional ML-based tools?"

**Relevant work**:
- [Bias in Large Language Models](https://arxiv.org/abs/2108.07258) (Bender et al., arXiv 2021) — General LLM bias
- **Gap**: LLMs in hiring specifically (very new)

---

#### **Option 4: Audits from Candidate Perspective**

**Idea**: Study how candidates experience and respond to AI hiring tools (trust, anxiety, gaming, opt-out).

**Why novel**:
- Most audits focus on algorithmic fairness (statistical measures)
- Candidate experience underexplored: Do candidates trust AI? Does it cause anxiety? Do they try to game the system?
- Human-centered approach

**Example research question**: "How do candidates from different demographic groups perceive and respond to AI video interviews, and does this affect their performance?"

**Relevant work**:
- [Candidate Reactions to AI Hiring](https://www.sciencedirect.com/science/article/pii/S0747563220302958) (Langer et al., Computers in Human Behavior, 2020)
- [Algorithmic Anxiety](https://dl.acm.org/doi/10.1145/3359321) (Sannon et al., CSCW 2019)
- **Gap**: Large-scale empirical studies across platforms

---

#### **Option 5: Intervention and Mitigation Studies**

**Idea**: Test interventions to reduce bias in AI hiring (debiasing algorithms, human oversight, transparency, candidate appeals).

**Why novel**:
- Most research documents problems; fewer test solutions
- Interventions underexplored: What actually works to reduce bias?
- Actionable for practitioners

**Example research question**: "Does providing candidates with explanations of AI hiring decisions reduce perceived unfairness by >30% and improve diversity outcomes?"

**Relevant work**:
- [Debiasing Techniques](https://arxiv.org/abs/1908.09635) (Mehrabi et al., arXiv 2019) — General debiasing
- **Gap**: Empirical tests of interventions in real hiring contexts

---

#### **Option 6: Comparative Audits Across Platforms**

**Idea**: Systematically compare bias across multiple platforms using standardized methodology.

**Why novel**:
- Most audits study single platforms
- Comparative analysis rare: Which platforms are most/least biased? Do they fail in similar ways?
- Useful for employers choosing platforms

**Example research question**: "How do the top 5 AI resume screening platforms (Greenhouse, Lever, Workday, iCIMS, Taleo) compare on racial and gender bias using standardized test resumes?"

**Relevant work**:
- [Comparative Algorithm Audits](https://dl.acm.org/doi/10.1145/3351095.3372878) (Bandy & Diakopoulos, CHI 2020) — General framework
- **Gap**: Comprehensive comparison of hiring platforms

---

#### **Option 7: Audits of Specific Occupations or Industries**

**Idea**: Focus on specific occupations (tech, healthcare, finance) or industries where AI hiring is prevalent and bias is particularly harmful.

**Why novel**:
- Most audits are general or focus on white-collar jobs
- Occupation-specific bias underexplored: Does AI discriminate differently for nurses vs. engineers vs. warehouse workers?
- Industry context matters (regulations, norms, stakes)

**Example research question**: "Do AI hiring tools for healthcare positions exhibit age bias that could exacerbate nursing shortages?"

**Relevant work**:
- [Occupation-Specific Bias](https://arxiv.org/abs/2010.03133) (De-Arteaga et al., arXiv 2020) — Bias in occupation classification
- **Gap**: Audits of hiring tools for specific occupations

---

#### **Option 8: Global/Cross-Cultural Audits**

**Idea**: Audit AI hiring tools across different countries/cultures to understand how bias manifests in different contexts.

**Why novel**:
- Most research is U.S.-centric
- Cross-cultural bias underexplored: Do tools trained on U.S. data discriminate in other countries? Do cultural norms affect bias?
- Important for global platforms

**Example research question**: "Do U.S.-developed AI hiring tools exhibit different bias patterns when used in India, Brazil, and Nigeria?"

**Relevant work**:
- [Cross-Cultural AI Fairness](https://arxiv.org/abs/2203.07177) (Sambasivan et al., arXiv 2022) — General framework
- **Gap**: Cross-cultural audits of hiring platforms

---

## How to Check Novelty Responsibly

### **Step 1: Systematic Literature Review**

**Search databases**:
- **Academic**: Google Scholar, ACM Digital Library, IEEE Xplore, arXiv, SSRN
- **Keywords**: "AI hiring bias," "algorithmic hiring discrimination," "automated employment decision," "resume screening fairness," "video interview bias"
- **Venues**: FAccT, CHI, CSCW, AIES, NeurIPS (fairness track), ICML, KDD

**Search strategy**:
1. **Broad search**: "AI hiring" OR "algorithmic hiring" (get landscape)
2. **Specific search**: Your specific angle (e.g., "intersectional bias hiring" OR "LLM resume screening")
3. **Citation tracking**: Find key papers, check who cites them (forward) and who they cite (backward)
4. **Recent work**: Filter for last 2-3 years (field moves fast)

**What to look for**:
- **Exact overlap**: Has someone already done your exact study?
- **Similar methods**: What audit methods are standard?
- **Gaps**: What hasn't been studied?
- **Contradictions**: Are there conflicting findings you could resolve?

**Time investment**: 1-2 weeks for thorough review

---

### **Step 2: Check Regulatory and Industry Landscape**

**Regulatory sources**:
- **EEOC**: [AI and Title VII guidance](https://www.eeoc.gov/laws/guidance/select-issues-assessing-adverse-impact-software-algorithms-and-artificial)
- **NYC DCWP**: [Local Law 144 compliance](https://www.nyc.gov/site/dca/about/automated-employment-decision-tools.page)
- **EU AI Act**: [High-risk AI systems](https://artificialintelligenceact.eu/)

**Industry reports**:
- Platform fairness reports (HireVue, Pymetrics, etc.)
- Audit firm reports (ORCAA, Holistic AI, etc.)
- Consulting reports (Gartner, Forrester on HR tech)

**What to look for**:
- **Compliance requirements**: What audits are legally required?
- **Industry standards**: What metrics/methods are standard?
- **Gaps**: What do regulations require that isn't being done?

**Time investment**: 3-5 days

---

### **Step 3: Stakeholder Interviews**

**Who to talk to**:
- **Researchers**: Authors of key papers (email, ask for 20-min call)
- **Practitioners**: HR professionals, recruiters using AI tools
- **Advocates**: Algorithmic Justice League, AI Now, etc.
- **Regulators**: EEOC staff, NYC DCWP (if accessible)
- **Platform vendors**: HireVue, Pymetrics, etc. (may be guarded)

**Questions to ask**:
- "What are the biggest open questions in AI hiring fairness?"
- "What audits are most needed but not being done?"
- "What barriers exist to conducting audits?"
- "What would make an audit most useful to you?"

**What to look for**:
- **Practitioner needs**: What do employers/candidates actually need?
- **Research gaps**: What do experts say is understudied?
- **Feasibility**: What's actually doable given access, resources?

**Time investment**: 2-3 weeks (scheduling, interviews, synthesis)

---

### **Step 4: Preliminary Scoping**

**Define your audit scope**:

**Dimensions to specify**:
1. **Platform(s)**: Which hiring tools? (resume screening, video interviews, skills tests, chatbots)
2. **Demographic groups**: Which protected characteristics? (race, gender, age, disability, intersections)
3. **Fairness metrics**: Which definitions of fairness? (demographic parity, equalized odds, calibration)
4. **Methodology**: How will you audit? (correspondence study, algorithmic audit, user study)
5. **Outcome**: What are you measuring? (hiring decisions, rankings, scores, candidate experience)
6. **Context**: Which jobs/industries? (tech, healthcare, retail, etc.)

**Example scoping**:
- **Broad** (risky): "Audit all AI hiring tools for all types of bias"
- **Focused** (better): "Audit top 3 resume screening platforms for intersectional race-gender bias in tech hiring using correspondence study with 500 synthetic resumes"

**Novelty check**:
- Search for your specific scope: Has this exact audit been done?
- If yes: Can you extend it (more platforms, more demographics, better methods)?
- If no: Why not? (Is it infeasible, unimportant, or genuinely novel?)

---

### **Step 5: Assess Feasibility and Impact**

**Feasibility questions**:
1. **Access**: Can you access the platforms? (Public APIs, paid accounts, partnerships)
2. **Data**: Can you generate test data (synthetic resumes, profiles)? Do you need real candidate data?
3. **Resources**: Do you have budget for platform fees, research assistants, statistical analysis?
4. **Ethics**: Can you get IRB approval? Are there ethical concerns (deception, platform ToS violations)?
5. **Timeline**: Can you complete the audit in your timeframe (6 months, 1 year, 2 years)?

**Impact questions**:
1. **Academic**: Will this contribute new knowledge? Is it publishable in top venues (FAccT, CHI)?
2. **Practical**: Will employers, platforms, or regulators use your findings?
3. **Social**: Will this help reduce discrimination and improve equity?
4. **Policy**: Could this inform regulation or litigation?

**Red flags** (low feasibility or impact):
- Platform access is impossible (no API, requires enterprise contract)
- Exact study has been done recently by credible researchers
- Findings unlikely to change practice or policy
- Ethical concerns are insurmountable

**Green lights** (high feasibility and impact):
- Platform is accessible (public API or affordable account)
- Specific angle is novel and important
- Stakeholders express need for this audit
- Findings could inform regulation (e.g., NYC Local Law 144 compliance)

---

## Concrete Audit Designs (Examples)

Assuming you choose **Option 3 (LLM-Based Hiring Tools)**, here are three audit designs:

### **Audit 1: Correspondence Study of LLM Resume Screening**

**Research question**: Do LLM-based resume screening tools (GPT-4, Claude) exhibit racial and gender bias compared to traditional ML tools?

**Methodology**:
- **Correspondence study**: Submit synthetic resumes to platforms, measure outcomes
- **Resumes**: Create 400 synthetic resumes (200 pairs)
  - Vary: Name (signaling race/gender), education, experience (matched pairs)
  - Control: Qualifications, formatting, length
  - Example: "Jamal Washington" vs. "Brad Johnson" (same qualifications)
- **Platforms**:
  - LLM-based: Custom GPT-4/Claude resume screeners (via API or platforms using them)
  - Traditional ML: Greenhouse, Lever, Workday (if accessible)
  - Human baseline: Recruit human recruiters to screen same resumes
- **Outcomes**: 
  - Ranking/scoring (1-10 scale)
  - Binary decision (interview/reject)
  - Explanation quality (if provided)
- **Analysis**:
  - Compare callback rates by race/gender (chi-square test)
  - Regression: Outcome ~ Race + Gender + Qualifications + Platform
  - Effect sizes (odds ratios)
  - Intersectional analysis (race × gender)

**Metrics**:
- **Demographic parity**: P(Interview | Black) vs. P(Interview | White)
- **Equalized odds**: TPR and FPR by race/gender
- **Calibration**: Are scores predictive of actual job performance? (hard to measure)

**Success criterion**: Detect statistically significant bias (p < 0.05) or demonstrate lack of bias with sufficient power (>80% to detect medium effect).

**Feasibility**: MEDIUM-HIGH
- **Access**: GPT-4/Claude APIs are public ($0.01-0.03 per resume)
- **Cost**: $500-2000 for API calls + resume creation
- **Timeline**: 3-6 months
- **Ethics**: IRB approval needed; may violate platform ToS (check carefully)

---

### **Audit 2: Intersectional Bias in Video Interview Platforms**

**Research question**: Do AI video interview platforms exhibit intersectional bias against Black women that is not captured by separate race and gender audits?

**Methodology**:
- **Experimental study**: Record standardized video interviews with actors
- **Actors**: Recruit 16 actors (2 race × 2 gender × 4 replicates)
  - Black women, Black men, White women, White men
  - Matched on: Age, education, experience, script
- **Videos**: Each actor records same interview (standardized script, setting, lighting)
  - Control: Content, length, background, clothing
  - Vary: Only race and gender (via actor identity)
- **Platforms**: HireVue, Modern Hire, Spark Hire (if accessible)
- **Outcomes**:
  - Overall score (1-100)
  - Trait scores (communication, enthusiasm, competence, etc.)
  - Hiring recommendation (yes/no)
- **Analysis**:
  - ANOVA: Score ~ Race × Gender × Platform
  - Intersectional effects: Is Black women's score lower than predicted by race + gender main effects?
  - Qualitative: Analyze AI-generated feedback for bias

**Metrics**:
- **Additive model**: Score = β₀ + β₁(Black) + β₂(Female) + ε
- **Intersectional model**: Score = β₀ + β₁(Black) + β₂(Female) + β₃(Black × Female) + ε
- **Test**: Is β₃ significant? (Intersectional penalty beyond additive effects)

**Success criterion**: Detect intersectional bias (β₃ < 0, p < 0.05) or rule it out with sufficient power.

**Feasibility**: MEDIUM
- **Access**: Platforms may require enterprise accounts ($$$)
- **Cost**: $5k-15k (actor fees, platform fees, video production)
- **Timeline**: 6-9 months
- **Ethics**: IRB approval needed; actors must consent; platform ToS may prohibit

---

### **Audit 3: Longitudinal Impact of AI Hiring on Workforce Diversity**

**Research question**: Do companies using AI resume screening show declining workforce diversity over 2-3 years compared to companies using traditional hiring?

**Methodology**:
- **Quasi-experimental design**: Compare companies with/without AI hiring tools over time
- **Sample**: 50-100 companies (25-50 using AI, 25-50 not using AI)
  - Match on: Industry, size, location, baseline diversity
  - Data sources: LinkedIn, Glassdoor, company diversity reports, EEOC filings
- **Intervention**: Companies adopt AI resume screening (treatment) vs. continue traditional hiring (control)
- **Outcomes**:
  - Workforce diversity (% women, % racial minorities) over time
  - Hiring rates by demographic group
  - Retention rates by demographic group
- **Analysis**:
  - Difference-in-differences: (Diversity_post - Diversity_pre)_AI vs. (Diversity_post - Diversity_pre)_NoAI
  - Regression: Diversity ~ AI × Time + Company FE + Industry FE
  - Subgroup analysis: By company size, industry, baseline diversity

**Metrics**:
- **Diversity change**: Δ% women, Δ% racial minorities
- **Hiring rate ratio**: (Hires_minority / Applicants_minority) / (Hires_majority / Applicants_majority)

**Success criterion**: Detect significant diversity decline in AI-using companies (p < 0.05) or rule it out.

**Feasibility**: LOW-MEDIUM
- **Access**: Company data is hard to get (proprietary, privacy concerns)
- **Cost**: $10k-50k (data collection, research assistants, analysis)
- **Timeline**: 2-3 years (longitudinal data collection)
- **Ethics**: IRB approval; data privacy; company partnerships needed

---

## Practical Considerations

### **1. Platform Access**

**Challenges**:
- Many platforms require enterprise contracts ($10k-100k/year)
- APIs may be limited or unavailable
- Terms of Service may prohibit auditing

**Strategies**:
- **Public APIs**: Use platforms with public APIs (LinkedIn, Indeed job search)
- **Free trials**: Sign up for free trials or basic accounts
- **Partnerships**: Partner with companies using platforms (get access via them)
- **Reverse engineering**: Analyze platform behavior via user interface (risky, may violate ToS)
- **Synthetic platforms**: Build your own AI hiring tool, audit it (less impactful but feasible)

**Legal considerations**:
- **ToS violations**: Auditing may violate Terms of Service (legal gray area)
- **CFAA**: Computer Fraud and Abuse Act (U.S.) may apply to unauthorized access
- **Precedent**: [hiQ Labs v. LinkedIn](https://en.wikipedia.org/wiki/HiQ_Labs_v._LinkedIn) (scraping public data is legal, but evolving)
- **Recommendation**: Consult legal counsel before auditing

---

### **2. Synthetic Data Creation**

**Resume generation**:
- **Names**: Use names that signal race/gender (validated by prior research)
  - Example: Lakisha, Jamal (Black); Emily, Greg (White)
  - Source: [Bertrand & Mullainathan (2004)](https://www.aeaweb.org/articles?id=10.1257/0002828042002561)
- **Qualifications**: Match education, experience, skills across pairs
- **Formatting**: Use consistent templates (avoid confounds)
- **Realism**: Ensure resumes are plausible (real companies, dates, etc.)

**Video creation**:
- **Actors**: Recruit diverse actors, pay fairly ($50-200/hour)
- **Scripts**: Standardize content (same answers to same questions)
- **Production**: Control lighting, background, audio, camera angle
- **Validation**: Pilot test to ensure videos are equivalent except for actor identity

**Challenges**:
- **Realism**: Synthetic data may not reflect real applicants
- **Confounds**: Hard to control all variables (accent, appearance, etc.)
- **Ethics**: Deception (platforms don't know resumes are fake)

---

### **3. Fairness Metrics**

**Common metrics**:
1. **Demographic parity**: P(Hire | Group A) = P(Hire | Group B)
   - Pro: Simple, intuitive
   - Con: Ignores qualifications (may be unfair if groups differ in qualifications)

2. **Equalized odds**: TPR and FPR equal across groups
   - Pro: Accounts for qualifications (if ground truth available)
   - Con: Requires ground truth (hard to get in hiring)

3. **Calibration**: P(Qualified | Score = s, Group A) = P(Qualified | Score = s, Group B)
   - Pro: Ensures scores mean the same thing across groups
   - Con: Requires ground truth

4. **Counterfactual fairness**: Would outcome change if only protected attribute changed?
   - Pro: Causal interpretation
   - Con: Requires causal model (hard to specify)

**Recommendation**: Use multiple metrics (no single metric captures all fairness concerns)

**Challenges**:
- **Impossibility results**: Some metrics are mutually exclusive ([Chouldechova, 2017](https://arxiv.org/abs/1703.00056))
- **Ground truth**: Hard to know who is truly "qualified" in hiring
- **Legal standards**: EEOC uses "adverse impact" (4/5ths rule), not ML fairness metrics

---

### **4. Ethics and IRB**

**Ethical considerations**:
1. **Deception**: Submitting fake resumes/videos to platforms
   - **Justification**: Public interest research (exposing discrimination)
   - **Mitigation**: Minimize harm (don't waste recruiter time, withdraw applications)

2. **Privacy**: Collecting data on real candidates or companies
   - **Mitigation**: Anonymize data, get consent where possible

3. **Platform harm**: Audits may violate ToS, harm platform reputation
   - **Mitigation**: Responsible disclosure (notify platform before publication)

4. **Candidate harm**: Exposing bias may discourage candidates from applying
   - **Mitigation**: Provide actionable recommendations, not just criticism

**IRB approval**:
- **Required**: If research involves human subjects (candidates, recruiters, actors)
- **Exemptions**: Some audit studies may be exempt (minimal risk, public behavior)
- **Process**: Submit protocol to IRB, address concerns, get approval (2-6 months)

**Recommendation**: Consult IRB early, be transparent about methods and risks

---

### **5. Responsible Disclosure**

**Best practices**:
1. **Notify platforms**: Share findings before publication, give time to respond (30-90 days)
2. **Collaborate**: Offer to work with platforms to fix issues
3. **Transparency**: Publish methods, data (if possible), code
4. **Actionable recommendations**: Don't just criticize; suggest solutions
5. **Stakeholder engagement**: Share findings with regulators, advocates, affected communities

**Example**: [Gender Shades](http://gendershades.org/) notified IBM, Microsoft, Face++ before publication; companies improved their systems

---

## Novelty Score: 6.5/10 (MODERATE-HIGH)

**Why moderate-high**:
- AI hiring bias is well-documented (not novel)
- Specific angles are underexplored (intersectionality, LLMs, longitudinal, cross-cultural)
- Rigorous, comprehensive audits are still relatively rare
- Regulatory landscape creates demand for audits

**Why not higher**:
- Field is active (50-100 papers/year)
- Major platforms have been audited (HireVue, LinkedIn, Indeed)
- Methods are established (correspondence studies, algorithmic audits)

**Where novelty lies**:
- **Specific platforms/technologies**: LLMs, emerging platforms
- **Specific demographics**: Intersectional groups, disability, age
- **Specific contexts**: Occupations, industries, countries
- **Interventions**: Testing solutions, not just documenting problems
- **Longitudinal**: Long-term impacts, not just snapshots

---

## Feasibility Score: 6/10 (MODERATE)

**Breakdown**:
- **Platform access**: 5/10 (varies by platform; some accessible, some not)
- **Data creation**: 7/10 (synthetic resumes feasible; videos more expensive)
- **Analysis**: 8/10 (statistical methods well-established)
- **Ethics/legal**: 5/10 (IRB approval needed; ToS violations risky)
- **Impact**: 7/10 (high demand from regulators, employers, advocates)

**Main barriers**:
- Platform access (enterprise contracts, ToS restrictions)
- Cost (platform fees, actor fees, research assistants)
- Legal risks (ToS violations, CFAA)
- Timeline (IRB approval, data collection, analysis = 6-18 months)

---

## Risk Assessment

### **Technical Risks (LOW-MEDIUM)**:

1. **Platform changes**: Platforms may change algorithms during audit
   - **Mitigation**: Document platform versions, conduct audits quickly

2. **Confounds**: Hard to control all variables in synthetic data
   - **Mitigation**: Careful experimental design, sensitivity analyses

3. **Statistical power**: May not detect bias if sample size too small
   - **Mitigation**: Power analysis, ensure adequate sample size (>200 per group)

### **Legal/Ethical Risks (MEDIUM-HIGH)**:

1. **ToS violations**: Platforms may sue or ban you
   - **Mitigation**: Legal counsel, use public data where possible, responsible disclosure

2. **CFAA liability**: Unauthorized access could be criminal (unlikely but possible)
   - **Mitigation**: Avoid hacking, use public APIs, consult legal counsel

3. **IRB rejection**: IRB may not approve deception-based audits
   - **Mitigation**: Engage IRB early, justify public interest, minimize harm

### **Reputational Risks (LOW-MEDIUM)**:

1. **Platform backlash**: Platforms may criticize your methods or findings
   - **Mitigation**: Rigorous methods, transparent reporting, peer review

2. **Null results**: May not find bias (hard to publish)
   - **Mitigation**: Frame as "testing for bias" not "exposing bias"; null results are valuable

---

## My Recommendation

**This is a VALUABLE, MODERATELY NOVEL area with HIGH SOCIAL IMPACT.** Here's how to scope responsibly:

### **Phase 1: Scoping and Literature Review (1-2 months)**

**Goals**:
- Systematic literature review (what's been done?)
- Stakeholder interviews (what's needed?)
- Identify specific novel angle (intersectionality, LLMs, longitudinal, etc.)
- Assess feasibility (platform access, resources, ethics)

**Deliverables**:
- Literature review summary
- Scoping document (specific research questions, methods, timeline)
- Preliminary IRB consultation

**Decision point**: Is there a novel, feasible, impactful angle? If yes, proceed. If no, pivot.

---

### **Phase 2: Pilot Audit (3-6 months)**

**Goals**:
- Small-scale audit (1-2 platforms, 100-200 synthetic resumes/videos)
- Test methods, identify challenges
- Preliminary findings

**Deliverables**:
- Pilot data and analysis
- Methods refinement
- Workshop or short paper (e.g., FAccT workshop, AIES)

**Decision point**: Do methods work? Are findings interesting? If yes, scale up. If no, iterate or pivot.

---

### **Phase 3: Full Audit (6-12 months)**

**Goals**:
- Large-scale audit (3-5 platforms, 400-1000 synthetic resumes/videos)
- Rigorous analysis (multiple fairness metrics, subgroup analysis)
- Responsible disclosure to platforms
- Publication in top venue

**Deliverables**:
- Full audit report
- Conference/journal paper (FAccT, CHI, CSCW, Science, Nature)
- Policy brief for regulators
- Public dataset/code (if possible)

---

### **Phase 4: Impact and Dissemination (ongoing)**

**Goals**:
- Share findings with stakeholders (employers, platforms, regulators, advocates)
- Testify at hearings, submit public comments on regulations
- Collaborate with platforms on improvements
- Media outreach (if appropriate)

**Deliverables**:
- Policy recommendations
- Platform partnerships
- Media coverage
- Follow-up research

---

## Bottom Line

**There is room for innovation, but you must find a specific angle.** Success depends on:

1. **Specific focus**: Don't audit "AI hiring" generally; focus on specific platforms, demographics, or contexts
2. **Rigorous methods**: Use established audit methods (correspondence studies, algorithmic audits)
3. **Responsible approach**: IRB approval, legal counsel, responsible disclosure
4. **Actionable findings**: Provide recommendations, not just criticism
5. **Stakeholder engagement**: Work with employers, platforms, regulators, advocates

**Novelty**: 6.5/10 (moderate-high; specific angles are novel, but field is active)
**Feasibility**: 6/10 (moderate; platform access and legal risks are barriers)
**Impact**: 9/10 (very high; addresses critical social problem, informs regulation)

**My recommendation**: PURSUE THIS with careful scoping. Start with literature review and stakeholder interviews to identify a specific novel angle (I recommend **LLM-based hiring tools** or **intersectional bias**—both are timely and underexplored). Then conduct a pilot audit to test methods before scaling up. This is important work that can make a real difference.

---

## Literature Anchors

1. **[Discrimination in Online Ad Delivery](https://arxiv.org/abs/1301.6822)** (Sweeney, arXiv 2013) — Early algorithmic discrimination work
2. **[Auditing Algorithms: Research Methods for Detecting Discrimination](https://dl.acm.org/doi/10.1145/3278721.3278723)** (Sandvig et al., ICA 2014) — Audit methodology
3. **[Gender Shades: Intersectional Accuracy Disparities](http://gendershades.org/)** (Buolamwini & Gebru, 2018) — Intersectional bias in facial recognition
4. **[Bias in Resume Screening Algorithms](https://arxiv.org/abs/1906.09208)** (Raghavan et al., arXiv 2019) — Resume screening bias
5. **[EEOC Guidance on AI and Title VII](https://www.eeoc.gov/laws/guidance/select-issues-assessing-adverse-impact-software-algorithms-and-artificial)** (EEOC, 2023) — Regulatory framework
6. **[NYC Local Law 144](https://www.nyc.gov/site/dca/about/automated-employment-decision-tools.page)** (NYC DCWP, 2023) — Bias audit requirements

---

## Sources
- [EEOC AI Guidance](https://www.eeoc.gov/laws/guidance/select-issues-assessing-adverse-impact-software-algorithms-and-artificial) — U.S. EEOC, 2023
- [NYC Local Law 144](https://www.nyc.gov/site/dca/about/automated-employment-decision-tools.page) — NYC Department of Consumer and Worker Protection, 2023
- [Gender Shades](http://gendershades.org/) — Buolamwini & Gebru, 2018
- [Auditing Algorithms](https://dl.acm.org/doi/10.1145/3278721.3278723) — Sandvig et al., ICA 2014
- [Bias in Resume Screening](https://arxiv.org/abs/1906.09208) — Raghavan et al., arXiv 2019
- [Intersectional Fairness](https://arxiv.org/abs/1807.08362) — Foulds et al., arXiv 2018
- [FAccT Conference](https://facctconference.org/) — ACM Conference on Fairness, Accountability, and Transparency