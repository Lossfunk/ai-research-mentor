[Turn 0] SYSTEM:
You're preparing to write your first conference paper and need help structuring the narrative, deciding what results to include, and understanding how to frame your contributions clearly.

[Turn 1] USER:
How do I decide which of my scattered experiments is worth turning into the paper’s core result?

[Turn 1] ASSISTANT:
Here’s a fast, evidence-based way to choose the core result when you have many scattered experiments.

A. Triage each experiment in one pass
- One-sentence claim: Can you state the central, testable claim in one sentence that could title a figure? If not, drop it from core consideration. Clear, single-message papers land better [1].
- Importance and novelty (Heilmeier test): For each experiment, answer: What are you trying to do? What’s new? Who cares? If it works, what difference will it make? How is it measured? If the answers are weak or vague, it’s not a core result [2].
- Evidential strength: Prefer experiments with (a) larger, precise effects (narrow CIs), (b) few researcher degrees of freedom, and (c) no post hoc story crafting. Undisclosed flexibility and HARKing inflate false positives; demote results that relied on many contingent choices or post hoc hypothesizing [3][4].
- Robustness to analysis choices: Run a quick multiverse/specification-curve check across plausible analysis decisions; choose results whose sign and magnitude are stable across most specifications [5].
- Falsifiability and negative evidence: Can the claim survive a targeted attempt to disconfirm it (e.g., negative controls, placebo outcomes, near-zero predicted conditions)? Prefer claims that have already passed such checks or can quickly be stress-tested [6][7].
- Communicability and buildability: Can the result be shown in one compelling figure; can others reproduce it without heavy bespoke tooling; and does it enable 3–5 obvious follow-up tests? Results that “scale” into a story and a program are better anchors [1][7][8].

B. Quick scoring rubric (pick the top one)
Score each candidate 1–5 on: (1) Clarity of core claim [1]; (2) Impact/novelty (Heilmeier) [2]; (3) Evidential strength and preregistration/analysis discipline [3][4]; (4) Robustness across specifications [5]; (5) Reproducibility/communicability [1][7]. Sum and pick the highest-scoring experiment as the provisional core. Use the runner-up as a robustness or ablation figure.

C. Before you commit: three fast falsification gates
Run these lightweight checks on the provisional core result. If any fail, switch to your runner-up.

1) Preregistered replication with a single primary endpoint
- Design: Same protocol, 1.5–2× sample size, preregister one primary outcome and analysis.
- Criterion to pass: The effect estimate remains within a pre-specified practical equivalence bound (e.g., |d| ≥ d_min) and the 90% CI excludes the equivalence region (TOST), or the estimate is consistent (directionally same, magnitude within, say, 70–130% of original) [6].
- Why: Guards against flexibility and HARKing; increases precision [3][4][6].

2) Multiverse/specification-curve robustness audit
- Design: Predefine a set of reasonable choices (e.g., 3–5 data filters × 2–3 model specs × 2–3 outcome definitions). Fit all cells and plot the distribution of effects.
- Criterion to pass: ≥80% of specifications preserve the effect’s sign and a majority remain practically meaningful (above d_min or policy-relevant threshold) [5].
- Why: Shows the claim isn’t propped up by one analytic path [5].

3) Stress tests with negative controls and placebo outcomes
- Design: Include outcomes the treatment should not affect (placebo), and/or exposures that should not affect the outcome (negative controls).
- Criterion to pass: No material effect on placebo outcomes; any signal there is grounds for confounding scrutiny [7].
- Why: Detects hidden biases and researcher degrees of freedom leaking into results [7].

Optional: Out-of-domain generalization check
- Design: Replicate in a distinct dataset, lab/site, or modality with minimal changes.
- Criterion to pass: Same qualitative effect; point estimate within a pre-specified tolerance band.
- Why: Supports external validity and a stronger “core” story [7].

D. How this maps to the paper
- Title and abstract follow the one-sentence claim [1].
- Figure 1: The core effect with CIs; prereg registered primary outcome [6].
- Figure 2: Specification curve/multiverse robustness [5].
- Figure 3: Stress tests (negative controls; key ablation) [7].
- Methods: Document analytic choices and guardrails to avoid undisclosed flexibility [3][4].
- Discussion: Bound the claim; explain what’s new and who should care (Heilmeier) [2].

Two literature anchors to guide your choices
- Mensh & Kording, Ten simple rules for structuring papers (clarity, single message) [1].
- Steegen et al., Increasing Transparency Through a Multiverse Analysis (robustness to analytic choices) [5].

Key takeaways
- Pick the result that you can state crisply, that matters to someone, and that survives preregistered replication, multiverse robustness, and negative-control stress tests.
- Use the runner-up experiment as a robustness/ablation figure to strengthen the story.
- Document choices to avoid undisclosed flexibility and HARKing so the core result is credible.

Citations
[1] Ten simple rules for structuring papers — https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005619
[2] Crafting Better Proposals Using the Heilmeier Catechism — https://www.depts.ttu.edu/research/ordc/Resources/heilmeier-catechism.php
[3] Simmons, Nelson, Simonsohn (2011). False-Positive Psychology (PDF) — https://springerlab.org/qmbc/files/Simmons2011.pdf
[4] Kerr (1998). HARKing: Hypothesizing After the Results are Known (PDF) — https://www2.psych.ubc.ca/~schaller/528Readings/Kerr1998.pdf
[5] Steegen et al. (2016). Increasing Transparency Through a Multiverse Analysis — https://journals.sagepub.com/doi/10.1177/1745691616658637
[6] Lakens (2017). Equivalence Tests — https://journals.sagepub.com/doi/10.1177/1948550617697177
[7] Munafò et al. (2017). A manifesto for reproducible science — https://pubmed.ncbi.nlm.nih.gov/33954258/
[8] Jeff Leek. The Elements of Data Analytic Style — https://worldpece.org/archive/artifact-pdf-document/elements-data-analytic-style

[Turn 2] USER:
If my top-scoring experiment is a negative result (no effect), can it still serve as the paper’s core storyline?

[Turn 2] ASSISTANT:
Short answer: yes—if you can show it is an informative null (evidence for no practically meaningful effect), not just absence of evidence. That can anchor a compelling paper when (a) the claim you rule out is theoretically or practically important, (b) your design has assay sensitivity (manipulations/measurements work), and (c) your estimate is precise enough to exclude meaningful effects using appropriate tests [1][2][4][5].

How to tell if your null can be the core
- Define a smallest effect size of interest (SESOI) and test equivalence: Use TOST or related equivalence tests to show your effect lies within ±SESOI with adequate power; this turns “no effect” into “evidence of no meaningful effect” [1].  
- Quantify evidence for the null: Complement equivalence tests with Bayes factors to show data favor H0 over H1 (e.g., BF01 > 3 or > 10, preregistered), making the inference explicit [2].  
- Assay sensitivity: Include manipulation/attention checks or a positive-control condition that is known to produce an effect so a null on your target outcome is interpretable (i.e., not a failed manipulation) [6].  
- Precision and power: Report the minimal detectable effect and achieved precision; a tight CI fully inside ±SESOI is far more persuasive than a wide CI that includes meaningful effects [4].  
- Credibility and transparency: Preregistration or a Registered Report strengthens a null-result narrative and avoids hindsight flexibility concerns [3].  
- Why it matters: Clear, well-powered, theory-motivated nulls advance science by pruning hypotheses and saving resources for others [5].

Three concrete, falsifiable experiments you can run now
1) Preregistered equivalence replication  
- Design: Same protocol, 1.5–2× N. Preregister one primary endpoint, an SESOI, and a TOST equivalence test.  
- Pass/fail: Pass if the 90% CI of the effect lies wholly within ±SESOI and both one-sided tests reject differences beyond the bounds [1].  
- What it shows: Evidence that any effect, if present, is too small to matter.

2) Bayes-factor confirmation with a minimally informative alternative  
- Design: Replicate with the same materials; preregister BF thresholds (e.g., BF01 ≥ 6 for H0), a sensible H1 (e.g., half-normal centered at 0 with SD = prior plausible effect), and a max N with optional stopping per BF rules.  
- Pass/fail: Pass if BF01 meets threshold; otherwise continue to max N or stop for H1 if BF10 crosses threshold [2].  
- What it shows: Direct evidence favoring the null over the alternative.

3) Assay-sensitivity/positive-control test  
- Design: Add a condition (or secondary outcome) known to produce a robust effect (e.g., canonical manipulation or benchmark model).  
- Pass/fail: Positive control shows the expected effect; target outcome remains equivalent to zero (TOST) [6][1].  
- What it shows: The null is not due to a broken manipulation, measurement, or sample.

Optional fourth test (helps generality)
4) Boundary-search generalization  
- Design: Vary one theoretically relevant dimension (dose/intensity/population) within feasible scope; repeat equivalence and BF analyses.  
- Pass/fail: Either you find where the effect emerges (boundary condition) or you extend the null across contexts with equivalence evidence [1][2].  
- What it shows: Where the theory holds or fails, informing revisions.

How to write the paper around a null
- Title/abstract: “X does not improve Y beyond ±δ under Z conditions” (state SESOI and context).  
- Figure 1: Core estimate with 90% CI fully inside ±SESOI (TOST) [1].  
- Figure 2: Bayes factors across replications (a BF-curve or sequential path) [2].  
- Figure 3: Assay sensitivity (positive control) + manipulation checks [6].  
- Methods: Preregistration/Registered Report details and power/precision rationale [3][4].  
- Discussion: Why ruling out ≥δ matters; implications for theory/practice and what to test next [5].

Two literature anchors
- Lakens (2017), Equivalence Tests — a practical framework to claim “no meaningful effect” rather than “no significant effect” [1].  
- Dienes (2014), Using Bayes to get the most out of non-significant results — how to quantify evidence for the null with Bayes factors [2].

Key takeaways
- A negative result can be your core when you show assay sensitivity and deliver precise, preregistered evidence that effects larger than your SESOI are unlikely (TOST and/or Bayes) [1][2][3][4].  
- Make it useful: specify δ, document power/precision, and demonstrate robustness, so others can stop chasing an effect that likely isn’t there [4][5][6].

Sources
[1] Equivalence Tests — https://journals.sagepub.com/doi/10.1177/1948550617697177  
[2] Dienes (2014). Using Bayes to get the most out of non-significant results — https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2014.00781/full  
[3] The past, present and future of Registered Reports — https://www.nature.com/articles/s41562-021-01193-7  
[4] Power failure: why small sample size undermines the reliability of neuroscience — https://pubmed.ncbi.nlm.nih.gov/23571845/  
[5] The importance of no evidence — https://www.nature.com/articles/s41562-019-0569-7  
[6] A manifesto for reproducible science — https://pubmed.ncbi.nlm.nih.gov/33954258/

[Turn 3] USER:
How do I weave the negative-result storyline into a coherent outline so each section still feels like it’s building toward something, not just reporting “nothing happened”?

[Turn 3] ASSISTANT:
Yes—treat the null as the thing you discovered: tight bounds on an effect that many expected, plus where it might still appear. Below is a concrete outline and writing templates that make each section build toward that contribution.

Narrative spine (what you’re “building toward”)
- The question: Does X meaningfully change Y under Z?
- The bar that matters: A smallest effect size of interest (SESOI, δ) that would matter in this context.
- The evidence: Assay sensitivity (positive control works), precise primary estimate, equivalence test, and/or Bayes factor favoring the null, plus robustness checks.
- The payoff: We rule out effects ≥ δ under Z; we map boundary conditions where the effect might still occur.

Section-by-section outline with roles and templates
1) Title and abstract: State the bounded claim and why it matters
- Title: “X does not improve Y beyond ±δ under Z conditions”
- Abstract arc:
  - Context/importance: Why δ matters (theory, decision, or cost).
  - Pre-commitment: Preregistered SESOI and primary analysis (if applicable).
  - Assay sensitivity: Positive control/manipulation checks succeed.
  - Core result: 90% CI lies within ±δ (TOST); Bayes factor favors H0; robustness holds.
  - Implication: What readers can stop doing or how theory must change.
- Template sentence: “We defined a smallest effect size of interest of δ = … based on … and preregistered an equivalence test. The 90% CI for the X→Y effect was [L, U], entirely within ±δ, and both one-sided tests rejected effects beyond ±δ; a Bayes factor of BF01 = … provided additional support for H0.”

2) Introduction: Set up a falsifiable expectation and the contribution
- Establish the live hypothesis and stakes (who cares, why now).
- Define δ explicitly and justify it (decision threshold, prior meta-analytic range, cost-benefit).
- Make concrete predictions: H1: |effect| ≥ δ vs H0: |effect| < δ.
- Promise the contribution: “We bound the effect below ±δ under Z, show assay sensitivity, and map boundary conditions.”
- Template: “Under widely used assumptions A–B, X is predicted to change Y by ≥ δ. We test that specific claim under Z and report bounds precise enough to inform theory and practice.”

3) Methods: Show assay sensitivity and the power/precision to be informative
- Design: Sample size for TOST at δ, measurement reliability, preregistration/primary endpoint, blinding/randomization.
- Assay sensitivity: Positive control or benchmark model expected to show a known effect; manipulation/attention checks; measurement invariance as needed.
- Analysis plan: Equivalence bounds, covariates, handling of missing data, and a limited multiverse/specification-curve grid pre-specified.
- Template: “We powered the study (1−β = 0.90) to declare equivalence at ±δ = … using TOST. A positive control (…→…) was included to verify assay sensitivity.”

4) Results: Lead with capability, then deliver the bound, then robustness
- Capability first: Positive control succeeds; manipulation checks pass.
- Core estimate: Show the primary model once (point, CI, equivalence test decision; Bayes factor optional).
- Robustness: Specification-curve/multiverse and negative/placebo outcomes; show that conclusions don’t depend on one analytic path.
- Boundary search (if run): Where the effect might appear (dose, population).
- Template: “Primary effect: b = …, 90% CI […, …] entirely within ±δ; TOST p_left < .05 and p_right < .05. Bayes factor BF01 = … indicates moderate/strong evidence for H0. Positive control: d = …, 95% CI […, …]. Robustness: 87% of specifications retained equivalence; placebo outcomes showed no effect.”

5) Discussion: Translate the bound into theory and practice
- What we ruled out: “Under Z, effects ≥ δ are unlikely.”
- Reconciliation: Why previous claims might differ (context, measurement, publication bias).
- Implications: Resource savings, redesign of interventions, or theory update.
- Boundaries and limitations: Where effects might still exist (other δ, contexts, populations); what you did not test.
- Concrete next steps: A short agenda (see experiments below).
- Template: “These results constrain models predicting ≥ δ under Z. Effects below δ could still matter in contexts with different utility; we outline when to reassess δ and how.”

6) Figures that tell a story at a glance
- Fig 1: Assay sensitivity (positive control) and manipulation checks.
- Fig 2: Primary effect with equivalence bounds and 90% CI; add Bayes factor inset.
- Fig 3: Specification-curve/multiverse; proportion of specs inside ±δ.
- Fig 4: Boundary-condition panel (dose/population/modality), if applicable.

Three-plus concrete, falsifiable experiments to strengthen the storyline
1) Preregistered equivalence replication
- Design: Same protocol; 1.5–2× N; preregister δ and TOST as the single primary analysis.
- Pass/fail: Pass if both TOST one-sided tests reject differences outside ±δ and the 90% CI lies within ±δ.
- Contribution: Confirms the bound isn’t a fluke and tightens precision (Lakens, 2017).

2) Assay-sensitivity positive control
- Design: Include a condition/outcome known to respond (canonical manipulation; benchmark model). Keep materials and analysis identical.
- Pass/fail: Positive control shows expected effect; target outcome remains equivalent to zero by TOST.
- Contribution: Shows “nothing happened” is not due to a broken assay; strengthens interpretability.

3) Specification-curve/multiverse audit
- Design: Predefine a grid of reasonable choices (e.g., 3 data filters × 3 outcome definitions × 2 model families); run them all.
- Pass/fail: ≥80% of specifications meet equivalence; no meaningful effects on placebo outcomes.
- Contribution: Demonstrates the conclusion isn’t path-dependent (Steegen et al., 2016).

4) Boundary-condition search (optional but powerful)
- Design: Vary dose/intensity, population, or timing along a theoretically motivated axis; repeat equivalence/BF analyses at each level.
- Pass/fail: Either locate a threshold where effects exceed δ, or extend the equivalence region across contexts.
- Contribution: Turns a null into a map of where an effect can/can’t be expected.

Practical writing tips to keep momentum
- Signal the destination early: “We bound the effect below ±δ” appears in the first paragraph of the abstract and end of the introduction.
- Use δ consistently: Put δ in the title, figures, and captions so readers see the bar you cleared.
- Lead with capability: Always show the positive control/manipulation checks before the null; it reassures readers the assay worked.
- Plot for decisions: Show equivalence bounds and 90% CI; for readers who prefer Bayesian framing, add a BF panel.
- Avoid hedging language: Replace “no significant effect” with “precision rules out effects ≥ δ under Z.”

Common pitfalls (and fixes)
- Wide CIs that still include meaningful effects: Increase N or improve measurement reliability before writing the paper.
- Missing δ justification: Tie δ to theory or decision thresholds; avoid arbitrary small numbers.
- No assay sensitivity: Add a positive control or benchmark; otherwise reviewers may interpret a null as a failed manipulation.
- Overlong robustness: Predefine a compact, principled multiverse; visualize clearly; keep narrative focused on the primary bound.

Key takeaways
- You are building toward a decision-relevant bound, not “nothing happened.”
- Make the bar explicit (δ), show the assay works, deliver a precise equivalence result, and demonstrate robustness.
- Frame the contribution as pruning theory and redirecting effort, with clear boundaries for where effects might still appear.

Literature anchors
- Lakens (2017). Equivalence Tests: A practical primer on using TOST and reporting 90% CIs for informative nulls. https://journals.sagepub.com/doi/10.1177/1948550617697177
- Steegen et al. (2016). Increasing Transparency Through a Multiverse Analysis: How to display robustness to analytic choices. https://journals.sagepub.com/doi/10.1177/1745691616658637
- Optional for Bayesian framing: Dienes (2014). Using Bayes to get the most out of non-significant results. https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00781/full

Sources
- Lakens, D. (2017). Equivalence Tests. Social Psychological and Personality Science. https://journals.sagepub.com/doi/10.1177/1948550617697177
- Steegen, S., et al. (2016). Increasing Transparency Through a Multiverse Analysis. Perspectives on Psychological Science. https://journals.sagepub.com/doi/10.1177/1745691616658637
- Dienes, Z. (2014). Using Bayes to get the most out of non-significant results. Frontiers in Psychology. https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00781/full
- Mensh, B., & Kording, K. (2017). Ten simple rules for structuring papers. PLoS Computational Biology. https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005619
- Munafò, M. R., et al. (2017). A manifesto for reproducible science. Nature Human Behaviour. https://www.nature.com/articles/s41562-016-0021

[Turn 4] USER:
Which experiment should anchor the Results section first—the preregistered equivalence replication or the positive-control assay?

[Turn 4] ASSISTANT:
Short answer: Open with the positive-control assay to establish assay sensitivity, then immediately anchor the Results with the preregistered equivalence replication as the primary result. The positive control reassures readers your assay can detect effects; the preregistered equivalence test delivers the core contribution (tight bounds on a meaningful effect).

Why this order works
- Assay sensitivity first: A brief, successful positive control (or manipulation check) prevents readers from attributing your null to a broken manipulation or measurement [ICH E10]. 
- Core outcome centered: The preregistered primary analysis (equivalence/TOST, plus CI and optionally Bayes factor) is your main finding and should be the anchor figure and headline estimate [Lakens, 2017].

Concrete structure you can use
- Results opening paragraph (2–4 sentences):
  - “Assay sensitivity: The positive control produced the expected effect (estimate and CI).”
  - “Primary preregistered test: The X→Y effect lay within ±δ (TOST: both one-sided tests significant; 90% CI entirely within bounds; optional BF01).”
  - “Robustness: Brief pointer to specification-curve/negative controls.”
- Figure 1 layout:
  - Panel A: Positive control/manipulation checks (short).
  - Panel B: Primary equivalence result with ±δ bounds and 90% CI (anchor).
  - Panel C: Optional Bayes factor/sequential-evidence panel.
  - Panel D: Compact specification-curve proportion inside ±δ.

When to deviate
- Registered Reports or hypothesis-first venues: Lead the text with the preregistered primary endpoint, but still show a one-panel positive control first or alongside in Figure 1A to establish assay sensitivity early.
- If the positive control is complex: Summarize one key metric in the main text/panel; move details to Methods/Supplement to avoid overshadowing the core result.

Three concrete, falsifiable checks to include (keep them tight)
- Positive-control capability gate
  - Design: Include a benchmark manipulation/outcome known to respond.
  - Pass/fail: Expected effect observed (e.g., CI excludes zero and exceeds a minimal meaningful size).
- Preregistered equivalence test (anchor)
  - Design: Predefine SESOI ±δ and use TOST with 90% CI; report exact CI and decisions.
  - Pass/fail: Both one-sided tests reject differences outside ±δ; CI entirely within ±δ.
- Placebo/negative-control outcomes
  - Design: Outcomes or exposures that should not change under your treatment.
  - Pass/fail: No material effects; any signal triggers confounding scrutiny.

Bottom line
- Lead with capability (positive control) to validate the assay, but center the preregistered equivalence result as the first, main figure and narrative anchor.

Citations
- Lakens (2017). Equivalence Tests — https://journals.sagepub.com/doi/10.1177/1948550617697177
- ICH E10: Choice of Control Group and Related Issues in Clinical Trials (assay sensitivity) — https://database.ich.org/sites/default/files/E10_Guideline.pdf

[Turn 5] USER:
Should I still label the positive-control figure as "Figure 1" even if the equivalence test is the real headline?

[Turn 5] ASSISTANT:
Recommended: make Figure 1 a composite and anchor it on the equivalence test. Put the positive control as panel 1A (assay sensitivity), and the preregistered equivalence result as 1B (headline). This lets readers see the assay works before they read the core bound, while ensuring the main claim is what Figure 1 emphasizes.

When to choose each layout
- Composite Figure 1 (1A positive control, 1B equivalence test): Default. Keeps capability upfront and centers the preregistered primary endpoint as the headline.
- Equivalence-only Figure 1; positive control as Extended Data/Supplement: Use if the positive control is routine and risks distracting from the main claim. Still mention in the first Results paragraph that the positive control passed and point to the supplement.
- Exception—venue conventions: Some journals require specific figures (e.g., CONSORT flow diagram as Fig. 1 in clinical trials). If so, renumber accordingly and make your equivalence/assay panel Figure 2; still open the Results text with assay sensitivity, then the primary equivalence result.

Practical tips
- Caption the hierarchy explicitly: “Figure 1. Assay sensitivity (A) and primary preregistered equivalence test (B).”
- Visual emphasis: Make panel B larger or more prominent; draw ±δ bounds and the 90% CI; put the panel letter in the text early (“Fig. 1A shows capability; Fig. 1B shows the primary bound”).
- Keep the positive-control panel concise (one key metric). Move its details to Methods/Supplement.

Bottom line
- You don’t need to label a standalone positive-control plot as “Figure 1.” Prefer a single Figure 1 that starts with assay sensitivity but clearly headlines the preregistered equivalence result.

Sources
- Lakens, D. (2017). Equivalence Tests. https://journals.sagepub.com/doi/10.1177/1948550617697177
- ICH E10: Choice of Control Group and Related Issues in Clinical Trials (assay sensitivity). https://database.ich.org/sites/default/files/E10_Guideline.pdf
- Mensh, B., & Kording, K. (2017). Ten simple rules for structuring papers. https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005619
- CONSORT 2010 Statement (flow diagram conventions for clinical trials). http://www.consort-statement.org/consort-2010

[Turn 6] USER:
How do I decide which of my scattered experiments is worth turning into the paper’s core result?

[Turn 6] ASSISTANT:
Here’s a fast, defensible way to choose the core result from scattered experiments and turn the runner‑ups into support.

Step 1 — 10‑minute triage per experiment
- One‑sentence claim: Can you state the experiment’s testable claim as a single figure title? If not, demote it (clarity matters for a coherent paper).
- Importance/novelty: Who cares if this claim is true or false? What changes if it holds? If you can’t name a real stakeholder or theory consequence, demote it.
- Evidential strength: Prefer results with larger, precise effects (narrow CIs), minimal analytic flexibility, and a prespecified primary endpoint. Demote results that required many contingent choices to “work.”
- Robustness: Quick specification‑curve check (a few reasonable data filters × model specs × outcome definitions). Favor results whose sign and practical meaning are stable across most specifications.
- Reproducibility/communicability: Can the result be shown in one figure, reproduced by others without bespoke tooling, and does it naturally suggest 3–5 follow‑ups? If yes, promote it.

Step 2 — Simple 5‑criterion score (1–5 each; pick the top one)
- Clarity of the core claim
- Impact/novelty (theory or decision relevance)
- Evidential strength (effect size, CIs, prereg/discipline)
- Robustness across plausible specifications
- Reproducibility/communicability (easy to show and build on)
Sum scores; make the winner your provisional core. Keep the runner‑up as a key robustness/ablation figure.

Step 3 — Before you commit: run three falsifiable gates
1) Preregistered replication (primary endpoint)
- Design: Same protocol; preregister one primary outcome/analysis; increase N by ~1.5–2× for precision.
- Pass/fail: Effect estimate consistent with the original (direction and within a pre‑specified tolerance band), or (for suspected nulls) 90% CI within a smallest effect size of interest (SESOI) using TOST.
- Outcome: Confirms the effect isn’t a fluke and tightens uncertainty.

2) Compact specification‑curve/multiverse audit
- Design: Define a grid of reasonable choices (e.g., 3 data filters × 2–3 model specs × 2–3 outcome definitions). Fit them all.
- Pass/fail: ≥80% of specifications preserve the effect’s sign and remain practically meaningful (above your SESOI or policy threshold).
- Outcome: Shows the claim doesn’t depend on a single analytic path.

3) Negative controls and placebo outcomes
- Design: Include outcomes the treatment should not affect, and/or exposures that should not affect the outcome.
- Pass/fail: No material effects on placebo outcomes or from negative‑control exposures.
- Outcome: Red flags hidden confounding or researcher degrees of freedom.

Optional strengthening experiment (use if feasible)
4) Out‑of‑domain generalization
- Design: Replicate in a distinct dataset, site, or modality with minimal changes.
- Pass/fail: Same qualitative effect; point estimate within a pre‑specified tolerance band.
- Outcome: Supports external validity for a stronger “core” story.

How to use the decision
- If the provisional core passes the three gates, it anchors Figure 1 and the Results opening. Use the runner‑up as robustness/ablation or boundary condition.
- If it fails any gate, switch to the runner‑up and rerun the gates on it.

Key takeaways
- Prioritize the experiment with a crisp, decision‑relevant claim that remains strong under replication, robustness checks, and negative controls.
- Promote what you can show in one compelling figure and that others can reproduce; demote fragile, path‑dependent results.
- Use runner‑ups strategically: ablations, stress tests, or boundary conditions to strengthen the core story.

Three concrete, falsifiable experiments to run now (templates)
- Preregistered replication: Single primary endpoint; 1.5–2× N; predefined tolerance band or SESOI; pass if estimate is consistent or CI lies within ±SESOI.
- Specification‑curve audit: Predefine reasonable choices; run all; pass if ≥80% of specs preserve the sign and practical magnitude; visualize the distribution of effects.
- Negative‑control/placebo checks: Add outcomes/exposures that should not change; pass if effects are null there; any signal triggers scrutiny or redesign.

Literature anchors
- Ten simple rules for structuring papers — clarity and single‑message focus: Mensh & Kording, 2017. https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005619
- Increasing Transparency Through a Multiverse Analysis — robustness to analytic choices: Steegen et al., 2016. https://journals.sagepub.com/doi/10.1177/1745691616658637

Sources
- Mensh, B., & Kording, K. (2017). Ten simple rules for structuring papers. https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005619
- Steegen, S., et al. (2016). Increasing Transparency Through a Multiverse Analysis. https://journals.sagepub.com/doi/10.1177/1745691616658637

[Turn 7] USER:
Should I run the triage on every pilot I’ve ever done, or only the ones that already look “publishable”?

[Turn 7] ASSISTANT:
Short answer: Don’t deep‑triage every pilot. Do a light screen on all pilots to avoid “file‑drawer” bias, then run the full triage only on the few that clear a clear, decision‑relevant bar.

Why this split works
- Light screen for all pilots reduces hindsight bias and keeps you from overlooking an informative null just because it looks “unpublishable.” Some “ugly” pilots become strong cores once you define a smallest effect size of interest (SESOI) and show equivalence with adequate precision (rather than chasing flashy effects).
- Deep triage is time‑intensive; reserve it for candidates that are (a) decision‑relevant, (b) assay‑sensitive, and (c) plausibly precise after a single replication.

Two‑stage funnel

Stage 1 — 15‑minute screen for every pilot (keep or archive)
Score 0/1 on each item; keep pilots scoring ≥4/6.
- One‑sentence claim: Can you title a figure with the testable claim?
- Decision relevance: Who cares, and what δ (SESOI) would matter in this context?
- Assay sensitivity: Do you have (or can you trivially add) a positive control/manipulation check?
- Precision forecast: With 1.5–2× the original N, would your 90% CI likely fit inside ±δ (or clearly exceed it)? If not, drop or redesign.
- Path dependence: Would the sign/magnitude survive reasonable alternative specifications?
- Feasibility: Can you reproduce this with available resources in <6–8 weeks?

Stage 2 — Full triage only on the shortlisted pilots (top 3–5)
Apply the 5‑criterion score you already use (clarity, impact, evidential strength, robustness, communicability) and run the pre‑commitment and stress tests below. Winner becomes your provisional core; runner‑up is a robustness/ablation figure.

What to skip
- Pilots with no plausible positive control or manipulation check.
- Pilots that cannot, even with 2× N or better measurement, deliver CIs tight enough to rule in/out ±δ.
- Pilots whose claims duplicate stronger evidence you already have (merge them or treat as supplemental replication).

Three concrete, falsifiable experiments to decide what advances from Stage 2
1) Preregistered mini‑replication with an equivalence decision rule
- Design: Same protocol; preregister one primary endpoint, SESOI ±δ, and TOST; increase N by ~1.5–2×.
- Pass/fail: Pass if the 90% CI lies wholly within ±δ (both one‑sided TOST p < .05) or, for “positive” candidates, if the effect exceeds δ with narrow CI.
- Use: Elevate pilots that can deliver an informative bound or a clearly practical effect.

2) Assay‑sensitivity positive control
- Design: Add a condition/outcome known to respond (benchmark manipulation or model) using the same pipeline.
- Pass/fail: Positive control shows the expected effect (and exceeds its own minimal δ); target effect meets your decision rule (equivalence or exceeds δ).
- Use: Filters out pilots where “nothing happened” likely reflects a broken assay.

3) Compact specification‑curve audit
- Design: Predefine a small grid of reasonable choices (e.g., 3 data filters × 2 model families × 2 outcome definitions) before looking.
- Pass/fail: ≥80% of specifications keep the effect’s sign and practical interpretation (above δ or within ±δ for equivalence); no effects on placebo outcomes.
- Use: Promotes pilots whose conclusions are not path‑dependent.

Practical tips
- Triaging by claim, not by “prettiness”: If several pilots test the same claim, pool them as a single candidate and plan one confirmatory replication rather than treating them separately.
- Timebox: Cap Stage 1 at a fixed time (e.g., one afternoon); cap Stage 2 experiments to a sprint (e.g., 4–6 weeks). If a candidate cannot pass the three tests in that window, demote it.
- Document drops: Keep a brief log (claim, δ, reason dropped). It protects against hindsight bias and helps future planning.

Bottom line
- Screen everything lightly to avoid bias; only invest deep triage (and new data) in pilots that can, with one replication and a positive control, deliver either a precise equivalence bound or a practically meaningful effect.

Literature anchors
- Lakens (2017). Equivalence Tests: A practical primer on using TOST and SESOI to make informative “no meaningful effect” claims. https://journals.sagepub.com/doi/10.1177/1948550617697177
- Steegen et al. (2016). Increasing Transparency Through a Multiverse Analysis: How to display robustness to analytic choices. https://journals.sagepub.com/doi/10.1177/1745691616658637

[Turn 8] USER:
After the light screen, how many pilots typically make it to the deep-triage round?

[Turn 8] ASSISTANT:
Rule of thumb
- Default target: 2–3 pilots into deep triage per paper cycle. That’s usually the most you can vet rigorously (mini-replication + robustness + controls) in 4–6 weeks without quality slipping.
- Proportion guide: Expect roughly 10–30% of pilots to clear a proper light screen. So if you have ~10–15 pilots, plan on 2–4 candidates for deep triage; cap at 5 maximum to keep the work tractable.
- Capacity check: Let resources set the ceiling.
  - Solo/one-RA team: 1–2 candidates.
  - Small team (2–3 people): 2–4 candidates (parallelized).
  - Larger team with automation: up to 5, but only if each will get the full confirmatory treatment.

When to adjust
- If >5 pass the light screen: raise the bar (stricter SESOI justification, require a feasible positive control, higher robustness expectations) or tie-break by decision relevance and ease of replication. Run the rest in the next wave.
- If 0–1 pass: revisit SESOI and measurement reliability; add/strengthen a positive control or redesign for precision so at least one candidate can deliver an informative bound/effect.

Three concrete, falsifiable checks each deep-triage candidate must pass
1) Preregistered mini-replication with an equivalence/effect decision rule
- Design: Same protocol; preregister one primary endpoint and SESOI ±δ; use TOST (90% CI) for null-like candidates or a superiority test for positive candidates.
- Pass/fail: Equivalence if the 90% CI lies wholly within ±δ (both TOST one-sided p < .05), or superiority if the CI exceeds δ with adequate precision.

2) Assay-sensitivity positive control
- Design: Include a condition/outcome known to respond using the same pipeline.
- Pass/fail: Positive control shows the expected effect (and exceeds its own minimal δ); the candidate’s primary conclusion (equivalence or superiority) still holds.

3) Compact specification-curve/multiverse audit
- Design: Predefine a small grid of reasonable analysis choices (e.g., 3 data filters × 2 model families × 2 outcome definitions) and run them all.
- Pass/fail: ≥80% of specifications preserve the practical interpretation (within ±δ for equivalence, or ≥δ for superiority); placebo outcomes remain null.

Bottom line
- After the light screen, expect 2–4 pilots to be deep-triage candidates; plan to fully vet 2–3, capped at 5. Let decision relevance and your ability to complete the three confirmatory checks in one cycle determine the exact number.

Sources
- Lakens, D. (2017). Equivalence Tests. https://journals.sagepub.com/doi/10.1177/1948550617697177
- Steegen, S., et al. (2016). Increasing Transparency Through a Multiverse Analysis. https://journals.sagepub.com/doi/10.1177/1745691616658637
