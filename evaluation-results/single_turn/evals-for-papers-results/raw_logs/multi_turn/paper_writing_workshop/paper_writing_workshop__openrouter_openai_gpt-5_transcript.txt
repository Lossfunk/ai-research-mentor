[Turn 0] SYSTEM:
You're preparing to write your first conference paper and need help structuring the narrative, deciding what results to include, and understanding how to frame your contributions clearly.

[Turn 1] USER:
I’ve run a bunch of experiments, but I’m not sure which results actually belong in the main paper. How do I decide what’s central enough to highlight in the main figures and what should go into the appendix?

[Turn 1] ASSISTANT:
Stage: C – Research Plan

Quick intake to tailor this well:
- What venue are you targeting and what’s the figure/table limit (e.g., 8 pages, 6 figures)?
- What are your 1–2 primary claims (phrased so a skeptical reviewer could paraphrase them in one sentence)?
- Which results, if removed, would force you to soften or change those claims?
- What’s your current blocker: too many ablations, too many datasets, or unclear narrative?

Strategic questions:
1 If a busy reviewer read only the abstract, intro, and Figures 1–3, would they be convinced of your main claim? If not, what single additional figure would fix that gap?
2 Which ablations change your interpretation (mechanism/attribution) versus those that only change the magnitude slightly? Only the former usually belong in the main text.
3 Can you summarize robustness (seeds/datasets/hyperparams) in one compact table or plot with CIs, leaving details to the appendix? If not, is the claim too strong for the current main text?
4 For each figure, what specific sentence in the paper does it prove? If you can’t map it, it likely goes to the appendix.

Recommendations (how to triage):
- Start with a claim–evidence map: list your 1–2 central claims; under each, list the minimum set of results that establish existence, size, and robustness of the effect. Anything not essential to that chain moves to the appendix.
- Figure budget rule: 1 overview/setup schematic, 1 main effect plot, 1 mechanism/ablation plot, and 1 robustness/performance summary. Everything else becomes appendix, unless removing it would change the claim’s strength or interpretation.
- Ablation funnel: keep only ablations that alter the causal story (e.g., “X drives the gain”). Group minor ablations into an appendix grid with one-line takeaways in the main text.
- Robustness condensation: show one CI/variance summary across seeds/datasets in main; move per-seed/per-dataset breakdowns, hyperparameter sweeps, and extended diagnostics to appendix with pointers in captions.
- Reader-path clarity: ensure each main figure’s title and first sentence of the caption state the claim it tests; defer procedure and tuning specifics to the appendix to match reader expectations and reduce cognitive load.

Three concrete, falsifiable “decision” experiments to determine main vs. appendix:
1 Narrative sufficiency stress test
- Hypothesis: Figures F1–F3 plus one robustness summary suffice for a skeptical reader to accept Claim C without the appendix.
- Setup: Recruit 3–5 labmates unfamiliar with the work. Give them abstract + intro + F1–F3 + a 1-paragraph methods summary. Ask them to rate “claim supported?” and “what’s missing?” on a 1–5 scale; time-box to 10 minutes.
- Metrics/success: Median ≥4 on “claim supported?” and ≤2 distinct missing-evidence items. Failure falsifies the hypothesis and indicates which figure (e.g., mechanism or robustness) must be promoted from appendix.
- Interpretation: If ratings are low, add exactly one figure that addresses the most frequent gap; re-run the test. If high, keep remaining details in appendix.
- Follow-ups: Blind the figure order; test whether removing any single figure drops support below threshold—only indispensable figures remain.

2 Robustness-summary sufficiency test
- Hypothesis: A single compact table/plot with means, 95% CIs across seeds and datasets is enough for the main text; full breakdowns can live in the appendix.
- Setup: Compute effect sizes with CIs across N seeds and M datasets; plot as a forest or beeswarm. Place extended per-seed/per-dataset plots in appendix.
- Metrics/success: All main-effect CIs are unambiguously above baseline; reviewers can recover the conclusion from the summary alone. If any CI overlaps zero or varies by dataset, the hypothesis is falsified—promote the problematic breakdown to main and soften the claim.
- Interpretation: Clear, consistent effects justify an appendix for details; heterogeneity demands more main-text space or a reframed claim.
- Follow-ups: Add sensitivity analyses (e.g., hyperparameter windows) to the appendix; keep only a one-line summary in main.

3 Mechanism necessity ablation
- Hypothesis: Component X (or assumption A) is necessary for the claimed gain; removing X collapses the effect by ≥Y%.
- Setup: Run ablations removing X, swapping X with a matched alternative, and adding X to a strong baseline. Report best practices (same compute budget, matched params) in appendix; show only the necessity/sufficiency result in main.
- Metrics/success: Pre-registered threshold (e.g., ≥30% drop vs. full model) with CI excludes zero. If falsified, demote the mechanism figure to appendix and state a narrower claim in main.
- Interpretation: Necessity demonstrated → mechanism plot belongs in main; otherwise, focus main on the performance effect and move mechanism speculation to appendix.
- Follow-ups: If borderline, include a small Shapley or ablation-importance summary in main and push per-variant sweeps to appendix.

Intuition:
- The main text should be a minimal, causal chain from claim to conviction: existence → size → mechanism (if strong) → robustness. Anything that doesn’t advance that chain adds friction and belongs in the appendix.

Why this is principled:
- Paper structure should foreground one central contribution and a small number of supporting figures; excessive detail impedes comprehension and dilutes the narrative. Reproducibility requires documenting variability and settings, but these can be summarized compactly in the main text with full details in the appendix/supplement.

Next actions (1–2 days):
- Draft a one-page claim–evidence map and allocate a 4-figure budget; tag each figure as existence/mechanism/robustness.
- Run the narrative sufficiency stress test and adjust the figure set once.
- Build a single robustness summary figure with CIs; push extended sweeps and per-case breakdowns to the appendix with clear cross-references.

Literature anchors:
- Ten simple rules for structuring papers (Mensh & Kording, 2017)
- Machine Learning Reproducibility Checklist (v2.0)

Citations
- Ten simple rules for structuring papers — https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005619
- The Science of Scientific Writing (Gopen & Swan, 1990) — https://cseweb.ucsd.edu/~swanson/papers/science-of-writing.pdf
- Machine Learning Reproducibility Checklist (v2.0) — https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf

[Turn 2] USER:
This makes sense, but I’m not sure how to actually write the claim–evidence map you mentioned. What level of detail should I include there — like full experiment descriptions, or just short bullet points mapping claim → figure → result? Could you show me what a simple example looks like for one claim?

[Turn 2] ASSISTANT:
Stage: A – Pre Idea Stage

Quick check to tailor the example:
- What is your single most important claim in one sentence (e.g., “Method M improves accuracy on benchmarks B by 3–5% under equal compute”)?
- How many figures can you afford in the main paper, and which ones you feel are “must have” today?

Guidance: what goes into a claim–evidence map and at what level
- Length/format: 1 page max. Use terse bullets, not full method prose. Each claim gets 6–8 bullets that form a minimal, causal chain.
- Scope: map only the essentials that a skeptical reviewer needs to accept the claim: existence of effect → size → robustness → (if strong) mechanism. Everything else is appendix.
- Granularity: include a one-line description per figure/table, the exact result you expect the figure to show (number or direction), and a decision rule for whether it must stay in main.
- Cross-links: for each main figure, point to 1–2 appendix items that hold details (sweeps, diagnostics). Keep the caption-level summary in main, the rest in appendix.
- Decision rules: add “if removed, we must weaken claim X” statements to make indispensability explicit. This forces a lean main narrative. Summarize variability compactly in main; relocate per-case breakdowns to appendix to meet reproducibility expectations without bloating the story.

Template for one claim (copy/paste)
- Claim C1: One-sentence, falsifiable claim.
- Evidence chain:
  - Existence: Figure F1 [name]. Shows baseline vs. method across N seeds; effect direction and that it’s non-zero.
  - Magnitude: Key metric and target effect size (e.g., +3–5% absolute).
  - Robustness: Figure/Table F2 summarizing seeds/datasets with CIs; heterogeneity flagged if any.
  - Mechanism (optional): Figure F3 ablation demonstrating necessity/sufficiency threshold.
- Indispensability test: What breaks about the claim if F1/F2/F3 is removed?
- Appendix pointers: A1 (per-dataset breakdown), A2 (hyperparameter sweeps), A3 (training curves/debug).
- Risks/assumptions: Short note on threats to validity and how the appendix items mitigate them.

Concrete example for one claim
- Claim C1: Under equal training budget, Method M improves average F1 by 3–5% over the strongest baseline on Benchmarks B1–B3.
- Evidence chain:
  - Existence: F1. Main effect plot (bar or beeswarm) comparing M vs. best baseline on B1–B3; 5 seeds each. Expect all three benchmarks show M > baseline by ≥2% per benchmark.
  - Magnitude: Report average ΔF1 and 95% CI; target +4% (CI [+2.5, +5.5]). If CI overlaps +0 on any benchmark, soften claim for that dataset.
  - Robustness: F2. Compact forest plot summarizing per-benchmark means with CIs and a meta-analytic average. If heterogeneity is high (I² > 50%), elevate one heterogeneity figure from appendix to main and qualify wording.
  - Mechanism: F3. Ablation removing component X reduces M’s gain by ≥30% under matched compute; swapping X with Y recovers at most half the gain. If drop <15%, move mechanism to appendix and narrow claim to performance only.
- Indispensability test:
  - Remove F1 → readers can’t see the effect exists across tasks; claim C1 is not established.
  - Remove F2 → readers can’t judge variability; must weaken “average” claim or add cautionary language.
  - Remove F3 → mechanism is speculative; keep claim to “performance” and relegate mechanism to appendix.
- Appendix pointers:
  - A1: Per-seed, per-dataset tables and exact training configs.
  - A2: Hyperparameter sensitivity (±20%) showing stability window; one-line summary in main.
  - A3: Training curves and failure case analysis with examples.
- Risks/assumptions:
  - Matching compute and tuning across methods; violations documented in A1/A2 with sensitivity checks.

Three quick, falsifiable “decision” experiments to validate your map
1 Map sufficiency review
- Hypothesis: With only F1–F3 and their captions, 3–5 naive readers will judge C1 as supported.
- Setup: Give them abstract + intro + F1–F3 + 1-paragraph methods. Collect 1–5 ratings for “claim supported?” and missing-evidence notes.
- Metric: Median ≥4/5 and ≤2 distinct missing items. If not met, promote the most-cited appendix item to main and rerun once.

2 Robustness-summary adequacy
- Hypothesis: A single forest plot with 95% CIs suffices; per-benchmark breakdowns can remain appendix.
- Setup: Compute means and CIs across seeds for each dataset; produce one forest plot; place per-seed plots in appendix.
- Metric: All CIs above zero. If any cross zero, either soften the claim or promote the problematic dataset breakdown to main.

3 Mechanism necessity threshold
- Hypothesis: Removing component X reduces the effect by ≥30% under matched compute; if not, mechanism stays appendix.
- Setup: Run full vs. -X vs. X→Y swap with identical budgets; report ΔF1 with CIs.
- Metric: If the drop is <15% or CI overlaps zero, demote mechanism to appendix and focus main on performance.

Intuition:
- A claim–evidence map is a lightweight contract: each main figure must prove a distinct link in the chain from “effect exists” to “it matters” to “it’s stable” (and optionally “why”). Anything that doesn’t pull its weight moves to the appendix.

Why this is principled:
- Clear, minimal figure–claim mapping improves reader comprehension and recall, reducing extraneous load in the main text. Reproducibility and robustness are essential, but compact summaries in the main text with detailed breakdowns in the supplement strike the right balance for reviewers and page limits.

Note: No sources found for “claim–evidence map” specifics in our mentorship-guidelines tool; using general writing and reproducibility sources instead. Conjecture elements are labeled as practice-oriented guidance.

Next actions (90 minutes)
- Draft one map for C1 using the template above.
- Identify exactly three main figures and write the indispensability test for each.
- Build the single robustness summary (forest/beeswarm with CIs) and move extended sweeps to appendix with cross-references.

Citations
- Ten simple rules for structuring papers — https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005619
- The Science of Scientific Writing (Gopen & Swan, 1990) — https://scholars.duke.edu/publication/881487
- Machine Learning Reproducibility Checklist (v2.0) — https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf

[Turn 3] USER:
This is really helpful — but I’m still a bit unsure about how early I should commit to this claim–evidence map. Should I lock it in now before running all my ablations, or keep it flexible and update it as I refine results? How much iteration is actually normal here for a first paper?

[Turn 3] ASSISTANT:
Stage: C – Research Plan

Quick questions so I can tailor the lock-in cadence:
- What’s your submission deadline and weekly time budget?
- Are your 1–2 claims primarily performance (“+X% under equal compute”) or mechanism/interpretability?
- How many co-authors need sign-off, and how risk-tolerant is the team (prefer stability vs. chasing bigger gains)?

Guidance on when to commit
- Use a soft-lock → update-windows → hard-lock cadence. Draft the claim–evidence map now, but commit only to the top-level claim and a 3–4 figure budget; allow predefined “update windows” (e.g., once per week) to revise details based on new results. Hard-lock the map 2–3 weeks before submission so writing can converge [G3][G6].
- Pre-register your stop rules in the map: what evidence would change the claim, what would demote a figure to appendix, and when to stop ablations (e.g., if two consecutive ablations shift the main CI by <1% absolute, stop) [G6].
- Keep a prediction log tied to each map bullet (expected direction/magnitude with confidence). If your predictions keep flipping, your claim is still exploratory; keep the map flexible until predictions stabilize across a few seeds/datasets [G7][G19].
- Iteration norms: Expect 2–4 substantive map revisions for a first paper, often more if claims mix performance and mechanism (explore→understand loops are normal). If you’re revising weekly without convergence for >4 weeks, either narrow the claim or change evaluation to get faster feedback [G7][G22].

Three concrete, falsifiable “process experiments” you can run this week
1 Soft-lock window trial
- Hypothesis: A 10-day soft lock with one scheduled update window reduces map reversals without weakening evidence.
- Setup: Freeze C1 and a 3–4 figure budget today; allow exactly one update window in 7–10 days. Keep a log of any claim/figure changes and their triggers.
- Metrics: Number of claim edits, time-to-draft, and reviewer-proxy ratings from 3 labmates (“claim supported?” 1–5). Success if edits ≤1 and median rating ≥4. If falsified, keep map flexible and shorten update cycles [G3][G6].

2 Ablation diminishing-returns test
- Hypothesis: After K key ablations, further variants change the main effect size by <1% absolute or don’t alter interpretation; stop rule is valid.
- Setup: Rank ablations by plausibility of changing the causal story; run top K first under matched compute. Track how the main CI and wording change with each added ablation.
- Metrics: Δ in CI and any wording changes to C1. If two consecutive ablations fail to change CI by ≥1% or alter interpretation, stop; else, keep mechanism in flux and do one more targeted ablation [G6][G22].

3 Claim stability under robustness expansion
- Hypothesis: Expanding from 3→5 seeds and 1→3 datasets won’t flip C1 (all CIs remain above baseline).
- Setup: Run extra seeds/datasets for the main effect figure; update the forest/CI summary.
- Metrics: Overlap of old vs. new CIs; count of datasets where CI crosses zero. If stability fails, keep the map flexible and narrow the claim scope (e.g., “on B1–B2 only”) before hard-locking [G3][G7].

Intuition:
- Locking too early risks polishing the wrong story; locking too late prevents a coherent narrative. A soft lock with explicit update windows gives you exploration room while forcing convergence before writing time sinks.

Why this is principled:
- Good papers crystallize a single contribution and a tight evidence chain; explicit figure budgets and pre-declared stop rules keep you from endless tinkering [G3][G6]. Iteration in explore→understand loops is expected; short feedback cycles (prediction logs, reviewer-proxy reads) help you pivot efficiently and know when to commit [G7][G19][G22].

Next actions (today)
- Draft your map, mark it “soft-locked,” and schedule one update window next week.
- Add stop rules and a prediction log entry for each main figure.
- Run the ablation diminishing-returns test on the top 2–3 ablations; decide if mechanism stays in main or moves to appendix.

Citations
- [G3] Highly Opinionated Advice On How To Write ML Papers — https://www.alignmentforum.org/posts/Xt8tMtwfsLo2jRCEj/highly-opinionated-advice-on-how-to-write-ml-papers
- [G6] Opinionated Guide to ML Research — http://joschu.net/blog/opinionated-guide-ml-research.html
- [G7] How I Think About My Research Process: Explore → Understand — https://www.alignmentforum.org/posts/hjMy4ZxS5ogA9cTYK/how-i-think-about-my-research-process-explore-understand
- [G19] Principles Of Effective Research — http://michaelnielsen.org/blog/principles-of-effective-research/
- [G22] Iterative nature of scientific practice — https://www.lifescied.org/doi/10.1187/cbe.20-12-0276

[Turn 4] USER:
This helps a lot — but I’m worried about over- or under-scoping. How do I know if my initial claim is too ambitious for a first paper? Are there any clear signs that I should narrow the scope early rather than trying to stabilize a big, unstable claim through endless iterations?

[Turn 4] ASSISTANT:
Stage: B - Idea Stage

Quick tailoring questions
- What’s your current average effect size and 95% CI across seeds/datasets for the main metric?
- How many datasets and seeds are in hand, and how sensitive is the result to modest hyperparameter changes (±20%)?
- What’s your submission deadline and weekly time budget?

Early signs you should narrow now (use these as pass/fail gates)
- Claim volatility: If your one-sentence claim wording changes week-to-week, you’re still in explore mode; narrow the scope until predictions stabilize across a few seeds/datasets [G11][G12].
- Fragile effect: If ≥1/3 datasets have CIs overlapping zero, or the win flips under benign hyperparameter windows, restrict the claim to the stable subset or reframe it as conditional (“for small-data regimes…”) [G15][G9].
- Baseline fairness gaps: If the gain depends on extra tuning/compute that the baseline didn’t get, you likely need to narrow or retune fairly; otherwise reviewers will discount the result [G15][G9].
- Narrative bloat: If you need more than ~4 indispensable main figures to convince a reviewer, the claim is over-scoped for a first paper; pick the tightest story [G1].
- Process warnings: If >50% of your last two weeks went to debugging/chasing regressions (not running preplanned tests), you’re paying an “unstable claim tax”—narrow to regain fast feedback loops [G12][G22].

Quantitative gates (commit to these now)
- Stability gate: After 5 seeds and 2 datasets, your meta-analytic CI should be strictly above baseline; if not, narrow or soften the claim before adding more complexity [G9][G15].
- Diminishing returns: If two consecutive ablations change the main CI by <1% absolute and don’t alter interpretation, stop ablations; if they keep moving the CI by >1%, your claim is unstable—narrow to the core component or fewer settings [G15].
- Hyperparameter window: If rank order flips within a ±20% sweep on key knobs, either narrow the claim to the stable window or reframe the contribution (e.g., “with calibrated LR schedules…”) [G9][G15].

Scope-down playbook (what to do if any gate fails)
- Reduce claim surface: Move from “state-of-the-art across B1–B5” to “consistent gains on B1–B2 under equal compute,” then add B3+ as future work [G1].
- Freeze evaluation: Pick 1 primary dataset + 1 metric for Phase 0; prove existence and robustness there before re-expanding [G15].
- Reframe contribution: If mechanism is unstable, commit to a performance-only claim and relegate mechanism to appendix; keep the mechanism exploration for a follow-up paper [G1][G15].
- Precommit stop rules: Write them in your claim–evidence map to avoid endless iteration [G6].

Three concrete, falsifiable experiments to decide scope
1 Stability gate experiment
- Objective: Test whether the main effect is stable enough for a broad claim.
- Setup: Run 5 seeds on 2 datasets for your method and strongest baseline under matched compute/tuning. Plot a forest with per-dataset means and 95% CIs plus a meta-analytic average.
- Metrics/success: All per-dataset CIs above zero; meta-analytic CI above zero. If any CI crosses zero, narrow to datasets with positive CIs or soften the claim and stop adding complexity [G9][G15].
- Interpretation: Passing → keep scope; failing → restrict to stable subsets and re-run once.
- Follow-ups: Add one more dataset only after passing the gate.

2 Hyperparameter window robustness sweep
- Objective: Check whether wins persist under benign tuning changes.
- Setup: For one dataset, sweep LR, weight decay, and batch size at ±20% around your chosen settings for both method and baseline; keep budget equal.
- Metrics/success: Method’s mean remains > baseline in ≥80% of the window; no rank flips at the center. Flips → reframe to a narrower, stability-qualified claim or standardize a tuning protocol [G9][G15].
- Interpretation: Robust windows justify keeping scope; sensitivity suggests narrowing.
- Follow-ups: Document full sweeps in appendix; keep one CI summary in main.

3 Reviewer-proxy minimal evidence test
- Objective: Determine if your mainstream figure set is already sufficient.
- Setup: Give 3–5 labmates just the abstract + F1–F3 + 1-paragraph methods. Ask “Is the claim supported?” (1–5) and “What’s missing?” in 10 minutes.
- Metrics/success: Median ≥4/5 and ≤2 distinct missing items. If not, either add one targeted figure or narrow the claim to match the strongest evidence [G1][G22].
- Interpretation: Persistent failure across two weeks → you’re overscoped; narrow and retest.
- Follow-ups: Repeat after any scope change to confirm sufficiency.

Intuition:
- Big, wobbly claims multiply surface area for failure (datasets, seeds, hyperparams, mechanisms). Small, stable claims let you build a clean causal chain and ship a coherent paper sooner; you can expand in follow-ups.

Why this is principled:
- Tight, single-contribution narratives are easier to evaluate and remember, and they reduce cognitive load for reviewers [G1]. Iterative explore→understand loops are normal, but explicit gates and stop rules keep you from pathological iteration and force convergence [G11][G6]. Fair baselines and robustness checks are standard expectations; if they fail, narrowing scope prevents overclaiming and improves credibility [G15][G9].

Next actions (this week)
- Apply the three gates above; if any fail, narrow the claim and update your claim–evidence map.
- Write two one-sentence claims (broad vs. narrow); ask 3 readers which is clearer/more credible and proceed with the winner.
- Pre-register your stop rules in the map and schedule one update window next week.

Citations
- [G1] Highly Opinionated Advice On How To Write ML Papers — https://www.alignmentforum.org/posts/Xt8tMtwfsLo2jRCEj/highly-opinionated-advice-on-how-to-write-ml-papers
- [G6] Manifesto For Doing Good Science — https://letters.lossfunk.com/p/manifesto-for-doing-good-science
- [G9] arXiv 2304.05585 — https://arxiv.org/abs/2304.05585
- [G11] How I Think About My Research Process: Explore → Understand — https://www.alignmentforum.org/posts/hjMy4ZxS5ogA9cTYK/how-i-think-about-my-research-process-explore-understand
- [G12] Principles Of Effective Research — http://michaelnielsen.org/blog/principles-of-effective-research/
- [G15] Opinionated Guide to ML Research — http://joschu.net/blog/opinionated-guide-ml-research.html
- [G22] Iterative nature of scientific practice — https://www.lifescied.org/doi/10.1187/cbe.20-12-0276

[Turn 5] USER:
Tailoring info: Current effect size: about +3.2% improvement over baseline, with a 95% CI of roughly [+1.0%, +5.1%] across current seeds/datasets. Datasets/seeds: 2 datasets, 5 seeds each. Hyperparameter sweeps of ±20% have shown some sensitivity, the ranking flips on one dataset but not the other. Deadline: 6 weeks out. I can budget roughly 8–10 hours/week for this paper. Follow-up question: Given that one of my datasets is already a bit shaky and I only have a modest effect size, do you think it's better to narrow early to just the more stable dataset and aim for a cleaner, smaller claim, or try to stabilize the second dataset with more tuning and seeds? I'm not sure what's the smarter trade-off this early in the timeline.

[Turn 5] ASSISTANT:
Stage: C – Research Plan

Short answer: With 6 weeks and ~60 hours total, I’d narrow early to the stable dataset for a clean, defensible claim, but run a tightly time-boxed “rescue” on the shaky dataset (≤10–12 hours) with fair, standardized tuning and a couple extra seeds. If it still flips or the CI crosses zero, keep it in the appendix as heterogeneity analysis. This balances credibility with limited time and avoids endless iteration.

Two quick checks before we lock this:
- Is your claim primarily performance (not mechanism)?
- Do you have enough compute to add 2–3 more seeds on the unstable dataset this week?

Recommendation (branching on your constraints)
- Default path (recommended): Narrow the main claim to the stable dataset now. Keep the cross-dataset story as “observed heterogeneity; see appendix” unless the rescue succeeds. This avoids overclaiming when one dataset flips under benign sweeps and makes the paper coherent under tight time/compute.
- Time-boxed rescue (≤10–12 hours total): One standardized tuning pass with compute parity + 2 seeds on the shaky dataset. Promote it back to main only if the CI is strictly above zero and rank is stable under the shared protocol.

Three concrete, falsifiable experiments to decide scope this week
1 Fair-tuning parity test on the shaky dataset
- Objective & hypothesis: With a shared, pre-declared tuning protocol and equal search budget, Method M remains better than the baseline on the shaky dataset.
- Setup: Define the same grid/ranges and trials per method (e.g., 20 trials, fixed budget; identical early-stopping rule). Re-run both methods; keep seeds fixed and add 2 new seeds for variance.
- Metrics/success: Best-of-equal-budget mean improvement with 95% CI strictly > 0; no rank flip at the protocol’s center setting. If fails, demote this dataset to appendix and narrow the claim to the stable dataset.
- Follow-ups: If close, report conditional wording in main and keep full sweeps in the appendix.

2 Seed expansion and CI stabilization
- Objective & hypothesis: Adding 2–3 seeds narrows variance without flipping the conclusion.
- Setup: On both datasets, train 2–3 additional seeds at the tuned center settings; compute per-dataset means and 95% CIs, plus a meta-analytic average.
- Metrics/success: Each dataset’s CI above zero; meta-analytic CI above zero. If the shaky dataset’s CI crosses zero, keep it out of the main claim (appendix only).
- Follow-ups: If stable, you can cautiously restore a two-dataset claim.

3 Hyperparameter window stability map (quick local audit)
- Objective & hypothesis: Within a ±20% neighborhood of LR/WD/batch size, M beats the baseline in ≥80% of the grid on the shaky dataset.
- Setup: 3×3 or 4×4 local grid around the chosen settings, same grid for both methods, equal epochs/steps. Use small-sample CIs or bootstrapped differences.
- Metrics/success: Proportion of grid where M > baseline ≥0.8 and no flip at the center. If not met, treat results as sensitive; narrow the main claim and move the map to the appendix.
- Follow-ups: Record the shared protocol and sensitivity figure to satisfy reproducibility expectations.

Intuition:
- A modest effect with one unstable dataset is a classic “surface area” trap: every extra dataset/knob multiplies ways the claim can wobble. A smaller, stable claim gets you a clean causal chain now, while a short, fair rescue attempt preserves upside.

Why this is principled:
- Reviewers expect compute/tuning parity, variance reporting, and protocol transparency; when those are met and a dataset still flips, narrowing scope is more credible than over-tuning. Time-boxed iterations and pre-declared stop rules prevent the endless-tinkering failure mode while keeping the door open to include the second dataset if it stabilizes.

Next actions (7–10 days)
- Today: Soft-lock the narrow claim on the stable dataset; write the caption for its main-effect figure. Pre-declare the fair-tuning protocol.
- Next 3–4 days: Run Experiments 1–3 on the shaky dataset (≤12 hours). Update CIs.
- Day 7–10: If the shaky dataset passes both parity and stability gates, restore it to main; otherwise, finalize it as an appendix heterogeneity analysis with a one-line summary in the main text.

Citations
- Machine Learning Reproducibility Checklist (v2.0) — https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf
- Improving Reproducibility in Machine Learning Research (JMLR 2021) — https://jmlr.org/papers/v22/20-303.html

[Turn 6] USER:
Tailoring info: My claim is primarily performance, not mechanism. Yes, I can afford to add 2–3 extra seeds on the shaky dataset this week with my current compute budget. Follow-up question: This plan sounds doable. If the rescue attempt barely passes, say the CI is just above zero but still narrower and more fragile than the stable dataset, would you recommend including it in the main claim with careful wording, or keeping it in the appendix and avoiding a weaker, more qualified claim in the main text?

[Turn 6] ASSISTANT:
Stage: D – First draft (scoping call with clear gates)

Short answer: Default to a crisp main claim on the stable dataset. Include the shaky dataset in the main paper only if it passes pre-declared gates: (a) compute/tuning parity, (b) CI strictly above zero with a practical margin (e.g., ≥+1% absolute), and (c) no rank flip at the center of a standardized ±20% window. If it only barely clears zero and remains fragile, keep it in the appendix as heterogeneity evidence and avoid weakening the main claim.

Two quick checks to confirm:
- What’s your minimal practical improvement (MPI) threshold that you’d consider “meaningful” (+1% is a common default for F1/accuracy)?
- Is the shaky dataset important for venue narrative (e.g., a standard benchmark reviewers expect), or just nice-to-have?

Recommendation with decision rule
- Main claim: Focus on the stable dataset now. It keeps the narrative clean and reduces reviewer attack surface.
- Promotion rule for the shaky dataset: Promote to main only if all three pass:
  1 Parity: Shared tuning protocol and equal search budget.
  2 Stability: Per-dataset 95% CI > 0 and ≥ MPI (e.g., ≥+1%).
  3 Robustness: No rank flip at the center setting; ≥80% wins in a small ±20% hyperparameter window.
- If any fail, keep the dataset in the appendix with a one-line main-text summary (“Dataset D2 shows smaller and sensitive gains; see App. A for full protocol and sensitivity”).

Wording templates
- Narrow main claim: “Under equal compute and standardized tuning, Method M improves F1 by 3.2% (95% CI [+1.0, +5.1]) on D1; D2 shows heterogeneous outcomes under benign tuning changes (Appendix A).”
- Qualified broader claim (use only if gates pass): “Across D1–D2, M improves F1 on average by 3.2% (meta-analytic CI), with a smaller but positive effect on D2 under a shared tuning protocol.”

Three concrete, falsifiable experiments (this week)
1 Compute/tuning parity re-run on the shaky dataset
- Objective & hypothesis: With a shared, pre-declared protocol and equal search budget, M’s mean improvement on D2 is > 0 and ≥ MPI.
- Setup: Define the same grid/ranges and trials for M and baseline; fixed early stopping and equal epochs/steps. Add 2–3 seeds. Record the best-of-equal-budget center and its CI.
- Metrics/success: Δ metric CI entirely above 0 and ≥ MPI; no rank flip at the center. If not, keep D2 in appendix.
- Interpretation: Passing supports optional inclusion as a secondary panel; failing implies heterogeneity.

2 Hyperparameter window robustness map (local)
- Objective & hypothesis: Within ±20% on LR/WD/batch size around the shared-protocol center, M wins in ≥80% of grid points on D2.
- Setup: 3×3 or 4×4 grid for both methods, identical training budget. Record the proportion of points where M > baseline.
- Metrics/success: Proportion ≥0.8 and center shows M > baseline. If not, treat D2 as sensitive; appendix.

3 Fragility index on D2
- Objective & hypothesis: The effect on D2 is not fragile to individual seed outcomes.
- Setup: With S total seeds, compute how many seed outcomes would need to reverse to make Δ ≤ 0 (or ≤ MPI). This is the fragility index.
- Metrics/success: Fragility index ≥2; if ≤1, the effect is too brittle for main inclusion.
- Interpretation: Low fragility → appendix; acceptable fragility → optional secondary panel with cautious wording.

Intuition:
- A borderline-positive, tuningsensitive result invites reviewer scrutiny and can derail the paper. A crisp, stable primary claim lands better; treating the shaky dataset as heterogeneity preserves transparency without diluting the core story.

Why this is principled:
- Performance claims are judged on fairness (compute/tuning parity), robustness (seeds/datasets), and clarity. Pre-declaring promotion gates and MPI prevents endless iteration and overclaiming, while an appendix heterogeneity analysis meets transparency and reproducibility expectations without bloating the main narrative.

Next actions (5–7 days)
- Lock the narrow main claim on D1; draft F1 (main-effect) and F2 (CI summary) for D1.
- Run the three experiments on D2 with a ≤12-hour cap; decide inclusion via the promotion rule.
- If D2 stays appendix, add a one-paragraph “heterogeneity and sensitivity” subsection with protocol details and a pointer to sensitivity plots.

Note: No sources found in the mentorship-guidelines tool for this specific scoping decision; guidance below includes Conjecture based on common practice.

Literature anchors
- Machine Learning Reproducibility Checklist (v2.0) — https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf
- Ten simple rules for structuring papers (Mensh & Kording, 2017) — https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005619