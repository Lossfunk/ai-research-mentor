[Turn 0] SYSTEM:
You're preparing to write your first conference paper and need help structuring the narrative, deciding what results to include, and understanding how to frame your contributions clearly.

[Turn 1] USER:
How do I decide which experiments are “core” enough to anchor the story when I’m still unsure what the main contribution actually is?

[Turn 1] ASSISTANT:
This is one of the most common—and most challenging—questions in research. The uncertainty you're feeling is normal, especially in exploratory phases. Here's a practical framework to help you identify core experiments even when your contribution isn't yet clear:

## 1. **Work backward from candidate contributions**

Even if you're unsure, you likely have 2–3 hypotheses about what the contribution *might* be. For each candidate:
- **What would need to be true?** List the minimal empirical claims.
- **What's the simplest experiment that tests that claim?** That's a core experiment for that storyline.

Example: If your contribution might be "Method X improves sample efficiency," the core experiment is a learning curve comparison. If it's "Method X generalizes better to distribution shift," the core experiment is out-of-distribution evaluation.

## 2. **Apply the "paper without it" test**

For each experiment you're considering:
- **Could you publish the paper without this result?** If no, it's core.
- **Would removing it change the main claim?** If yes, it's core.

Core experiments are *load-bearing*—they support the central claim. Everything else is supplementary (ablations, additional domains, robustness checks).

## 3. **Look for the "minimal surprising result"**

Ask yourself:
- **What's the smallest result that would make a reader say "I didn't expect that"?**
- **What's the simplest demonstration that challenges existing assumptions?**

Often the core experiment is the one that establishes the phenomenon or capability, not the one with the most impressive numbers.

## 4. **Prioritize experiments that discriminate between explanations**

If you're unsure what's driving your results:
- Design experiments that can **rule out alternative explanations**.
- The experiment that most clearly isolates the mechanism or effect is often core.

Example: If you observe improved performance, is it due to your architectural change, better optimization, or more parameters? An ablation that controls for parameter count becomes core.

## 5. **Start with the "existence proof" + one comparison**

A minimal core often consists of:
- **Existence proof**: Does the thing work at all? (Proof of concept on one task/dataset)
- **Comparison**: Is it better than the obvious baseline? (Head-to-head on a standard benchmark)

Everything else—scaling studies, additional datasets, failure cases—can be added once you've established these two anchors.

## 6. **Use the "elevator pitch" heuristic**

Try to explain your work in 2–3 sentences:
- **Which experimental results appear in that pitch?** Those are your core experiments.
- If you can't pitch it without mentioning an experiment, that experiment is core.

## 7. **Embrace iteration: Core experiments can change**

It's okay if your sense of "core" evolves:
- **Early exploration**: Cast a wide net. Run multiple small experiments.
- **Mid-stage**: Patterns emerge. Double down on the experiments that reveal something interesting.
- **Late stage**: Crystallize the story. Relegate experiments that don't support the main thread to appendices or cut them.

The core experiments are the ones that survive this winnowing process.

## Practical next steps

1. **Write 2–3 one-sentence candidate contributions** (even if speculative).
2. **For each, identify the single experiment that would most directly support it.**
3. **Run those experiments first** (or analyze existing results through that lens).
4. **See which storyline is best supported by the data**—that reveals your actual contribution and core experiments.

---

### Key takeaway
You don't need to know the contribution before running experiments—often the experiments *reveal* the contribution. But you do need to run experiments that are **decisive**: they should clearly support or refute a specific claim. Start with the simplest, most direct tests of your candidate claims, and let the data guide you to the story.

If you'd like, share a bit about your project (domain, methods, early results) and I can help you brainstorm which experiments might be core for your specific case.

[Turn 2] USER:
What if I have two candidate contributions that seem equally supported—how do I pick which one becomes the paper’s spine without feeling like I’m abandoning the other?

[Turn 2] ASSISTANT:
This is a sophisticated problem that many researchers face, especially when the work is rich enough to support multiple narratives. The good news: having two strong contributions is a luxury. The challenge: choosing feels like leaving value on the table. Here's how to navigate this:

## 1. **You're not abandoning—you're prioritizing**

Reframe the decision:
- **The "spine" is about narrative clarity, not scientific completeness.** The secondary contribution doesn't disappear; it becomes supporting evidence or a secondary claim.
- **Readers remember one main idea.** Papers that try to tell two equal stories often leave readers confused about what to cite the work for.

Think of it as: "What do I want this paper to be *known* for?" vs. "What does this paper contain?"

## 2. **Test which contribution has stronger "pull"**

Ask these diagnostic questions:

### **Novelty & surprise**
- Which contribution would surprise your target audience more?
- Which challenges existing assumptions or opens new directions?

### **Generality**
- Which contribution applies to a broader set of problems or methods?
- Which has more potential for follow-up work (by you or others)?

### **Clarity of evidence**
- Which contribution has cleaner, more decisive experimental support?
- Which requires fewer caveats or auxiliary assumptions?

### **Community readiness**
- Which contribution addresses a question the community is actively asking?
- Which fits better into ongoing conversations (recent workshops, trending topics)?

The contribution that scores higher on most of these dimensions is usually the better spine.

## 3. **Consider the "two-paper test"**

Imagine splitting your work into two papers, each anchored by one contribution:

- **Paper A (Contribution 1 as spine)**: What would the abstract say? What experiments are core? Does it feel complete?
- **Paper B (Contribution 2 as spine)**: Same questions.

Often one paper feels more coherent, self-contained, or impactful. That reveals your spine.

If *both* feel like complete papers, you might actually have two papers' worth of work—consider whether splitting is feasible (though this depends on venue, timeline, and whether the contributions share infrastructure).

## 4. **Use the "primary + secondary" structure**

You don't have to choose exclusively. Many strong papers have:
- **Primary contribution** (the spine): Introduced early, supported by core experiments, emphasized in abstract/intro/conclusion.
- **Secondary contribution**: Presented as "Additionally, we find..." or "A consequence of our approach is..." Supported by 1–2 experiments, discussed but not over-emphasized.

This structure lets you:
- Maintain narrative focus (readers know what the paper is "about").
- Preserve the secondary contribution (it's still published, citable, and valuable).
- Avoid the "two papers crammed into one" feeling.

**Example framing:**
- Spine: "We propose method X, which improves sample efficiency by 40%."
- Secondary: "Interestingly, method X also exhibits better robustness to distribution shift, suggesting a connection between sample efficiency and generalization."

## 5. **Let the venue guide you**

Different venues reward different contributions:

- **Top-tier ML conferences (NeurIPS, ICML, ICLR)**: Favor novelty, theoretical insight, or strong empirical gains. Pick the contribution that's more surprising or methodologically innovative.
- **Application-focused venues**: Favor practical impact. Pick the contribution that solves a real problem more directly.
- **Workshops or specialized tracks**: May welcome dual contributions if they're both relevant to the theme.

If you're targeting a specific venue, look at recent best papers—what kind of "spine" do they have?

## 6. **Run a "colleague test"**

Explain both candidate stories to 2–3 colleagues (or your advisor):
- **"This paper is about [Contribution 1]..."** → Gauge their reaction.
- **"Actually, this paper is about [Contribution 2]..."** → Gauge their reaction.

Which framing generates more excitement, follow-up questions, or "aha" moments? That's often your spine.

## 7. **Prototype both abstracts**

Write two versions of your abstract, each leading with a different contribution:
- **Abstract A**: Contribution 1 is the main claim; Contribution 2 is mentioned as a secondary finding.
- **Abstract B**: Contribution 2 is the main claim; Contribution 1 is mentioned as a secondary finding.

Which abstract:
- Feels more compelling?
- Has a clearer logical flow?
- Would make *you* want to read the paper?

This exercise often makes the right choice obvious.

## 8. **Consider strategic positioning for your career**

If both contributions are equally strong scientifically, think about:
- **Which aligns better with your research identity?** (What do you want to be known for?)
- **Which opens more doors for future work?** (Which contribution has more follow-up questions you're excited to pursue?)
- **Which differentiates you more in your subfield?** (Which is less crowded?)

This isn't about gaming the system—it's about being intentional with your research narrative.

## 9. **Accept that you can revisit the other contribution later**

If Contribution 2 doesn't become the spine now:
- **It can anchor a follow-up paper.** Use this work as a preliminary result and expand it.
- **It can be a chapter in your thesis.** (If you're a PhD student.)
- **It can be a workshop paper or short paper.** Get it out there in a lighter-weight venue.

Choosing a spine for *this* paper doesn't mean the other contribution is lost—it just means it might have its own moment later.

## 10. **When in doubt, pick the simpler story**

If you're truly stuck:
- **Choose the contribution that's easier to explain** in one sentence.
- **Choose the contribution with fewer moving parts** (fewer experimental conditions, fewer assumptions).
- **Choose the contribution that feels more "done"** (less hand-waving, fewer "future work" caveats).

Simpler stories are easier to write, easier to review, and easier to remember.

---

## A decision framework (practical checklist)

| **Criterion**                          | **Contribution 1** | **Contribution 2** | **Winner** |
|----------------------------------------|--------------------|--------------------|------------|
| More novel/surprising?                 |                    |                    |            |
| Broader applicability?                 |                    |                    |            |
| Cleaner experimental support?          |                    |                    |            |
| Better fit for target venue?           |                    |                    |            |
| Generates more follow-up questions?    |                    |                    |            |
| Easier to explain in one sentence?     |                    |                    |            |
| Aligns better with my research goals?  |                    |                    |            |

Tally the checkmarks. If one contribution wins on ≥5 criteria, make it the spine.

---

## Key takeaway

**Choosing a spine is not about which contribution is "better"—it's about which story you can tell most clearly and compellingly.** The secondary contribution doesn't vanish; it enriches the paper. But readers need one anchor to hold onto.

If you're comfortable sharing, I'd be happy to help you think through your specific two contributions—sometimes an outside perspective can quickly clarify which should be the spine.

[Turn 3] USER:
If I pick Contribution A as the spine, how do I keep Contribution B from swelling into a second equal storyline while still giving it a fair supporting role?

[Turn 3] ASSISTANT:
This is an excellent structural question—and one that separates well-organized papers from sprawling, unfocused ones. The key is **intentional asymmetry**: you want Contribution B to be visible and valuable without competing for narrative control. Here's how to do it:

---

## 1. **Establish hierarchy early and explicitly**

### **In the abstract:**
- Lead with Contribution A in the first 1–2 sentences.
- Mention Contribution B as a secondary finding: "Additionally, we show..." or "As a consequence..."
- Use **1 sentence max** for Contribution B in the abstract.

**Example:**
> "We propose Method X, which achieves 40% better sample efficiency on benchmark Y (Contribution A). Additionally, we find that Method X exhibits improved robustness to distribution shift, suggesting a link between sample efficiency and generalization (Contribution B)."

### **In the introduction:**
- **Contribution A**: Motivate it, explain why it matters, preview the approach (2–3 paragraphs).
- **Contribution B**: Introduce it as a "bonus" or "interesting consequence" (1 paragraph, later in the intro).
- Use framing language: "Beyond our primary contribution, we also observe..." or "An unexpected benefit of our approach is..."

This sets reader expectations: A is the main event, B is a valuable side dish.

---

## 2. **Control real estate: Allocate space asymmetrically**

### **Section/page budget:**
- **Contribution A**: Gets its own method section, multiple core experiments, detailed analysis (e.g., 3–4 pages).
- **Contribution B**: Gets a subsection within results or a standalone section that's clearly shorter (e.g., 1–1.5 pages).

### **Figure budget:**
- **Contribution A**: 3–5 figures (main results, ablations, comparisons).
- **Contribution B**: 1–2 figures (key result + maybe one supporting plot).

If Contribution B starts demanding equal space, it's trying to become a co-spine. Trim ruthlessly.

---

## 3. **Use structural cues to signal hierarchy**

### **Section titles:**
- **For Contribution A**: Use assertive, primary titles:
  - "Method X: A Sample-Efficient Approach to Y"
  - "Experimental Validation of Method X"
  
- **For Contribution B**: Use subordinate or exploratory titles:
  - "Additional Benefit: Robustness to Distribution Shift"
  - "Emergent Property: Generalization Gains"
  - "Beyond Sample Efficiency: Robustness Analysis"

The title itself signals "this is secondary."

### **Placement:**
- **Contribution A**: Core experiments appear in main results section (Section 4 or 5).
- **Contribution B**: Place it *after* the main results, either as:
  - A subsection: "4.3 Robustness Analysis"
  - A separate section: "5 Additional Findings"
  - Or even in an appendix (if it's truly supplementary).

Readers skim linearly—what comes first and gets more space is perceived as more important.

---

## 4. **Limit the experimental depth for Contribution B**

### **For Contribution A (spine):**
- Multiple datasets/tasks
- Comparisons to several baselines
- Ablation studies
- Statistical significance tests
- Failure case analysis

### **For Contribution B (supporting):**
- **One or two experiments** that clearly demonstrate the phenomenon
- **Minimal baselines** (maybe just one comparison)
- **No deep ablations** (save those for Contribution A)
- **Acknowledge limitations briefly** without over-analyzing

**Example:**
- Contribution A: "We evaluate on 5 benchmarks, compare to 8 baselines, and ablate 4 design choices."
- Contribution B: "We evaluate robustness on 2 distribution-shift scenarios and compare to the strongest baseline from our main experiments."

This keeps B substantiated but not sprawling.

---

## 5. **Frame Contribution B as a consequence, not a co-equal claim**

Use language that positions B as *derived from* or *enabled by* A:

- "As a consequence of improved sample efficiency, Method X also..."
- "The design choices that enable Contribution A have an additional benefit..."
- "Interestingly, the same mechanism that drives A also leads to B..."

This framing makes B feel like a natural extension rather than a separate pillar.

**Avoid:**
- "We also make a major contribution by showing..."
- "Equally important, we demonstrate..."
- Giving B its own motivation/related work subsection (that signals co-equal status).

---

## 6. **Integrate B into A's narrative, don't create a parallel track**

### **Bad structure (parallel storylines):**
```
1. Intro (A and B both motivated equally)
2. Related Work
   2.1 Work related to A
   2.2 Work related to B
3. Method for A
4. Method for B
5. Experiments for A
6. Experiments for B
7. Conclusion (A and B both emphasized)
```
This reads like two papers stapled together.

### **Good structure (B as supporting):**
```
1. Intro (A is primary, B mentioned briefly)
2. Related Work (focused on A; B mentioned in 1 paragraph if needed)
3. Method (focused on A; B emerges naturally from design)
4. Experiments
   4.1 Main results (A)
   4.2 Ablations (A)
   4.3 Robustness analysis (B, presented as "additional finding")
5. Discussion (A is main takeaway; B discussed as implication)
6. Conclusion (A emphasized; B mentioned as bonus)
```

B is woven in, not given its own parallel track.

---

## 7. **Resist the urge to "do B justice" with exhaustive analysis**

You'll be tempted to:
- Add more baselines for B
- Run B on more datasets
- Write a long discussion of B's implications

**Don't.** This is how B swells into a co-spine.

Instead:
- **Present B's core result clearly and concisely.**
- **Acknowledge it's preliminary or exploratory** if appropriate: "While a full investigation of B is beyond this paper's scope, our initial results suggest..."
- **Point to future work:** "A deeper exploration of B is an exciting direction for future research."

This signals: "B is real and interesting, but not the focus here."

---

## 8. **Use the related work section to reinforce hierarchy**

### **For Contribution A:**
- Dedicate 2–3 paragraphs to prior work on A's problem space.
- Position your work clearly against this literature.

### **For Contribution B:**
- **Option 1 (minimal):** Mention relevant work in 1 paragraph, or even just in the section where you present B's results.
- **Option 2 (integrated):** Weave B's related work into A's discussion if they overlap (e.g., "Prior work on sample efficiency has also noted connections to robustness...").

**Avoid:** Giving B its own multi-paragraph related work subsection. That signals co-equal importance.

---

## 9. **Control the conclusion and takeaways**

### **In the conclusion:**
- **First paragraph:** Restate Contribution A and its implications.
- **Second paragraph (optional):** Briefly mention Contribution B as an additional insight.
- **Final paragraph:** Future work, with A as the main launching point.

**Example:**
> "We introduced Method X, demonstrating significant gains in sample efficiency across five benchmarks. Our ablation studies reveal that design choice Y is critical to these gains. Additionally, we observed that Method X exhibits robustness to distribution shift, suggesting a deeper connection between sample efficiency and generalization that warrants further investigation. Future work will explore scaling Method X to larger domains and understanding the theoretical underpinnings of its efficiency gains."

Notice: A gets the spotlight, B is acknowledged but not over-emphasized.

---

## 10. **Test the balance with the "cover-up" method**

After drafting:
- **Cover up all mentions of Contribution B** (literally, or mentally).
- **Does the paper still feel complete?** Does it have a clear arc from motivation → method → results → conclusion?

If **yes**: B is appropriately supporting. The paper stands on A alone.

If **no**: Either (a) B has become load-bearing (you've accidentally made it a co-spine), or (b) A isn't strong enough on its own (you may need to rethink the spine).

---

## 11. **Watch for warning signs that B is swelling**

### **Red flags:**
- You're writing "Contribution B" in section headers or figure captions.
- Reviewers or colleagues say "I'm not sure what this paper is about."
- The abstract mentions A and B in equal detail.
- You have as many figures for B as for A.
- You're running the same experimental protocol (multiple datasets, many baselines) for both A and B.

### **Fix:**
- **Cut or move to appendix:** If B has 3 figures, move 1–2 to the appendix.
- **Simplify B's experiments:** Reduce the number of conditions, baselines, or datasets.
- **Reframe B's language:** Change "We demonstrate..." to "We observe..." or "Preliminary results suggest..."

---

## 12. **Example: A well-balanced structure**

Here's a concrete example of how to structure a paper with A as spine and B as supporting:

### **Title:**
"Method X: Improving Sample Efficiency in Reinforcement Learning"
*(Title mentions only A)*

### **Abstract (150 words):**
- Sentences 1–4: Contribution A (problem, method, results)
- Sentence 5: Contribution B (brief mention)
- Sentence 6: Implications

### **Introduction (2 pages):**
- Paragraphs 1–2: Motivate sample efficiency problem (A)
- Paragraph 3: Introduce Method X (A)
- Paragraph 4: Preview main results (A)
- Paragraph 5: "Additionally, we find that Method X improves robustness..." (B)
- Paragraph 6: Contributions and outline

### **Related Work (1.5 pages):**
- Subsection 2.1: Sample efficiency methods (A) — 3 paragraphs
- Subsection 2.2: Robustness in RL (B) — 1 paragraph

### **Method (2 pages):**
- Focus entirely on Method X and how it improves sample efficiency (A)
- Design choices that enable A
- *(B emerges naturally from the method, no separate subsection)*

### **Experiments (4 pages):**
- 4.1 Experimental Setup (1 page)
- 4.2 Sample Efficiency Results (1.5 pages, 3 figures) — **Contribution A**
- 4.3 Ablation Studies (1 page, 1 figure) — **Contribution A**
- 4.4 Robustness Analysis (0.5 pages, 1 figure) — **Contribution B**

### **Discussion (0.5 pages):**
- Implications of A
- Brief mention of A–B connection

### **Conclusion (0.3 pages):**
- Restate A
- Mention B in one sentence
- Future work focused on A

### **Appendix:**
- Additional ablations for A
- Maybe one extra robustness experiment for B

---

## Key takeaway

**The secret is asymmetry in every dimension: space, depth, framing, and emphasis.** Contribution B should be:
- **Visible** (readers know it exists)
- **Substantiated** (backed by at least one solid experiment)
- **Subordinate** (clearly secondary in scope and narrative weight)

Think of it this way: Contribution A is the protagonist; Contribution B is a compelling supporting character. The supporting character gets memorable scenes and adds richness to the story, but the plot revolves around the protagonist.

---

**Practical exercise:** Draft your paper outline right now with explicit space budgets:
- Contribution A: X pages, Y figures
- Contribution B: X/3 pages, Y/3 figures

Stick to those budgets ruthlessly. If B starts exceeding its budget, you're letting it swell—cut or move content to the appendix.

Would it help to see your specific outline or draft structure? I can give targeted advice on where B might be swelling and how to trim it.

[Turn 4] USER:
Which experiments should I treat as the immovable anchors for each section so the spine doesn’t drift?

[Turn 4] ASSISTANT:
This is a brilliant question—it gets at the **structural integrity** of your paper. Without immovable anchors, the narrative can drift as you revise, respond to feedback, or add new results. Here's how to identify and lock down the experiments that keep your spine rigid:

---

## **The Anchor Framework: Three Types of Immovable Experiments**

Every well-structured paper needs anchors at three levels:

1. **Existence anchor** – Proves the core phenomenon/capability exists
2. **Comparison anchor** – Establishes superiority/difference vs. baselines
3. **Mechanism anchor** – Explains *why* it works (isolates the key factor)

These three experiments form the **minimal complete argument** for Contribution A. Everything else is elaboration.

---

## **1. The Existence Anchor: "Does it work at all?"**

### **Purpose:**
Demonstrates that your method/finding is real, not an artifact or fluke.

### **What it looks like:**
- **For a new method:** Performance on at least one standard benchmark/task
- **For a theoretical claim:** One empirical validation that the predicted behavior occurs
- **For an analysis/insight:** One clear demonstration of the phenomenon

### **Where it goes:**
- **First result in your main experiments section** (e.g., Section 4.1 or the first figure in Results)
- This is the "proof of concept" that everything else builds on

### **Characteristics of a good existence anchor:**
- **Simple and clean:** Minimal confounds, straightforward setup
- **Standard setting:** Uses a well-known benchmark or task (builds credibility)
- **Unambiguous outcome:** Clear success/failure or strong effect size
- **Self-contained:** A reader could understand your contribution from this experiment alone

### **Example (sample efficiency contribution):**
- **Anchor:** Learning curves on Atari Breakout showing Method X reaches 80% performance with 50% fewer samples than baseline
- **Figure:** Line plot with your method vs. strongest baseline, error bars, clear gap
- **Why immovable:** Without this, you haven't shown sample efficiency gains exist

### **Red flag:** If you're tempted to remove or heavily modify this experiment during revision, your spine is drifting. Resist.

---

## **2. The Comparison Anchor: "Is it better than alternatives?"**

### **Purpose:**
Establishes that your contribution is not just real, but **meaningfully different** from existing approaches.

### **What it looks like:**
- **Head-to-head comparison** with 2–4 strong baselines on a standard benchmark
- Typically a **table or bar chart** showing your method vs. competitors
- Should include the **current state-of-the-art** or most relevant prior work

### **Where it goes:**
- **Second major result** in your experiments section (e.g., Section 4.2 or Table 1)
- Often the most-cited figure/table in your paper

### **Characteristics of a good comparison anchor:**
- **Fair comparison:** Same compute budget, hyperparameter tuning, evaluation protocol
- **Multiple baselines:** At least one "obvious" baseline + one strong competitor
- **Standard metrics:** Use community-accepted metrics (accuracy, F1, sample efficiency, etc.)
- **Statistical rigor:** Error bars, significance tests, or multiple seeds

### **Example (sample efficiency contribution):**
- **Anchor:** Table comparing sample efficiency (steps to 80% performance) across 5 Atari games for your method vs. DQN, Rainbow, PPO, and one recent sample-efficient method
- **Why immovable:** This is the quantitative evidence that your method is better, not just different

### **Red flag:** If reviewers question whether your method is actually better, this anchor is weak. Strengthen it before submission.

---

## **3. The Mechanism Anchor: "Why does it work?"**

### **Purpose:**
Isolates the **key design choice or factor** that drives your contribution. This is what separates a method paper from a "bag of tricks."

### **What it looks like:**
- **Ablation study:** Remove or modify the key component and show performance degrades
- **Controlled experiment:** Vary one factor while holding others constant
- **Analysis:** Visualization or measurement that reveals the underlying mechanism

### **Where it goes:**
- **Third major result** or a dedicated ablation section (e.g., Section 4.3 or Figure 3)
- Sometimes integrated into the discussion

### **Characteristics of a good mechanism anchor:**
- **Isolates one variable:** Changes only the factor you claim is important
- **Shows necessity:** Performance drops significantly when the factor is removed
- **Connects to theory/intuition:** The result should make sense given your explanation

### **Example (sample efficiency contribution):**
- **Anchor:** Ablation showing that removing your novel replay buffer design reduces sample efficiency by 30%, while other design choices have <5% impact
- **Figure:** Bar chart or line plot comparing full method vs. ablated versions
- **Why immovable:** This proves your claimed mechanism (replay buffer design) is actually responsible for the gains

### **Red flag:** If you can't clearly explain *why* your method works, you don't have a mechanism anchor. Reviewers will ask for this.

---

## **How to Lock Down Anchors in Each Section**

Now let's map these three anchors to specific paper sections and make them immovable:

---

### **Introduction: Anchor the motivation and preview**

**Immovable element:**
- **One concrete example or statistic** that motivates Contribution A

**What it looks like:**
- "Current methods require 10M samples to reach human-level performance on Atari; we show this can be reduced to 5M."
- Or: A teaser figure showing your main result (learning curve comparison)

**Why immovable:**
- This sets the reader's expectation for what the paper will deliver
- If you change this, you're changing the paper's promise

**Placement:**
- First or second paragraph of the introduction
- Optionally: A teaser figure (Figure 1) showing the existence or comparison anchor

**Test:**
- Does this motivation directly connect to your existence anchor experiment?
- If you removed the existence anchor, would this motivation still make sense?
- If no to either, realign them.

---

### **Method: Anchor the key design choice**

**Immovable element:**
- **The design choice or component** that will be tested in your mechanism anchor

**What it looks like:**
- A subsection or paragraph explaining the novel replay buffer design (continuing the example)
- Pseudocode or a diagram highlighting this component

**Why immovable:**
- This is what you'll ablate in the mechanism anchor
- If you change the method description, the ablation becomes invalid

**Placement:**
- Dedicated subsection (e.g., "3.2 Prioritized Replay with Temporal Diversity")
- Or: Highlighted in a figure/algorithm box

**Test:**
- Can you draw a straight line from this method subsection to your mechanism anchor experiment?
- Does the ablation study test exactly what you describe here?

---

### **Experiments: Anchor the three core results**

This is where the three anchors live. Structure your experiments section around them:

#### **Section 4.1: Existence Anchor**
**Immovable experiment:**
- Performance on one canonical task/benchmark

**Immovable figure/table:**
- Figure 2 or Table 1: Learning curves or performance comparison on this task

**Immovable claim:**
- "Method X achieves [specific metric] on [benchmark], demonstrating [core capability]."

**Test:**
- If you removed this experiment, could you still claim your contribution exists?
- If yes, this isn't your existence anchor—find the experiment that is truly load-bearing.

---

#### **Section 4.2: Comparison Anchor**
**Immovable experiment:**
- Head-to-head comparison with baselines across multiple tasks/datasets

**Immovable figure/table:**
- Table 2 or Figure 3: Performance comparison (your method vs. 3–5 baselines)

**Immovable claim:**
- "Method X outperforms [baseline 1] by [X%] and [baseline 2] by [Y%] on average across [N] tasks."

**Test:**
- If a reviewer asks "How does this compare to [obvious baseline]?", is the answer in this table?
- If no, add that baseline or explain why it's not relevant.

---

#### **Section 4.3: Mechanism Anchor**
**Immovable experiment:**
- Ablation study isolating the key component

**Immovable figure/table:**
- Figure 4 or Table 3: Ablation results showing performance drop when key component is removed

**Immovable claim:**
- "Removing [key component] reduces performance by [X%], confirming its necessity."

**Test:**
- Does this ablation test the specific design choice you highlighted in the method section?
- If you removed this experiment, could you still explain *why* your method works?

---

### **Discussion/Conclusion: Anchor the takeaway**

**Immovable element:**
- **One-sentence summary** of Contribution A, referencing the three anchors

**What it looks like:**
- "We demonstrated that Method X improves sample efficiency by 50% (Section 4.1), outperforms state-of-the-art baselines across five benchmarks (Section 4.2), and achieves these gains through its novel replay buffer design (Section 4.3)."

**Why immovable:**
- This is the claim readers will remember and cite
- It should map directly to your three anchors

**Test:**
- Can you trace each clause of this sentence back to a specific anchor experiment?
- If no, either the sentence is too vague or you're missing an anchor.

---

## **How to Prevent Drift During Revision**

Even with anchors in place, the spine can drift as you:
- Add new experiments (in response to reviews or new ideas)
- Reframe the contribution (after feedback)
- Cut content (to meet page limits)

Here's how to keep the spine rigid:

### **1. Create an "anchor checklist" before any major revision**

Before changing anything, verify:
- [ ] Existence anchor is still present and prominent (first result)
- [ ] Comparison anchor includes the most relevant baselines
- [ ] Mechanism anchor tests the design choice emphasized in the method section
- [ ] Introduction motivation aligns with existence anchor
- [ ] Conclusion summary references all three anchors

If any checkbox is unchecked after revision, you've drifted—realign.

---

### **2. Treat anchors as "protected" during cuts**

When cutting for space:
- **Never cut an anchor experiment entirely**
- **Never move an anchor to the appendix**
- **Never reduce an anchor to a single sentence**

Instead, cut:
- Additional datasets/tasks beyond the anchors
- Secondary baselines (keep the strongest 2–3)
- Supplementary analyses that don't support the anchors

---

### **3. When adding new experiments, classify them**

For every new experiment you consider adding, ask:
- **Does this strengthen an existing anchor?** (e.g., adds a stronger baseline to the comparison anchor) → **Add it**
- **Does this support Contribution A but isn't an anchor?** (e.g., additional dataset, robustness check) → **Add if space allows, otherwise appendix**
- **Does this support Contribution B?** → **Keep minimal, per previous advice**
- **Does this introduce a new claim not tied to Contribution A?** → **Cut or save for another paper**

This prevents scope creep.

---

### **4. Use anchor experiments to resolve disputes**

When co-authors or reviewers suggest changes that feel like they're shifting the contribution:

**Ask:**
- "Does this change affect one of our three anchor experiments?"
- If **yes**: "Does it strengthen the anchor or weaken it?"
  - Strengthen → Consider it
  - Weaken → Reject or modify
- If **no**: "Is this change essential to the story, or is it scope creep?"

This gives you a principled way to say no to suggestions that would dilute the spine.

---

### **5. Write the "anchor narrative" and don't deviate**

Draft a 3-sentence narrative that connects your anchors:

**Example:**
1. "We show that Method X achieves 50% better sample efficiency on Atari Breakout (existence anchor)."
2. "This advantage holds across five games and outperforms four baselines, including the state-of-the-art (comparison anchor)."
3. "Ablations reveal that our novel replay buffer design is responsible for these gains (mechanism anchor)."

**Pin this narrative** to your monitor, whiteboard, or paper draft. Every section should support this narrative. If a paragraph or experiment doesn't connect to one of these three sentences, question whether it belongs.

---

## **Advanced: Anchors for Different Contribution Types**

The three-anchor framework (existence, comparison, mechanism) works for most papers, but here's how to adapt it for different contribution types:

### **For a new method/algorithm:**
- **Existence:** Performance on one benchmark
- **Comparison:** Head-to-head with baselines on multiple benchmarks
- **Mechanism:** Ablation isolating the key algorithmic component

### **For a theoretical contribution:**
- **Existence:** One empirical validation of the theory's prediction
- **Comparison:** Show the theory explains phenomena that prior theories don't
- **Mechanism:** Proof or analysis showing why the theory holds

### **For a dataset/benchmark contribution:**
- **Existence:** Show the dataset captures the intended phenomenon (e.g., distribution shift, long-tail)
- **Comparison:** Existing methods fail or perform poorly on your dataset
- **Mechanism:** Analysis showing what makes your dataset challenging (e.g., feature analysis, human study)

### **For an empirical analysis/insight:**
- **Existence:** Demonstrate the phenomenon on one setting
- **Comparison:** Show the phenomenon holds across multiple settings/models
- **Mechanism:** Controlled experiment isolating the factor that causes the phenomenon

---

## **Red Flags That Your Anchors Are Weak**

Watch for these warning signs:

### **Existence anchor is weak if:**
- You're using a toy dataset or non-standard benchmark
- The effect size is small (<10% improvement)
- You can't replicate it consistently (high variance)

**Fix:** Switch to a more standard benchmark or strengthen the effect.

---

### **Comparison anchor is weak if:**
- You're only comparing to one baseline, or weak baselines
- You're using different evaluation protocols for your method vs. baselines
- Baselines aren't tuned or are from old papers

**Fix:** Add stronger baselines, ensure fair comparison, or re-run baselines with proper tuning.

---

### **Mechanism anchor is weak if:**
- The ablation shows all components matter equally (no clear key factor)
- Removing the "key" component only drops performance by <5%
- You have multiple ablations but no clear story about what matters most

**Fix:** Identify the single most important design choice and make that the focus. If everything matters equally, your contribution may be a "bag of tricks" rather than a principled method—consider reframing.

---

## **Practical Exercise: Identify Your Anchors Right Now**

Fill in this template for Contribution A:

**Existence Anchor:**
- Experiment: _______________
- Figure/Table: _______________
- One-sentence claim: _______________

**Comparison Anchor:**
- Experiment: _______________
- Figure/Table: _______________
- One-sentence claim: _______________

**Mechanism Anchor:**
- Experiment: _______________
- Figure/Table: _______________
- One-sentence claim: _______________

**Narrative (connect the three):**
_______________

If you can't fill this out clearly, your spine isn't well-defined yet. That's okay—use this as a guide to design the missing anchor experiments.

---

## **Key Takeaway**

**The three anchor experiments—existence, comparison, mechanism—are the load-bearing pillars of your paper.** Everything else is decoration. Lock them down early:
1. Design them to directly support Contribution A
2. Make them the first, second, and third major results in your experiments section
3. Protect them ruthlessly during revision
4. Ensure every other section (intro, method, conclusion) points back to them

If you keep these three experiments immovable, your spine won't drift—no matter how much feedback you get or how many revisions you go through.

---

Would it help to workshop your specific experiments? If you share what you're working on (even in broad strokes), I can help you identify which experiments should be your three anchors.

[Turn 5] USER:
Can the mechanism anchor ever be a theoretical proof instead of an ablation, or does that break the load-bearing rule?

[Turn 5] ASSISTANT:
Excellent question—and the answer is **yes, a theoretical proof can absolutely serve as a mechanism anchor**, but with important caveats. The load-bearing rule doesn't break; it just shifts what kind of weight the anchor needs to carry.

Let me break down when this works, when it doesn't, and how to structure it properly.

---

## **When a Theoretical Proof Works as a Mechanism Anchor**

A proof can be a mechanism anchor when it:

### **1. Explains the empirical phenomenon you demonstrated**
- Your existence and comparison anchors show *that* something works
- The proof explains *why* it works by establishing necessary/sufficient conditions

**Example:**
- **Existence anchor:** New optimization algorithm converges faster on benchmark X
- **Comparison anchor:** Outperforms SGD, Adam, etc. across multiple tasks
- **Mechanism anchor (proof):** Theorem showing the algorithm has provably faster convergence rate under certain smoothness assumptions

The proof is load-bearing because it **grounds the empirical gains in theory**—without it, you're just reporting "it works" without understanding why.

---

### **2. Makes a falsifiable prediction that you validate empirically**
- The proof predicts specific behavior (e.g., "convergence rate scales as O(1/k²)")
- You show empirically that this prediction holds

**Example:**
- **Mechanism anchor (proof):** Theorem proving your method's sample complexity is O(n log n) vs. O(n²) for baselines
- **Empirical validation:** Plot showing sample complexity scaling matches the theoretical prediction across problem sizes

This is load-bearing because the proof + validation together explain the mechanism. The proof alone wouldn't be enough (it's just math); the validation alone wouldn't be enough (it's just a curve). Together, they anchor the explanation.

---

### **3. Identifies the critical design choice and proves its necessity**
- The proof shows that a specific component is **necessary** for a desirable property
- This is the theoretical equivalent of an ablation

**Example:**
- **Mechanism anchor (proof):** Theorem showing that without your novel regularization term, the loss function is non-convex; with it, the loss is convex
- **Implication:** The regularization term is provably necessary for convergence guarantees

This is load-bearing because it isolates the key factor (just like an ablation would), but uses mathematical reasoning instead of empirical removal.

---

## **When a Proof Alone Is NOT Sufficient as a Mechanism Anchor**

A proof fails as a mechanism anchor when:

### **1. It's too abstract or disconnected from the empirical results**
- The proof makes assumptions that don't hold in practice
- The theorem applies to a simplified setting that doesn't match your experiments

**Example (bad):**
- **Existence anchor:** Method works on real-world image classification
- **Mechanism anchor (proof):** Theorem assuming convex loss, linear models, and i.i.d. data
- **Problem:** Real-world image classification violates all these assumptions—the proof doesn't actually explain why your method works in practice

**Fix:** Either (a) add empirical analysis showing the proof's insights transfer to the non-convex case, or (b) use an ablation instead.

---

### **2. It proves something orthogonal to the empirical contribution**
- The proof establishes a property (e.g., privacy guarantees) that isn't the main contribution
- The empirical results focus on something else (e.g., accuracy)

**Example (bad):**
- **Existence anchor:** Method achieves state-of-the-art accuracy on benchmark X
- **Mechanism anchor (proof):** Theorem proving the method satisfies differential privacy
- **Problem:** The proof doesn't explain *why accuracy improved*—it explains a different property

**Fix:** If privacy is important, make it part of Contribution A (or Contribution B). If accuracy is the spine, the mechanism anchor should explain accuracy gains.

---

### **3. The proof is trivial or doesn't add insight**
- The theorem is a straightforward consequence of known results
- It doesn't reveal anything non-obvious about why the method works

**Example (bad):**
- **Mechanism anchor (proof):** "Our method converges because it's a special case of gradient descent, which is known to converge."
- **Problem:** This doesn't explain why *your* method is better than standard gradient descent

**Fix:** Either prove something non-trivial (e.g., faster convergence rate, better sample complexity) or use an empirical ablation to isolate the key factor.

---

## **The Hybrid Approach: Proof + Empirical Validation**

In practice, the strongest mechanism anchors often **combine theory and empirics**:

### **Structure:**
1. **Theorem:** Proves a key property (e.g., convergence rate, sample complexity, approximation guarantee)
2. **Empirical validation:** Shows the theorem's prediction holds in practice
3. **Ablation (optional):** Confirms that removing the component the theorem focuses on degrades performance

This triple-anchor is extremely robust because:
- The **proof** provides mathematical insight
- The **validation** shows the insight is practically relevant
- The **ablation** confirms the component is necessary

### **Example (strong mechanism anchor):**

**Setting:** You propose a new neural network architecture with a novel attention mechanism.

**Mechanism anchor (hybrid):**
- **Theorem (Section 5.1):** Prove that your attention mechanism has O(n log n) complexity vs. O(n²) for standard attention
- **Empirical validation (Section 5.2, Figure 4):** Plot showing wall-clock time scales as predicted by the theorem across sequence lengths
- **Ablation (Section 5.3, Table 3):** Replacing your attention with standard attention increases runtime by 3× and reduces accuracy by 5%

**Why this is load-bearing:**
- The theorem explains *why* your method is faster (better complexity)
- The validation shows the theorem is practically relevant (not just asymptotic)
- The ablation shows the component is necessary (not just sufficient)

Without any one of these three, the mechanism anchor is weaker—but the theorem is still a critical part.

---

## **How to Structure a Proof-Based Mechanism Anchor**

If you're using a proof as your mechanism anchor, here's how to make it load-bearing:

### **1. State the theorem in the main text, proof in appendix**
- **Main text (Section 4.3 or 5):** Theorem statement, intuition, and implications (0.5–1 page)
- **Appendix:** Full proof with technical details

**Why:** Keeps the main narrative focused on the insight, not the technical machinery.

### **2. Lead with intuition, not formalism**
- Before stating the theorem, explain in plain language what it says and why it matters
- Use a concrete example or diagram to build intuition

**Example:**
> "Intuitively, our method works because it decouples the exploration and exploitation phases, allowing each to be optimized independently. We formalize this intuition in Theorem 1, which shows that this decoupling provably reduces sample complexity."

### **3. Connect the theorem to the empirical results**
- After stating the theorem, immediately show how it explains the existence/comparison anchors

**Example:**
> "Theorem 1 predicts that our method should require O(√n) samples vs. O(n) for baselines. Figure 4 confirms this prediction: across problem sizes n = 100 to 10,000, our method's sample complexity scales as √n (dashed line), while baselines scale linearly (solid lines)."

### **4. Acknowledge assumptions and limitations**
- Be explicit about what the theorem assumes (e.g., convexity, bounded gradients, i.i.d. data)
- Discuss whether these assumptions hold in your experiments

**Example:**
> "Theorem 1 assumes the loss function is L-smooth and μ-strongly convex. While real neural networks violate these assumptions, our empirical results (Section 4.2) suggest the theorem's insights extend to the non-convex setting, likely due to local convexity near minima."

This honesty strengthens the anchor—it shows you understand the gap between theory and practice.

---

## **Examples from Real Papers**

Here are examples of papers where a proof serves as the mechanism anchor:

### **Example 1: Adam optimizer (Kingma & Ba, 2015)**
- **Existence anchor:** Adam converges on several benchmarks
- **Comparison anchor:** Outperforms SGD, RMSprop, etc.
- **Mechanism anchor:** Theorem showing Adam's adaptive learning rates lead to better convergence in the convex case (though the proof's assumptions don't fully hold for deep learning)
- **Note:** The proof is somewhat disconnected from practice, which is why later work (e.g., "On the Convergence of Adam and Beyond") revisited the theory. But the original proof still provided valuable intuition.

### **Example 2: Neural Tangent Kernels (Jacot et al., 2018)**
- **Existence anchor:** Infinite-width neural networks behave like kernel methods
- **Comparison anchor:** NTK predictions match finite-width network behavior in certain regimes
- **Mechanism anchor (proof):** Theorem proving that in the infinite-width limit, gradient descent on a neural network is equivalent to kernel regression with the NTK
- **Strength:** The proof is the main contribution—it explains *why* neural networks can be analyzed as kernels

### **Example 3: Lottery Ticket Hypothesis (Frankle & Carbin, 2019)**
- **Existence anchor:** Pruned subnetworks can match full network performance
- **Comparison anchor:** This holds across multiple architectures and datasets
- **Mechanism anchor:** Mostly empirical (ablations on initialization, pruning strategies), but later work added theoretical analysis (e.g., "The Lottery Ticket Hypothesis at Scale")
- **Note:** The original paper is mostly empirical; theory came later to explain the mechanism

---

## **When to Choose Proof vs. Ablation as Mechanism Anchor**

Here's a decision guide:

| **Use a proof when:**                                      | **Use an ablation when:**                                |
|------------------------------------------------------------|----------------------------------------------------------|
| You have a clean mathematical model of the problem         | The problem is too complex to model theoretically        |
| The key insight is about fundamental properties (complexity, convergence, guarantees) | The key insight is about which component matters most    |
| You can make realistic assumptions                         | Assumptions would be too restrictive                     |
| The proof provides non-obvious insight                     | The proof would be trivial or well-known                 |
| Your audience values theory (e.g., COLT, ALT, NeurIPS theory track) | Your audience values empirics (e.g., applied ML, vision, NLP) |

**Rule of thumb:** If your contribution is primarily empirical (new method, better performance), default to ablation. If your contribution is primarily conceptual (new understanding, theoretical insight), a proof can be the mechanism anchor.

---

## **Can You Have Both Proof and Ablation?**

**Yes—and this is often the strongest approach.** Structure it as:

### **Section 4.3: Understanding the Mechanism**
- **4.3.1 Theoretical Analysis (Proof):** Theorem explaining why the key component should help
- **4.3.2 Empirical Validation (Ablation):** Experiments confirming the theorem's prediction and showing the component is necessary

This gives you:
- **Theoretical grounding** (the proof)
- **Empirical confirmation** (the ablation)
- **Robustness** (if one is questioned, the other still supports the mechanism)

---

## **Red Flags for Proof-Based Mechanism Anchors**

Watch out for these issues:

### **1. The proof is in the appendix and never mentioned in the main text**
- **Problem:** If the proof is invisible, it's not load-bearing
- **Fix:** Summarize the theorem and its implications in the main text (even if the proof is in the appendix)

### **2. The proof's assumptions are unrealistic and you don't acknowledge it**
- **Problem:** Reviewers will notice and question whether the proof is relevant
- **Fix:** Explicitly state assumptions and discuss when they hold/don't hold

### **3. The proof doesn't connect to the empirical results**
- **Problem:** The proof feels like a separate contribution, not an explanation of the empirical findings
- **Fix:** Add a figure or experiment that validates the proof's prediction

### **4. The proof is the *only* mechanism anchor (no empirical validation)**
- **Problem:** For empirical papers, theory alone may not convince readers
- **Fix:** Add at least one experiment that confirms the proof's insight (e.g., scaling plot, ablation)

---

## **Key Takeaway**

**A theoretical proof can absolutely be a mechanism anchor—it doesn't break the load-bearing rule.** But it must:

1. **Explain the empirical phenomenon** you demonstrated in the existence/comparison anchors
2. **Make falsifiable predictions** that you validate empirically
3. **Provide non-obvious insight** (not just restate known results)
4. **Connect clearly to the practical setting** (acknowledge assumption gaps)

The strongest mechanism anchors often **combine proof + empirical validation**: the proof provides the "why" (mathematical insight), and the validation shows the "so what" (practical relevance).

**Litmus test:** If you removed the proof, would you still understand *why* your method works? If yes, the proof isn't load-bearing (it's supplementary). If no, it's a true mechanism anchor.

---

Does this help clarify when a proof can serve as an anchor? If you're working on a specific theoretical contribution, I'd be happy to help you think through whether a proof alone is sufficient or whether you need empirical validation to make it load-bearing.

[Turn 6] USER:
If my proof predicts a scaling law, is a single log-log plot enough to validate it, or do reviewers expect more?

[Turn 6] ASSISTANT:
Great question—and the answer is **a single log-log plot is rarely enough on its own**, though it can be sufficient in specific cases. What reviewers expect depends on how strong your theoretical claim is and how much you're asking them to believe. Let me break down what makes a scaling law validation convincing.

---

## **Why a Single Plot Is Often Insufficient**

A log-log plot showing your data fits a predicted scaling law (e.g., y ∝ x^α) faces several challenges:

### **1. Overfitting to noise**
- With enough flexibility (choosing the range, cherry-picking the metric), you can make almost any data look linear on a log-log plot
- Reviewers know this and will be skeptical of a single curve

### **2. Confounding factors**
- Is the scaling due to your theoretical mechanism, or something else (e.g., implementation details, hardware constraints, numerical precision)?
- A single plot doesn't rule out alternative explanations

### **3. Limited generality**
- Does the scaling hold only for this specific setting (dataset, hyperparameters, random seed)?
- Or is it a robust phenomenon?

### **4. Statistical uncertainty**
- Without error bars or confidence intervals, reviewers can't assess whether the fit is meaningful or just noise

**Bottom line:** A single log-log plot is a good *starting point*, but reviewers will want to see that the scaling is **robust, reproducible, and not an artifact**.

---

## **What Reviewers Typically Expect**

To make a scaling law validation convincing, you generally need:

### **1. Multiple problem sizes/scales**
- **Minimum:** 5–7 data points spanning at least 2 orders of magnitude
- **Better:** 8–10 points spanning 3+ orders of magnitude

**Why:** With only 3–4 points, you can fit almost any curve. With 7+, the scaling relationship becomes much more credible.

**Example:**
- **Weak:** n = {100, 200, 500, 1000} (only 1 order of magnitude)
- **Strong:** n = {100, 200, 500, 1000, 2000, 5000, 10000, 20000} (2+ orders of magnitude)

---

### **2. Error bars or confidence intervals**
- Show variance across multiple runs (e.g., different random seeds, initializations)
- Fit the scaling law to the mean and show that individual runs are consistent

**Why:** This demonstrates the scaling isn't a fluke from one lucky run.

**How to present:**
- Shaded regions (mean ± std) on the log-log plot
- Or: Error bars at each data point

---

### **3. Goodness-of-fit metrics**
- Report R² or mean squared error of the fit
- Show that the predicted exponent α matches the theoretical prediction within confidence intervals

**Example:**
> "Our theorem predicts sample complexity scales as O(n^{0.5}). Fitting a power law to our data yields an exponent of 0.48 ± 0.03 (R² = 0.98), consistent with the theoretical prediction."

**Why:** This quantifies how well the data matches the theory.

---

### **4. Comparison to baselines**
- Show that your method follows the predicted scaling, while baselines follow a different scaling law

**Why:** This demonstrates that the scaling is specific to your method, not a general property of the problem.

**Example:**
- Your method: O(n^{0.5}) (as predicted by your theorem)
- Baseline 1: O(n^{1.0}) (linear scaling)
- Baseline 2: O(n^{0.7}) (intermediate scaling)

Plot all three on the same log-log plot with different colors/markers.

---

### **5. Robustness checks**
Show the scaling holds across:
- **Different datasets or tasks** (at least 2–3)
- **Different hyperparameters** (e.g., learning rate, batch size)
- **Different random seeds** (at least 3–5)

**Why:** This rules out the possibility that the scaling is an artifact of one specific setting.

**How to present:**
- Main plot: One representative task with error bars
- Supplementary plots (main text or appendix): Same scaling on 2–3 other tasks
- Or: A table showing fitted exponents across tasks, all close to the theoretical prediction

---

### **6. Validation in the regime where theory applies**
- If your theorem assumes certain conditions (e.g., large n, small learning rate), show the scaling holds in that regime
- Acknowledge when you're extrapolating beyond the theorem's assumptions

**Example:**
> "Our theorem assumes n → ∞. Figure 4 shows the predicted scaling emerges for n > 1000; for smaller n, finite-size effects dominate (gray region)."

**Why:** This shows intellectual honesty and helps reviewers trust your interpretation.

---

## **The Minimal Convincing Validation**

If you're constrained (e.g., compute budget, page limits), here's the **minimal set** that's usually convincing:

### **One main plot with:**
1. **7+ data points** spanning 2+ orders of magnitude
2. **Error bars** (mean ± std over 3–5 runs)
3. **Fitted curve** with the predicted exponent annotated
4. **Baseline comparison** (at least one baseline with different scaling)
5. **Goodness-of-fit metric** (R² or similar) in the caption

### **One supplementary validation:**
- Either: Same plot on a second task/dataset (shows generality)
- Or: Ablation showing the scaling breaks when you remove the key component (shows the mechanism is responsible)

**Example caption:**
> "Figure 4: Sample complexity scaling. Our method (blue) scales as O(n^{0.5}) as predicted by Theorem 1 (dashed line, R² = 0.98), while the baseline (orange) scales as O(n^{1.0}). Shaded regions show mean ± std over 5 runs. See Appendix B for results on two additional tasks."

---

## **When a Single Plot Might Be Enough**

A single log-log plot can be sufficient if:

### **1. The scaling is extremely clean and spans many orders of magnitude**
- Example: 10+ data points spanning 4+ orders of magnitude with R² > 0.99
- The fit is so good that alternative explanations seem implausible

### **2. The result replicates a well-known scaling law**
- Example: You're showing your method exhibits the same scaling as a famous result (e.g., neural scaling laws, power laws in language models)
- Reviewers are already familiar with this type of scaling, so less validation is needed

### **3. The scaling validation is a secondary contribution**
- Your main contribution is the method/algorithm, and the scaling law is a "bonus" insight
- In this case, a single plot might suffice, with a note that "further investigation is future work"

### **4. You're submitting to a venue that values theory over empirics**
- Example: COLT, ALT, or a theory-focused workshop
- Reviewers may be more interested in the proof than extensive empirical validation

**But even in these cases, adding error bars and a baseline comparison strengthens the claim significantly.**

---

## **Common Pitfalls to Avoid**

### **1. Cherry-picking the range**
- **Bad:** Only showing the range where the fit looks good, hiding the range where it breaks down
- **Fix:** Show the full range and acknowledge where the scaling starts/stops

**Example:**
> "The predicted scaling holds for n > 500 (solid region). For n < 500, finite-size effects cause deviations (gray region)."

---

### **2. Ignoring the intercept**
- **Bad:** Only checking that the slope matches the predicted exponent, ignoring the constant factor
- **Fix:** If your theorem predicts y = C · n^α, validate both α (slope) and C (intercept)

**Why:** Sometimes the exponent matches by coincidence, but the constant is way off—this suggests the mechanism is different than predicted.

---

### **3. Using too few data points**
- **Bad:** 3–4 points on a log-log plot (you can fit almost any curve)
- **Fix:** Aim for 7+ points

---

### **4. Not showing baselines**
- **Bad:** Only plotting your method's scaling
- **Fix:** Include at least one baseline to show your scaling is different/better

---

### **5. Overstating the conclusion**
- **Bad:** "This proves our theorem is correct."
- **Better:** "This is consistent with our theorem's prediction."

**Why:** Empirical validation can support a theory but rarely "proves" it (there could be other explanations). Use cautious language.

---

## **How to Structure the Validation in Your Paper**

Here's a recommended structure:

### **Main text (Section 4.3 or 5):**

**Paragraph 1: State the prediction**
> "Theorem 1 predicts that our method's sample complexity scales as O(n^{0.5}), compared to O(n) for standard approaches. We now validate this prediction empirically."

**Paragraph 2: Describe the experiment**
> "We evaluate sample complexity across problem sizes n ∈ {100, 200, 500, 1000, 2000, 5000, 10000} on the CartPole task. For each n, we measure the number of samples required to reach 90% success rate, averaged over 5 random seeds."

**Paragraph 3: Present the result**
> "Figure 4 shows the results on a log-log plot. Our method (blue) exhibits a power-law scaling with exponent 0.48 ± 0.03 (R² = 0.98), closely matching the predicted exponent of 0.5. In contrast, the baseline (orange) scales with exponent 0.97 ± 0.05, consistent with linear scaling."

**Paragraph 4: Robustness**
> "This scaling is robust across tasks: we observe similar exponents on Acrobot (0.51 ± 0.04) and MountainCar (0.49 ± 0.05) (see Appendix C, Figures 8–9)."

**Figure 4:**
- Log-log plot with your method + baseline
- Error bars
- Fitted lines with exponents annotated
- Caption with R² values

---

### **Appendix:**

**Additional validations:**
- Same plot on 2–3 other tasks
- Ablation: Scaling breaks when you remove the key component
- Sensitivity analysis: Scaling holds across different hyperparameters
- Table of fitted exponents across all settings

---

## **Examples from Real Papers**

Here are examples of how strong papers validate scaling laws:

### **Example 1: "Scaling Laws for Neural Language Models" (Kaplan et al., 2020)**
- **Validation:** 50+ data points spanning 6 orders of magnitude (10^6 to 10^12 parameters)
- **Robustness:** Tested across multiple architectures, datasets, and training procedures
- **Presentation:** Multiple log-log plots with clean power-law fits (R² > 0.99)
- **Strength:** The extensive validation made the scaling laws highly credible, even without a formal proof

### **Example 2: "The Lottery Ticket Hypothesis" (Frankle & Carbin, 2019)**
- **Validation:** Scaling of pruning ratio vs. performance across multiple architectures and datasets
- **Robustness:** Tested on MNIST, CIFAR-10, and various network sizes
- **Presentation:** Multiple plots showing consistent trends, with error bars
- **Note:** The scaling wasn't derived from a theorem, but the extensive empirical validation made it convincing

### **Example 3: "Neural Tangent Kernel" (Jacot et al., 2018)**
- **Validation:** Showed that finite-width networks approach the NTK prediction as width increases
- **Presentation:** Plots showing convergence to the theoretical prediction as width → ∞
- **Strength:** The validation directly tested the theorem's key assumption (infinite width limit)

---

## **Practical Recommendations**

### **If you have limited compute:**
1. **Prioritize range over density:** Better to have 7 points spanning 3 orders of magnitude than 20 points spanning 1 order
2. **Run multiple seeds at key points:** At least 3–5 seeds for the largest and smallest problem sizes
3. **Include one baseline:** Even a simple baseline is better than none

### **If you have more resources:**
1. **Add a second task/dataset:** Shows the scaling is general
2. **Add an ablation:** Shows the scaling is due to your mechanism
3. **Vary hyperparameters:** Shows the scaling is robust

### **If you're really constrained:**
- A single plot with 7+ points, error bars, and one baseline can be acceptable
- But acknowledge in the text that "further validation across tasks is future work"

---

## **Key Takeaway**

**A single log-log plot is a good start, but reviewers typically expect:**
1. **7+ data points** spanning 2+ orders of magnitude
2. **Error bars** (multiple runs)
3. **Baseline comparison** (different scaling)
4. **Goodness-of-fit metric** (R²)
5. **Robustness check** (second task or ablation)

**The minimal convincing validation is: one main plot with all of the above + one supplementary validation (second task or ablation).**

If your scaling law is a **core contribution** (not just a side observation), invest in thorough validation—it's worth it. If it's a **secondary finding**, a single well-executed plot with error bars and a baseline might suffice, but acknowledge the limitation.

---

Would it help to see a mock-up of what a strong scaling validation figure should look like? Or if you share details about your specific theorem and data, I can give more targeted advice on what validation would be most convincing.

[Turn 7] USER:
How do I decide which experiments are “core” enough to anchor the story when I’m still unsure what the main contribution actually is?

[Turn 7] ASSISTANT:
I notice you've asked this exact question before—it was the first question in our conversation. We've since had a detailed discussion about:

1. **Choosing between two candidate contributions** (which one becomes the spine)
2. **Keeping a secondary contribution from swelling** (maintaining narrative hierarchy)
3. **Identifying immovable anchor experiments** (existence, comparison, mechanism anchors)
4. **Using theoretical proofs as mechanism anchors** (when they work vs. when ablations are better)
5. **Validating scaling laws** (what makes empirical validation convincing)

Are you:
- **Circling back because you're still uncertain** about how to identify core experiments in your specific project?
- **Working on a different project** now and want to apply the framework fresh?
- **Looking for a different angle** on the original question that we haven't covered?

If you're still wrestling with identifying your core experiments, it might help to **share some specifics about your work**:
- What's the domain/problem area?
- What are your 2–3 candidate contributions?
- What experiments have you run (or are considering)?
- What's making it hard to decide which are "core"?

With concrete details, I can give you much more targeted advice—like "Experiment X should be your existence anchor because..." or "Contribution A is stronger than B because..."

Alternatively, if there's a **specific aspect of identifying core experiments** that our earlier discussion didn't address, let me know and I'll dig deeper into that angle.

[Turn 8] USER:
If I pick Contribution A as spine, how do I keep Contribution B from swelling into a second equal storyline while still giving it a fair supporting role?

[Turn 8] ASSISTANT:
I notice you've asked this exact question before—it was actually your **second question** in our conversation. We had a detailed discussion where I covered:

1. **Establishing hierarchy early** (abstract, intro framing)
2. **Controlling real estate asymmetrically** (page/figure budgets)
3. **Using structural cues** (section titles, placement)
4. **Limiting experimental depth for B** (fewer datasets, baselines, ablations)
5. **Framing B as a consequence** (not a co-equal claim)
6. **Integrating B into A's narrative** (avoiding parallel tracks)
7. **Resisting the urge to "do B justice"** (keeping it concise)
8. **Using the conclusion to reinforce hierarchy**
9. **The "cover-up" test** (does the paper work without B?)
10. **Warning signs that B is swelling** (red flags to watch for)

Are you:
- **Re-reading to reinforce the framework** before applying it to your draft?
- **Encountering a specific challenge** with your draft that the earlier advice didn't address?
- **Working on a different paper** now with a similar two-contribution structure?
- **Finding that B keeps swelling despite following the advice**, and need troubleshooting?

If you're facing a **specific structural problem** with your current draft, it would help to know:
- Where is B currently appearing in your paper? (which sections, how much space)
- What's making it hard to keep B subordinate? (e.g., reviewers asking for more, co-authors pushing for equal emphasis, B's experiments are surprisingly strong)
- Do you have a draft outline or table of contents you could share?

With specifics, I can give you targeted advice like "Move Section 4.3 to the appendix" or "Reframe this paragraph to make B sound more exploratory."

Or if there's a **new angle** on managing the A/B hierarchy that we didn't cover in the earlier discussion, let me know what aspect you'd like to explore further.
