1. Problem framing and goals
- Objective: Develop a robust, reproducible compression toolkit and methodology for large language models (LLMs) that achieves substantial memory and latency reductions with minimal accuracy loss. Target three compression dimensions: (a) quantization (weights, activations, KV cache), (b) sparsity/pruning (unstructured and structured), and (c) knowledge distillation for small students. Demonstrate end-to-end deployment speedups on modern inference stacks.
- Primary targets and success criteria:
  - Memory: ≥4× reduction vs. BF16 (e.g., 4-bit weights + 3–4 bit KV cache), with ≤1–2 point absolute drop on MMLU for 7–13B models and ≤0.5–1 point for 30–70B models.
  - Latency/throughput: ≥1.8× tokens/s speedup on A100/H100 with compatible kernels and serving (vLLM + FlashAttention-2) while preserving output quality on standard benchmarks.
  - Quality: Retain ≥98% of baseline across zero/few-shot tasks (e.g., MMLU, HellaSwag, ARC, GSM8K without CoT) and perplexity on Wikitext2/C4, with calibrated uncertainty where applicable. Use lm-eval-harness and long-context evals where relevant [16].
- Rationale and evidence: High-fidelity post-training quantization (GPTQ, AWQ, SmoothQuant/OmniQuant) and outlier-aware methods enable 4–6 bit weight/activation compression with limited degradation [1–4,10]. One-shot/pruning-aware methods can induce 30–60% sparsity with modest loss, especially with brief finetuning [5–7]. KV-cache quantization to 2–4 bits substantially cuts memory for long context with minor quality loss using asymmetric, group-wise designs [8–9]. Efficient serving (vLLM PagedAttention, FlashAttention-2) is critical to realize system speedups [13–14].

2. Experiments
Experiment 1: Systematic PTQ for weights and activations across methods and bit-widths
- Hypothesis: For 7–13B base models, 4-bit weight PTQ with outlier handling (GPTQ/AWQ) retains ≥98% baseline task accuracy; combining SmoothQuant/OmniQuant for activations enables 8–6-bit activation quantization with ≤1 point average drop on MMLU and negligible perplexity drift [1–4,10].
- Setup:
  - Models: Llama-3/3.1 8B/70B (or Mistral 7B) base and instruct.
  - Methods: GPTQ, AWQ (W-only and W+partial-A), SmoothQuant, OmniQuant, AQLM, QuIP# [1–4,10–12].
  - Calibration: 128–2048 samples from a mixture (C4, The Pile subsets, ShareGPT for instruct) and ablate calibration set size.
  - Knobs: group size, per-channel vs per-tensor, symmetric vs asymmetric, rounding (SGD rounding for AQLM/QuIP#).
- Baselines: BF16; LLM.int8 (outlier-aware matmuls) and ZeroQuant(-V2) as strong 8-bit baselines [3,15].
- Metrics: lm-eval-harness scores on MMLU, HellaSwag, ARC-Ch/E, BoolQ, WinoGrande; GSM8K (no CoT); perplexity on Wikitext2/C4; max throughput (tok/s), latency (p50/p99), peak VRAM.
- Expected outcomes: 4-bit weights (W4A16) via GPTQ/AWQ within ≤1 point MMLU drop; SmoothQuant/OmniQuant enable A8–A6 with small loss; AQLM/QuIP# offer incremental gains at <6-bit regimes [1–4,10–12].

Experiment 2: KV-cache quantization for long-context inference
- Hypothesis: Asymmetric, group-wise KV quantization to 2–3 bits (e.g., KIVI, KVQuant) preserves quality on long-context tasks with ≥2× memory savings and ≥1.3× throughput at 32–64k context on A100/H100 [8–9].
- Setup:
  - Methods: KIVI (2–3b asymmetric per-channel), KVQuant; compare naive static INT8 and per-token dynamic schemes [8–9].
  - Tasks: LongBench/Needle-in-a-Haystack variants; retrieval QA over 32k–64k context; perplexity vs. context length curves.
  - Serving: vLLM with PagedAttention and FlashAttention-2 kernels [13–14].
- Baselines: FP16/BF16 KV, LLM.int8 KV; no-quantization.
- Metrics: Task accuracy/F1; perplexity drift vs. context; GPU memory per request and batch; tokens/s vs. context length.
- Expected outcomes: 2–3b KV with KIVI/KVQuant yields ≤1–2% relative accuracy drop and meaningful throughput/memory gains; worst-case drift at very long contexts mitigated by outlier channel exceptions [8–9].

Experiment 3: Pruning for sparsity—unstructured vs. structured with light recovery
- Hypothesis: 40–60% unstructured sparsity via SparseGPT/Wanda maintains ≥95–98% of baseline accuracy with ≤3 epochs of LoRA recovery; structured pruning via LLM-Pruner at 10–30% yields real wall-clock speedups with smaller accuracy loss [5–7].
- Setup:
  - Methods: SparseGPT (one-shot Hessian-aware), Wanda (magnitude × activation), LLM-Pruner (attention/MLP head/channel pruning) [5–7].
  - Recovery: LoRA or QLoRA 1–3 epochs on 20–50k instruction + 100–200k pretraining mix.
  - Kernels: Evaluate sparse kernels (CUTLASS/cuSPARSELt) vs dense; block-sparse layouts for structured pruning.
- Baselines: No pruning; small-scale structured pruning heuristics.
- Metrics: Same accuracy suite; actual tokens/s and energy per token; FLOPs reduction; sparsity-pattern realizable speedup.
- Expected outcomes: 50% unstructured sparsity recovers to within ~1–2 points on MMLU with LoRA; structured 20% delivers consistent 1.2–1.5× speedup on GPU with ≤1 point average accuracy drop [5–7].

Experiment 4: Compositional recipe—prune → quantize → brief distillation
- Hypothesis: A staged pipeline (30–40% structured pruning → W4A8 PTQ → 2–3b KV → 1–2 epoch KD) matches or exceeds accuracy of quantization-only while achieving additional 1.2–1.5× speedup from structure and ≥1.5× memory savings from KV [1–4,7–9,10].
- Setup:
  - Teacher: BF16 baseline; Student: compressed model with LoRA adapters for KD.
  - KD Objective: KL on next-token distribution + response-level KD on instruction data (MiniLLM-style) [17].
  - Evaluate sensitivity to stage order and small tuning data sizes (10k–100k).
- Baselines: Best from Exp. 1–3 individually; distillation-only small model.
- Metrics: Same accuracy suite; calibration (ECE/Brier); robustness across domains.
- Expected outcomes: Combined pipeline closes most accuracy gaps vs. best individual method; KD most beneficial for reasoning-heavy tasks with W4A8.

Experiment 5: Quantization-aware finetuning (QAT) under small compute
- Hypothesis: Lightweight QAT (few thousand steps, layer-wise freezing, PEFT) reduces W4A6 degradation by 20–50% vs PTQ-only while limiting overfitting [15].
- Setup:
  - Start from PTQ models (Exp. 1); apply parameter-efficient QAT (freeze attention/key layers; train scaling/zero-points or small adapters).
  - Data: 50–200k mixed-domain tokens; early stopping on eval.
- Baselines: PTQ-only.
- Metrics: Same accuracy suite; training stability, time-to-quality.
- Expected outcomes: Consistent small gains (0.2–0.8 MMLU points) and better tail-task robustness, in line with prior QAT reports [15].

Ablations to include across experiments
- Calibration set size and domain shift sensitivity for PTQ [1–4,10].
- Group size, per-channel vs per-tensor, rounding schemes (AQLM/QuIP#) [11–12].
- Layer-wise bit-width allocation and mixed precision; outlier-channel exemption ratios [3–4,10].
- Pruning location (attention vs MLP), structured block sizes, recovery data scale [5–7].
- Serving stack: vLLM vs vanilla HF, FlashAttention-2 on/off; paged KV vs monolithic [13–14].

3. Timeline for the next 6 months with milestones
- Month 1: Foundations and baselines
  - Stand up eval stack (lm-eval-harness; long-context benchmarks) [16].
  - Implement PTQ baselines (LLM.int8, ZeroQuant, GPTQ, AWQ, SmoothQuant) on 7B model; collect calibration sets. Milestone: W8A8 and W4A16 results on 7B across 5+ tasks with a reproducible script [1–4,15–16].
- Month 2: Activation and KV quantization
  - Add OmniQuant and AQLM/QuIP#; run activation bit sweeps (A8→A6). Integrate KIVI/KVQuant and evaluate at 32k–64k context in vLLM [8–10,11–13]. Milestone: W4A8 + KV-3b reproducible recipe with ≤1pt avg MMLU drop; long-context memory/throughput plots.
- Month 3: Pruning track
  - Run SparseGPT and Wanda sweeps (30–60%); evaluate LoRA recovery; introduce LLM-Pruner for 10–30% structured sparsity [5–7]. Milestone: Pareto curves of sparsity vs accuracy vs tokens/s (with and without sparse kernels).
- Month 4: Compose and distill
  - Combine best pruning + quantization + KV; implement MiniLLM-style KD on 50–200k instructions [17]. Milestone: End-to-end compressed model with ≥1.8× speedup, ≥4× memory reduction, and ≤1pt MMLU drop on 7B.
- Month 5: QAT and scaling
  - Apply lightweight QAT to close residual gaps; scale to 13–34B; add domain robustness tests (code/math subsets). Milestone: Stable W4A6 recipe with small QAT that generalizes to 13–34B.
- Month 6: Hardening, reporting, and deployment
  - Stress-test on diverse tasks and long contexts; profile throughput/latency under vLLM; package open-source recipes, logs, and configs. Milestone: Camera-ready report with ablations, and a public repo with scripts and model cards.

4. Resources (compute, tools, datasets)
- Compute: 4× A100/H100 80GB recommended; 10–20 TB fast storage; for 70B-scale validations, short bursts on 8× H100. Mixed-precision kernels and vLLM serving nodes (>=80GB VRAM) for long-context tests.
- Tools:
  - Quantization/pruning: AutoGPTQ/GPTQ, AWQ, SmoothQuant (MIT-Han-Lab), OmniQuant, AQLM/QuIP#, SparseGPT, Wanda, LLM-Pruner [1–7,10–12].
  - Serving and kernels: vLLM (PagedAttention), FlashAttention-2; cuSPARSELt/CUTLASS for sparse benchmarking [13–14].
  - Evaluation: lm-eval-harness; LongBench; profiling scripts (Nsight, PyTorch profiler) [16].
- Datasets:
  - Calibration: C4 slices, The Pile subsets, ShareGPT/UltraChat-style instruction data (if licensing permits).
  - Evaluation: Wikitext2/C4 for perplexity; MMLU, HellaSwag, ARC, WinoGrande, BoolQ, GSM8K (no CoT) in lm-eval-harness; LongBench/Needle tasks for 32–64k contexts [16].
Note: For any instruction data, ensure licensing compliance and document data sources in a data card.

5. Risks and mitigations table
- Risk: Kernel support gaps reduce realized speedups (e.g., sparse matmul not accelerating on target GPU).
  - Mitigations: Prefer structured pruning patterns with supported kernels; benchmark on A100/H100 with vendor libraries; fall back to density-aware tiling [7,13–14].
- Risk: PTQ sensitivity to calibration data causing domain-specific degradations.
  - Mitigations: Use diverse calibration sets; layer-wise adaptive bit allocation; outlier-channel exemptions; small QAT/PEFT to correct drift [3–4,10,15].
- Risk: KV 2–3b quantization harming very-long-context reasoning.
  - Mitigations: Hybrid schemes (higher bits in early layers or key heads); dynamic error monitoring and selective dequantization; asymmetric per-channel quantization [8–9].
- Risk: Pruning harms reasoning/math disproportionately.
  - Mitigations: Prune later/MLP layers more; protect attention heads important for math/code (saliency-guided); brief LoRA recovery on math/code mixes [5–7].
- Risk: Distillation data quality and licensing.
  - Mitigations: Curate clean, diverse instruction mixes; prioritize permissive datasets; document licenses; add small task-specific KD.
- Risk: Reported gains don’t transfer to larger models.
  - Mitigations: Scale ablations to a 13–34B checkpoint mid-project; report scaling laws; maintain compute budget buffers.

6. Stretch ideas or follow-up directions
- Mixed-precision allocation via bilevel optimization: learn per-layer bit-widths with a latency/VRAM budget constraint; integrate with OmniQuant/AQLM rounding [10–11].
- Token-level early-exit and dynamic token pruning combined with quantization to further reduce latency (evaluate on vLLM).
- MoE-specific compression: gate sparsity and expert merging; quantize experts with per-expert calibration.
- Quantized speculative decoding: profile benefits in tandem with W4A8 + KV-3b.
- Learned KV eviction policies under PagedAttention for multi-tenant serving.
- Ultra-low-bit research: 3-bit weights with QuIP#/AQLM and small QAT on select layers [11–12].

Citations
[1] GPTQ: Accurate Post-Training Quantization for Generative Pretrained Transformers — https://arxiv.org/abs/2210.17323
[2] AWQ: Activation-Aware Weight Quantization for LLMs — https://arxiv.org/abs/2306.00978
[3] LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale — https://arxiv.org/pdf/2208.07339
[4] SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models — https://arxiv.org/abs/2211.10438
[5] SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot — https://arxiv.org/abs/2301.00774
[6] Wanda: An Efficient Post-Training Pruning Method for Large Language Models — https://arxiv.org/pdf/2306.11695
[7] LLM-Pruner: On the Structural Pruning of Large Language Models — https://arxiv.org/abs/2305.11627
[8] KIVI: A Tuning-Free Asymmetric 2-bit Quantization for KV Cache — https://arxiv.org/abs/2402.02750
[9] KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Compression — https://proceedings.neurips.cc/paper_files/paper/2024/file/028fcbcf85435d39a40c4d61b42c99a4-Paper-Conference.pdf
[10] OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models — https://arxiv.org/abs/2308.13137
[11] AQLM: Extreme Compression of Large Language Models via Additive Quantization — https://arxiv.org/abs/2401.06118
[12] QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks — https://arxiv.org/abs/2402.04396
[13] Efficient Memory Management for Large Language Model Serving (vLLM/PagedAttention) — https://arxiv.org/abs/2309.06180
[14] FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning — https://arxiv.org/abs/2307.08691
[15] ZeroQuant and ZeroQuant-V2: Post-Training Quantization in LLMs — https://arxiv.org/abs/2206.01861 and https://dl.acm.org/doi/10.1609/aaai.v38i17.29908
[16] EleutherAI lm-evaluation-harness — https://github.com/EleutherAI/lm-evaluation-harness
[17] MiniLLM: Knowledge Distillation of Large Language Models — https://arxiv.org/abs/2306.08543

Optional next step
- Greenlight Month 1: set up eval harness and implement PTQ baselines (GPTQ/AWQ/SmoothQuant) on a 7B model, then run the first W4A16 vs BF16 benchmark suite and publish a preliminary reproducibility report.