Stage: B → C (we’re moving from a well-defined idea to a concrete research plan with gated Phase 0)

Intuition: Multilingual fact-checking stresses both knowledge access (retrieval) and reasoning. Open-weight LLMs can be competitive with careful retrieval, prompting, and calibration, especially when evaluation controls for evidence grounding and cross-lingual retrieval quality. Leveraging compact retrievers and quantized 7–8B models lets you run credible experiments on limited GPUs.

Why this is principled: RAG mitigates hallucinations but needs careful multilingual evaluation and bias control [P5]. X-FACT is a strong multilingual fact verification dataset; coupling FEVER-style metrics with evidence grounding and calibration provides robust assessment [P4][n]. Modern multilingual retrievers (e.g., MINERS, bge-m3) and language-agnostic embeddings (LaBSE) are effective and efficient for cross-lingual retrieval [P8][n][n].

1) Problem framing and goals
- Problem: Benchmark open-weight LLMs for multilingual claim verification with evidence grounding. Compare direct zero-shot verification vs. retrieval-augmented verification across diverse languages under limited GPU.
- Scope:
  - Languages: prioritize 6–10 spanning high- and low-resource (e.g., en, es, de, ar, hi, tr, vi, sw).
  - Tasks: claim label (Supported/Refuted/Not Enough Info), evidence sentence retrieval, calibrated confidence.
  - Settings: zero-shot prompting, RAG with multilingual retrieval, translation-pivot vs native-language pipelines.
- Goals:
  - G1: Establish a reproducible, compute-efficient benchmark for multilingual fact-checking with open models (7–8B preferred).
  - G2: Quantify retrieval’s impact on accuracy and calibration, especially in low-resource languages.
  - G3: Provide an evidence-grounded evaluation suite: FEVER-style scoring, macro-F1, evidence precision/recall, MRR@k, ECE/Brier, and language-wise error analysis [n].
  - G4: Release code, prompts, and per-language results with sanity checks for leakage and overfitting.
- Deliverable outputs:
  - For each claim: JSON with {claim_lang, label, confidence, evidence_ids, evidence_text, retrieval_scores, rationale}.
  - For the benchmark: per-language metric tables, calibration curves, ablation figures (retriever/model/prompt/translation).

2) Experiments
Experiment 1: Does RAG improve multilingual verification and calibration?
- Hypothesis: RAG improves FEVER score and reduces ECE vs. direct zero-shot prompting, with larger gains in lower-resource languages [P5][n].
- Setup: Models: Qwen2.5-7B-Instruct, Llama-3.1-8B-Instruct (or closest open 8B), Mistral-7B-Instruct (4-bit quantized). Retrievers: bge-m3, LaBSE, BM25 (Pyserini) [n][n]. Corpora: Wikipedia per language (snapshot fixed date). Datasets: X-FACT (primary), MultiFC (English), CheckThat! tasks if claim-level available [P4][n][n]. Prompt: minimal chain-of-thought off + rationale-on-demand to control compute. Inference via vLLM or llama.cpp for quantized runs.
- Baselines: Zero-shot no retrieval; BM25 + linear/LogReg verifier; Cross-encoder XLM-RoBERTa-base verifier; RAG with bge-m3 vs. BM25.
- Evaluation metrics: FEVER score; macro-F1 on labels; evidence precision/recall@5, MRR@10; ECE and Brier score; latency per claim; per-language breakdown [n].
- Expected outcomes: RAG yields +3–8 FEVER points on low-resource languages, smaller but positive gains on high-resource; ECE drops with RAG and further with temperature scaling. If no gains, indicates retrieval mismatch or prompt misalignment; analyze evidence recall vs. label accuracy.

Experiment 2: Retriever comparison and its effect on downstream verification
- Hypothesis: Dense multilingual retrievers (bge-m3, LaBSE, MINERS-like setups) outperform BM25 for cross-lingual evidence, translating into better final verification, especially for non-English claims [P8][n][n].
- Setup: Build FAISS IVF-PQ indices for each language’s Wiki (and a pooled multilingual index). Compare BM25 (Pyserini), LaBSE, bge-m3, and mContriever variants [n][n][n]. Fix the verifier (e.g., Qwen2.5-7B) and prompts; vary only retrieval.
- Baselines: BM25; LaBSE; bge-m3; bge-m3 + cross-encoder rerank (XLM-R-base) limited to top-50.
- Metrics: Evidence recall@5/10, precision@5; MRR@10; end-to-end FEVER score; compute cost (embedding time, index size).
- Expected outcomes: bge-m3 ≥ LaBSE ≥ BM25 on cross-lingual recall, with moderate cost. If BM25 wins in some languages, likely due to tokenization/orthography; consider language-specific analyzers or hybrid dense+BM25.

Experiment 3: Translation pivot vs. native-language pipelines
- Hypothesis: Translating claims to English and retrieving from English Wikipedia gives competitive or superior performance for low-resource languages, but underperforms native retrieval in high-resource ones [P4][P5].
- Setup: Two pipelines per non-English claim: (A) Native retrieval + native verification; (B) Translate claim to English (small open MT or LLM translation), retrieve English evidence, verify in English, optionally translate rationale back. Control translation quality via back-translation BLEU/chrF.
- Baselines: Native-only RAG; Pivot-only RAG; Hybrid (retrieve in both and merge).
- Metrics: FEVER score; evidence precision/recall; calibration (ECE); translation fidelity metrics (chrF or COMETkiwi-lite if available).
- Expected outcomes: Pivot improves low-resource languages where native corpora sparse; native wins in high-resource. Hybrid often best but costlier. Divergences suggest translation brittleness or cultural domain gaps.

Experiment 4: Calibration without degrading accuracy (lightweight)
- Hypothesis: Temperature scaling or vector scaling reduces ECE/Brier with ≤0.5 FEVER loss.
- Setup: Fit calibrators on dev set predictions per language; apply at test.
- Baselines: Uncalibrated; temperature scaling; Dirichlet calibration (if feasible).
- Metrics: ECE, Brier, FEVER; NLL; reliability diagrams.
- Expected outcomes: Material ECE reduction with negligible accuracy change; if not, confidence outputs may be poorly aligned and need re-prompting for probability elicitation.

Experiment 5: Evidence grounding checks and hallucination control
- Hypothesis: Evidence-constrained prompting (require citation IDs) reduces unsupported rationales and yields higher evidence precision [P1][P5].
- Setup: Require models to cite retrieved passage IDs in JSON schema; reject answers without valid citation and re-ask with narrower context. Compare free-form vs. citation-constrained prompts.
- Metrics: Evidence precision/recall; fraction of unsupported rationales; FEVER; latency.
- Expected outcomes: Higher evidence precision with minimal FEVER trade-off; if latency rises, prune context or top-k.

3) Timeline for the next 6 months with milestones
- Phase 0 (Weeks 1–2, gate): 
  - Deliverables: (1) Prediction log with ≥14 entries; (2) Reproduce a baseline FEVER-style score on a 1k-claim multilingual slice from X-FACT with ≤10% relative gap across two seeds; (3) One ablation (BM25 vs bge-m3) with a one-page post-mortem.
  - Artifacts: Experiment card templates; dataset cards; fixed Wikipedia snapshot; prompt registry.
- Month 2:
  - Build multilingual indices (BM25, LaBSE, bge-m3); run Experiment 1 on 3 languages (en/es/ar); draft calibration curves.
  - Milestone: RAG vs zero-shot comparison report; decision on default retriever.
- Month 3:
  - Run Experiment 2 across all chosen languages; index optimization (IVF-PQ parameters); release interim retrieval metrics.
  - Milestone: Retriever leaderboard with per-language recall and MRR.
- Month 4:
  - Run Experiment 3 (pivot vs native); add lightweight calibrations (Experiment 4).
  - Milestone: Full ablation matrix; calibration results; initial draft figures.
- Month 5:
  - Run Experiment 5 (evidence grounding); finalize prompt schemas; stress-test on CheckThat! topics.
  - Milestone: End-to-end results with sanity checks; error taxonomy by language.
- Month 6:
  - Writing, reproducibility pack, and camera-ready materials: code release, Docker, model cards; submit to an appropriate venue or workshop.
  - Milestone: Public repo and report with artifact badges.

4) Resources (compute, tools, datasets)
- Compute:
  - Minimal GPU: single 16–24 GB GPU (e.g., RTX 3090/4090) for 4-bit quantized 7–8B models via vLLM or llama.cpp; CPU fallback for indexing/embedding. Batch inference with KV-caching; context ≤4k tokens.
  - Storage: ~200–400 GB for Wikipedia dumps (10 languages) + FAISS indices (IVF-PQ). 
  - Time: Indexing a medium dump with bge-m3 on CPU ~12–24h per language; can be parallelized on CPUs.
- Tools:
  - LLM inference: vLLM or llama.cpp (quantized); HuggingFace transformers.
  - Retrieval: FAISS; Pyserini BM25; sentence-transformers (bge-m3, LaBSE).
  - Evaluation: fever-score implementation; sklearn for F1; calibration libraries (netcal).
  - Data processing: WikiExtractor; ftlangdetect or fastText for language ID sanity checks.
- Datasets/Corpora:
  - X-FACT (25 languages) for claim verification [P4].
  - MultiFC (English; evidence-based fact-checking) [n].
  - CheckThat! CLEF tasks for claim verification/check-worthiness (use claim-level subsets) [n].
  - FEVER (English) for metric alignment and sanity checks [n].
  - Wikipedia snapshots per language (fixed date); optionally CCNet or news for out-of-domain probing.
- Models:
  - Open LLMs: Qwen2.5-7B-Instruct (strong multilingual) [n]; Llama-3.x-8B-Instruct (or closest open multilingual); Mistral-7B-Instruct.
  - Retrievers: bge-m3 [n]; LaBSE [n]; mContriever/variants [n].
  - Re-ranker (optional): XLM-R-base cross-encoder if compute allows.

5) Risks and mitigations table
- Risk: Dataset mismatch or leakage (e.g., claims tied to post-snapshot content).
  - Mitigation: Freeze a Wikipedia snapshot date; filter evidence by timestamp; run leakage probe by removing all retrieved evidence and checking residual accuracy.
- Risk: Translation artifacts bias pivot pipeline.
  - Mitigation: Back-translation checks; report translation fidelity (chrF/COMET-lite); ablate with native-only subset; manual spot checks per language.
- Risk: Retrieval dominates performance; model differences obscured.
  - Mitigation: Fix retriever to best-in-class and compare models; also fix model and vary retrievers (two-way ablation).
- Risk: Calibration metrics unstable due to small per-language test sizes.
  - Mitigation: Use bootstrapped CIs; aggregate across similar languages; predefine stop rules for reporting.
- Risk: GPU scarcity slows runs.
  - Mitigation: Prefer 4-bit inference; shard evaluation (100–500 claim batches); prioritize languages and run adaptive sampling.
- Risk: Evidence hallucinations with plausible but uncited rationales.
  - Mitigation: Enforce citation schema; reject and reprompt without citations; compute evidence precision; manual audit sample.
- Risk: Generic or off-topic references.
  - Mitigation: Maintain a living bibliography; if sources are preprints or tangential, flag and replicate claims on your data; seek task-specific shared tasks (CLEF) for anchoring [n].

6) Stretch ideas or follow-up directions
- Cross-lingual RAG bias analysis: Measure cultural or regional bias in retrieval and answer selection; evaluate with culturally sensitive tasks [P5].
- Long-context evidence chains: Test if models can reason over multi-hop multilingual evidence; adapt from long-context multilingual evaluations [P2].
- Hallucination-oriented evaluation: Add Poly-FEVER-style hallucination tests for multilingual settings (note: recent preprint) [P3].
- Knowledge editing stress tests: After editing model knowledge (MLaKE), re-evaluate verification to test robustness to updates [P7].
- Semantic retrievers as LLMs: Explore MINERS-style LLM-as-retriever approaches in multilingual setups [P8].

Sanity checks (run continuously)
- Label priors: Compare predicted label distribution to gold per language; large skews flag miscalibration.
- Retrieval sanity: Oracle vs. no-retrieval bounds; if “no retrieval” is close to oracle, evidence signals aren’t used.
- Evidence locality: Ensure cited evidence actually contains claim entities; compute entity overlap and lexical overlap.
- Adversarial slices: Test time-shifted claims and OOD topics; expect graceful degradation.
- Reproducibility: Two seeds or prompt variants per setting; ≤10% relative metric variance.

Expected outputs (by end of Month 6)
- Benchmark report with:
  - Per-language tables: FEVER, macro-F1, evidence P/R, MRR@10, ECE, Brier.
  - Plots: RAG gain by language resource level; calibration reliability diagrams; retriever recall vs. FEVER.
  - Ablation matrix: model × retriever × prompt × pivot.
- Open repository with:
  - Docker, configs, prompt registry, experiment cards, and fixed Wikipedia snapshot hashes.
  - JSONL predictions with evidence IDs and confidences.
  - Reproducibility script to re-run on 1k-claim subset.

Citations within plan
- RAG benchmarking and multilingual evaluation framing [P1][P5].
- X-FACT dataset [P4].
- FEVER dataset/metric [n].
- MultiFC [n]; CheckThat! CLEF [n].
- Multilingual retrieval (MINERS), bge-m3, LaBSE, mContriever [P8][n][n][n].
- Note: Some sources (e.g., P2, P3) are recent preprints; use as directional references and validate on your data.

Optional next step: Start Phase 0 by indexing a single language (Arabic) with bge-m3 and BM25, run 1k-claim X-FACT slice through zero-shot vs. RAG with Qwen2.5-7B (4-bit), and produce a one-page ablation memo with FEVER, evidence recall, and ECE per setting.

Citations
- [P1] MIRAGE-Bench: Automatic Multilingual Benchmark Arena for Retrieval-Augmented Generation Systems — http://arxiv.org/abs/2410.13716v2
- [P2] Can LLMs reason over extended multilingual contexts? Towards long-context evaluation beyond retrieval and haystacks — http://arxiv.org/abs/2504.12845v1
- [P3] Poly-FEVER: A Multilingual Fact Verification Benchmark for Hallucination Detection in Large Language Models — http://arxiv.org/abs/2503.16541v2
- [P4] X-FACT: A New Benchmark Dataset for Multilingual Fact Checking — http://arxiv.org/abs/2106.09248v1
- [P5] Multilingual Retrieval Augmented Generation for Culturally-Sensitive Tasks: A Benchmark for Cross-lingual Robustness — http://arxiv.org/abs/2410.01171v3
- [P7] MLaKE: Multilingual Knowledge Editing Benchmark for Large Language Models — http://arxiv.org/abs/2404.04990v3
- [P8] MINERS: Multilingual Language Models as Semantic Retrievers — http://arxiv.org/abs/2406.07424v3
- [n] FEVER: a Large-scale Dataset for Fact Extraction and VERification — https://aclanthology.org/N18-1074/
- [n] MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking — https://arxiv.org/abs/1909.03242
- [n] CLEF CheckThat! overview (2024–2025) — https://ceur-ws.org/Vol-3740/paper-64.pdf ; https://arxiv.org/abs/2503.14828
- [n] bge-m3 embedding — https://arxiv.org/abs/2402.03216
- [n] LaBSE — https://arxiv.org/abs/2007.01852
- [n] mContriever — https://arxiv.org/pdf/2112.09118
- [n] Qwen2.5 blog/tech report — https://qwenlm.github.io/blog/qwen2.5 ; https://arxiv.org/abs/2412.15115