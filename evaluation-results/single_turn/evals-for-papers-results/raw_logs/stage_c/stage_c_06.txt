Stage: B – Idea moving to C – Research plan

Intuition: In low-resource settings, the binding constraints are uplink bandwidth, intermittent connectivity, device heterogeneity, and privacy. Federated learning only works reliably when the algorithm, communication, and privacy stack are co-designed for those constraints—using asynchronous aggregation, update compression, and on-device DP so the server never sees raw gradients. As a result, “recipes” that trade off accuracy for orders-of-magnitude savings in communication and energy—and that quantify privacy loss—are more valuable than marginal accuracy gains.

Why this is principled: Canonical FL results show simple FedAvg is brittle under non-IID data and system heterogeneity, motivating methods like FedProx, SCAFFOLD, and server-side adaptive optimizers (FedOpt) [1][2][3][4]. Communication and energy are first-order costs, so quantization/sparsification (e.g., QSGD, STC) and asynchronous/buffered aggregation (e.g., FedBuff) materially improve viability under poor connectivity [9][10][17]. Privacy and trust rely on DP-SGD and secure aggregation protocols to bound leakage while hiding individual updates [5][6], and system design lessons at scale emphasize client sampling, failure handling, and practical deployments [7]. Benchmarks like WILDS and widely used datasets (Common Voice, MedMNIST, ChestX-ray14) let us simulate non-IID, low-resource distributions and measure fairness across clients [11][12][13][14].

1) Problem framing and goals
- Problem: Design, evaluate, and open-source practical FL methods that remain accurate, private, and affordable under: low/variable bandwidth, intermittent connectivity, heterogeneous/low-end devices, non-IID data, and limited engineering support typical in low-resource countries.
- Primary goals (ranked):
  1) Communication/energy efficiency: ≥10× reduction in uplink bytes and ≥30% device energy savings vs. FedAvg with ≤2–3 percentage-point accuracy loss on target tasks [9][10][17].
  2) Robustness to non-IID and dropouts: Maintain ≥90% of centralized accuracy with 30–50% client dropout; improve 10th-percentile client accuracy by ≥5 points via personalization [1][2][3][4][11].
  3) Privacy by design: Achieve ε ≈ 3–8 (δ ≤ 1e−5) with secure aggregation and <5-point accuracy drop vs. non-DP FL [5][6].
- Secondary goals:
  - Field-viable recipes: Asynchronous training + update compression + DP + SecAgg + lightweight monitoring [6][7][8][9][17].
  - Fairness: Report tail performance (10th percentile client accuracy) and participation equity [11].
  - Reproducibility: Release code, configs, and seeds; reproduce ≥1 standard baseline within 10% relative gap across ≥3 seeds [7].

2) Experiments
Experiment 1: Async + compressed FL under intermittent connectivity
- Hypothesis: Buffered asynchronous aggregation with quantized updates (QSGD or sparse ternary) matches FedAvg accuracy within ≤2 pts while reducing uplink by ≥10× and tolerating 30–50% client dropouts [9][10][17].
- Setup: Flower or FedML server with FedBuff; clients simulated and on-device (10–20 Raspberry Pi 4/Android phones). Datasets: MedMNIST (multi-task, lightweight) and WILDS (e.g., iWildCam or Camelyon17 for non-IID) [11][13]. Methods: FedAvg (baseline) [1]; FedBuff (async) [17]; FedOpt (server-side Adam) [4]; QSGD and STC compression [9][10].
- Baselines: FedAvg (sync); FedAvg + FedOpt; FedAvg + compression.
- Metrics: Test accuracy/F1; time-to-90%-of-centralized accuracy; uplink/downlink bytes per round; client energy (battery drain or power meter); convergence stability under 30–50% random dropouts [7][9][17].
- Expected outcomes: FedBuff + QSGD/STC achieves ≥10× comm reduction, ≤2–3 pt accuracy loss, and faster wall-clock to target accuracy under stragglers [9][10][17]. If accuracy degrades >3 pts, increase buffer size or mix occasional full-precision rounds.
- Interpretation: If async + compression preserves accuracy, it validates a deployment-ready recipe. If not, tighten staleness bounds or mix periodic synchronous rounds. Follow-ups: evaluate adaptive compression (error feedback) and staleness-aware weighting [3][9][17].

Experiment 2: Personalization for non-IID clients
- Hypothesis: FedProx or SCAFFOLD with local fine-tuning improves 10th-percentile client accuracy by ≥5 points over FedAvg at the same communication budget on highly skewed label distributions (Dirichlet α=0.2) [2][3].
- Setup: Create non-IID partitions on MedMNIST and WILDS datasets [11][13]. Methods: FedAvg [1], FedProx (μ tuned) [2], SCAFFOLD [3], and simple personalization head or local FT (1–3 epochs) after global aggregation.
- Baselines: Centralized upper bound; local-only training lower bound.
- Metrics: Median and 10th-percentile client accuracy; fairness gap (median − 10th percentile); bytes per round; rounds-to-target; ablation on α and client availability [11].
- Expected outcomes: SCAFFOLD and FedProx stabilize under heterogeneity; adding a small personalized head or local fine-tuning yields the largest tail gains with limited extra cost [2][3]. If fairness doesn’t improve, try client clustering and per-cluster models.
- Interpretation: Positive tail gains with small overhead support personalization as a first-class ingredient. Follow-ups: cluster-based personalization; meta-learning warm starts; conditional computation [3][11].

Experiment 3: Privacy–utility with DP-SGD + Secure Aggregation
- Hypothesis: Client-side DP-SGD with secure aggregation achieves ε ≈ 3–8 with ≤5-point accuracy loss vs. non-DP FL and <10% training-time overhead on small images/speech tasks [5][6].
- Setup: Implement DP-SGD on clients via Opacus or TensorFlow Privacy; moments accountant to report (ε, δ) [5]. Secure aggregation protocol as per Bonawitz et al. (simulate if library support is limited) [6]. Run on MedMNIST and a small Common Voice subset for two low-resource languages [12][13].
- Baselines: Non-DP FL with/without SecAgg; centralized DP-SGD.
- Metrics: Accuracy/F1; ε, δ; communication and training-time overhead; attack proxies (membership inference risk). Report accuracy-vs-ε Pareto curves [5][6].
- Expected outcomes: Moderate accuracy drop with DP-SGD; SecAgg overhead primarily in protocol setup/round-trip [5][6]. If utility drops >5 pts, reduce clipping, tune noise multipliers, or apply partial DP (DP on last layers).
- Interpretation: Establishes defensible privacy budgets for practical deployments. Follow-ups: privacy amplification via subsampling, secure aggregation robustness to dropouts [5][6][7].

Experiment 4: Energy/availability-aware client sampling
- Hypothesis: Sampling policies that prioritize high-battery/low-latency windows cut average client energy by ≥30% with ≤1-point loss in accuracy compared to uniform sampling [7][17].
- Setup: Android clients exposing battery/network state; Raspberry Pi with power meters. Compare uniform sampling vs. availability- and energy-aware sampling; combine with FedBuff to absorb stragglers [7][17].
- Baselines: Uniform client sampling; time-of-day heuristic.
- Metrics: Client energy (Wh), participation rate, rounds-to-target, accuracy.
- Expected outcomes: Energy-aware sampling reduces energy and improves participation with minimal accuracy loss. If accuracy suffers, add reweighting or quotas to maintain client diversity [7][17].

Note: We did not find robust, peer-reviewed case studies specifically in low-resource countries beyond general FL systems and methods; emerging preprints target Africa and clinical settings but may be early-stage [P1][P2]. We propose gathering deployment constraints via a small pilot with local institutions to calibrate bandwidth, availability, and device distributions before large-scale trials.

3) Timeline for the next 6 months with milestones
- Phase 0 (Weeks 1–2) — Gate before scaling:
  - Deliverables: (1) Prediction log with ≥14 entries; (2) Reproduce FedAvg and centralized baselines on MedMNIST with ≤10% relative gap across ≥3 seeds; (3) One experiment card and one ablation/negative result written up [7].
  - Infra: Set up Flower/FedML; network emulation (tc/netem); energy measurement harness; CI for reproducibility [7].
- Month 1–2: Experiment 1 (Async + compression)
  - Milestones: 10× comm reduction with ≤3-pt accuracy loss; dropout robustness (30–50%). Write ablation report (compression level, staleness, buffer size) [9][10][17].
- Month 2–3: Experiment 2 (Personalization under non-IID)
  - Milestones: +5 pts in 10th-percentile client accuracy; fairness report; release configs for WILDS split [2][3][11].
- Month 3–4: Experiment 3 (DP + SecAgg)
  - Milestones: ε ≈ 3–8 with ≤5-pt accuracy loss; privacy-utility curve; overhead characterization; draft privacy appendix [5][6].
- Month 4–5: Experiment 4 (Energy/availability-aware sampling)
  - Milestones: ≥30% energy reduction; stable accuracy; on-device demo on 10–20 devices; systems appendix [7][17].
- Month 5–6: Integration + writing
  - Milestones: Consolidated “recipe” with knobs; end-to-end scripts; red-teaming (membership inference proxy); paper draft with artifact appendix; open-source release; optional small NGO/clinic pilot planning [7][11].

4) Resources (compute, tools, datasets)
- Compute:
  - Server: 1 workstation with 2× RTX 3090/4090 or 1–2 A100s (or equivalent cloud), 128 GB RAM; 2 TB SSD. This suffices for MedMNIST/WILDS-scale experiments and rapid iteration.
  - Clients: 10–20 Raspberry Pi 4 (4–8 GB) and/or Android phones; optional power meters for a subset.
- Tools:
  - Frameworks: Flower or FedML; PyTorch; Opacus or TensorFlow Privacy; network emulation via Linux tc/netem; logging/metrics (Weights & Biases).
  - Privacy/SecAgg: Implement Bonawitz et al. protocol or use available libraries; if unavailable, simulate SecAgg and document assumptions [5][6][7].
- Datasets:
  - Generalization and non-IID: WILDS (iWildCam, Camelyon17) [11].
  - Health: MedMNIST v2 (lightweight, multi-task) [13]; ChestX-ray14 (subset) [14].
  - Speech: Mozilla Common Voice for low-resource languages [12].
  - Agriculture: PlantVillage (smartphone imagery); if licensing allows, create non-IID splits by region.
- Baselines and methods:
  - FedAvg [1], FedProx [2], SCAFFOLD [3], FedOpt [4], Asynchronous FedOpt [8], FedBuff [17], QSGD [9], STC [10], DP-SGD [5], SecAgg [6].

5) Risks and mitigations
- Connectivity too unstable for sync rounds
  - Mitigation: Prefer buffered async (FedBuff); opportunistic upload windows; increase staleness tolerance [7][17].
- Severe non-IID harms convergence
  - Mitigation: FedProx/SCAFFOLD; local personalization; client clustering; server adaptive optimizers [2][3][4].
- Communication cost still too high
  - Mitigation: Stronger compression (QSGD/STC), error feedback, periodic full-precision sync [9][10].
- Privacy budgets unacceptable to partners
  - Mitigation: Tune DP noise/clipping; apply layer-wise DP; combine with SecAgg; present privacy-utility curves [5][6].
- Limited device energy and participation
  - Mitigation: Energy-aware sampling; time-of-day scheduling; small local epochs; partial client participation [7][17].
- Reproducibility gaps
  - Mitigation: Open configs, seeds, and CI; report 3-seed means/variances; adherence to systems reproducibility checklists [7].
- Regulatory and data governance hurdles
  - Mitigation: Early IRB/ethics engagement; data minimization; privacy impact assessment; on-device logging only.
- Security/poisoning attacks
  - Mitigation: Basic robust aggregators (median/trimmed mean); anomaly detection; limit per-round influence; future byzantine-robust methods.
- Limited context-specific literature
  - Mitigation: Run a small scoping study with a local partner to measure bandwidth/availability/device profiles; adapt parameters accordingly. Note: domain-specific, in-country FL case studies are sparse [P1][P2].

6) Stretch ideas or follow-up directions
- Parameter-efficient FL: LoRA/adapter-style updates to further cut communication; combine with compression.
- Fairness-aware FL: Optimize for worst-group or quantile performance; evaluate with WILDS [11].
- Cross-modal deployments: Speech (Common Voice) + clinical imaging; test multi-task FL with modality-specific heads.
- Hybrid cross-device/cross-silo: Mix a few stable institutions (cross-silo) with many transient phones (cross-device) using hierarchical aggregation [7].
- Opportunistic transport: SMS/USSD or delay-tolerant networking for tiny gradient sketches in ultra-poor connectivity contexts (conjectural; prototype and measure).
- Field pilot: Collaborate with an NGO/clinic to validate the recipe in one country; log real dropout/latency to refine simulation distributions.

Literature anchors
- Communication-Efficient Learning of Deep Networks from Decentralized Data (FedAvg) — https://proceedings.mlr.press/v54/mcmahan17a.html [1]
- Towards Federated Learning at Scale: System Design — https://mlsys.org/Conferences/2019/doc/2019/193.pdf [7]

Citations
- [P1] Democratizing AI in Africa: Federated Learning for Low-Resource Edge Devices — https://arxiv.org/abs/2408.17216
- [P2] Federated learning in low-resource settings: A chest imaging study in Africa — https://arxiv.org/html/2505.14217v1
- [P3] A Practical Recipe for Federated Learning Under Statistical Heterogeneity Experimental Design — https://arxiv.org/abs/2307.15245
- [1] Communication-Efficient Learning of Deep Networks from Decentralized Data (FedAvg) — https://proceedings.mlr.press/v54/mcmahan17a.html
- [2] FedProx: Federated Optimization in Heterogeneous Networks — https://arxiv.org/abs/1812.06127
- [3] SCAFFOLD: Stochastic Controlled Averaging for Federated Learning — https://proceedings.mlr.press/v119/karimireddy20a.html
- [4] Adaptive Federated Optimization (FedOpt) — https://arxiv.org/abs/2003.00295
- [5] Deep Learning with Differential Privacy (DP-SGD) — https://arxiv.org/pdf/1607.00133
- [6] Practical Secure Aggregation for Privacy-Preserving Machine Learning — https://dl.acm.org/doi/10.1145/3133956.3133982
- [7] Towards Federated Learning at Scale: System Design — https://mlsys.org/Conferences/2019/doc/2019/193.pdf
- [8] Asynchronous Federated Optimization — https://arxiv.org/abs/1903.03934
- [9] QSGD: Communication-Efficient SGD via Gradient Quantization — https://proceedings.neurips.cc/paper/2017/file/6c340f25839e6acdc73414517203f5f0-Paper.pdf
- [10] Sparse Ternary Compression for Communication-Efficient FL — https://export.arxiv.org/pdf/2003.03564v2.pdf
- [11] WILDS: A Benchmark of in-the-Wild Distribution Shifts — https://arxiv.org/abs/2012.07421
- [12] Mozilla Common Voice (low-resource languages) — https://datacollective.mozillafoundation.org/datasets/cmfzu8u8wa555eq8onrk334h4
- [13] MedMNIST v2 — https://arxiv.org/abs/2110.14795
- [14] ChestX-ray14 (NIH) — https://arxiv.org/abs/1705.02315
- [15] Advances and Open Problems in Federated Learning (survey) — https://arxiv.org/pdf/1912.04977
- [16] signSGD: Compressed Optimization for Non-Convex Problems — https://proceedings.mlr.press/v80/bernstein18a/bernstein18a.pdf
- [17] Federated Learning with Buffered Asynchronous Aggregation (FedBuff) — https://arxiv.org/abs/2106.06639

Limitations and how to improve evidence
- The mentorship guideline tool returned no specific guidance for this exact scenario; we rely on standard systems/ML reproducibility and ablation practices plus surveys [15]. For context-specific deployments in low-resource countries, peer-reviewed, large-scale case studies appear sparse; to strengthen external validity, we propose a scoping pilot with a local partner to measure real connectivity/availability/energy distributions and iterate parameters before any scale-up.

Optional next step: Stand up Experiment 1 in Flower using MedMNIST with FedAvg vs. FedBuff + QSGD, log bytes/energy with tc/netem-induced 2G/3G conditions, and produce a 2-page ablation report with time-to-90%-of-centralized accuracy and a 10× communication target.