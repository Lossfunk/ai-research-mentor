1. Problem framing and goals
- Objective: Deliver a privacy-preserving, open-source mental health chatbot stack and evaluation program that (a) prioritizes on-device or end-to-end encrypted (E2EE) inference, (b) prevents sensitive data leakage via PII redaction and privacy-preserving learning, and (c) demonstrates safety guardrails for self-harm, clinical claims, and misinformation. This is not a clinical device; the system must clearly disclaim that it does not provide diagnosis/treatment and must escalate crises to appropriate resources [10][11].
- Privacy and safety pillars:
  - Local-first inference where feasible (llama.cpp with GGUF models) to keep user text on device [3].
  - If learning from user data, use federated learning with secure aggregation and client-level differential privacy (DP) to bound leakage risk [1][2].
  - E2EE transport and storage for messaging/logs (Matrix/Olm/Megolm), plus PII detection/redaction (Presidio) before any optional aggregation [7][8][9].
  - Safety guardrails for crisis detection and response flows (NeMo Guardrails), aligned with health AI governance (WHO) and AI risk management (NIST AI RMF) [5][6][8].
- Six-month success criteria:
  - Privacy: Client-level DP training with a cumulative ε≤5, δ=1e−5 in a federated prototype, with secure aggregation, and verified E2EE transport [1][2][9].
  - Safety: ≥95% correct escalation for simulated self-harm scenarios without giving clinical instructions; zero disallowed content in a held-out harmful prompt set; clear non-diagnostic disclaimers [5][8][11].
  - Utility: Comparable helpfulness to a baseline open-source LLM on empathetic dialogue tasks (e.g., EmpatheticDialogues) with ≤2–3 point drop in human-rated empathy under privacy constraints [4][12].
  - Transparency: Public model/data cards and an AI risk register per NIST AI RMF; documented guardrail policies and incident response SOPs [6].

2. Experiments (each with hypothesis, setup, baselines, evaluation metrics, expected outcomes)
Experiment 1: Local-first inference vs. server inference for privacy and latency
- Hypothesis: On-device inference via llama.cpp achieves acceptable latency on consumer laptops/phones and eliminates server-side data exposure; E2EE client–server inference preserves similar privacy for transit but increases attack surface on the server [3][9].
- Setup:
  - Models: Quantized small/medium open LLMs in GGUF (e.g., 3–8B class) served via llama.cpp; identical models served remotely behind an E2EE Matrix client [3][9].
  - Data: Non-sensitive empathetic dialogue prompts (EmpatheticDialogues test splits) and synthetic mental health–style prompts vetted by clinicians for research use [4].
  - Security: Matrix E2EE (Olm/Megolm) for client–server; local logging disabled; disk encryption enabled [9].
- Baselines: Remote inference over TLS without E2EE; CPU-only local inference.
- Metrics: p50/p95 latency; tokens/s; local memory/CPU; qualitative privacy risk analysis (data at rest/in transit, threat model coverage); user preference in a small blinded study (n≈30).
- Expected outcomes: Local inference meets <1.5 s first-token and ≥15 tok/s on laptops; zero network exposure for content; E2EE server path shows larger attack surface at rest but secured in transit [3][9].

Experiment 2: PII detection and safe logging pipeline
- Hypothesis: A Presidio-based PII pipeline can remove identifiable information with high recall before optional analytics, reducing re-identification risk without materially harming utility for safety analytics [7].
- Setup:
  - Pipeline: Presidio analyzers + custom rules (mental-health entities) + hashing/tokenization; evaluate pre/post redaction effects on safety classifiers [7].
  - Data: Synthetic PII-embedded mental-health dialogues; small samples from open datasets augmented with named entities (no real PHI).
- Baselines: No redaction; regex-only.
- Metrics: PII detection precision/recall/F1; residual PII rate in samples; impact on downstream safety detection (self-harm intent); manual audit of 200 messages by two reviewers (κ agreement).
- Expected outcomes: ≥0.9 recall and ≥0.85 precision for PII; ≤0.5% residual PII in audited samples; negligible drop (<1–2 points) in safety classifier performance [7].

Experiment 3: Safety guardrails for self-harm and clinical claims
- Hypothesis: Rule- and LLM-based guardrails (NeMo Guardrails) configured with crisis response templates improve safe refusals and correct escalation to crisis resources relative to an unguarded baseline [8][11].
- Setup:
  - Guardrails: Flows for self-harm, violence, illegal instructions, and clinical claims; refusal templates; crisis escalation providing hotline and immediate help guidance; never provide diagnosis/treatment [8][11].
  - Evaluation set: Simulated crisis prompts (incl. CLPsych-like self-harm patterns), misinformation checks, and clinical claim traps; ensure no real user data [13].
- Baselines: Same model without guardrails; simple keyword filters.
- Metrics: Correct escalation rate on self-harm prompts; refusal rate on clinical diagnosis requests; harmful content rate; false-positive refusal; human rater agreement.
- Expected outcomes: ≥95% correct escalation; 0 harmful responses; ≤10% over-refusal on benign items (to be tuned) [8][11][13].

Experiment 4: Privacy-preserving learning with FL + secure aggregation + DP
- Hypothesis: Cross-device/cross-silo federated fine-tuning with secure aggregation and client-level DP (DP-SGD) can achieve near-baseline helpfulness/empathy with bounded privacy loss (ε≤5) [1][2].
- Setup:
  - Framework: Flower (or similar) for FL; secure aggregation for model updates; Opacus-like DP accounting (noise multiplier/clip norm sweeps) [1][2].
  - Data: Non-clinical empathetic dialogue (EmpatheticDialogues); no real mental-health chat; simulate non-IID clients [4].
- Baselines: Centralized fine-tuning without DP; FL without DP.
- Metrics: Empathy/helpfulness via crowd ratings; perplexity; ε,δ accounting; membership inference risk proxy; convergence rounds and bytes transferred.
- Expected outcomes: ≤2–3 point drop in empathy/helpfulness vs. non-DP baseline at ε≤5; secure aggregation prevents server-side gradient inspection [1][2][4].

Experiment 5: Governance and risk management drill (WHO + NIST)
- Hypothesis: A formal risk register, model/data cards, and incident response runbooks aligned with WHO guidance for health AI and NIST AI RMF reduce deployment risk and improve reviewer confidence [5][6].
- Setup:
  - Produce AI system card, training data card, privacy threat model, and misuse/abuse scenarios; conduct a tabletop incident drill (e.g., guardrail failure) [5][6].
- Baselines: None (process evaluation).
- Metrics: Coverage of WHO/NIST checklists; time-to-mitigation in drill; reviewer checklist completeness.
- Expected outcomes: Complete documentation set and a rehearsed incident response path acceptable for an academic artifact appendix [5][6].

3. Timeline for the next 6 months with milestones
- Month 1: Foundations and governance
  - Lock scope, disclaimers, and non-clinical use; set up repo and risk register per NIST AI RMF; outline WHO-aligned safeguards and data governance [5][6].
  - Milestones: System card v0.1; ethics review; dataset curation plan (non-clinical).
- Month 2: Local inference and E2EE prototypes
  - Integrate llama.cpp for on-device; stand up Matrix E2EE demo; measure latency/footprint; disable persistent logs [3][9].
  - Milestones: Latency and privacy threat model report; on-device demo video.
- Month 3: PII redaction and guardrails
  - Implement Presidio redaction; configure NeMo Guardrails flows (self-harm, clinical claims); build red-team evaluation sets [7][8][11][13].
  - Milestones: PII precision/recall and residual audit; guardrail v1 safe-response rates.
- Month 4: Federated DP training prototype
  - Stand up Flower with secure aggregation; run DP-SGD sweeps; evaluate empathy/helpfulness on held-out data [1][2][4].
  - Milestones: Privacy–utility frontier (ε vs. performance); recommended operating point.
- Month 5: Integrated pilot and user study
  - Combine local inference + guardrails + PII pipeline; small lab user study (n≈30–50) for helpfulness/empathy and perceived privacy; finalize crisis escalation templates [4][8][11].
  - Milestones: Pilot report; incident drill report; updated SOPs.
- Month 6: Consolidation and artifact release
  - Freeze models/configs; publish open-source toolkit (scripts, configs, prompts, evaluation sets), model/data cards, and preprint; document limitations and non-clinical scope [5][6].
  - Milestones: Camera-ready preprint; public repo with reproducibility checklist.

4. Resources (compute, tools, datasets)
- Compute and hosting
  - Local devices for llama.cpp (laptop/mobile); optional CPU/GPU server for federated coordinator (no user data stored). All components open-source.
- Open-source tools
  - On-device inference: llama.cpp and GGUF models [3].
  - E2EE messaging: Matrix (Olm/Megolm) libraries and homeserver [9].
  - PII redaction: Microsoft Presidio [7].
  - Safety: NeMo Guardrails (open-source) [8].
  - Privacy-preserving learning: Flower (FL), DP-SGD accounting (Opacus-like), secure aggregation protocol following Bonawitz et al. [1][2].
  - Risk management: WHO health AI guidance; NIST AI RMF templates [5][6].
- Datasets (non-clinical, open)
  - EmpatheticDialogues for empathy-oriented conversations (license check required) [4].
  - Synthetic/self-authored safety and crisis prompts inspired by CLPsych task patterns (no real user data) [13].
- Notes on constraints
  - We did not locate high-confidence, peer-reviewed work evaluating llama.cpp on mental-health tasks specifically; we will measure utility and document limitations (Conjecture where direct evidence is lacking).
  - Clinical screening tools like C-SSRS are referenced to inform response flows but must not be administered by the chatbot; any use requires appropriate licensing and clinical oversight [10][11].

5. Risks and mitigations table
- Privacy leakage via logs or transport
  - Mitigations: Default no-logging; local-only inference; E2EE (Matrix) if server used; PII redaction before any analytics; periodic privacy audits [7][9].
- DP utility loss in FL
  - Mitigations: Tune clipping/noise; increase local steps; personalize heads; report ε–utility frontier for informed choice [1][2].
- Harmful or clinical advice slips past guardrails
  - Mitigations: Multi-layer guardrails (rules + LLM checks); crisis-only templates with immediate escalation; periodic red-teaming; incident response SOPs [8][11].
- Dataset bias and domain mismatch
  - Mitigations: Use empathy-focused, non-clinical corpora; diverse synthetic prompts; fairness audits; clearly scope limitations [4][6].
- Regulatory/ethical misalignment
  - Mitigations: WHO and NIST-aligned governance; disclaimers; non-clinical labeling; avoid PHI; external review [5][6].
- Security of federated coordinator
  - Mitigations: Secure aggregation; hardened server; minimal metadata retention; key management reviews [2][9].
- User misunderstanding (believes chatbot is a clinician)
  - Mitigations: Prominent non-clinical disclaimers; periodic reminders; mandatory acknowledgment; fast paths to human help [5][11].

6. Stretch ideas or follow-up directions
- On-device fine-tuning with parameter-efficient methods and DP (e.g., LoRA + DP-SGD) to avoid central aggregation.
- Formal verification of guardrail flows using property tests; continuous safety evaluation pipeline.
- Emotion/state tracking fully on-device with no persistence; weekly transparency reports of blocked content (aggregated, differentially private).
- Multilingual extensions with localized crisis resources and culturally adapted guardrails (governance co-design).

Sources
[1] Abadi et al. Deep Learning with Differential Privacy — https://arxiv.org/abs/1607.00133  
[2] Bonawitz et al. Practical Secure Aggregation for Federated Learning on User-Held Data — https://arxiv.org/abs/1611.04482  
[3] llama.cpp (local, on-device LLM inference for GGUF models) — https://github.com/NousResearch/nous-llama.cpp  
[4] Rashkin et al. Towards Empathetic Open-domain Conversation Models (EmpatheticDialogues) — https://aclanthology.org/P19-1534/  
[5] WHO guidance on AI ethics and governance for health — https://www.who.int/publications/i/item/9789240084759  
[6] NIST AI Risk Management Framework (AI RMF 1.0) — https://nvlpubs.nist.gov/nistpubs/ai/nist.ai.100-1.pdf  
[7] Microsoft Presidio (open-source PII detection/redaction) — https://github.com/microsoft/presidio  
[8] NVIDIA NeMo Guardrails (open-source safety toolkit) — https://github.com/NVIDIA-NeMo/Guardrails  
[9] Matrix end-to-end encryption (Olm/Megolm) — https://matrix.org/docs/matrix-concepts/end-to-end-encryption/  
[10] Columbia-Suicide Severity Rating Scale (C-SSRS) overview — https://cssrs.columbia.edu/  
[11] Scoping and evaluations of mental health chatbots (review examples) — https://www.jmir.org/2023/1/e47217/  
[12] Additional EmpatheticDialogues references — https://arxiv.org/abs/1811.00207  
[13] CLPsych shared task (self-harm risk patterns; for inspiration only) — https://aclanthology.org/W19-3003/

Optional next step
- Approve Months 1–2 to stand up local inference + E2EE prototypes and implement PII redaction/guardrails; we will return with latency, safety, and privacy threat-model reports and a proposed DP/FL operating point for Month 4.