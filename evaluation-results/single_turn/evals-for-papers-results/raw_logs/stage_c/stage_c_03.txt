1. Problem framing and goals
- Objective: Not “cure cancer tomorrow,” but deliver a translationally meaningful, prospective-ready AI pipeline in 6 months that (a) improves outcome prediction and treatment response stratification, and (b) nominates repurposable drugs for one or two high-burden indications (e.g., NSCLC, breast), with external validation and a registered analysis plan. Foundation pathology models and multimodal benchmarks show feasibility for survival and diagnosis; we’ll build on them and target external generalization and calibration suitable for a pilot study [1][2][6].  
- Primary goals (measurable):  
  - Survival prediction (WSI+omics): c-index ≥0.70 on TCGA/CPTAC internal validation; within ≤10% relative gap on external cohort (e.g., METABRIC or MSK-IMPACT) with good calibration (ECE ≤0.05) [1][6][11].  
  - Immunotherapy response (CT): AUC ≥0.75 for binary response; RECIST-compatible lesion change estimation; decision-curve net benefit > standard-of-care models [8][9].  
  - Drug repurposing: Top-50 candidates enriched for known/ongoing trials; AUROC ≥0.75 on held-out known indication links; at least 3 mechanism-plausible candidates per target disease for expert review [12][13].  
- Phase 0 gates (≤14 days): (1) Reproduce a literature baseline (e.g., WSI-only Cox with Virchow embeddings on TCGA; c-index within 10% of reference) [1], and (2) produce one ablation or negative result plus an experiment card and a 14-entry prediction log.

Intuition
- Most near-term clinical uplift comes from better patient selection: who benefits, from what, and when. Multimodal models (pathology images + genomics) and radiology-based predictors already show signal; combining them with rigorous external validation and calibration can change decisions sooner than de novo drug discovery [1][2][6][8][11].

Why this is principled
- It leverages strong foundation models and curated benchmarks (Virchow, MLOmics) and prioritizes generalization, calibration, and external validation, which are prerequisites for clinical impact and regulatory pathways [1][6][14]. Retrospective replication + external validation + pre-registered analysis are standard translational steps before a prospective pilot [11][14].

2. Experiments
Experiment 1: Multimodal survival prediction (WSI + multi-omics)  
- Hypothesis: Late-fusion of pathology foundation model embeddings with omics improves c-index over WSI-only and omics-only baselines on TCGA/CPTAC and generalizes to METABRIC/MSK-IMPACT [1][6][11].  
- Setup: Extract WSI embeddings from a foundation model (e.g., Virchow) and gene panels (e.g., RNA-seq, mutation) from TCGA; train Cox PH and DeepSurv variants with late fusion; use CPTAC as secondary internal validation [1][6].  
- Baselines: WSI-only (linear Cox on Virchow embeddings), omics-only (elastic-net Cox), clinical-only (TNM, stage).  
- Metrics: c-index, integrated Brier score (IBS), calibration (ECE), decision-curve analysis.  
- Expected outcomes: +0.03–0.06 c-index over strongest single-modality baseline; better calibration after Platt/temperature scaling; ablation shows both WSI and omics contribute [1][6]. Follow-ups: replace late fusion with cross-attention; test site-robust training with stain normalization.

Experiment 2: Immunotherapy response prediction from pre-treatment CT  
- Hypothesis: Pre-treatment CT with 3D CNNs or diffusion backbones improves response AUC and RECIST change prediction versus classical radiomics in NSCLC [8][9].  
- Setup: Use TCIA/other public NSCLC cohorts with PD-1/PD-L1 labels; train 3D CNN/diffusion model; lesion-level auxiliary loss for RECIST diameter change estimation [8][9].  
- Baselines: Radiomics + XGBoost; 3D ResNet without lesion guidance.  
- Metrics: AUC/AUPRC for response, MAE for RECIST change, net benefit, subgroup fairness (smoking status, stage).  
- Expected outcomes: AUC ≥0.75 and RECIST MAE improvement versus radiomics; calibration curve close to diagonal; if gains vanish on external site, apply domain adaptation and harmonization [8][9].

Experiment 3: External generalization on MSK-IMPACT/real-world data  
- Hypothesis: Models trained on TCGA/CPTAC maintain ≤10% relative performance drop on MSK-IMPACT or other RWD cohorts when paired with simple domain adjustments and clinical covariates [10][11].  
- Setup: Refit risk models using overlapping features (e.g., gene panels) and batch-correct distributions; evaluate on MSK-IMPACT or similar cohorts; integrate structured EHR via MSK-CHORD-like pipelines for improved outcome prediction [11].  
- Baselines: OncoCast and clinical-only risk models (stage, age) [10].  
- Metrics: c-index, calibration, time-dependent AUC; report hazard ratios for risk strata.  
- Expected outcomes: ≤10% relative c-index drop; improved calibration with Platt scaling; if failure, quantify shift and retrain with transfer learning [10][11].

Experiment 4: Graph-based drug repurposing  
- Hypothesis: Knowledge-graph + LINCS expression signatures with GNNs yield enriched top-k lists of plausible repurposing candidates for target indications versus Connectivity Map scoring [12][13].  
- Setup: Build a disease–gene–drug graph (LINCS, DrugBank, DGIdb, HPO); train GNN for link prediction; validate against known indication links and ongoing trials; shortlist candidates for the target cancer (e.g., NSCLC, breast) [12][13].  
- Baselines: CMap signature matching; logistic regression on hand-crafted features.  
- Metrics: AUROC/AUPRC for known links; top-k enrichment against trial databases; mechanistic plausibility (target pathways).  
- Expected outcomes: AUROC ≥0.75; 3–5 high-plausibility hits sent for expert triage; negative controls (toxic, non-oncology) down-weighted [12][13].

3. Timeline for the next 6 months with milestones
- Phase 0 (Weeks 1–2): Data access (TCGA/CPTAC/TCIA/licensing), DUA/IRB checks, reproduce a literature baseline on TCGA WSI-only survival; deliver experiment cards, prediction log (≥14 entries), and one ablation.  
- Month 2: Train/validate Experiment 1 (internal CV on TCGA/CPTAC); run ablations and calibration; draft Methods for preprint.  
- Month 3: Run Experiment 2 (CT immunotherapy) with external validation; produce decision-curve analysis; draft Results v1.  
- Month 4: Run Experiment 3 (external generalization on MSK-IMPACT or equivalent); pre-register analysis plan; calibration and fairness checks.  
- Month 5: Run Experiment 4 (drug repurposing); triage top candidates with domain experts; compile mechanistic rationale.  
- Month 6: Integrate all results; finalize preprint and code release; submit to a translational venue; prepare IRB for a small prospective observational pilot.

4. Resources (compute, tools, datasets)
- Compute: Recommended 2–4× A100 80GB or equivalent for WSI/3D-CT; 10–20 TB fast storage; 256–512 GB RAM for slide tiling and training.  
- Tools: MONAI, TIAToolbox/Slideflow for WSI; PyTorch/Lightning; scikit-survival/lifelines; Captum/SHAP; Weights & Biases + DVC; Great Expectations for data QA.  
- Datasets: TCGA WSIs/omics; CPTAC proteogenomics; METABRIC (external breast); TCIA for CT; MSK-IMPACT (NGS, outcomes) where permitted; LINCS L1000; CTRP/GDSC for drug sensitivity; cBioPortal for harmonized clinical data [1][6][8][10][11][12].  
- Regulatory: Align with SaMD expectations early (locked model versions, change control, real-world monitoring) [14].  
Note: A prior “unified” literature query returned off-topic items; we therefore used targeted queries and will perform a systematic review using search strings combining method+task+dataset (e.g., “foundation model pathology survival TCGA”, “immunotherapy CT RECIST NSCLC”, “LINCS graph neural drug repurposing”), screened by venue/date.

5. Risks and mitigations table
- Data leakage or label leakage → Strict patient-level splits; nested CV; audit pipelines; preregister splits.  
- Distribution shift (scanner/site/assay) → Stain/feature harmonization; domain adaptation; external validation before claims [1][11].  
- Small N and overfitting → Parsimonious models; heavy regularization; bootstrap CIs; report variance across seeds.  
- Poor calibration → Post-hoc calibration (Platt/temperature); decision-curve analysis to quantify clinical utility.  
- Missing modalities → Late-fusion with missingness-aware encoders; train imputation baselines.  
- Confounding in observational data → Causal sensitivity analyses; target trial emulation for treatment effect claims; limit scope to prognostic/risk stratification unless causal assumptions justified.  
- IRB/DUA delays → Start applications in Week 1; prioritize public datasets first; design for portability.  
- Reproducibility gaps → Versioned data/code, run sheets, experiment cards; open-sourced baselines.  
- Regulatory misalignment → Map artifacts to SaMD guidance (intended use, performance, change management) [14].

6. Stretch ideas or follow-up directions
- Tri-modal fusion (WSI+omics+CT) with cross-attention and uncertainty quantification; evaluate OOD detectors for safe deployment.  
- Target-trial emulation for treatment effect estimates in RWD, pre-registered with sensitivity bounds.  
- Federated evaluation across institutions for site-robustness; privacy-preserving fine-tuning.  
- Active learning to prioritize wet-lab validations (e.g., CRISPR screens) for top drug candidates.  
- Trial enrichment optimization: model-based inclusion criteria to maximize power and patient benefit [11].

Literature anchors
- Virchow: a whole-slide foundation model enabling survival and diagnosis signals in real-world pathology [1].  
- MLOmics: a multi-omics benchmark clarifying modeling choices and evaluation for oncology [6].

Citations
[1] A whole-slide foundation model for digital pathology from real-world data — https://www.nature.com/articles/s41586-024-07441-w  
[2] A foundation model for generalizable cancer diagnosis and survival prediction from histopathological images — https://www.nature.com/articles/s41467-025-57587-y  
[3] A multi-modal deep learning framework for pan-cancer prognosis — https://arxiv.org/abs/2501.07016  
[4] Path-GPTOmic: A Balanced Multi-modal Learning Framework for Survival Outcome Prediction — https://arxiv.org/abs/2403.11375  
[5] Self-Normalizing Foundation Model for Enhanced Multi-Omics Data Analysis in Oncology — https://arxiv.org/abs/2405.08226  
[6] MLOmics: Benchmark for Machine Learning on Cancer Multi-Omics Data — https://arxiv.org/abs/2409.02143  
[7] Deep learning-driven survival prediction integrating histology and genomics — https://academic.oup.com/bib/article/doi/10.1093/bib/bbaf121/8089949  
[8] Deep Learning to Estimate RECIST in NSCLC Treated with PD-1 Blockade — https://pmc.ncbi.nlm.nih.gov/articles/PMC7981277/  
[9] ImmunoDiff: A Diffusion Model for Immunotherapy Response Prediction in Lung Cancer — https://arxiv.org/pdf/2505.23675.pdf  
[10] Integration of Next Generation Sequencing Data to Inform Survival Prediction Using OncoCast Algorithm — https://pmc.ncbi.nlm.nih.gov/articles/PMC12249426/  
[11] Automated real-world data integration improves cancer outcome prediction using MSK-CHORD — https://www.nature.com/articles/s41586-024-08167-5  
[12] Drug repurposing against breast cancer with LINCS + GNNs — https://pmc.ncbi.nlm.nih.gov/articles/PMC8479657/  
[13] A foundation model for clinician-centered drug repurposing — https://www.nature.com/articles/s41591-024-03233-x  
[14] Artificial Intelligence in Software as a Medical Device (SaMD) — FDA — https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-software-medical-device

Optional next step
- In the next 48 hours, pick one target disease (e.g., NSCLC), lock the datasets (TCGA + TCIA + one external cohort), and draft the Experiment 1 and 2 cards (hypothesis, falsifier, minimal test, variables, analysis plan, stop rule). Then start Phase 0 by reproducing the TCGA WSI-only survival baseline with Virchow embeddings and logging your first 14 predictions.