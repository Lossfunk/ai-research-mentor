1. Problem framing and goals
Framing: With no local compute, focus on research contributions that rely on rigor over training: evaluation methodology, reproducibility audits, benchmark design/critique, and contamination analysis using public model outputs or small API calls. These areas are high-leverage, undersupplied, and publishable when done carefully [P10][G5]. Recent work shows impactful progress can come from better experimental design and robust evaluations (e.g., tiny-sample evaluation, benchmark agreement, contamination-limited test sets) without training any models [P1][P4][P5].

Goals (6 months):
- Deliver one reproducibility audit with strong negative/qualified claims (with CIs and ablations) [P2][G8].
- Deliver one benchmark-method artifact (tiny-sample evaluation protocol or contamination audit toolkit) with public data/code [P1][P3].
- Submit to a Datasets & Benchmarks/evaluation venue or workshop; release a reproducible toolkit and report [P10].

Intuition: You can substitute compute with careful design: small, statistically efficient samples; replication across seeds/prompts; and contamination-aware test sets reveal real signal without training new models [P1][P3][P5].

Why this is principled: Methodological rigor and problem selection drive impact even when resources are limited (Hamming; Schulman). Reproducibility work has repeatedly corrected over-claims in LLM prompting/“reasoning,” and robust benchmarking is an active frontier [P2][G5][G7].

2. Experiments
Experiment 1 — Tiny-sample evaluation replication and extension
- Hypothesis: A tiny-sample protocol (e.g., 20–50 items per task with bootstrap CIs) preserves model rankings within τ ≥ 0.7 of full benchmarks, cutting evaluation cost by >80% [P1].
- Setup: Select 3–4 public tasks with available items (e.g., subsets of LiveBench and classic tasks), and 3–5 public models via free/demo endpoints or existing outputs; evaluate with 3 seeds and report CIs [P1][P4].
- Baselines: Full-sample leaderboard rankings where available; random ranking baseline.
- Metrics: Kendall/Spearman rank correlations to full benchmarks, CI width, cost/runtime.
- Expected outcomes: Tiny-sample ranks correlate highly with full sets; CI-based decision rules reduce overclaiming [P1]. If correlation is low, analyze item selection and task instability [P5].

Experiment 2 — Reproducibility audit of CoT/self-consistency claims
- Hypothesis: Claimed gains from CoT/self-consistency shrink when controlling for sampling seeds, temperatures, and prompt templates; some effects are benchmark- or prompt-specific [P2].
- Setup: For 2–3 small tasks (e.g., 50-item arithmetic/reasoning subsets), compare Direct Answer vs CoT with fixed and varied templates, 5–10 seeds, and temperature grid; use only public APIs or public outputs where possible.
- Baselines: Majority-class/random; best non-CoT prompt per task.
- Metrics: Absolute accuracy/EM; effect sizes with bootstrapped CIs; multiple-comparison correction.
- Expected outcomes: Reduced, task-dependent CoT gains; clear reporting templates and stop rules. If gains persist, identify the conditions and publish as stability evidence [P2].

Experiment 3 — Benchmark data contamination audit (lightweight)
- Hypothesis: Older benchmarks show measurable overlap with public web sources; contamination-limited sets (e.g., LiveBench) reduce this exposure and yield different model orderings [P3][P4].
- Setup: Choose 2 benchmarks (one older, one contamination-limited). For 200–500 items, run exact/fuzzy n-gram searches on the web/archives to flag potential leakage; stratify analyses by suspected overlap.
- Baselines: Random web pages; null of no differential performance across overlap vs non-overlap items.
- Metrics: Overlap rate; performance difference Δ across overlap strata; rank shifts; simple logistic regression of correctness ~ overlap flag.
- Expected outcomes: Higher overlap in older sets; attenuated performance and/or different ranking on contamination-limited data [P3][P4]. If no effect, document methods and propose stronger tests.

Experiment 4 — Do benchmarks agree? A meta-evaluation probe
- Hypothesis: Different benchmarks disagree on model ranking more than expected; agreement improves after removing high-variance items and harmonizing scoring [P5][P10].
- Setup: Aggregate small, public subsets from 3–4 benchmarks; use public model outputs or low-cost API calls with a unified scoring rubric.
- Baselines: Unharmonized scoring; full-item versions without pruning.
- Metrics: Inter-benchmark Kendall/Spearman correlations; instability indices (bootstrap); proportion of pairwise rank inversions.
- Expected outcomes: Moderate disagreement that decreases with harmonization and item pruning; a compact protocol others can adopt [P5][P10].

3. Timeline for the next 6 months with milestones
- Phase 0 (Weeks 1–2): Stand up repo; define experiment cards and stop rules; pilot tiny-sample evaluation on one task; produce a prediction log (≥14 entries) and one reproduced metric with ≤10% gap; 1 ablation or negative result write-up [G8][G9].
- Month 1: Run Experiment 1 across 3 tasks and 3–5 models; pre-register analysis; draft short report section on statistical efficiency [P1].
- Month 2: Run Experiment 2; build prompt-variance harness; write reproducibility checklist and reporting tables [P2].
- Month 3: Run Experiment 3; release overlap-detection script and contamination notes; compare rank shifts [P3][P4].
- Month 4: Run Experiment 4; synthesize cross-benchmark insights; draft overall paper skeleton [P5][P10].
- Month 5: Polish: ablations, robustness checks, sensitivity analyses; external replication ask; prepare artifact and ethical statements.
- Month 6: Submit to a Datasets & Benchmarks/evaluation venue or workshop; post preprint; open-source toolkit.

4. Resources (compute, tools, datasets)
- Compute: None locally. Use free notebooks (Kaggle/Colab) for light scripting only; keep per-run <30 minutes and <1GB RAM. All core results should be reproducible via scripts + public outputs.
- Tools: Python + pandas; bootstrap CI utilities; lm-eval-harness or minimal custom scorer; reproducibility templates; simple web/archival search for contamination; BenchBench-inspired correlation scripts [P5][P10].
- Datasets/benchmarks: LiveBench (contamination-limited) [P4]; small, classic subsets (e.g., arithmetic, QA); your tiny-sample subsets per [P1].
- Method references: tinyBenchmarks protocol [P1]; Sober Look reproducibility checklist [P2]; contamination audit guidance [P3]; benchmark agreement practices [P5]; best-practice checklists [P10].

5. Risks and mitigations table
- Risk: No API access or costs. Mitigation: Use public model outputs/leaderboards; focus on re-analysis and contamination audits; seek small academic credits later.
- Risk: Tiny-sample instability. Mitigation: Predefine item sampling; use bootstrap CIs; report decision uncertainty; increase n only as needed [P1].
- Risk: Non-replicable prompt results. Mitigation: Multi-seed, multi-template, temperature grids; preregister; share full prompts and seeds [P2].
- Risk: Contamination detection false positives/negatives. Mitigation: Transparent heuristics; human adjudication on a sample; sensitivity analyses [P3].
- Risk: Reviewer skepticism about “no training.” Mitigation: Position as methodology/meta-evaluation with rigorous stats, ablations, and public artifacts [P10][G5].
- Risk: Scope creep. Mitigation: Phase-gated plan; stop rules tied to CI widths and replication thresholds [G9].

6. Stretch ideas or follow-up directions
- Build a contamination-limited, continuously refreshed micro-benchmark (monthly refresh pipeline) and show drift-resilient evaluation [P4].
- Propose a benchmark agreement scorecard and release a lightweight tool others can run on any leaderboard [P5].
- Domain application: adopt ELEVATE-AI-like evaluation frameworks for a specific field (e.g., HEOR) to demonstrate external validity and stakeholder-relevant metrics [P9].
- Synthesize a best-practices report on benchmark construction pitfalls and fixes, aligned with BetterBench [P10].

Intuition: This plan trades compute for rigor—small, carefully designed samples; multi-seed prompt audits; contamination-aware evaluation; and cross-benchmark meta-analyses provide real insight without training models [P1][P3][P5].

Why this is principled: It aligns with strong research taste and methodology guidance: pick important, tractable problems; use clear hypotheses and falsifiers; report uncertainty; and make artifacts reusable [G5][G7][G8]. Recent evaluations have revealed over-claims and benchmark fragility, signaling fertile ground for careful, low-compute work [P2][P10].

Citations
- [P1] tinyBenchmarks: evaluating LLMs with fewer examples — https://arxiv.org/html/2402.14992v1
- [P2] A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility — https://arxiv.org/abs/2504.07086
- [P3] The Emperor’s New Clothes in Benchmarking? A Rigorous Examination of Mitigation Strategies for LLM Benchmark Data Contamination — https://arxiv.org/html/2503.16402v1
- [P4] LiveBench: A Challenging, Contamination-Limited LLM Benchmark — https://arxiv.org/abs/2406.19314
- [P5] Do These LLM Benchmarks Agree? Fixing Benchmark Evaluation with BenchBench — https://arxiv.org/abs/2407.13696
- [P10] BetterBench: Assessing AI Benchmarks, Uncovering Issues, and Establishing Best Practices — https://arxiv.org/html/2411.12990v1
- [G5] Schulman, An Opinionated Guide to ML Research — http://joschu.net/blog/opinionated-guide-ml-research.html
- [G7] Hamming, You and Your Research — https://gwern.net/doc/science/1986-hamming
- [G8] Best Practices Research: A Methodological Guide — https://www.researchgate.net/publication/31052323_Best_Practices_Research_A_Methodological_Guide_for_the_Perplexed
- [G9] Explore→Understand Research Process — https://www.alignmentforum.org/posts/hjMy4ZxS5ogA9cTYK/how-i-think-about-my-research-process-explore-understand

Optional next step
- Create a one-page experiment card for Experiment 1 (hypothesis, minimal test, variables, analysis plan, stop rule) and sample 30 items for your first tiny-sample evaluation run.