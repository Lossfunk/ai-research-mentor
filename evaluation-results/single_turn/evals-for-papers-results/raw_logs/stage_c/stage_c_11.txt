1. Problem framing and goals
Objective: Establish a cross-lab, reproducible benchmark and evaluation protocol for statistical downscaling of climate model output with calibrated uncertainty quantification (UQ), with special emphasis on extremes and spatial coherence. The collaboration will compare families of methods (parametric quantile models, deep ensembles, diffusion/generative approaches) and calibration strategies (conformal prediction, EVT tails) across geographies, variables, and climate models, using proper scoring rules and scale-dependent spatial metrics. Deep ensembles have been proposed to improve SD UQ under climate change conditions [P2], and conditional diffusion models are an emerging class for precipitation downscaling and bias correction [P8]. Evaluation will center on CRPS, reliability/coverage, and spatial Fractions Skill Score (FSS), which are standard for probabilistic forecasts and high-resolution precipitation verification [1], [2].

Deliverables:
- A public benchmark suite (data loaders, standardized splits, metrics, and baselines)
- A curated suite of UQ-enabled downscalers with unified interfaces
- Reproducible evaluation pipelines with pre-registered analysis plans and ablation cards
- A cross-lab paper with open code and artifacts

Intuition: Downscaling adds fine-scale structure and extremes that coarse models miss; credible decision support requires not just point skill but calibrated probabilities and spatial realism. Comparing methods under controlled, cross-model and cross-region shifts reveals what generalizes, while proper scoring rules and scale-aware metrics prevent over-optimism from over-smoothing or noise.

Why this is principled: Proper scoring rules like CRPS reward both calibration and sharpness [1], while FSS targets spatial realism at relevant neighborhood scales [2]. Deep ensembles and generative models are the main modern SD paradigms for representing predictive distributions [P2], [P8]; evaluating them with strict splits and external calibration (e.g., conformal prediction) tests coverage guarantees under distribution shift [3], [4].

2. Experiments
All experiments use pre-registered experiment cards (hypothesis, falsifier, minimal test, variables, analysis plan, stop rule) and are run across multiple regions (e.g., CONUS, Western Europe, South Asia) and variables (daily precipitation, daily max/min temperature).

Experiment 1 — Calibration and sharpness of probabilistic downscalers across regions
- Hypothesis: Deep ensembles and diffusion-based downscalers yield lower CRPS and better reliability than deterministic quantile regression baselines for daily precipitation; improvements persist across climates [P2], [P8].
- Setup: Inputs: CMIP6 GCM outputs (historical 1995–2014; test 2015–2020) downscaled to 0.1°–0.25°. Targets: PRISM/Daymet (land), GPM IMERG/NOAA Stage IV (precipitation), ERA5-Land (temperature). Models: (a) Bilinear+quantile mapping (QM/BCSD), (b) UNet quantile regression (multiple quantiles), (c) UNet deep ensembles (N=5–10) [P2], (d) conditional diffusion model [P8]. Hindcast-style splits by year and region; leave-one-region-out for external validity.
- Baselines: BCSD/QM; analog methods; NEX-GDDP as a reference product; CORDEX RCM as dynamical reference where available.
- Evaluation metrics: CRPS and quantile loss; PIT histograms and reliability diagrams; coverage and interval width; sharpness (variance of predictive dist.); FSS at 5–50 km scales (precip) [1], [2].
- Expected outcomes: Ensembles improve reliability and reduce CRPS vs. deterministic quantile models; diffusion models improve FSS and spatial structure at similar CRPS. Falsifier: no statistically significant CRPS reduction or improved reliability after block-bootstrap tests.

Experiment 2 — Extreme precipitation tail calibration with conformal and EVT corrections
- Hypothesis: Conformalized quantile regression and EVT-tail augmentation achieve nominal coverage for high quantiles (e.g., 95th–99.5th) with minimal loss in sharpness, outperforming uncalibrated ensembles in the tails [3], [4].
- Setup: Start from models in Exp. 1. Apply split-conformal calibration on a held-out calibration period/region, and compare to EVT-based tail rescaling. Focus on extreme thresholds (e.g., 25 mm/day) and return levels. Evaluate by season and region.
- Baselines: Uncalibrated ensembles; empirical quantile mapping.
- Evaluation metrics: Tail coverage (q≥0.95), exceedance Brier score, reliability curves conditioned on intensity, CRPScore tail-weighted variants, extreme FSS at neighborhood scales [1], [2].
- Expected outcomes: Conformal methods meet nominal coverage by construction and preserve ranking/sharpness reasonably; EVT corrections improve bias at the highest intensities. Falsifier: coverage falls outside 95% tolerance bands after conformalization or sharpness collapses (intervals become too wide).

Experiment 3 — Generalization across GCMs and scenarios (model-as-truth and OOD tests)
- Hypothesis: Under leave-one-GCM-out and scenario shift (SSP2-4.5/SSP3-7.0), conformal calibration maintains coverage, while CRPS degrades modestly; diffusion models degrade less in spatial FSS than deterministic models [P2], [P8].
- Setup: Train on multiple CMIP6 GCMs; test on an unseen GCM and on future scenarios. Model-as-truth: where available, treat higher-res CORDEX convection-permitting or 0.11° RCM as pseudo-truth to quantify relative fidelity under forced climate change.
- Baselines: Same as Exp. 1; add “train single GCM” vs “multi-GCM pooled” training.
- Evaluation metrics: Coverage and CRPS delta vs. in-distribution; spatial FSS delta; energy score for multivariate (Tmin/Tmax/Pr) fields; significance via block bootstrap and paired tests [1], [2].
- Expected outcomes: Conformalized predictors sustain nominal coverage across OOD shifts; ensembles/diffusion show smaller FSS degradation than deterministic quantile models. Falsifier: coverage breaks consistently across OOD or spatial skill collapses.

Experiment 4 — Physics-aware constraints and diagnostics
- Hypothesis: Adding hard constraints (e.g., non-negativity, mass/water balance surrogates) or physics-informed losses improves physical diagnostics with ≤5% CRPS penalty [P7].
- Setup: Compare unconstrained vs. hard-constrained UNet and diffusion variants (e.g., bounded outputs, conservation penalties). Evaluate hydrologic diagnostics against basin aggregates and water balance checks.
- Baselines: Unconstrained counterparts; dynamical reference where available (RCM).
- Evaluation metrics: CRPS; bias in seasonal totals; water-balance residuals at basin scale; spatial FSS; constraint violation rates.
- Expected outcomes: Constraint violations reduced substantially with minimal deterioration in probabilistic scores. Falsifier: constraints induce large CRPS increases or unrealistic artifacts.

3. Timeline for the next 6 months with milestones
Phase 0 (Weeks 1–4): Governance and baseline reproducibility
- Cross-lab MOU: roles (Data Lab, Modeling Lab, Audit/Eval Lab), authorship, code of conduct
- Data ingestion and standardized splits (hindcast, tail-calibration, OOD)
- Baselines (BCSD/QM, bilinear) and metric suite (CRPS, coverage, PIT, FSS) implemented with unit tests [1], [2]
- Gate to Phase 1 only if:
  - Prediction log with ≥14 entries and one reproduced metric/figure within ≤10% of reported baseline
  - One experiment card (Exp. 1) plus one ablation/negative result with post-mortem

Month 2: Initial probabilistic models
- Train UNet quantile and deep ensembles; preliminary reliability and CRPS across 2 regions
- Draft preregistration for Exp. 1 (analysis plan, stop rules)

Month 3: Diffusion and conformal calibration
- Train conditional diffusion baseline; implement split-conformal and EVT tails
- Complete Exp. 1 across all regions; start Exp. 2 tail calibration

Month 4: OOD and model-as-truth tests
- Run leave-one-GCM-out; CORDEX pseudo-truth where available
- Complete Exp. 2; midterm cross-lab results review, adjust ablation plan

Month 5: Physics-aware constraints and diagnostics
- Run Exp. 3 and Exp. 4; finalize significance tests
- Draft figures: reliability, PIT, CRPS by region, FSS by scale, tail coverage

Month 6: Synthesis and writing
- Aggregate multi-region results, uncertainty budgets, and ablations
- Release benchmark repository and data loaders; internal review and preprint submission

Ongoing cadence: Weekly 45-min tri-lab standup; biweekly artifact freeze; writing cadence ≥1 page/week per lab; scoreboard tracked for Reproduction fidelity, Ablation clarity, and Writing cadence.

4. Resources (compute, tools, datasets)
- Compute: 4–8 GPUs per modeling lab (A100 40–80GB or V100 32GB); 50–150 GPU-days total; 50–100 CPU cores for ETL; 20–40 TB storage with object store (S3/GS) and Zarr access via Pangeo.
- Tools:
  - Data/compute: xarray, zarr, dask/pangeo, intake-esm; Docker/Conda; Snakemake/Prefect; MLflow/W&B; DVC for data versioning
  - Modeling: PyTorch/Lightning; Pyro or TensorFlow Probability for probabilistic layers; diffusion libs (e.g., OpenAI ADM or equivalent); conformal libs (MAPIE, jackknife+); spatial verification (xskillscore, properscoring, fss utilities)
- Datasets:
  - Predictors: CMIP6 (historical + SSPs), multiple GCMs
  - Targets: ERA5-Land (temp), PRISM/Daymet (temp/precip over land), GPM IMERG and NOAA Stage IV (precip), CORDEX (0.11°–0.44°) for pseudo-truth dynamical references
  - Reference products: NEX-GDDP for bias-corrected comparisons
- Documentation: central handbook with data licenses, variable harmonization, and split definitions

5. Risks and mitigations
- Data leakage across time/space
  - Mitigation: Hindcast splits; blocked cross-validation; strict region/year holds with leakage tests
- Misleading skill from spatial smoothing
  - Mitigation: Report FSS at multiple scales; show CRPS-sharpness trade-offs; PIT diagnostics [1], [2]
- Extremes underestimation
  - Mitigation: Tail-focused metrics; conformal and EVT corrections; report coverage and sharpness [3]
- OOD failure across GCMs/scenarios
  - Mitigation: Leave-one-GCM-out; model-as-truth with CORDEX; conformal calibration for coverage [P2], [3]
- Reproducibility gaps across labs
  - Mitigation: Containerized pipelines; DVC; experiment cards; preregistration; artifact snapshots
- Compute and storage overruns
  - Mitigation: Early down-selection via Phase 0 gates; subsampled pilots; cloud burst with quotas
- Governance/authorship disputes
  - Mitigation: Pre-agreed MOU; contribution log; weekly decision records

6. Stretch ideas or follow-up directions
- Multivariate generative downscaling with energy score optimization (joint Tmin/Tmax/Pr) [1]
- Physics-guided diffusion with hard constraints or differentiable diagnostics (e.g., water balance) [P7]
- Impact-aware evaluation: feed probabilistic downscales into hydrologic/flood risk models; assess decision metrics
- Active calibration: region-season adaptive conformal sets for improved sharpness at fixed coverage [3], [4]
- Fairness and equity audit: differential performance across vulnerable regions; targeted calibration strategies
- Benchmark expansion: hourly precip in convection-permitting regions; geographies beyond CONUS/EU

Literature anchors
- Deep Ensembles to Improve Uncertainty Quantification of Statistical Downscaling Models under Climate Change Conditions — http://arxiv.org/abs/2305.00975 [P2]
- Conditional diffusion models for downscaling & bias correction of Earth system model precipitation — http://arxiv.org/abs/2404.14416 [P8]

Citations
- [P1] Climate Downscaling: A Deep-Learning Based Super-resolution Model of Precipitation Data with Attention Block and Skip Connections — http://arxiv.org/abs/2403.17847v1
- [P2] Deep Ensembles to Improve Uncertainty Quantification of Statistical Downscaling Models under Climate Change Conditions — http://arxiv.org/abs/2305.00975v1
- [P3] A Generative Deep Learning Approach to Stochastic Downscaling of Precipitation Forecasts — http://arxiv.org/abs/2204.02028v2
- [P4] On the modern deep learning approaches for precipitation downscaling — http://arxiv.org/abs/2207.00808v1
- [P5] Downscaling Precipitation with Bias-informed Conditional Diffusion Model — http://arxiv.org/abs/2412.14539v1
- [P6] Urban precipitation downscaling using deep learning: a smart city application over Austin, Texas, USA — http://arxiv.org/abs/2209.06848v1
- [P7] Hard-Constrained Deep Learning for Climate Downscaling — http://arxiv.org/abs/2208.05424v9
- [P8] Conditional diffusion models for downscaling & bias correction of Earth system model precipitation — http://arxiv.org/abs/2404.14416v1
- [1] Gneiting, T. & Raftery, A. E. (2007). Strictly Proper Scoring Rules, Prediction, and Estimation — https://sites.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdf
- [2] Roberts, N. M., & Lean, H. W. (2008). Scale-selective verification of rainfall accumulations — https://centaur.reading.ac.uk/31220/1/Roberts2008a.pdf
- [3] Quantifying uncertainty in climate projections with conformal ensembles — https://arxiv.org/abs/2408.06642
- [4] Valid Error Bars for Neural Weather Models using Conformal Prediction — https://arxiv.org/abs/2406.14483

Optional next step
Share your preferred regions/variables and any fixed compute limits so I can instantiate Phase 0 (data menu, splits, and the first experiment card) and generate a concrete weekly task board for each lab.