Based on the retrieved evidence on human-centered AI evaluation [1,2,3], legal AI benchmarks [4,5,6], IRB ethics protocols [7,8], and human-AI collaboration frameworks [9,10], I can now construct a comprehensive research execution plan.

---

## 1. Problem Framing and Goals

**Core Problem:**  
AI legal assistants (LLMs fine-tuned for legal tasks) are rapidly deployed in law firms, corporate legal departments, and public legal aid, yet their evaluation focuses primarily on technical accuracy metrics (F1, BLEU, exact match) rather than human-centered outcomes [1,4,5]. Critical gaps include: (1) **Trust calibration**—lawyers may over-rely on incorrect AI outputs or dismiss correct ones [9,10]; (2) **Task appropriateness**—unclear which legal tasks benefit from AI assistance vs. full automation [3]; (3) **Equity and access**—AI tools may exacerbate disparities for self-represented litigants or under-resourced legal aid organizations [1]; (4) **Ethical risks**—hallucinations, bias, and unauthorized practice of law concerns require human-subjects evaluation [7,8].

**Key Gaps from Literature:**
- **Evaluation mismatch:** LegalBench [4] and CUAD [6] measure model performance, not human-AI team performance or user satisfaction [2,3]
- **Lack of ecological validity:** Most studies use synthetic tasks; few evaluate AI assistants in realistic legal workflows [1,5]
- **IRB/ethics guidance sparse:** Limited frameworks for evaluating AI systems with legal professionals as human subjects [7,8]
- **Trust and reliance understudied:** Few studies measure appropriate reliance, over-trust, or calibration in legal AI [9,10]

**Primary Goals (6-month scope):**
1. **Develop human-centered evaluation framework** for AI legal assistants across 4 dimensions:
   - **Task performance:** Accuracy, efficiency, quality of legal work product
   - **User experience:** Usability, trust, satisfaction, cognitive load
   - **Collaboration dynamics:** Appropriate reliance, complementarity, human agency [3]
   - **Ethical/equity outcomes:** Fairness, access to justice, professional responsibility
2. **Conduct IRB-approved user studies** with 3 stakeholder groups:
   - Licensed attorneys (corporate, litigation, legal aid)
   - Law students (novice users)
   - Self-represented litigants (access to justice focus)
3. **Benchmark 3–5 AI legal assistants** (GPT-4, Claude, domain-specific models) on realistic legal tasks
4. **Produce actionable guidelines** for deploying AI legal assistants responsibly
5. **Release open-source evaluation toolkit** with protocols, metrics, and anonymized data

**Scientific Contributions:**
- First comprehensive human-centered evaluation framework for legal AI
- Empirical evidence on trust calibration and appropriate reliance in legal AI
- IRB-approved protocols for evaluating AI systems with legal professionals
- Benchmark comparing technical vs. human-centered metrics
- Guidelines for responsible deployment in legal practice

**Scope Constraints:**
- **Legal tasks:** Contract review, legal research, document drafting, case analysis (not courtroom advocacy or client counseling)
- **Jurisdictions:** U.S. law (federal and 2–3 states); international law as stretch goal
- **Participants:** 60–120 total (20–40 per stakeholder group)
- **AI systems:** 3–5 commercial/open-source LLMs (GPT-4, Claude-3, Llama-3-70B, legal-specific models)
- **Timeline:** 6 months (IRB approval, pilot, main study, analysis, writing)

---

## 2. Experiments

### **Experiment 1: Baseline Technical Evaluation (Month 1–2)**

**Hypothesis:**  
AI legal assistants will achieve 70–85% accuracy on standardized legal reasoning tasks (LegalBench, CUAD) but exhibit systematic failures on edge cases (ambiguous contracts, conflicting precedents, jurisdiction-specific rules).

**Setup:**
- **AI systems (5 total):**
  - GPT-4 (OpenAI, general-purpose LLM)
  - Claude-3 Opus (Anthropic, long-context)
  - Llama-3-70B (Meta, open-source)
  - Legal-BERT (domain-pretrained encoder)
  - CaseHOLD or LexGLUE fine-tuned model (legal-specific)
- **Benchmarks (4 datasets):**
  - LegalBench [4]: 162 tasks across 6 legal reasoning types (issue-spotting, rule-recall, interpretation, etc.)
  - CUAD [6]: Contract review (510 contracts, 41 clause types)
  - CaseHOLD: Legal reasoning over case law (53K multiple-choice questions)
  - Custom edge cases: 100 adversarial examples (ambiguous clauses, conflicting precedents, jurisdiction-specific nuances)
- **Prompting strategies:**
  - Zero-shot: "Identify all indemnification clauses in this contract."
  - Few-shot: 3–5 examples per task type
  - Chain-of-thought: "Explain your reasoning step-by-step."
  - Retrieval-augmented: Provide relevant case law or statutes

**Baselines:**
- Random guessing (lower bound)
- Majority class (dataset-specific)
- Human expert performance (from published benchmarks or pilot annotation)

**Evaluation Metrics:**
- **Accuracy:** Exact match, F1, precision, recall (task-dependent)
- **Calibration:** Expected Calibration Error (ECE), Brier score
- **Robustness:** Performance on edge cases, adversarial examples
- **Efficiency:** Tokens used, latency (seconds per task)
- **Cost:** API cost per task (for commercial models)

**Expected Outcomes:**
- GPT-4 achieves 75–85% on LegalBench, 70–80% on CUAD (vs. 90–95% human expert)
- Legal-specific models (Legal-BERT, CaseHOLD) outperform general LLMs on domain tasks by 5–10%
- All models struggle with edge cases (50–65% accuracy vs. 70–85% on standard tasks)
- Calibration poor (ECE 0.10–0.20, overconfident on errors)
- Identify failure modes: Hallucinated case citations, jurisdiction confusion, ambiguous clause interpretation

---

### **Experiment 2: IRB Protocol Development and Pilot Study (Month 2–3)**

**Hypothesis:**  
An IRB-approved protocol with informed consent, data anonymization, and risk mitigation will enable ethical evaluation of AI legal assistants with minimal risk to participants (classified as "minimal risk" or "exempt" research).

**Setup:**
- **IRB submission (Month 2):**
  - **Study design:** Mixed-methods (quantitative task performance + qualitative interviews)
  - **Participants:** 10–15 pilot participants (5 attorneys, 5 law students, 5 self-represented litigants)
  - **Recruitment:** Law firm partnerships, law school clinics, legal aid organizations
  - **Informed consent:** Disclose AI involvement, data use, risks (e.g., reliance on incorrect AI output)
  - **Data protection:** Anonymize all legal documents, remove client-identifying information, secure storage (encrypted, access-controlled)
  - **Risk mitigation:** Participants informed AI outputs are not legal advice, supervision by licensed attorney for self-represented litigants
  - **Compensation:** $50–$100/hour (market rate for legal professionals)
- **Pilot study (Month 3):**
  - **Tasks (3 types):**
    - Contract review: Identify risky clauses in NDA (30 min)
    - Legal research: Find relevant case law for hypothetical scenario (30 min)
    - Document drafting: Draft motion to dismiss with AI assistance (45 min)
  - **Conditions (2×2 design):**
    - AI assistance: Yes vs. No (control)
    - AI transparency: Black-box vs. Explainable (show reasoning)
  - **Data collection:**
    - Task performance: Accuracy, completeness, time
    - User experience: NASA-TLX (cognitive load), SUS (usability), trust scale [9,10]
    - Qualitative: Semi-structured interviews (15–20 min)
- **IRB approval criteria:**
  - Minimal risk: No greater than everyday legal practice
  - Informed consent: Clear disclosure of AI use, voluntary participation
  - Privacy: Anonymization, secure storage, limited access
  - Beneficence: Potential to improve legal services, no harm to participants

**Baselines:**
- No AI assistance (human-only performance)
- Standard legal research tools (Westlaw, LexisNexis)

**Evaluation Metrics:**
- **IRB approval:** Time to approval (weeks), revisions required
- **Recruitment:** Enrollment rate (% of contacted participants who enroll)
- **Retention:** Completion rate (% who finish all tasks)
- **Usability:** SUS score (>68 = acceptable), NASA-TLX (cognitive load <50)
- **Trust:** Trust in Automation scale [9] (appropriate calibration, not over-trust)
- **Qualitative insights:** Thematic analysis (pain points, preferences, concerns)

**Expected Outcomes:**
- IRB approval in 4–8 weeks (minimal risk or exempt classification)
- Recruitment: 60–80% enrollment rate (legal professionals interested in AI)
- Retention: >90% completion rate (tasks realistic, compensation adequate)
- Pilot reveals usability issues: Confusing AI explanations, over-reliance on incorrect outputs
- Refine protocol: Adjust task difficulty, improve AI transparency, add debiasing interventions

---

### **Experiment 3: Main User Study – Attorneys and Law Students (Month 3–5)**

**Hypothesis:**  
AI assistance will improve task efficiency (20–40% time reduction) and accuracy (10–20% improvement) for routine legal tasks, but attorneys will exhibit over-reliance on AI for complex tasks (accepting 30–50% of incorrect AI suggestions), while law students will under-rely (rejecting 20–30% of correct AI suggestions).

**Setup:**
- **Participants (60–80 total):**
  - 30–40 licensed attorneys (5+ years experience, diverse practice areas: corporate, litigation, legal aid)
  - 30–40 law students (2L/3L, some legal research experience)
  - Recruitment: Law firm partnerships, law school clinics, bar associations
- **Tasks (6 realistic scenarios):**
  - *Contract review:* Identify problematic clauses in employment agreement (30 min)
  - *Legal research:* Find precedents for motion to dismiss (45 min)
  - *Document drafting:* Draft demand letter with AI assistance (30 min)
  - *Case analysis:* Summarize key holdings from 5 appellate opinions (45 min)
  - *Regulatory compliance:* Identify GDPR violations in privacy policy (30 min)
  - *Due diligence:* Review M&A documents for red flags (45 min)
- **Conditions (3×2 design):**
  - AI assistance level: None (control), Suggestions (AI recommends, human decides), Automation (AI drafts, human edits)
  - AI transparency: Black-box vs. Explainable (show reasoning, cite sources)
  - Randomized within-subjects (each participant does 2 tasks per condition)
- **AI systems:** GPT-4, Claude-3 (best performers from Experiment 1)
- **Data collection:**
  - Task performance: Accuracy (expert-annotated gold standard), completeness, time
  - Reliance behavior: % of AI suggestions accepted (correct vs. incorrect)
  - User experience: SUS, NASA-TLX, trust scale, satisfaction (Likert 1–7)
  - Qualitative: Post-task interviews (10–15 min, 20% of participants)

**Baselines:**
- No AI assistance (human-only)
- Standard legal tools (Westlaw, LexisNexis, Word)

**Evaluation Metrics:**
- **Task performance:**
  - Accuracy: F1 score vs. expert gold standard
  - Efficiency: Time to completion (minutes)
  - Quality: Expert ratings (1–5 scale) on completeness, clarity, legal soundness
- **Reliance and trust:**
  - Appropriate reliance: % of correct AI suggestions accepted + % of incorrect AI suggestions rejected
  - Over-reliance: % of incorrect AI suggestions accepted (should be <10%)
  - Under-reliance: % of correct AI suggestions rejected (should be <20%)
  - Trust calibration: Correlation between AI confidence and user reliance
- **User experience:**
  - Usability: SUS score (target >70)
  - Cognitive load: NASA-TLX (target <50)
  - Satisfaction: Mean Likert score (target >5/7)
- **Equity:**
  - Performance gap: Novice (students) vs. expert (attorneys) improvement with AI
  - Access: Time/cost savings for legal aid attorneys

**Expected Outcomes:**
- **Efficiency:** AI assistance reduces time by 25–35% for routine tasks (contract review, research)
- **Accuracy:** AI improves accuracy by 10–15% for students, 5–10% for attorneys (ceiling effect)
- **Over-reliance:** Attorneys accept 35–45% of incorrect AI suggestions (especially for complex tasks)
- **Under-reliance:** Students reject 25–35% of correct AI suggestions (lack of trust)
- **Transparency helps:** Explainable AI reduces over-reliance by 10–15% (users scrutinize reasoning)
- **Equity:** AI narrows novice-expert gap by 20–30% (students benefit more)
- **Qualitative insights:** Attorneys want control, students want guidance; both concerned about hallucinations

---

### **Experiment 4: Access to Justice Study – Self-Represented Litigants (Month 4–5)**

**Hypothesis:**  
AI legal assistants will improve access to justice for self-represented litigants (30–50% improvement in document quality, 40–60% time savings) but require careful design to avoid over-reliance and ensure appropriate disclaimers (not legal advice).

**Setup:**
- **Participants (20–30 total):**
  - Self-represented litigants (no attorney, handling own case)
  - Recruitment: Legal aid clinics, court self-help centers, community organizations
  - Inclusion: English-speaking, basic digital literacy, civil cases (family law, housing, small claims)
  - Exclusion: Criminal cases (higher stakes, ethical concerns)
- **Tasks (3 realistic scenarios):**
  - *Form completion:* Fill out small claims complaint with AI assistance (30 min)
  - *Legal research:* Understand tenant rights for eviction defense (20 min)
  - *Document drafting:* Draft response to landlord demand letter (30 min)
- **Conditions (2×2 design):**
  - AI assistance: Yes vs. No (control, paper forms + instructions)
  - AI design: Standard (legal jargon) vs. Plain language (simplified explanations)
  - Between-subjects (each participant does 1 condition)
- **AI system:** GPT-4 with custom prompt (plain language, disclaimers, cite sources)
- **Supervision:** Licensed attorney reviews all AI-assisted outputs (ethical safeguard)
- **Data collection:**
  - Document quality: Expert attorney ratings (1–5 scale) on completeness, accuracy, persuasiveness
  - Efficiency: Time to completion, number of errors
  - User experience: SUS, confidence in output (Likert 1–7), perceived fairness
  - Qualitative: Interviews (15–20 min) on trust, concerns, willingness to use

**Baselines:**
- No AI assistance (paper forms, online instructions)
- Legal aid attorney assistance (gold standard, if available)

**Evaluation Metrics:**
- **Document quality:**
  - Completeness: % of required fields filled correctly
  - Accuracy: % of legal arguments/facts correct (expert-rated)
  - Persuasiveness: Expert rating (1–5 scale)
- **Efficiency:**
  - Time to completion (minutes)
  - Number of errors requiring correction
- **Access to justice:**
  - Confidence: Self-reported confidence in filing (Likert 1–7)
  - Perceived fairness: "AI helped me understand my rights" (Likert 1–7)
  - Willingness to use: % who would use AI again
- **Safety:**
  - Over-reliance: % who submit AI output without review (should be 0% with disclaimers)
  - Misunderstanding: % who believe AI provides legal advice (should be <10%)

**Expected Outcomes:**
- **Quality:** AI improves document completeness by 30–40%, accuracy by 20–30% (vs. no assistance)
- **Efficiency:** AI reduces time by 40–50% (30 min → 15–18 min)
- **Plain language helps:** Simplified AI explanations improve quality by 10–15% vs. standard
- **Safety concerns:** 15–25% of participants over-rely on AI without review (need stronger disclaimers)
- **Access to justice:** 70–80% report increased confidence, 60–70% would use AI again
- **Equity:** AI partially substitutes for attorney assistance (70–80% of attorney quality at 5–10% of cost)
- **Qualitative insights:** Participants value speed and cost savings but worry about errors and lack of personalized advice

---

### **Experiment 5: Longitudinal Deployment and Trust Dynamics (Month 5–6)**

**Hypothesis:**  
Over 4–8 weeks of repeated use, attorneys will calibrate trust appropriately (reliance correlates with AI accuracy), but initial over-trust will persist for 2–3 weeks before correction, and trust will degrade if AI makes salient errors (e.g., hallucinated case citations).

**Setup:**
- **Participants (15–20 attorneys):**
  - Subset from Experiment 3 (volunteers for longitudinal study)
  - Weekly tasks over 4–8 weeks (1–2 tasks per week)
- **Tasks (rotating set of 12):**
  - Contract review, legal research, document drafting (similar to Experiment 3)
  - Difficulty varies: Easy (AI 90% accurate), medium (70%), hard (50%)
- **AI system:** GPT-4 with calibrated confidence scores
- **Interventions (randomized):**
  - Control: No intervention
  - Debiasing: Weekly feedback on reliance accuracy ("You accepted 40% of incorrect AI suggestions last week")
  - Transparency: AI shows confidence scores and reasoning
- **Data collection:**
  - Weekly: Task performance, reliance behavior, trust scale
  - Bi-weekly: Interviews (10 min) on trust evolution, critical incidents
  - Final: Retrospective survey on overall experience

**Baselines:**
- Week 1 performance (initial trust)
- No AI assistance (human-only, measured at Week 0 and Week 8)

**Evaluation Metrics:**
- **Trust calibration:**
  - Correlation between AI accuracy and user reliance (target r >0.70)
  - Over-trust: % of incorrect AI suggestions accepted (target <15% by Week 8)
  - Under-trust: % of correct AI suggestions rejected (target <20% by Week 8)
- **Trust dynamics:**
  - Trust trajectory: Trust scale over time (linear, U-shaped, or declining?)
  - Critical incidents: Impact of salient errors (e.g., hallucinated citation) on trust
  - Recovery: Time to restore trust after error (weeks)
- **Intervention effectiveness:**
  - Debiasing: Reduction in over-reliance (% improvement vs. control)
  - Transparency: Improvement in calibration (correlation increase)
- **Performance:**
  - Learning curve: Accuracy improvement over time (human + AI team)
  - Efficiency: Time reduction over time (as users learn to use AI effectively)

**Expected Outcomes:**
- **Initial over-trust:** Week 1–2, participants accept 40–50% of incorrect AI suggestions
- **Calibration:** By Week 6–8, over-reliance drops to 15–25% (appropriate reliance 70–80%)
- **Trust degradation:** Salient errors (hallucinated citations) reduce trust by 20–30% (1–2 weeks to recover)
- **Debiasing helps:** Feedback reduces over-reliance by 10–15% vs. control
- **Transparency helps:** Confidence scores improve calibration (r=0.50 → 0.65)
- **Performance:** Human-AI team accuracy improves by 5–10% over 8 weeks (learning to collaborate)
- **Qualitative insights:** Attorneys develop heuristics ("Always verify case citations," "Trust AI for routine tasks, not complex ones")

---

## 3. Timeline for the Next 6 Months with Milestones

| **Month** | **Milestone** | **Deliverables** |
|-----------|---------------|------------------|
| **Month 1** | Baseline Technical Evaluation (Exp 1) + IRB Preparation | - Evaluate 5 AI systems on LegalBench, CUAD, CaseHOLD, edge cases<br>- Identify top 2–3 systems for user studies<br>- Draft IRB protocol (study design, consent forms, data protection plan)<br>- **Deliverable:** Technical evaluation report, IRB application draft |
| **Month 2** | IRB Submission + Pilot Study Preparation (Exp 2 Part 1) | - Submit IRB application (target: minimal risk or exempt)<br>- Recruit 10–15 pilot participants (attorneys, students, self-represented litigants)<br>- Develop task materials (contracts, research scenarios, evaluation rubrics)<br>- Build user study platform (web interface, data logging)<br>- **Deliverable:** IRB submission, pilot participant roster, study platform |
| **Month 3** | IRB Approval + Pilot Study (Exp 2 Part 2) + Main Study Launch (Exp 3 Part 1) | - Obtain IRB approval (revise if needed)<br>- Conduct pilot study (10–15 participants, 3 tasks each)<br>- Analyze pilot data, refine protocol (task difficulty, AI transparency, consent)<br>- Recruit 60–80 participants for main study (attorneys, students)<br>- **Deliverable:** IRB approval letter, pilot study report, main study recruitment |
| **Month 4** | Main User Study – Attorneys & Students (Exp 3 Part 2) + Access to Justice Study (Exp 4 Part 1) | - Conduct main study (60–80 participants, 6 tasks, 3×2 conditions)<br>- Collect task performance, reliance behavior, user experience data<br>- Recruit 20–30 self-represented litigants<br>- Conduct access to justice study (3 tasks, 2×2 conditions)<br>- **Deliverable:** Main study data (60–80 participants), access to justice data (20–30 participants) |
| **Month 5** | Data Analysis (Exp 3 & 4) + Longitudinal Study (Exp 5 Part 1) | - Analyze main study: Task performance, reliance, trust, user experience<br>- Analyze access to justice study: Document quality, efficiency, safety<br>- Recruit 15–20 attorneys for longitudinal study<br>- Launch longitudinal study (Week 1–4 of 8-week deployment)<br>- **Deliverable:** Main study analysis, access to justice analysis, longitudinal study launch |
| **Month 6** | Longitudinal Study Completion (Exp 5 Part 2) + Writing & Dissemination | - Complete longitudinal study (Week 5–8)<br>- Analyze trust dynamics, calibration, intervention effectiveness<br>- Synthesize findings across all experiments<br>- Write manuscript (intro, methods, results, discussion)<br>- Prepare open-source release (evaluation toolkit, protocols, anonymized data)<br>- Submit to conferences (CHI, FAccT, CSCW, Law & AI workshops)<br>- **Deliverable:** Paper submitted, open-source toolkit, deployment guidelines |

**Key Decision Points:**
- End of Month 2: IRB approval status; if delayed, proceed with pilot as "practice" (no data collection)
- Month 3: Pilot results; if major usability issues, delay main study by 2–4 weeks
- Month 4: Recruitment progress; if <50 participants enrolled, extend recruitment or reduce sample size
- Month 5: Longitudinal study feasibility; if <10 participants, shorten to 4 weeks or cancel

---

## 4. Resources (Compute, Tools, Datasets)

### **Compute Requirements**
- **AI inference (Months 1–6):**
  - Commercial APIs: GPT-4, Claude-3 ($0.01–$0.10 per task, 500–1,000 tasks total)
  - Estimated cost: $500–$1,500
  - Open-source models (Llama-3-70B): 1–2 GPUs (A100 40GB or equivalent)
  - Estimated GPU-hours: 100–200 hours ($400–$800 on cloud)
- **User study platform (Months 2–6):**
  - Web hosting: AWS, Google Cloud, or university server ($50–$200/month)
  - Database: PostgreSQL or MongoDB (managed service, $20–$100/month)
- **Total compute budget:** $1,500–$3,000

### **Software & Tools**
- **AI systems:**
  - OpenAI API (GPT-4, GPT-3.5-turbo)
  - Anthropic API (Claude-3 Opus, Sonnet)
  - Hugging Face Transformers (Llama-3-70B, Legal-BERT)
  - LangChain (prompt engineering, retrieval-augmented generation)
- **User study platform:**
  - Frontend: React, Vue.js, or Qualtrics (survey platform)
  - Backend: Flask, Django, or Node.js
  - Database: PostgreSQL, MongoDB
  - Authentication: OAuth, university SSO
- **Data collection:**
  - Screen recording: OBS Studio, Loom (with consent)
  - Logging: Custom event tracking (clicks, edits, time)
  - Surveys: Qualtrics, Google Forms
  - Interviews: Zoom (recorded, transcribed with consent)
- **Analysis:**
  - Quantitative: Python (pandas, scipy, statsmodels), R (lme4, ggplot2)
  - Qualitative: NVivo, Atlas.ti, or manual coding (thematic analysis)
  - Visualization: Matplotlib, Seaborn, Plotly
- **IRB/ethics:**
  - Consent forms: Qualtrics, DocuSign
  - Data anonymization: Custom scripts (remove names, case numbers, client info)
  - Secure storage: Encrypted drives, university-approved cloud (Box, OneDrive)

### **Datasets**
1. **Technical evaluation (Experiment 1):**
   - LegalBench [4]: 162 tasks, 6 reasoning types (public, GitHub)
   - CUAD [6]: 510 contracts, 41 clause types (public, Atticus Project)
   - CaseHOLD: 53K legal reasoning questions (public, Harvard)
   - Custom edge cases: 100 adversarial examples (self-created or crowdsourced)
2. **User study tasks (Experiments 2–5):**
   - Contracts: NDAs, employment agreements, M&A documents (anonymized from public sources or law firm partnerships)
   - Case law: Appellate opinions (public, Caselaw Access Project, CourtListener)
   - Statutes/regulations: GDPR, employment law, housing law (public)
   - Hypothetical scenarios: Self-created (based on real cases, anonymized)
3. **Gold standards:**
   - Expert annotations: Hire 2–3 licensed attorneys to create gold standard answers ($100–$150/hour, 20–40 hours total)
   - Inter-annotator agreement: Cohen's kappa >0.70 (substantial agreement)

### **Participants and Compensation**
- **Attorneys (30–40 main study + 15–20 longitudinal):**
  - Recruitment: Law firm partnerships, bar associations, LinkedIn
  - Compensation: $75–$100/hour (market rate)
  - Total: 50 attorneys × 2 hours = 100 hours × $75 = $7,500
- **Law students (30–40):**
  - Recruitment: Law school clinics, student organizations
  - Compensation: $25–$50/hour (student rate)
  - Total: 35 students × 2 hours = 70 hours × $35 = $2,450
- **Self-represented litigants (20–30):**
  - Recruitment: Legal aid clinics, court self-help centers
  - Compensation: $25–$50/hour (community rate)
  - Total: 25 participants × 1.5 hours = 37.5 hours × $35 = $1,312
- **Expert annotators (2–3):**
  - Compensation: $100–$150/hour
  - Total: 30 hours × $125 = $3,750
- **Total participant budget:** $15,000–$20,000

### **Partnerships and Collaborations**
- **Law firms:** Partner with 2–3 firms for attorney recruitment, task materials (anonymized contracts)
- **Law schools:** Partner with 1–2 law schools for student recruitment, clinic access
- **Legal aid organizations:** Partner with 1–2 organizations for self-represented litigant recruitment
- **IRB:** University IRB or independent IRB (e.g., WCG IRB, Sterling IRB)
- **Funding:** Apply for grants (NSF, ABA, legal tech foundations, university internal grants)

---

## 5. Risks and Mitigations Table

| **Risk** | **Likelihood** | **Impact** | **Mitigation** |
|----------|----------------|------------|----------------|
| **IRB approval delayed (>8 weeks)** | Medium | High | - Submit IRB early (Month 2)<br>- Prepare thorough application (minimize revisions)<br>- Consult IRB staff before submission<br>- Fallback: Conduct pilot as "practice" (no data collection) until approval |
| **Recruitment fails (<50 participants)** | Medium | High | - Diversify recruitment channels (law firms, bar associations, LinkedIn, law schools)<br>- Increase compensation ($100/hour for attorneys)<br>- Extend recruitment period (2–4 weeks)<br>- Reduce sample size (power analysis: 40–50 participants may suffice) |
| **AI systems hallucinate or produce harmful outputs** | High | High | - Pre-screen AI outputs (filter obvious errors)<br>- Supervision: Licensed attorney reviews all outputs for self-represented litigants<br>- Disclaimers: "AI outputs are not legal advice, verify all information"<br>- Informed consent: Participants aware of AI limitations |
| **Participants over-rely on AI, submit incorrect work** | Medium | Medium | - Debiasing interventions (feedback on reliance accuracy)<br>- Transparency: Show AI confidence, reasoning<br>- Training: Brief tutorial on AI limitations before tasks<br>- Supervision: Attorney review for high-stakes tasks (self-represented litigants) |
| **Data privacy breach (client information leaked)** | Low | High | - Anonymize all legal documents (remove names, case numbers, client info)<br>- Secure storage: Encrypted drives, access-controlled<br>- IRB-approved data protection plan<br>- Legal review: Ensure compliance with attorney-client privilege, confidentiality rules |
| **Longitudinal study attrition (>30% dropout)** | Medium | Medium | - Incentivize completion (bonus payment for finishing all 8 weeks)<br>- Reduce burden (1 task/week instead of 2)<br>- Regular check-ins (weekly emails, reminders)<br>- Backup: Recruit 20–25 participants (expect 15–20 to complete) |
| **AI systems too expensive (>$3,000 API costs)** | Low | Medium | - Use open-source models (Llama-3-70B) for some tasks<br>- Optimize prompts (reduce tokens)<br>- Negotiate academic discounts (OpenAI, Anthropic)<br>- Fallback: Reduce number of tasks or participants |
| **Findings not generalizable (U.S.-only, limited practice areas)** | Medium | Low | - Acknowledge limitations in paper<br>- Diversify participants (corporate, litigation, legal aid)<br>- Include multiple task types (contract review, research, drafting)<br>- Stretch goal: International replication (UK, EU law) |
| **Publication rejected from top-tier venues** | Medium | Low | - Target multiple venues (CHI, FAccT, CSCW, Law & AI workshops)<br>- Submit to journals (ACM TOCHI, Law & AI journals)<br>- Post preprint on arXiv, SSRN for visibility<br>- Incorporate reviewer feedback, resubmit |

---

## 6. Stretch Ideas or Follow-Up Directions

1. **Cross-Jurisdictional Evaluation:**  
   Extend to international law (UK, EU, Canada) to assess generalizability of findings and AI performance across legal systems.

2. **Adversarial Robustness Testing:**  
   Evaluate AI legal assistants against adversarial inputs (ambiguous clauses, conflicting precedents, trick questions) to identify failure modes.

3. **Explainable AI for Legal Reasoning:**  
   Develop and evaluate explanation methods (attention visualization, case-based reasoning, counterfactual explanations) to improve trust calibration.

4. **Bias and Fairness Audits:**  
   Assess AI legal assistants for demographic bias (race, gender, socioeconomic status) in contract review, sentencing recommendations, or legal advice.

5. **Multi-Stakeholder Evaluation:**  
   Include judges, paralegals, legal tech vendors, and clients to capture diverse perspectives on AI legal assistants.

6. **Longitudinal Field Deployment:**  
   Deploy AI legal assistants in real law firms or legal aid organizations for 6–12 months, measuring productivity, quality, and client outcomes.

7. **Regulatory and Professional Responsibility Analysis:**  
   Investigate unauthorized practice of law (UPL) concerns, attorney supervision requirements, and malpractice liability for AI-assisted legal work.

8. **Human-AI Teaming Optimization:**  
   Develop adaptive AI systems that learn from user feedback, adjust to individual preferences, and optimize task allocation (human vs. AI).

9. **Access to Justice at Scale:**  
   Pilot AI legal assistants in underserved communities (rural areas, low-income populations) and measure impact on access to justice outcomes.

10. **Benchmark Suite Release:**  
    Create comprehensive benchmark combining technical metrics (LegalBench) and human-centered metrics (trust, usability, equity) for reproducible evaluation.

---

## Concrete Action Plan (First Month)

**Week 1:**
1. **Day 1–2:** Set up compute environment (OpenAI API, Anthropic API, Hugging Face)
2. **Day 3–4:** Download LegalBench, CUAD, CaseHOLD datasets
3. **Day 5–7:** Implement evaluation pipeline (prompting, scoring, logging)

**Week 2:**
1. **Day 8–10:** Evaluate GPT-4, Claude-3, Llama-3-70B on LegalBench (162 tasks)
2. **Day 11–12:** Evaluate on CUAD (contract review, 510 contracts)
3. **Day 13–14:** Evaluate on CaseHOLD (legal reasoning, 53K questions)

**Week 3:**
1. **Day 15–17:** Create 100 adversarial edge cases (ambiguous clauses, conflicting precedents)
2. **Day 18–19:** Evaluate AI systems on edge cases
3. **Day 20–21:** Analyze results, identify top 2–3 systems for user studies

**Week 4:**
1. **Day 22–24:** Draft IRB protocol (study design, consent forms, data protection)
2. **Day 25–26:** Consult with IRB staff, legal experts (review protocol)
3. **Day 27–28:** Finalize IRB application, prepare for Month 2 submission

---

## Sources

[1] [Human Centered AI for Indian Legal Text Analytics](https://arxiv.org/html/2403.10944v1)  
[2] [Bridging HCI and AI Research for the Evaluation of Conversational SE Assistants](https://arxiv.org/html/2502.07956)  
[3] [Scalable Evaluation of Human Agency Support in AI Assistants](https://arxiv.org/html/2509.08494v1)  
[4] [LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models](https://arxiv.org/abs/2308.11462)  
[5] [LegalAgentBench: Evaluating LLM Agents in Legal Domain](http://arxiv.org/pdf/2412.17259.pdf)  
[6] [CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review](https://arxiv.org/abs/2103.06268)  
[7] [Informing the ethical review of human subjects research utilizing artificial intelligence](https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2023.1235226/full)  
[8] [IRB Considerations on the Use of Artificial Intelligence in Human Subjects Research](https://www.hhs.gov/ohrp/sachrp-committee/recommendations/irb-considerations-use-artificial-intelligence-human-subjects-research/index.html)  
[9] [Adaptive trust calibration for human-AI collaboration](https://pmc.ncbi.nlm.nih.gov/articles/PMC7034851/)  
[10] [Trust Development and Repair in AI-Assisted Decision-Making during Complementary Expertise](https://www.jorgegoncalves.com/docs/facct24.pdf)  

**Note on additional references:** The web search provided strong coverage of legal AI benchmarks [4,5,6], IRB ethics protocols [7,8], and human-AI collaboration frameworks [9,10]. For more specific guidance on legal ethics and professional responsibility, I recommend:

- **Legal ethics:** Consult ABA Model Rules of Professional Conduct (Rule 1.1 on competence, Rule 5.5 on unauthorized practice of law)
- **AI and legal practice:** Review ABA Formal Opinion 512 (2024) on generative AI in legal practice
- **Access to justice:** Search for "legal aid AI evaluation" or consult Legal Services Corporation (LSC) reports on technology innovation

---

**Optional next step:**  
Contact the authors of LegalBench [4] and the human-AI collaboration trust calibration study [9,10] to explore collaboration on extending their frameworks to include human-centered evaluation metrics and longitudinal trust dynamics. Also reach out to law firms, legal aid organizations, and law schools to establish partnerships for participant recruitment and task material development (anonymized contracts, realistic scenarios).