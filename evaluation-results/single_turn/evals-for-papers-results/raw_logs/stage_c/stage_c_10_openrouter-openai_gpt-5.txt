1. Problem framing and goals
- Objective: Design and evaluate a bandwidth‑aware, multimodal (image+text) misinformation detection pipeline that works under intermittent connectivity and low-end devices. Focus on (a) compact on-device inference, (b) minimal‑byte feature exchange when servers are necessary, and (c) robustness to out‑of‑context media and cross‑lingual text.
- Evidence anchors and constraints:
  - Public multimodal misinformation/rumor datasets exist for benchmarking: Fakeddit (fine‑grained labels across images+text) and NewsCLIPpings (out‑of‑context image–text pairs) [1][3]. A Chinese social media multimodal dataset (Weibo23) supports cross‑lingual evaluation [7]. Surveys summarize modeling/evaluation baselines [2].
  - Model compression via quantization and distillation is effective for resource‑constrained deployment; TensorFlow Lite provides pragmatic toolchains for int8 quantization on mobile/edge devices [4][6].
  - Federated learning with asynchronous participation and communication compression is suitable under low bandwidth and intermittent clients [5].
- Six‑month success criteria:
  - Utility: Macro‑F1 ≥ baseline unimodal alternatives on Fakeddit and NewsCLIPpings; demonstrate ≥5–10 point improvement of multimodal vs text‑only for out‑of‑context detection on NewsCLIPpings [3].
  - Efficiency: 4–10× model size reduction and ≤20% accuracy drop from full‑precision to int8 models; ≤200 ms/image inference on mid‑range Android; bytes per decision reduced by ≥10× vs uploading full images (measured) [4][6]. Where we do not find high‑confidence priors for byte savings, we will report empirical measurements (Conjecture).
  - Bandwidth‑robust training: In FL simulations, achieve within ≤2–4 Macro‑F1 points of centralized training while cutting per‑round uplink by ≥5× via quantization/sparsification; tolerate ≥30% client dropouts [5].

2. Experiments (each with hypothesis, setup, baselines, evaluation metrics, and expected outcomes)
Experiment 1: Multimodal vs. unimodal baselines under quantization on Fakeddit
- Hypothesis: A compact multimodal model (int8) retains most of the performance gap over unimodal text‑only or image‑only baselines while cutting model size ≥4× [1][4][6].
- Setup:
  - Data: Fakeddit train/val/test with standard splits (use coarse and fine‑grained labels) [1].
  - Models: Text encoder (small Transformer) + image encoder ( MobileNetV3‑Small) with late fusion; train full‑precision, then apply post-training int8 quantization (TFLite) and compare to a distilled student [4][6].
  - Deployment: Benchmark inference on a mid‑range Android device using TFLite.
- Baselines: Text‑only (Transformer) and image‑only (MobileNetV3) in fp32 and int8; centralized (server) fp32 inference.
- Metrics: Macro‑F1/ROC‑AUC; model size (MB); latency (ms/image+text); tokens/images per second; energy proxy; bandwidth (bytes transmitted per decision for server vs on‑device).
- Expected outcomes: Multimodal > unimodal Macro‑F1; int8 achieves ≥80–90% of fp32 Macro‑F1 with ≥4–10× size reduction and mobile latency ≤200 ms/image [4][6]. Fakeddit supports fine‑grained evaluation across labels [1].

Experiment 2: Out‑of‑context detection with minimal bytes on NewsCLIPpings
- Hypothesis: Multimodal modeling is substantially more effective than text‑only for out‑of‑context detection; compressing models to int8 maintains most of the gain [3][6].
- Setup:
  - Data: NewsCLIPpings (train/dev/test), which pairs headlines/captions with mismatched images [3].
  - Models: Compare (a) text‑only classifier, (b) image‑only, (c) multimodal fusion (e.g., shallow fusion of CLIP‑like features with light MLP); evaluate fp32 vs int8.
  - Bandwidth protocol: Evaluate two inference paths: on‑device (zero media upload) vs. server‑side (upload image+text).
- Baselines: Text‑only; random; majority class.
- Metrics: Accuracy, Macro‑F1; bytes per decision (media upload vs none); latency on device vs server round‑trip; failure analysis by manipulation type [3].
- Expected outcomes: Multimodal improves Macro‑F1 by ≥5–10 points over text‑only on NewsCLIPpings [3]; int8 retains most gains with minimal latency increase [6].

Experiment 3: Feature‑upload vs raw‑media upload (bandwidth measurement study)
- Hypothesis: Uploading compact on‑device embeddings (e.g., 512‑D float/int8 vectors) instead of raw images/captions reduces bytes ≥10× with comparable accuracy when a lightweight server‑side head is used (Conjecture; no high‑confidence quantitative sources found).
- Setup:
  - Data: Fakeddit and NewsCLIPpings [1][3].
  - Protocols: (A) Upload raw image (compressed JPEG) + text; (B) Compute int8 image and text embeddings on device (e.g., MobileNet features + text averaged embeddings), upload embeddings only; server applies small MLP.
- Baselines: Raw upload.
- Metrics: Bytes per sample; end‑to‑end Macro‑F1; latency; privacy surface (threat model note).
- Expected outcomes: ≥10× byte reduction with ≤3‑point Macro‑F1 drop (to be empirically verified; Conjecture).

Experiment 4: Federated learning under low bandwidth with compressed/asynchronous updates
- Hypothesis: Asynchronous FL with quantized/sparse updates achieves near‑centralized performance with ≥5× uplink reduction and resilience to 30–50% client dropouts [5].
- Setup:
  - Clients: Simulated regional clients (5–20) with non‑IID splits over Fakeddit/Weibo23; intermittent participation [1][7].
  - Methods: FL baselines (FedAvg); add asynchronous server, 8‑bit update quantization, and top‑k sparsification; rounds with K clients per step [5].
- Baselines: Centralized training; synchronous FedAvg (fp32).
- Metrics: Macro‑F1 gap to centralized; uplink bytes/round; rounds/day under dropout; convergence stability.
- Expected outcomes: ≤2–4 point Macro‑F1 gap to centralized with ≥5× communication savings; stable learning with 30–50% client unavailability [5].

Experiment 5: Cross‑lingual robustness and regional generalization (Weibo23)
- Hypothesis: Multimodal methods trained/tuned on Western datasets transfer poorly to Chinese social media; light regional fine‑tuning recovers performance [2][7].
- Setup:
  - Data: Weibo23 multimodal fake news dataset [7].
  - Models: Evaluate zero‑shot transfer from Exp. 1/2; then fine‑tune the text encoder with multilingual tokenizers; keep image encoder fixed.
- Baselines: Text‑only multilingual model; image‑only.
- Metrics: Macro‑F1; per‑class recall; ablation by modality; error taxonomy (culture‑specific references).
- Expected outcomes: Drop in zero‑shot performance; multimodal fine‑tuning improves Macro‑F1 over text‑only; documents domain shift issues [2][7].

Experiment 6: Human‑in‑the‑loop triage and active learning
- Hypothesis: Selecting uncertain/borderline posts for human verification cuts annotation budget by ≥40% for the same performance vs. random sampling (Conjecture; no high‑confidence LMIC‑specific sources located).
- Setup:
  - Loop: Calibrate uncertainty; route top‑k% uncertain posts to annotators; retrain periodically.
- Baselines: Random sampling of the same size.
- Metrics: Macro‑F1 vs. labeled samples; annotation hours; inter‑annotator agreement.
- Expected outcomes: Higher label efficiency under uncertainty sampling (to be empirically verified; Conjecture).

3. Timeline for the next 6 months with milestones
- Month 1: Setup and baselines
  - Acquire datasets (Fakeddit, NewsCLIPpings, Weibo23) and licenses; implement unimodal/multimodal baselines; define non‑IID client partitions.
  - Milestones: Reproducible pipeline; baseline Macro‑F1 on Fakeddit and NewsCLIPpings [1][3][7].
- Month 2: Compression and on‑device prototypes
  - Apply post‑training int8 quantization and small‑student distillation; deploy to Android with TFLite; measure latency and accuracy [4][6].
  - Milestones: Int8 models within ≤20% accuracy of fp32; ≤200 ms latency per image.
- Month 3: Bandwidth measurements and feature‑upload A/B
  - Implement raw vs embedding upload; instrument byte counters; run end‑to‑end tests on cellular links.
  - Milestones: Bandwidth report with accuracy trade‑offs (Exp. 3).
- Month 4: Out‑of‑context and cross‑lingual evaluation
  - Complete NewsCLIPpings experiments; run zero‑shot and fine‑tuned evaluations on Weibo23 [3][7].
  - Milestones: Report on modality gains and cross‑lingual gaps.
- Month 5: Federated learning under constraints
  - Implement asynchronous FL with compressed updates; simulate dropouts; tune for convergence [5].
  - Milestones: FL vs centralized performance and uplink savings; recommended operating point.
- Month 6: Human‑in‑the‑loop and consolidation
  - Pilot active learning; integrate governance notes on privacy/data handling; prepare preprint and open artifacts.
  - Milestones: End‑to‑end evaluation report; artifact (models, scripts), and governance checklist.

4. Resources (compute, tools, datasets)
- Compute
  - Development: 1–2 modest GPUs (e.g., 24–48 GB total) or cloud equivalents for training; CPU‑only for mobile tests.
  - Devices: 2–3 mid‑range Android phones for on‑device benchmarking.
- Tools
  - Modeling: PyTorch + TFLite converters; ONNX Runtime Mobile as alternative [6].
  - Compression: Distillation/quantization utilities (post‑training int8; optional QAT) [4][6].
  - FL: Framework supporting asynchronous/quantized updates (or custom implementation), following efficient async FL literature [5].
  - Evaluation: Byte/latency logging; fairness dashboards; error taxonomy scripts.
- Datasets
  - Fakeddit (multimodal, fine‑grained labels) [1].
  - NewsCLIPpings (out‑of‑context image–text) [3].
  - Weibo23 (Chinese multimodal fake news) [7].
  - Survey references for method selection and pitfalls [2].

5. Risks and mitigations table
- Domain shift and cultural references harm accuracy
  - Mitigation: Region‑specific fine‑tuning; multilingual tokenizers; per‑region thresholds; explicit error taxonomies [2][7].
- Quantization degrades multimodal fusion more than expected
  - Mitigation: Layer‑wise sensitivity tests; selectively keep sensitive layers in fp16; student‑teacher distillation to recover accuracy [4][6].
- Bandwidth/latency gains vary by network conditions
  - Mitigation: Measure across 2G/3G/4G; cache results offline; adaptive protocol (on‑device if slow; server when fast). Byte‑savings claims reported empirically (no strong priors; Conjecture).
- Federated training unstable under high dropout
  - Mitigation: Asynchronous aggregation; increase local steps; compressed updates; patience/early stopping [5].
- Dataset licensing or content sensitivity
  - Mitigation: Review licenses; avoid storing raw user media; consider embedding‑only storage for experiments (with privacy caveats).
- Annotation quality in active learning
  - Mitigation: Clear guidelines; dual‑review for contentious items; periodic calibration.

6. Stretch ideas or follow-up directions
- Energy‑aware deployment: measure mWh per decision on phones; schedule background batching to save power.
- Robustness: Add adversarial and manipulation tests (splicing, text overlays) and simple OCR to detect meme text inconsistencies.
- Content provenance: integrate lightweight hash‑based media deduplication to flag recirculated misinformation with negligible bytes.
- Privacy: Explore client‑side DP on embeddings before upload; quantify utility loss (Conjecture—no high‑confidence sources identified for DP on multimodal embeddings in this context).

Citations
[1] Fakeddit: A New Multimodal Benchmark Dataset for Fine-grained Fake News Detection — https://aclanthology.org/2020.lrec-1.755.pdf
[2] Multimodal Fake News Detection (survey) — https://arxiv.org/abs/2112.04831
[3] NewsCLIPpings: Automatic Generation of Out-of-Context Multimodal Media — https://aclanthology.org/2021.emnlp-main.545.pdf
[4] Polino et al., Model compression via quantization and distillation — https://arxiv.org/abs/1802.05668
[5] Efficient Asynchronous Federated Learning (low-bandwidth friendly FL) — https://arxiv.org/abs/2312.15186
[6] TensorFlow Lite Model Optimization for On-Device Machine Learning — https://learnopencv.com/tensorflow-lite-model-optimization-for-on-device-machine-learning/
[7] Multimodal fake news dataset Weibo23 — https://ieee-dataport.org/documents/multimodal-fake-news-dataset-weibo23

Optional next step
- Approve Months 1–2 to stand up baselines and int8 on-device prototypes on Fakeddit/NewsCLIPpings, with a measurement harness for accuracy/latency/bytes; we will return with a down‑selected architecture and a detailed FL compression plan for Month 5.