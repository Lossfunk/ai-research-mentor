Stage: B – Idea moving to C – Research plan

1. Problem framing and goals
- Problem: For small e-commerce stores in emerging markets, many sellers receive little to no exposure due to popularity bias and sparse data. We aim to evaluate and improve fairness-aware recommendation methods that increase equitable exposure for sellers without unacceptable losses in user relevance and business KPIs. Fairness will focus primarily on provider (seller) exposure, with optional user-side checks if demographic or region labels are available. Fairness of exposure is a well-established paradigm for ranking systems and aligns directly with seller equity goals [1].
- Primary goals:
  - Quantify current exposure disparity and quality-adjusted exposure disparity across sellers (overall and by lifecycle: cold-start, low-inventory, rural/under-served regions).
  - Benchmark re-ranking and training-time fairness methods on relevance–fairness trade-offs under realistic small-data constraints.
  - Provide off-policy, statistically principled offline estimates of online impact to de-risk pilots using IPS/DR estimators [2].
- Success criteria:
  - Reduce seller exposure Gini by ≥20% with ≤2% relative drop in Recall@K on at least two datasets; uncertainty-bounded via bootstrap CIs.
  - Demonstrate robust gains for cold-start sellers (≥30% increase in calibrated exposure share) with acceptable utility trade-off [P5].
  - Deliver a reproducible pipeline with ≥3-seed variance reporting and clear ablation evidence for the dominant factor behind gains.

Intuition
- Small stores suffer from head–tail dynamics: a few popular items and sellers dominate exposure. Adding exposure constraints or penalties can redirect a portion of attention to qualified tail sellers while maintaining user utility, especially via lightweight re-ranking on top of strong baselines.
Why this is principled
- Exposure-based fairness provides a direct, interpretable link between ranking positions and provider outcomes [1]. Off-policy evaluation via IPS/doubly-robust estimators is the standard to estimate counterfactual policy performance from logs without running risky online tests [2]. Cold-start fairness leverages transfer/meta-learning to protect new providers from being starved by feedback loops [P5].

2. Experiments
Experiment 1: Exposure-constrained re-ranking (post-hoc)
- Hypothesis: A re-ranking layer that enforces exposure constraints by seller reduces exposure disparity with minimal relevance loss [1].
- Setup: Train a baseline recommender (e.g., BPR-MF, LightFM, SASRec). Apply Top-K re-ranking with exposure budgets per seller (proportional to estimated quality via predicted relevance) using constrained optimization (e.g., greedy with exposure caps). Datasets: Olist (Brazil), RetailRocket, Yoochoose; derive seller IDs from items.
- Baselines: Pure utility ranking; diversity-focused MMR/xQuAD; uniform rotation without quality adjustments.
- Evaluation metrics: Utility (Recall@K/NDCG@K), Fairness (exposure Gini/Theil across sellers; quality-adjusted exposure ratio vs predicted relevance), Coverage@K (sellers surfaced), Head–tail exposure ratio. Estimate CTR/GMV via IPS/DR with clipping and SNIPS variants, report CIs [2].
- Expected outcomes: Pareto front showing 10–30% exposure disparity reduction at ≤2% utility loss; failure (large utility loss) indicates constraints too strict or poor quality estimates; follow-ups include adaptive budgets by lifecycle and position-weighted exposure modeling.

Experiment 2: Training-time exposure regularization
- Hypothesis: Incorporating an exposure disparity penalty during training yields smoother trade-offs than post-hoc re-ranking and improves calibration of exposure to item quality.
- Setup: Add a regularizer to the ranking loss: L = L_rec + λ·ExposureDisparity(seller), where exposure is approximated from predicted scores and position bias. Train across λ∈{0,0.01,0.1,1}. Use same datasets, identical negatives, and hyperparameters across conditions.
- Baselines: Experiment 1 utility-only and re-ranking-only policies.
- Evaluation metrics: Same as E1 plus calibration error between predicted relevance share and realized exposure share per seller. Use DR OPE with an estimated logging policy to reduce variance [2].
- Expected outcomes: For moderate λ, expect similar fairness to re-ranking with smaller utility drops; if unstable, diagnose sensitivity to position-bias modeling and switch to doubly robust estimation with better logging policy estimation [2].

Experiment 3: Cold-start seller fairness via transferable fairness
- Hypothesis: Transfer/meta-learning that encodes seller-invariant priors improves cold-start exposure fairness without hurting overall utility [P5].
- Setup: Split sellers by join time; treat newest 20% as cold-start. Pretrain user/item encoders on head sellers, adapt with few-shot updates or meta-learning to unseen sellers; optionally add a fairness-aware objective that boosts uncertainty-aware exposure for new sellers.
- Baselines: Utility-only model; re-ranking from E1 applied to cold-start sellers; naive smoothing (add-k exposure).
- Evaluation metrics: Cold-start segment utility (Recall@K), exposure share vs expected quality proxy (category-level click-through or item metadata), ramp-up time to reach 80% of mature-seller exposure; fairness–utility trade-off curves by seller age. OPE with stratified variance reporting [P5][2].
- Expected outcomes: 30%+ improvement in cold-start calibrated exposure with ≤3% utility loss; if not, examine whether item metadata is too sparse and test auxiliary text/image features or hierarchical category priors.

Experiment 4: Counterfactual fairness analysis and guardrails
- Hypothesis: Recommendations contain unfair pathways (e.g., region → shipping complexity → reduced exposure) that can be mitigated by path-specific constraints without blanket removal of useful signals [P1][P2].
- Setup: Build a simple causal graph over features (region, price, shipping time, category) and predicted scores. Conduct path-specific counterfactual probes by intervening on sensitive or proxy attributes and measuring score/exposure changes. Implement a guardrail re-ranking that limits exposure changes attributable to sensitive paths while preserving permissible paths.
- Baselines: No guardrails; naive feature removal (drop region).
- Evaluation metrics: Counterfactual fairness gap (Δ exposure/score under interventions), utility and standard fairness metrics. Qualitative inspection of affected sellers to ensure plausible business rationales [P1][P2].
- Expected outcomes: Guardrails reduce path-specific unfairness with smaller utility loss than naive feature removal; if unstable, simplify graph or limit to high-confidence paths.

3. Timeline for the next 6 months with milestones
- Phase 0 (Weeks 1–2): Repro and gating
  - Deliverables: (1) Prediction log with ≥14 entries; (2) Reproduce a baseline Recall@K on one dataset within 10% of reported benchmarks and one ablation or negative result with post-mortem; (3) Experiment card template finalized for E1–E3.
- Month 2: Data audit and baselines
  - Complete data prep for Olist, RetailRocket, Yoochoose; build seller mappings. Establish position-bias model. Baseline utility models trained with 3 seeds; initial logging policy estimation for OPE. Milestone: Baseline metrics dashboard + OPE sanity checks (SNIPS ≈ IPS when weights near 1).
- Month 3: E1 re-ranking
  - Implement exposure-constrained re-ranking; generate fairness–utility Pareto curves across K and budgets. Milestone: Achieve ≥10% exposure Gini reduction with ≤3% utility loss on 2 datasets; archive configs and seeds.
- Month 4: E2 regularization
  - Train exposure-regularized models across λ grid; DR OPE integrated. Milestone: Match or exceed E1 trade-off on at least one dataset; ablation shows λ explains ≥50% of observed fairness gains.
- Month 5: E3 cold-start fairness
  - Split by seller age; implement transfer/meta variant; evaluate ramp-up metrics. Milestone: 30% improvement in cold-start calibrated exposure with ≤3% utility loss on at least one dataset; uncertainty intervals reported.
- Month 6: E4 counterfactual guardrails + synthesis
  - Build causal probes; test guardrail re-ranking. Prepare paper: methods, ablations, limitations, and deployment checklist. Milestone: Draft with figures; reproducibility package (code, configs, data notes).

4. Resources (compute, tools, datasets)
- Compute
  - CPU: 16–32 vCPU; RAM: 64–128 GB for joins and OPE.
  - GPU: 1×T4/3090-class optional for deep sequential models (SASRec, transformers).
  - Storage: 200 GB for datasets, logs, artifacts; fast SSD recommended.
- Tools
  - Modeling: RecBole or TensorFlow Recommenders; implicit/LightFM for MF; PyTorch Lightning for training loops.
  - Re-ranking: Custom exposure constraint module; OR-Tools or greedy heuristics.
  - OPE: Custom IPS/SNIPS/DR estimators with propensity clipping; bootstrap CI.
  - Experiment tracking: Weights & Biases or MLflow; DVC for data versioning.
- Datasets (public, seller-aware)
  - Olist (Brazil) with sellers and orders; derive seller exposure targets (Kaggle).
  - RetailRocket and Yoochoose (RecSys 2015); infer sellers via item–merchant mapping if available; if not, simulate provider groups by brand/category and note limitation.
  - Diginetica; optional Taobao small sample. Note: If seller IDs are missing, use category/brand as provider proxy and document as a limitation to be improved via partnerships.

5. Risks and mitigations table
- Sparse or missing seller IDs
  - Mitigation: Use brand/category as provider proxy; run sensitivity analyses; seek a partner dataset with explicit seller fields.
- Selection bias and MNAR logs
  - Mitigation: Use IPS/SNIPS/DR with clipping, logging-policy estimation, and diagnostics; report variance and CIs [2].
- Over-constraining harms utility
  - Mitigation: Sweep constraint strength; report Pareto fronts; adopt quality-adjusted exposure so high-quality tail sellers get priority [1].
- Cold-start metadata too weak
  - Mitigation: Add text/image features; hierarchical category priors; uncertainty-aware exploration component in re-ranking [P5].
- Metric gaming or mis-specification
  - Mitigation: Triangulate fairness (Gini, Theil, head–tail ratios, calibrated exposure error); pre-register analysis plan; keep guardrail thresholds conservative.
- Latency and engineering overhead
  - Mitigation: Cache re-ranked slates; batch updates; keep training-time regularization as a no-latency alternative.
- Ethical/regulatory concerns (user demographics)
  - Mitigation: Prioritize provider fairness; if user fairness is explored, use coarse, consented region labels and aggregate reporting; apply path-specific guardrails rather than dropping predictive features [P1][P2].

6. Stretch ideas or follow-up directions
- Multi-objective constrained bandits for real-time fair exposure with OPE-safety checks before online rollout.
- Path-specific causal audits and interactive dashboards for stakeholders to explore harms and trade-offs [P3].
- Explainable fairness: counterfactual explanations to communicate why exposure changed for a seller [P6]; GNN-based fairness explanation for catalog graphs [P8].
- Temporal fairness: enforce stability/equity over time to avoid oscillations in seller visibility [P7].

Citations
- [P1] Path-Specific Counterfactual Fairness for Recommender Systems — http://arxiv.org/abs/2306.02615v1
- [P2] Personalized Counterfactual Fairness in Recommendation — http://arxiv.org/abs/2105.09829v3
- [P3] Interactive Counterfactual Exploration of Algorithmic Harms in Recommender Systems — http://arxiv.org/abs/2409.06916v1
- [P5] Transferable Fairness for Cold-Start Recommendation — http://arxiv.org/abs/2301.10665v1
- [P6] Counterfactual Explanation for Fairness in Recommendation — http://arxiv.org/abs/2307.04386v1
- [P7] Recommendation Fairness in Social Networks Over Time — http://arxiv.org/abs/2402.03450v2
- [P8] GNNUERS: Fairness Explanation in GNNs for Recommendation via Counterfactual Reasoning — http://arxiv.org/abs/2304.06182v4
- [1] Fairness of Exposure in Rankings — https://arxiv.org/abs/1802.07281
- [2] Doubly-Robust Off-Policy Evaluation with Estimated Logging Policy — https://arxiv.org/abs/2404.01830

Optional next step
- Kick off Phase 0 by selecting two datasets (Olist + RetailRocket), standing up a simple BPR-MF baseline with 3 seeds, and drafting the experiment cards for E1–E3 so we can lock metrics, falsifiers, and stop rules before implementation.