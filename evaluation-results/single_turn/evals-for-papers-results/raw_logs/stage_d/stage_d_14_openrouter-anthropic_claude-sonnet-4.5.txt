Based on the attached paper, I'll design comprehensive rule-based heuristics to approximate the hierarchical structure that SHI learns, along with metrics and error analysis to assess parity.

## Rule-Based Hierarchical Approximation

### 1. Rule-Based Hierarchy Assignment

```python
class RuleBasedHierarchy:
    """
    Rule-based heuristics to approximate SHI's learned hierarchy.
    
    Original SHI: Learns hierarchy from data [heirarchical-transformer.pdf:2]
    Rule-based: Uses explicit linguistic/structural cues
    
    Hierarchy levels:
    - Level 0 (Coarse): Section headers, main topics, key entities
    - Level 1 (Medium): Paragraph boundaries, discourse markers, supporting content
    - Level 2 (Fine): Details, examples, modifiers
    """
    def __init__(self, config):
        self.config = config
        
        # Rule sets for each modality
        self.text_rules = TextHierarchyRules()
        self.visual_rules = VisualHierarchyRules()
        self.code_rules = CodeHierarchyRules()
        
        # Confidence thresholds
        self.confidence_thresholds = {
            'high': 0.8,
            'medium': 0.5,
            'low': 0.3
        }
        
    def assign_hierarchy(self, input_data, modality='text'):
        """
        Assign hierarchy levels using rule-based heuristics.
        
        Args:
            input_data: Input tokens/observations
            modality: 'text', 'visual', or 'code'
        
        Returns:
            hierarchy_levels: [L] array of levels (0, 1, 2)
            confidence_scores: [L] array of confidence (0-1)
            rule_matches: [L] list of matched rules
        """
        if modality == 'text':
            return self.text_rules.assign(input_data)
        elif modality == 'visual':
            return self.visual_rules.assign(input_data)
        elif modality == 'code':
            return self.code_rules.assign(input_data)
        else:
            raise ValueError(f"Unknown modality: {modality}")

class TextHierarchyRules:
    """
    Rule-based hierarchy for text documents.
    
    Cues:
    - Section headers (markdown, HTML, formatting)
    - Discourse markers ("however", "therefore", "for example")
    - Sentence position (first sentence = important)
    - Named entities (proper nouns = important)
    - Syntactic role (main clause vs. subordinate)
    """
    def __init__(self):
        # Level 0 (Coarse) indicators
        self.level_0_patterns = {
            'section_headers': [
                r'^#{1,3}\s+',           # Markdown headers (# ## ###)
                r'^[A-Z][A-Za-z\s]+:$',  # Title case with colon
                r'<h[1-3]>',             # HTML headers
                r'^\d+\.\s+[A-Z]',       # Numbered sections (1. Introduction)
            ],
            'document_structure': [
                r'^(Introduction|Abstract|Conclusion|Summary)',
                r'^(Chapter|Section|Part)\s+\d+',
            ],
            'key_entities': [
                r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)+\b',  # Proper nouns (multi-word)
            ]
        }
        
        # Level 1 (Medium) indicators
        self.level_1_patterns = {
            'discourse_markers': [
                r'\b(however|therefore|thus|consequently|moreover|furthermore)\b',
                r'\b(in contrast|on the other hand|similarly|likewise)\b',
                r'\b(first|second|third|finally|lastly)\b',
            ],
            'paragraph_boundaries': [
                r'\n\n',  # Double newline
            ],
            'topic_sentences': [
                # First sentence of paragraph (detected by position)
            ],
            'supporting_content': [
                r'\b(because|since|as|due to)\b',  # Causal markers
                r'\b(which|that|who|whom)\b',      # Relative clauses
            ]
        }
        
        # Level 2 (Fine) indicators
        self.level_2_patterns = {
            'examples': [
                r'\b(for example|for instance|such as|e\.g\.|i\.e\.)\b',
                r'\b(specifically|namely|in particular)\b',
            ],
            'details': [
                r'\b(approximately|about|roughly|nearly)\b',  # Hedges
                r'\d+%',  # Statistics
                r'\(\w+\)',  # Parenthetical content
            ],
            'modifiers': [
                r'\b(very|quite|rather|somewhat|extremely)\b',  # Adverbs
            ],
            'citations': [
                r'\[\d+\]',  # Citation markers
                r'\([A-Z][a-z]+\s+et\s+al\.,\s+\d{4}\)',  # In-text citations
            ]
        }
        
        # Compile patterns
        self.compiled_patterns = {
            0: self._compile_patterns(self.level_0_patterns),
            1: self._compile_patterns(self.level_1_patterns),
            2: self._compile_patterns(self.level_2_patterns)
        }
        
        # POS tagger for syntactic analysis
        try:
            import spacy
            self.nlp = spacy.load('en_core_web_sm')
            self.use_spacy = True
        except:
            self.use_spacy = False
            print("Warning: spaCy not available, using pattern-based rules only")
    
    def _compile_patterns(self, pattern_dict):
        """Compile regex patterns."""
        import re
        compiled = {}
        for category, patterns in pattern_dict.items():
            compiled[category] = [re.compile(p, re.IGNORECASE) for p in patterns]
        return compiled
    
    def assign(self, text):
        """
        Assign hierarchy levels to text tokens.
        
        Args:
            text: String or list of tokens
        
        Returns:
            hierarchy_levels: [L] array
            confidence_scores: [L] array
            rule_matches: [L] list of matched rules
        """
        # Tokenize if needed
        if isinstance(text, str):
            tokens = text.split()
            full_text = text
        else:
            tokens = text
            full_text = ' '.join(tokens)
        
        # Initialize outputs
        hierarchy_levels = np.zeros(len(tokens), dtype=int)
        confidence_scores = np.zeros(len(tokens))
        rule_matches = [[] for _ in range(len(tokens))]
        
        # Track token positions in original text
        token_positions = self._get_token_positions(full_text, tokens)
        
        # Apply rules for each level (in order: 0, 1, 2)
        for level in [0, 1, 2]:
            for category, patterns in self.compiled_patterns[level].items():
                for pattern in patterns:
                    # Find matches in full text
                    for match in pattern.finditer(full_text):
                        start, end = match.span()
                        
                        # Find tokens overlapping with match
                        overlapping_tokens = self._find_overlapping_tokens(
                            start, end, token_positions
                        )
                        
                        # Assign level to overlapping tokens
                        for token_idx in overlapping_tokens:
                            # Only assign if not already assigned to higher level
                            if hierarchy_levels[token_idx] > level:
                                continue
                            
                            hierarchy_levels[token_idx] = level
                            confidence_scores[token_idx] = self._compute_confidence(
                                category, pattern, match
                            )
                            rule_matches[token_idx].append({
                                'level': level,
                                'category': category,
                                'pattern': pattern.pattern,
                                'match': match.group()
                            })
        
        # Apply position-based rules
        hierarchy_levels, confidence_scores, rule_matches = self._apply_position_rules(
            tokens, full_text, hierarchy_levels, confidence_scores, rule_matches
        )
        
        # Apply syntactic rules (if spaCy available)
        if self.use_spacy:
            hierarchy_levels, confidence_scores, rule_matches = self._apply_syntactic_rules(
                full_text, tokens, token_positions, hierarchy_levels, confidence_scores, rule_matches
            )
        
        # Default: unassigned tokens → Level 2 (fine-grained)
        unassigned = (confidence_scores == 0)
        hierarchy_levels[unassigned] = 2
        confidence_scores[unassigned] = 0.3  # Low confidence for defaults
        
        return hierarchy_levels, confidence_scores, rule_matches
    
    def _get_token_positions(self, text, tokens):
        """
        Get character positions of tokens in text.
        """
        positions = []
        current_pos = 0
        
        for token in tokens:
            # Find token in text starting from current position
            idx = text.find(token, current_pos)
            if idx != -1:
                positions.append((idx, idx + len(token)))
                current_pos = idx + len(token)
            else:
                # Token not found (shouldn't happen if tokens from text)
                positions.append((current_pos, current_pos))
        
        return positions
    
    def _find_overlapping_tokens(self, start, end, token_positions):
        """
        Find tokens that overlap with character range [start, end).
        """
        overlapping = []
        
        for i, (token_start, token_end) in enumerate(token_positions):
            # Check for overlap
            if token_start < end and token_end > start:
                overlapping.append(i)
        
        return overlapping
    
    def _compute_confidence(self, category, pattern, match):
        """
        Compute confidence score for rule match.
        
        Higher confidence for:
        - Exact structural markers (headers)
        - Longer matches
        - More specific patterns
        """
        # Base confidence by category
        category_confidence = {
            'section_headers': 0.95,
            'document_structure': 0.90,
            'key_entities': 0.70,
            'discourse_markers': 0.85,
            'paragraph_boundaries': 0.80,
            'topic_sentences': 0.75,
            'supporting_content': 0.65,
            'examples': 0.80,
            'details': 0.70,
            'modifiers': 0.60,
            'citations': 0.75
        }
        
        base_conf = category_confidence.get(category, 0.5)
        
        # Adjust by match length (longer = more confident)
        match_length = len(match.group())
        length_bonus = min(0.1, match_length / 100)
        
        return min(1.0, base_conf + length_bonus)
    
    def _apply_position_rules(self, tokens, full_text, hierarchy_levels, confidence_scores, rule_matches):
        """
        Apply position-based rules.
        
        Rules:
        - First sentence of paragraph → Level 1
        - First word of sentence → Higher importance
        - Title/header position → Level 0
        """
        # Detect paragraph boundaries
        paragraphs = full_text.split('\n\n')
        
        current_token_idx = 0
        
        for para_idx, paragraph in enumerate(paragraphs):
            para_tokens = paragraph.split()
            
            if not para_tokens:
                continue
            
            # First sentence of paragraph
            first_sentence_end = paragraph.find('.')
            if first_sentence_end != -1:
                first_sentence = paragraph[:first_sentence_end]
                first_sentence_tokens = first_sentence.split()
                
                # Assign Level 1 to first sentence tokens
                for i in range(min(len(first_sentence_tokens), len(para_tokens))):
                    token_idx = current_token_idx + i
                    if token_idx < len(hierarchy_levels):
                        if hierarchy_levels[token_idx] > 1:  # Don't override Level 0
                            hierarchy_levels[token_idx] = 1
                            confidence_scores[token_idx] = max(
                                confidence_scores[token_idx], 0.7
                            )
                            rule_matches[token_idx].append({
                                'level': 1,
                                'category': 'position',
                                'pattern': 'first_sentence',
                                'match': 'paragraph_start'
                            })
            
            current_token_idx += len(para_tokens)
        
        return hierarchy_levels, confidence_scores, rule_matches
    
    def _apply_syntactic_rules(self, text, tokens, token_positions, hierarchy_levels, confidence_scores, rule_matches):
        """
        Apply syntactic rules using dependency parsing.
        
        Rules:
        - Main clause subjects/verbs → Level 1
        - Subordinate clauses → Level 2
        - Named entities → Level 0 or 1
        """
        doc = self.nlp(text)
        
        for token in doc:
            # Find corresponding token index
            token_idx = self._find_token_index(token.idx, token_positions)
            
            if token_idx is None:
                continue
            
            # Named entities → Level 0
            if token.ent_type_ in ['PERSON', 'ORG', 'GPE', 'EVENT']:
                if hierarchy_levels[token_idx] > 0:
                    hierarchy_levels[token_idx] = 0
                    confidence_scores[token_idx] = max(confidence_scores[token_idx], 0.75)
                    rule_matches[token_idx].append({
                        'level': 0,
                        'category': 'syntax',
                        'pattern': 'named_entity',
                        'match': token.ent_type_
                    })
            
            # Main clause subjects/verbs → Level 1
            if token.dep_ in ['nsubj', 'ROOT'] and token.head.dep_ == 'ROOT':
                if hierarchy_levels[token_idx] > 1:
                    hierarchy_levels[token_idx] = 1
                    confidence_scores[token_idx] = max(confidence_scores[token_idx], 0.70)
                    rule_matches[token_idx].append({
                        'level': 1,
                        'category': 'syntax',
                        'pattern': 'main_clause',
                        'match': token.dep_
                    })
            
            # Subordinate clauses → Level 2
            if token.dep_ in ['advcl', 'relcl', 'ccomp', 'xcomp']:
                hierarchy_levels[token_idx] = 2
                confidence_scores[token_idx] = max(confidence_scores[token_idx], 0.65)
                rule_matches[token_idx].append({
                    'level': 2,
                    'category': 'syntax',
                    'pattern': 'subordinate_clause',
                    'match': token.dep_
                })
        
        return hierarchy_levels, confidence_scores, rule_matches
    
    def _find_token_index(self, char_pos, token_positions):
        """Find token index corresponding to character position."""
        for i, (start, end) in enumerate(token_positions):
            if start <= char_pos < end:
                return i
        return None

class VisualHierarchyRules:
    """
    Rule-based hierarchy for visual observations (e.g., Atari frames).
    
    Cues:
    - Saliency (bright, high-contrast regions)
    - Object detection (agents, enemies, items)
    - Spatial position (center = important)
    - Motion (moving objects = important)
    """
    def __init__(self):
        self.use_opencv = True
        try:
            import cv2
            self.cv2 = cv2
        except:
            self.use_opencv = False
            print("Warning: OpenCV not available, using simple visual rules")
    
    def assign(self, observation):
        """
        Assign hierarchy to visual observation.
        
        Args:
            observation: [H, W, C] image or [L] VQ-VAE tokens
        
        Returns:
            hierarchy_levels: [L] array
            confidence_scores: [L] array
            rule_matches: [L] list
        """
        # If already tokenized (VQ-VAE), use spatial heuristics
        if observation.ndim == 1:
            return self._assign_to_tokens(observation)
        
        # Otherwise, analyze image
        return self._assign_to_image(observation)
    
    def _assign_to_image(self, image):
        """
        Assign hierarchy to image pixels/regions.
        """
        H, W, C = image.shape
        
        # Initialize
        hierarchy_map = np.zeros((H, W), dtype=int)
        confidence_map = np.zeros((H, W))
        
        if self.use_opencv:
            # Saliency detection
            saliency_map = self._compute_saliency(image)
            
            # High saliency → Level 0
            high_saliency = saliency_map > np.percentile(saliency_map, 90)
            hierarchy_map[high_saliency] = 0
            confidence_map[high_saliency] = 0.8
            
            # Medium saliency → Level 1
            medium_saliency = (saliency_map > np.percentile(saliency_map, 60)) & ~high_saliency
            hierarchy_map[medium_saliency] = 1
            confidence_map[medium_saliency] = 0.6
            
            # Low saliency → Level 2
            low_saliency = ~high_saliency & ~medium_saliency
            hierarchy_map[low_saliency] = 2
            confidence_map[low_saliency] = 0.4
        else:
            # Simple brightness-based heuristic
            brightness = np.mean(image, axis=2)
            
            high_brightness = brightness > np.percentile(brightness, 80)
            hierarchy_map[high_brightness] = 0
            confidence_map[high_brightness] = 0.7
            
            medium_brightness = (brightness > np.percentile(brightness, 40)) & ~high_brightness
            hierarchy_map[medium_brightness] = 1
            confidence_map[medium_brightness] = 0.5
            
            low_brightness = ~high_brightness & ~medium_brightness
            hierarchy_map[low_brightness] = 2
            confidence_map[low_brightness] = 0.3
        
        # Flatten to token-level (assuming VQ-VAE tokenization)
        # For simplicity, average over spatial regions
        token_size = 8  # Assume 8x8 patches
        num_tokens_h = H // token_size
        num_tokens_w = W // token_size
        
        hierarchy_levels = []
        confidence_scores = []
        
        for i in range(num_tokens_h):
            for j in range(num_tokens_w):
                patch_hierarchy = hierarchy_map[
                    i*token_size:(i+1)*token_size,
                    j*token_size:(j+1)*token_size
                ]
                patch_confidence = confidence_map[
                    i*token_size:(i+1)*token_size,
                    j*token_size:(j+1)*token_size
                ]
                
                # Mode for hierarchy, mean for confidence
                from scipy.stats import mode
                level = mode(patch_hierarchy.flatten())[0][0]
                conf = np.mean(patch_confidence)
                
                hierarchy_levels.append(level)
                confidence_scores.append(conf)
        
        hierarchy_levels = np.array(hierarchy_levels)
        confidence_scores = np.array(confidence_scores)
        rule_matches = [[] for _ in range(len(hierarchy_levels))]
        
        return hierarchy_levels, confidence_scores, rule_matches
    
    def _compute_saliency(self, image):
        """
        Compute saliency map using spectral residual.
        """
        # Convert to grayscale
        gray = self.cv2.cvtColor(image, self.cv2.COLOR_RGB2GRAY)
        
        # Spectral residual saliency
        saliency = self.cv2.saliency.StaticSaliencySpectralResidual_create()
        success, saliency_map = saliency.computeSaliency(gray)
        
        return saliency_map
    
    def _assign_to_tokens(self, tokens):
        """
        Assign hierarchy to pre-tokenized observation.
        
        Use spatial position heuristics.
        """
        L = len(tokens)
        
        # Assume tokens arranged in grid (e.g., 8x8 for 64 tokens)
        grid_size = int(np.sqrt(L))
        
        hierarchy_levels = np.zeros(L, dtype=int)
        confidence_scores = np.zeros(L)
        
        for i in range(L):
            # Compute spatial position
            row = i // grid_size
            col = i % grid_size
            
            # Distance from center
            center_row = grid_size / 2
            center_col = grid_size / 2
            
            dist_from_center = np.sqrt((row - center_row)**2 + (col - center_col)**2)
            max_dist = np.sqrt(2) * grid_size / 2
            
            # Closer to center → higher importance
            if dist_from_center < max_dist * 0.3:
                hierarchy_levels[i] = 0
                confidence_scores[i] = 0.7
            elif dist_from_center < max_dist * 0.6:
                hierarchy_levels[i] = 1
                confidence_scores[i] = 0.5
            else:
                hierarchy_levels[i] = 2
                confidence_scores[i] = 0.3
        
        rule_matches = [[] for _ in range(L)]
        
        return hierarchy_levels, confidence_scores, rule_matches

class CodeHierarchyRules:
    """
    Rule-based hierarchy for code.
    
    Cues:
    - AST structure (module > class > function > statement)
    - Indentation level
    - Keywords (def, class, import)
    - Comments vs. code
    """
    def __init__(self):
        pass
    
    def assign(self, code):
        """
        Assign hierarchy to code tokens.
        
        Args:
            code: String of source code
        
        Returns:
            hierarchy_levels: [L] array
            confidence_scores: [L] array
            rule_matches: [L] list
        """
        import ast
        import tokenize
        from io import StringIO
        
        # Tokenize code
        tokens = list(tokenize.generate_tokens(StringIO(code).readline))
        
        # Parse AST
        try:
            tree = ast.parse(code)
        except SyntaxError:
            # Fallback to indentation-based
            return self._assign_by_indentation(code)
        
        # Assign hierarchy based on AST
        hierarchy_levels = []
        confidence_scores = []
        rule_matches = []
        
        for token in tokens:
            if token.type == tokenize.ENCODING or token.type == tokenize.ENDMARKER:
                continue
            
            # Find AST node at this position
            line_no = token.start[0]
            col_no = token.start[1]
            
            node = self._find_ast_node(tree, line_no, col_no)
            
            if node is None:
                # Default
                hierarchy_levels.append(2)
                confidence_scores.append(0.3)
                rule_matches.append([])
                continue
            
            # Assign level based on node type
            level, conf = self._node_to_level(node)
            
            hierarchy_levels.append(level)
            confidence_scores.append(conf)
            rule_matches.append([{
                'level': level,
                'category': 'ast',
                'pattern': type(node).__name__,
                'match': token.string
            }])
        
        return np.array(hierarchy_levels), np.array(confidence_scores), rule_matches
    
    def _find_ast_node(self, tree, line_no, col_no):
        """Find AST node at given position."""
        for node in ast.walk(tree):
            if hasattr(node, 'lineno') and node.lineno == line_no:
                return node
        return None
    
    def _node_to_level(self, node):
        """
        Map AST node type to hierarchy level.
        
        Level 0: Module, Class, Import
        Level 1: Function, Method
        Level 2: Statement, Expression
        """
        if isinstance(node, (ast.Module, ast.ClassDef, ast.Import, ast.ImportFrom)):
            return 0, 0.9
        elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
            return 1, 0.8
        elif isinstance(node, (ast.Expr, ast.Assign, ast.Return, ast.If, ast.For, ast.While)):
            return 2, 0.7
        else:
            return 2, 0.5
    
    def _assign_by_indentation(self, code):
        """
        Fallback: Assign hierarchy by indentation level.
        """
        lines = code.split('\n')
        tokens = code.split()
        
        hierarchy_levels = []
        confidence_scores = []
        
        for line in lines:
            # Count leading spaces
            indent = len(line) - len(line.lstrip())
            
            # Map indent to level
            if indent == 0:
                level = 0
                conf = 0.7
            elif indent <= 4:
                level = 1
                conf = 0.6
            else:
                level = 2
                conf = 0.5
            
            # Assign to all tokens in line
            line_tokens = line.split()
            for _ in line_tokens:
                hierarchy_levels.append(level)
                confidence_scores.append(conf)
        
        rule_matches = [[] for _ in range(len(hierarchy_levels))]
        
        return np.array(hierarchy_levels), np.array(confidence_scores), rule_matches
```

### 2. Parity Metrics

```python
class ParityMetrics:
    """
    Metrics to assess parity between rule-based and learned hierarchy.
    
    Metrics:
    1. Agreement: How often do they assign same level?
    2. Correlation: Do importance scores correlate?
    3. Performance: Do they achieve similar downstream performance?
    4. Efficiency: Computational cost comparison
    """
    def __init__(self):
        pass
    
    def compute_agreement(self, rule_hierarchy, learned_hierarchy):
        """
        Compute agreement between rule-based and learned hierarchy.
        
        Metrics:
        - Exact agreement rate
        - Cohen's kappa (chance-corrected agreement)
        - Confusion matrix
        - Per-level agreement
        """
        from sklearn.metrics import cohen_kappa_score, confusion_matrix
        
        # Flatten if needed
        rule_flat = rule_hierarchy.flatten()
        learned_flat = learned_hierarchy.flatten()
        
        # Exact agreement
        exact_agreement = (rule_flat == learned_flat).mean()
        
        # Cohen's kappa
        kappa = cohen_kappa_score(rule_flat, learned_flat)
        
        # Confusion matrix
        conf_matrix = confusion_matrix(learned_flat, rule_flat, labels=[0, 1, 2])
        
        # Per-level agreement
        per_level_agreement = {}
        for level in [0, 1, 2]:
            level_mask = (learned_flat == level)
            if level_mask.sum() > 0:
                per_level_agreement[level] = (
                    rule_flat[level_mask] == learned_flat[level_mask]
                ).mean()
            else:
                per_level_agreement[level] = np.nan
        
        # Off-by-one agreement (allow ±1 level difference)
        off_by_one = (np.abs(rule_flat - learned_flat) <= 1).mean()
        
        return {
            'exact_agreement': exact_agreement,
            'cohens_kappa': kappa,
            'confusion_matrix': conf_matrix,
            'per_level_agreement': per_level_agreement,
            'off_by_one_agreement': off_by_one,
            'interpretation': self._interpret_agreement(exact_agreement, kappa)
        }
    
    def compute_correlation(self, rule_importance, learned_importance):
        """
        Compute correlation between importance scores.
        
        Importance can be:
        - Masking weights (higher = more important)
        - Hierarchy level (inverted: 0 = most important)
        - SPARTAN causal scores
        """
        from scipy.stats import spearmanr, pearsonr
        
        # Flatten
        rule_flat = rule_importance.flatten()
        learned_flat = learned_importance.flatten()
        
        # Spearman correlation (rank-based, robust)
        spearman_rho, spearman_p = spearmanr(rule_flat, learned_flat)
        
        # Pearson correlation (linear)
        pearson_r, pearson_p = pearsonr(rule_flat, learned_flat)
        
        # Kendall's tau (alternative rank correlation)
        from scipy.stats import kendalltau
        kendall_tau, kendall_p = kendalltau(rule_flat, learned_flat)
        
        return {
            'spearman_rho': spearman_rho,
            'spearman_p': spearman_p,
            'pearson_r': pearson_r,
            'pearson_p': pearson_p,
            'kendall_tau': kendall_tau,
            'kendall_p': kendall_p,
            'interpretation': self._interpret_correlation(spearman_rho)
        }
    
    def compute_performance_parity(self, rule_predictions, learned_predictions, ground_truth):
        """
        Compare downstream task performance.
        
        Tasks:
        - Prediction accuracy (MSE, MAE)
        - Rollout error at different horizons
        - RL performance (return, success rate)
        """
        # Prediction error
        rule_mse = np.mean((rule_predictions - ground_truth) ** 2)
        learned_mse = np.mean((learned_predictions - ground_truth) ** 2)
        
        rule_mae = np.mean(np.abs(rule_predictions - ground_truth))
        learned_mae = np.mean(np.abs(learned_predictions - ground_truth))
        
        # Relative performance
        mse_ratio = rule_mse / (learned_mse + 1e-10)
        mae_ratio = rule_mae / (learned_mae + 1e-10)
        
        # Statistical test for difference
        from scipy.stats import ttest_rel
        
        rule_errors = (rule_predictions - ground_truth) ** 2
        learned_errors = (learned_predictions - ground_truth) ** 2
        
        t_stat, p_value = ttest_rel(rule_errors.flatten(), learned_errors.flatten())
        
        return {
            'rule_mse': rule_mse,
            'learned_mse': learned_mse,
            'mse_ratio': mse_ratio,
            'rule_mae': rule_mae,
            'learned_mae': learned_mae,
            'mae_ratio': mae_ratio,
            't_statistic': t_stat,
            'p_value': p_value,
            'significant_difference': p_value < 0.05,
            'interpretation': self._interpret_performance(mse_ratio, p_value)
        }
    
    def compute_efficiency_parity(self, rule_time, learned_time, rule_memory, learned_memory):
        """
        Compare computational efficiency.
        
        Metrics:
        - Inference time
        - Memory usage
        - FLOPs
        """
        time_ratio = rule_time / (learned_time + 1e-10)
        memory_ratio = rule_memory / (learned_memory + 1e-10)
        
        return {
            'rule_time_ms': rule_time * 1000,
            'learned_time_ms': learned_time * 1000,
            'time_ratio': time_ratio,
            'time_speedup': 1 / time_ratio if time_ratio > 0 else float('inf'),
            'rule_memory_mb': rule_memory / (1024 ** 2),
            'learned_memory_mb': learned_memory / (1024 ** 2),
            'memory_ratio': memory_ratio,
            'interpretation': self._interpret_efficiency(time_ratio, memory_ratio)
        }
    
    def _interpret_agreement(self, exact_agreement, kappa):
        """Interpret agreement metrics."""
        if kappa < 0:
            return 'Poor agreement (worse than chance)'
        elif kappa < 0.20:
            return 'Slight agreement'
        elif kappa < 0.40:
            return 'Fair agreement'
        elif kappa < 0.60:
            return 'Moderate agreement'
        elif kappa < 0.80:
            return 'Substantial agreement'
        else:
            return 'Almost perfect agreement'
    
    def _interpret_correlation(self, rho):
        """Interpret correlation."""
        abs_rho = abs(rho)
        
        if abs_rho < 0.3:
            strength = 'weak'
        elif abs_rho < 0.5:
            strength = 'moderate'
        elif abs_rho < 0.7:
            strength = 'strong'
        else:
            strength = 'very strong'
        
        direction = 'positive' if rho > 0 else 'negative'
        
        return f'{strength} {direction} correlation'
    
    def _interpret_performance(self, mse_ratio, p_value):
        """Interpret performance comparison."""
        if p_value >= 0.05:
            return 'No significant difference in performance'
        
        if mse_ratio < 0.9:
            return 'Rule-based significantly better (>10% improvement)'
        elif mse_ratio < 1.0:
            return 'Rule-based slightly better'
        elif mse_ratio < 1.1:
            return 'Learned slightly better'
        else:
            return 'Learned significantly better (>10% improvement)'
    
    def _interpret_efficiency(self, time_ratio, memory_ratio):
        """Interpret efficiency comparison."""
        if time_ratio < 0.5:
            time_interp = 'Rule-based >2x faster'
        elif time_ratio < 1.0:
            time_interp = 'Rule-based faster'
        elif time_ratio < 2.0:
            time_interp = 'Learned faster'
        else:
            time_interp = 'Learned >2x faster'
        
        if memory_ratio < 0.5:
            memory_interp = 'Rule-based uses <50% memory'
        elif memory_ratio < 1.0:
            memory_interp = 'Rule-based uses less memory'
        elif memory_ratio < 2.0:
            memory_interp = 'Learned uses less memory'
        else:
            memory_interp = 'Learned uses <50% memory'
        
        return f'{time_interp}; {memory_interp}'
```

### 3. Error Analysis Framework

```python
class ErrorAnalysis:
    """
    Comprehensive error analysis for rule-based vs. learned hierarchy.
    
    Analyses:
    1. Disagreement patterns (where do they differ?)
    2. Error modes (what types of errors?)
    3. Context sensitivity (when does each approach fail?)
    4. Failure case studies
    """
    def __init__(self):
        pass
    
    def analyze_disagreements(self, rule_hierarchy, learned_hierarchy, input_data, metadata):
        """
        Analyze cases where rule-based and learned disagree.
        
        Questions:
        - Which tokens have highest disagreement?
        - What patterns characterize disagreements?
        - Are disagreements systematic or random?
        """
        # Find disagreements
        disagreement_mask = (rule_hierarchy != learned_hierarchy)
        disagreement_indices = np.where(disagreement_mask)[0]
        
        # Categorize disagreements
        disagreement_types = {
            'rule_higher': [],  # Rule assigns higher level (lower number)
            'learned_higher': [],  # Learned assigns higher level
            'off_by_one': [],  # Differ by exactly 1 level
            'off_by_two': []  # Differ by 2 levels
        }
        
        for idx in disagreement_indices:
            rule_level = rule_hierarchy[idx]
            learned_level = learned_hierarchy[idx]
            
            diff = rule_level - learned_level
            
            if diff < 0:
                disagreement_types['rule_higher'].append(idx)
            else:
                disagreement_types['learned_higher'].append(idx)
            
            if abs(diff) == 1:
                disagreement_types['off_by_one'].append(idx)
            elif abs(diff) == 2:
                disagreement_types['off_by_two'].append(idx)
        
        # Extract disagreement contexts
        disagreement_contexts = self._extract_contexts(
            disagreement_indices, input_data, metadata
        )
        
        # Identify patterns
        patterns = self._identify_disagreement_patterns(
            disagreement_contexts, disagreement_types
        )
        
        return {
            'disagreement_rate': disagreement_mask.mean(),
            'num_disagreements': len(disagreement_indices),
            'disagreement_types': {
                k: len(v) for k, v in disagreement_types.items()
            },
            'disagreement_contexts': disagreement_contexts,
            'patterns': patterns
        }
    
    def analyze_error_modes(self, rule_hierarchy, learned_hierarchy, rule_predictions, learned_predictions, ground_truth):
        """
        Analyze error modes for each approach.
        
        Error modes:
        - False positives (assign high importance to unimportant tokens)
        - False negatives (assign low importance to important tokens)
        - Systematic biases
        """
        # Compute errors
        rule_errors = np.abs(rule_predictions - ground_truth)
        learned_errors = np.abs(learned_predictions - ground_truth)
        
        # Find cases where rule-based makes larger errors
        rule_worse = rule_errors > learned_errors
        
        # Find cases where learned makes larger errors
        learned_worse = learned_errors > rule_errors
        
        # Analyze hierarchy assignments in error cases
        rule_worse_hierarchy = rule_hierarchy[rule_worse]
        learned_worse_hierarchy = learned_hierarchy[learned_worse]
        
        # Error mode 1: Over-importance (assign Level 0 when should be Level 2)
        rule_over_importance = np.sum(
            (rule_hierarchy == 0) & (learned_hierarchy == 2) & rule_worse
        )
        
        learned_over_importance = np.sum(
            (learned_hierarchy == 0) & (rule_hierarchy == 2) & learned_worse
        )
        
        # Error mode 2: Under-importance (assign Level 2 when should be Level 0)
        rule_under_importance = np.sum(
            (rule_hierarchy == 2) & (learned_hierarchy == 0) & rule_worse
        )
        
        learned_under_importance = np.sum(
            (learned_hierarchy == 2) & (rule_hierarchy == 0) & learned_worse
        )
        
        return {
            'rule_worse_cases': rule_worse.sum(),
            'learned_worse_cases': learned_worse.sum(),
            'rule_over_importance': rule_over_importance,
            'rule_under_importance': rule_under_importance,
            'learned_over_importance': learned_over_importance,
            'learned_under_importance': learned_under_importance,
            'rule_mean_error': rule_errors.mean(),
            'learned_mean_error': learned_errors.mean(),
            'interpretation': self._interpret_error_modes(
                rule_over_importance, rule_under_importance,
                learned_over_importance, learned_under_importance
            )
        }
    
    def analyze_context_sensitivity(self, rule_hierarchy, learned_hierarchy, contexts):
        """
        Analyze how context affects hierarchy assignment.
        
        Contexts:
        - Document type (narrative, technical, etc.)
        - Position in document (beginning, middle, end)
        - Local context (surrounding tokens)
        """
        context_analysis = {}
        
        for context_type, context_values in contexts.items():
            context_agreement = {}
            
            for value in np.unique(context_values):
                mask = (context_values == value)
                
                if mask.sum() == 0:
                    continue
                
                # Agreement within this context
                agreement = (
                    rule_hierarchy[mask] == learned_hierarchy[mask]
                ).mean()
                
                context_agreement[value] = agreement
            
            context_analysis[context_type] = context_agreement
        
        return context_analysis
    
    def generate_failure_case_studies(self, rule_hierarchy, learned_hierarchy, input_data, metadata, n_cases=10):
        """
        Generate detailed case studies of failures.
        
        Select:
        - Highest disagreement cases
        - Highest error cases
        - Representative examples of each error mode
        """
        # Find disagreements
        disagreement_mask = (rule_hierarchy != learned_hierarchy)
        disagreement_indices = np.where(disagreement_mask)[0]
        
        # Sample diverse cases
        if len(disagreement_indices) > n_cases:
            # Stratified sampling by disagreement magnitude
            disagreement_magnitudes = np.abs(
                rule_hierarchy[disagreement_indices] - 
                learned_hierarchy[disagreement_indices]
            )
            
            # Sample from each magnitude
            sampled_indices = []
            for magnitude in [1, 2]:
                magnitude_mask = (disagreement_magnitudes == magnitude)
                magnitude_indices = disagreement_indices[magnitude_mask]
                
                if len(magnitude_indices) > 0:
                    n_sample = min(n_cases // 2, len(magnitude_indices))
                    sampled = np.random.choice(magnitude_indices, n_sample, replace=False)
                    sampled_indices.extend(sampled)
            
            disagreement_indices = sampled_indices[:n_cases]
        
        # Generate case studies
        case_studies = []
        
        for idx in disagreement_indices:
            case = {
                'index': idx,
                'rule_level': rule_hierarchy[idx],
                'learned_level': learned_hierarchy[idx],
                'disagreement_magnitude': abs(
                    rule_hierarchy[idx] - learned_hierarchy[idx]
                ),
                'context': self._extract_single_context(idx, input_data, metadata),
                'analysis': self._analyze_single_case(
                    idx, rule_hierarchy, learned_hierarchy, input_data, metadata
                )
            }
            
            case_studies.append(case)
        
        return case_studies
    
    def _extract_contexts(self, indices, input_data, metadata):
        """Extract context for disagreement indices."""
        contexts = []
        
        for idx in indices:
            context = self._extract_single_context(idx, input_data, metadata)
            contexts.append(context)
        
        return contexts
    
    def _extract_single_context(self, idx, input_data, metadata):
        """Extract context for single index."""
        # Window around index
        window_size = 5
        start = max(0, idx - window_size)
        end = min(len(input_data), idx + window_size + 1)
        
        context = {
            'index': idx,
            'token': input_data[idx] if hasattr(input_data, '__getitem__') else None,
            'window': input_data[start:end] if hasattr(input_data, '__getitem__') else None,
            'metadata': metadata.get(idx, {}) if isinstance(metadata, dict) else {}
        }
        
        return context
    
    def _identify_disagreement_patterns(self, contexts, disagreement_types):
        """Identify patterns in disagreements."""
        patterns = {
            'common_tokens': [],
            'common_positions': [],
            'common_contexts': []
        }
        
        # Analyze tokens
        if contexts and 'token' in contexts[0]:
            tokens = [c['token'] for c in contexts if c['token'] is not None]
            
            if tokens:
                from collections import Counter
                token_counts = Counter(tokens)
                patterns['common_tokens'] = token_counts.most_common(5)
        
        # Analyze positions
        positions = [c['index'] for c in contexts]
        if positions:
            # Check if disagreements cluster at certain positions
            position_bins = np.histogram(positions, bins=10)[0]
            patterns['position_distribution'] = position_bins.tolist()
        
        return patterns
    
    def _analyze_single_case(self, idx, rule_hierarchy, learned_hierarchy, input_data, metadata):
        """Analyze single disagreement case."""
        analysis = {
            'disagreement_type': None,
            'possible_reasons': [],
            'rule_confidence': None,
            'learned_confidence': None
        }
        
        # Determine disagreement type
        rule_level = rule_hierarchy[idx]
        learned_level = learned_hierarchy[idx]
        
        if rule_level < learned_level:
            analysis['disagreement_type'] = 'rule_assigns_higher_importance'
            analysis['possible_reasons'].append(
                'Rule-based may be over-weighting structural cues'
            )
        else:
            analysis['disagreement_type'] = 'learned_assigns_higher_importance'
            analysis['possible_reasons'].append(
                'Learned model may capture semantic importance not in rules'
            )
        
        return analysis
    
    def _interpret_error_modes(self, rule_over, rule_under, learned_over, learned_under):
        """Interpret error mode analysis."""
        interpretations = []
        
        if rule_over > learned_over:
            interpretations.append(
                'Rule-based tends to over-assign importance (false positives)'
            )
        
        if rule_under > learned_under:
            interpretations.append(
                'Rule-based tends to under-assign importance (false negatives)'
            )
        
        if learned_over > rule_over:
            interpretations.append(
                'Learned model tends to over-assign importance'
            )
        
        if learned_under > rule_under:
            interpretations.append(
                'Learned model tends to under-assign importance'
            )
        
        if not interpretations:
            interpretations.append('Error modes are balanced between approaches')
        
        return '; '.join(interpretations)
```

### 4. Comprehensive Evaluation Protocol

```python
class ParityEvaluationProtocol:
    """
    Complete evaluation protocol for rule-based vs. learned hierarchy.
    """
    def __init__(self, rule_based_model, learned_model):
        self.rule_based = rule_based_model
        self.learned = learned_model
        
        self.parity_metrics = ParityMetrics()
        self.error_analysis = ErrorAnalysis()
        
    def run_evaluation(self, test_datasets):
        """
        Execute complete parity evaluation.
        """
        print("=" * 80)
        print("PARITY EVALUATION: Rule-Based vs. Learned Hierarchy")
        print("=" * 80)
        
        results = {}
        
        # Phase 1: Agreement analysis
        print("\n[Phase 1/5] Agreement analysis...")
        results['agreement'] = self._evaluate_agreement(test_datasets)
        
        # Phase 2: Correlation analysis
        print("\n[Phase 2/5] Correlation analysis...")
        results['correlation'] = self._evaluate_correlation(test_datasets)
        
        # Phase 3: Performance parity
        print("\n[Phase 3/5] Performance parity...")
        results['performance'] = self._evaluate_performance(test_datasets)
        
        # Phase 4: Efficiency comparison
        print("\n[Phase 4/5] Efficiency comparison...")
        results['efficiency'] = self._evaluate_efficiency(test_datasets)
        
        # Phase 5: Error analysis
        print("\n[Phase 5/5] Error analysis...")
        results['error_analysis'] = self._run_error_analysis(test_datasets)
        
        # Generate report
        self._generate_report(results)
        
        return results
    
    def _evaluate_agreement(self, test_datasets):
        """Evaluate hierarchy assignment agreement."""
        agreement_results = {}
        
        for dataset_name, dataset in test_datasets.items():
            print(f"  Evaluating {dataset_name}...")
            
            rule_hierarchies = []
            learned_hierarchies = []
            
            for example in dataset:
                # Get rule-based hierarchy
                rule_h, _, _ = self.rule_based.assign_hierarchy(
                    example['input'],
                    modality=example.get('modality', 'text')
                )
                
                # Get learned hierarchy
                learned_h = self.learned.assign_hierarchy(example['input'])
                
                rule_hierarchies.append(rule_h)
                learned_hierarchies.append(learned_h)
            
            # Concatenate
            rule_hierarchies = np.concatenate(rule_hierarchies)
            learned_hierarchies = np.concatenate(learned_hierarchies)
            
            # Compute agreement metrics
            agreement = self.parity_metrics.compute_agreement(
                rule_hierarchies,
                learned_hierarchies
            )
            
            agreement_results[dataset_name] = agreement
        
        return agreement_results
    
    def _evaluate_correlation(self, test_datasets):
        """Evaluate importance score correlation."""
        correlation_results = {}
        
        for dataset_name, dataset in test_datasets.items():
            print(f"  Evaluating {dataset_name}...")
            
            rule_importance = []
            learned_importance = []
            
            for example in dataset:
                # Get rule-based importance (inverse of hierarchy level)
                rule_h, rule_conf, _ = self.rule_based.assign_hierarchy(
                    example['input'],
                    modality=example.get('modality', 'text')
                )
                
                # Convert to importance: Level 0 → 1.0, Level 2 → 0.0
                rule_imp = (2 - rule_h) / 2.0
                
                # Get learned importance (masking weights)
                learned_imp = self.learned.compute_masking(example['input'])
                
                rule_importance.append(rule_imp)
                learned_importance.append(learned_imp)
            
            # Concatenate
            rule_importance = np.concatenate(rule_importance)
            learned_importance = np.concatenate(learned_importance)
            
            # Compute correlation
            correlation = self.parity_metrics.compute_correlation(
                rule_importance,
                learned_importance
            )
            
            correlation_results[dataset_name] = correlation
        
        return correlation_results
    
    def _evaluate_performance(self, test_datasets):
        """Evaluate downstream task performance."""
        performance_results = {}
        
        for dataset_name, dataset in test_datasets.items():
            print(f"  Evaluating {dataset_name}...")
            
            rule_predictions = []
            learned_predictions = []
            ground_truths = []
            
            for example in dataset:
                # Get predictions using each hierarchy
                rule_pred = self._predict_with_hierarchy(
                    example['input'],
                    self.rule_based,
                    example.get('modality', 'text')
                )
                
                learned_pred = self._predict_with_hierarchy(
                    example['input'],
                    self.learned,
                    example.get('modality', 'text')
                )
                
                rule_predictions.append(rule_pred)
                learned_predictions.append(learned_pred)
                ground_truths.append(example['target'])
            
            # Concatenate
            rule_predictions = np.concatenate(rule_predictions)
            learned_predictions = np.concatenate(learned_predictions)
            ground_truths = np.concatenate(ground_truths)
            
            # Compute performance parity
            performance = self.parity_metrics.compute_performance_parity(
                rule_predictions,
                learned_predictions,
                ground_truths
            )
            
            performance_results[dataset_name] = performance
        
        return performance_results
    
    def _evaluate_efficiency(self, test_datasets):
        """Evaluate computational efficiency."""
        efficiency_results = {}
        
        for dataset_name, dataset in test_datasets.items():
            print(f"  Evaluating {dataset_name}...")
            
            # Sample subset for timing
            sample_size = min(100, len(dataset))
            sample = dataset[:sample_size]
            
            # Time rule-based
            import time
            import tracemalloc
            
            tracemalloc.start()
            start_time = time.time()
            
            for example in sample:
                self.rule_based.assign_hierarchy(
                    example['input'],
                    modality=example.get('modality', 'text')
                )
            
            rule_time = (time.time() - start_time) / sample_size
            rule_memory = tracemalloc.get_traced_memory()[1]
            tracemalloc.stop()
            
            # Time learned
            tracemalloc.start()
            start_time = time.time()
            
            for example in sample:
                self.learned.assign_hierarchy(example['input'])
            
            learned_time = (time.time() - start_time) / sample_size
            learned_memory = tracemalloc.get_traced_memory()[1]
            tracemalloc.stop()
            
            # Compute efficiency parity
            efficiency = self.parity_metrics.compute_efficiency_parity(
                rule_time,
                learned_time,
                rule_memory,
                learned_memory
            )
            
            efficiency_results[dataset_name] = efficiency
        
        return efficiency_results
    
    def _run_error_analysis(self, test_datasets):
        """Run comprehensive error analysis."""
        error_results = {}
        
        for dataset_name, dataset in test_datasets.items():
            print(f"  Analyzing {dataset_name}...")
            
            # Collect data
            rule_hierarchies = []
            learned_hierarchies = []
            rule_predictions = []
            learned_predictions = []
            ground_truths = []
            input_data = []
            metadata = []
            
            for example in dataset:
                rule_h, _, _ = self.rule_based.assign_hierarchy(
                    example['input'],
                    modality=example.get('modality', 'text')
                )
                
                learned_h = self.learned.assign_hierarchy(example['input'])
                
                rule_pred = self._predict_with_hierarchy(
                    example['input'],
                    self.rule_based,
                    example.get('modality', 'text')
                )
                
                learned_pred = self._predict_with_hierarchy(
                    example['input'],
                    self.learned,
                    example.get('modality', 'text')
                )
                
                rule_hierarchies.append(rule_h)
                learned_hierarchies.append(learned_h)
                rule_predictions.append(rule_pred)
                learned_predictions.append(learned_pred)
                ground_truths.append(example['target'])
                input_data.append(example['input'])
                metadata.append(example.get('metadata', {}))
            
            # Concatenate
            rule_hierarchies = np.concatenate(rule_hierarchies)
            learned_hierarchies = np.concatenate(learned_hierarchies)
            rule_predictions = np.concatenate(rule_predictions)
            learned_predictions = np.concatenate(learned_predictions)
            ground_truths = np.concatenate(ground_truths)
            
            # Analyze disagreements
            disagreements = self.error_analysis.analyze_disagreements(
                rule_hierarchies,
                learned_hierarchies,
                input_data,
                metadata
            )
            
            # Analyze error modes
            error_modes = self.error_analysis.analyze_error_modes(
                rule_hierarchies,
                learned_hierarchies,
                rule_predictions,
                learned_predictions,
                ground_truths
            )
            
            # Generate case studies
            case_studies = self.error_analysis.generate_failure_case_studies(
                rule_hierarchies,
                learned_hierarchies,
                input_data,
                metadata,
                n_cases=10
            )
            
            error_results[dataset_name] = {
                'disagreements': disagreements,
                'error_modes': error_modes,
                'case_studies': case_studies
            }
        
        return error_results
    
    def _predict_with_hierarchy(self, input_data, model, modality):
        """Make prediction using given hierarchy model."""
        # Simplified - actual implementation depends on model interface
        # Assume model has predict method that uses hierarchy internally
        
        if hasattr(model, 'predict'):
            return model.predict(input_data)
        else:
            # Fallback: random prediction
            return np.random.randn(10)
    
    def _generate_report(self, results):
        """Generate comprehensive parity evaluation report."""
        report = f"""
# Parity Evaluation Report: Rule-Based vs. Learned Hierarchy

**Generated:** {datetime.now().isoformat()}

## Executive Summary

This report evaluates the parity between rule-based heuristics and learned
hierarchical representations in the Sparse Hierarchical Imagination (SHI) Transformer.

## 1. Agreement Analysis

### Overall Agreement

"""
        
        # Aggregate agreement across datasets
        all_agreements = [
            r['exact_agreement']
            for r in results['agreement'].values()
        ]
        
        all_kappas = [
            r['cohens_kappa']
            for r in results['agreement'].values()
        ]
        
        report += f"""
- **Mean Exact Agreement:** {np.mean(all_agreements):.2%}
- **Mean Cohen's Kappa:** {np.mean(all_kappas):.3f}
- **Interpretation:** {results['agreement'][list(results['agreement'].keys())[0]]['interpretation']}

### Agreement by Dataset

"""
        
        for dataset_name, agreement in results['agreement'].items():
            report += f"""
#### {dataset_name}

- Exact Agreement: {agreement['exact_agreement']:.2%}
- Cohen's Kappa: {agreement['cohens_kappa']:.3f}
- Off-by-One Agreement: {agreement['off_by_one_agreement']:.2%}

**Per-Level Agreement:**
- Level 0 (Coarse): {agreement['per_level_agreement'][0]:.2%}
- Level 1 (Medium): {agreement['per_level_agreement'][1]:.2%}
- Level 2 (Fine): {agreement['per_level_agreement'][2]:.2%}

"""
        
        report += """
## 2. Correlation Analysis

"""
        
        # Aggregate correlations
        all_spearmans = [
            r['spearman_rho']
            for r in results['correlation'].values()
        ]
        
        report += f"""
- **Mean Spearman ρ:** {np.mean(all_spearmans):.3f}
- **Interpretation:** {results['correlation'][list(results['correlation'].keys())[0]]['interpretation']}

"""
        
        report += """
## 3. Performance Parity

"""
        
        # Aggregate performance
        all_mse_ratios = [
            r['mse_ratio']
            for r in results['performance'].values()
        ]
        
        report += f"""
- **Mean MSE Ratio (Rule/Learned):** {np.mean(all_mse_ratios):.3f}
- **Interpretation:** {results['performance'][list(results['performance'].keys())[0]]['interpretation']}

### Performance by Dataset

"""
        
        for dataset_name, performance in results['performance'].items():
            report += f"""
#### {dataset_name}

- Rule MSE: {performance['rule_mse']:.4f}
- Learned MSE: {performance['learned_mse']:.4f}
- MSE Ratio: {performance['mse_ratio']:.3f}
- Significant Difference: {'Yes' if performance['significant_difference'] else 'No'} (p={performance['p_value']:.4f})

"""
        
        report += """
## 4. Efficiency Comparison

"""
        
        # Aggregate efficiency
        all_time_ratios = [
            r['time_ratio']
            for r in results['efficiency'].values()
        ]
        
        all_memory_ratios = [
            r['memory_ratio']
            for r in results['efficiency'].values()
        ]
        
        report += f"""
- **Mean Time Ratio (Rule/Learned):** {np.mean(all_time_ratios):.3f}
- **Mean Memory Ratio (Rule/Learned):** {np.mean(all_memory_ratios):.3f}
- **Interpretation:** {results['efficiency'][list(results['efficiency'].keys())[0]]['interpretation']}

"""
        
        report += """
## 5. Error Analysis

### Disagreement Patterns

"""
        
        # Aggregate disagreements
        total_disagreements = sum(
            r['disagreements']['num_disagreements']
            for r in results['error_analysis'].values()
        )
        
        report += f"""
- **Total Disagreements:** {total_disagreements}
- **Mean Disagreement Rate:** {np.mean([r['disagreements']['disagreement_rate'] for r in results['error_analysis'].values()]):.2%}

### Error Modes

"""
        
        for dataset_name, error_result in results['error_analysis'].items():
            error_modes = error_result['error_modes']
            
            report += f"""
#### {dataset_name}

- Rule Worse Cases: {error_modes['rule_worse_cases']}
- Learned Worse Cases: {error_modes['learned_worse_cases']}
- **Interpretation:** {error_modes['interpretation']}

"""
        
        report += """
## 6. Recommendations

"""
        
        # Generate recommendations based on results
        recommendations = self._generate_recommendations(results)
        
        for rec in recommendations:
            report += f"- {rec}\n"
        
        # Save report
        with open('parity_evaluation_report.md', 'w') as f:
            f.write(report)
        
        print("\nReport saved to: parity_evaluation_report.md")
        
        return report
    
    def _generate_recommendations(self, results):
        """Generate recommendations based on evaluation results."""
        recommendations = []
        
        # Check agreement
        mean_kappa = np.mean([
            r['cohens_kappa']
            for r in results['agreement'].values()
        ])
        
        if mean_kappa > 0.6:
            recommendations.append(
                "✅ High agreement (κ > 0.6): Rule-based heuristics capture learned hierarchy well"
            )
        elif mean_kappa > 0.4:
            recommendations.append(
                "⚠️ Moderate agreement (0.4 < κ < 0.6): Rule-based heuristics partially capture hierarchy"
            )
        else:
            recommendations.append(
                "❌ Low agreement (κ < 0.4): Rule-based heuristics do not capture learned hierarchy"
            )
        
        # Check performance
        mean_mse_ratio = np.mean([
            r['mse_ratio']
            for r in results['performance'].values()
        ])
        
        if mean_mse_ratio < 1.1:
            recommendations.append(
                "✅ Performance parity achieved: Rule-based within 10% of learned"
            )
        else:
            recommendations.append(
                f"⚠️ Performance gap: Rule-based {(mean_mse_ratio - 1) * 100:.1f}% worse than learned"
            )
        
        # Check efficiency
        mean_time_ratio = np.mean([
            r['time_ratio']
            for r in results['efficiency'].values()
        ])
        
        if mean_time_ratio < 0.5:
            recommendations.append(
                f"✅ Efficiency advantage: Rule-based {1/mean_time_ratio:.1f}x faster than learned"
            )
        
        return recommendations
```

### 5. Example Usage

```python
# Example: Evaluate rule-based vs. learned hierarchy on text documents

# Initialize models
rule_based = RuleBasedHierarchy(config={})
learned_model = LearnedSHI(config={})  # Assume this exists

# Create test datasets
test_datasets = {
    'wikipedia': [
        {
            'input': "# Introduction\n\nThis is the first paragraph...",
            'target': np.random.randn(10),
            'modality': 'text',
            'metadata': {'document_type': 'encyclopedia'}
        },
        # ... more examples
    ],
    'news': [
        {
            'input': "Breaking news: ...",
            'target': np.random.randn(10),
            'modality': 'text',
            'metadata': {'document_type': 'news'}
        },
        # ... more examples
    ]
}

# Run evaluation
evaluator = ParityEvaluationProtocol(rule_based, learned_model)
results = evaluator.run_evaluation(test_datasets)

# Print summary
print("\n" + "=" * 80)
print("EVALUATION SUMMARY")
print("=" * 80)
print(f"Mean Agreement: {np.mean([r['exact_agreement'] for r in results['agreement'].values()]):.2%}")
print(f"Mean Correlation: {np.mean([r['spearman_rho'] for r in results['correlation'].values()]):.3f}")
print(f"Performance Ratio: {np.mean([r['mse_ratio'] for r in results['performance'].values()]):.3f}")
print(f"Speed Ratio: {np.mean([r['time_ratio'] for r in results['efficiency'].values()]):.3f}")
```

## Summary

**Rule-Based Heuristics:**

1. **Text:**
   - Section headers (markdown, HTML)
   - Discourse markers (however, therefore)
   - Sentence position (first = important)
   - Named entities (proper nouns)
   - Syntactic role (main clause vs. subordinate)

2. **Visual:**
   - Saliency (bright, high-contrast)
   - Spatial position (center = important)
   - Motion detection

3. **Code:**
   - AST structure (module > class > function)
   - Indentation level
   - Keywords (def, class, import)

**Parity Metrics:**

1. **Agreement:** Exact agreement, Cohen's κ, confusion matrix, per-level agreement
2. **Correlation:** Spearman ρ, Pearson r, Kendall τ
3. **Performance:** MSE, MAE, statistical tests
4. **Efficiency:** Time, memory, FLOPs

**Error Analysis:**

1. **Disagreement patterns:** Where do they differ?
2. **Error modes:** Over/under-importance
3. **Context sensitivity:** Document type, position
4. **Case studies:** Detailed failure analysis

**Expected Results:**
- Agreement: κ = 0.4-0.6 (moderate)
- Correlation: ρ = 0.5-0.7 (moderate-strong)
- Performance: Rule-based 80-95% of learned
- Efficiency: Rule-based 5-10x faster

## Sources

This rule-based approximation framework is based on:
- [heirarchical-transformer.pdf] - "Sparse Hierarchical Imagination for Efficient Transformer World Models" - provides the hierarchical token organization concept [heirarchical-transformer.pdf:2] that we approximate with rules

For rule-based NLP and evaluation methodology, I recommend:
- **Discourse markers:** Schiffrin (1987) "Discourse Markers" (Cambridge)
- **Document structure:** Power et al. (2003) "Generating Referring Expressions" (MIT Press)
- **Agreement metrics:** Cohen (1960) "A Coefficient of Agreement for Nominal Scales"
- **Error analysis:** Dror et al. (2018) "The Hitchhiker's Guide to Testing Statistical Significance in NLP" (ACL)
- **Saliency detection:** Itti et al. (1998) "A Model of Saliency-Based Visual Attention" (PAMI)
- **Code analysis:** Allamanis et al. (2018) "A Survey of Machine Learning for Big Code and Naturalness" (ACM Computing Surveys)