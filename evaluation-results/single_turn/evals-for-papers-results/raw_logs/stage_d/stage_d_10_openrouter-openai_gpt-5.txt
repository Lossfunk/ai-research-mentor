Below is a focused test plan to measure whether adding a privacy/governance constraint—concretely, offline on-device inference—changes “Verification-like” failure rates for the attached Hierarchical Transformer, while keeping the original datasets and primary outcomes intact for apples-to-apples comparisons.

Anchor to the paper’s evaluation
- Keep datasets and primary outcomes unchanged: Atari 100k and Crafter; report rollout compute cost, rollout error, and final RL performance, plus the paper’s ablations (hierarchical masking, causal guidance, uncertainty masking) [heirarchical-transformer.pdf:3].
- Add privacy/governance constraints as orthogonal deployment variants; do not alter training/eval seeds, rollout depth, or retained-token budget so differences can be attributed to the constraint [heirarchical-transformer.pdf:3].

Privacy/governance constraints to test (deployment-only, unless noted; Conjecture)
- Offline on-device inference (primary): no network, no external retrieval/telemetry; post-training static int8 quantization for the world model and controller; fixed memory budget to emulate edge deployment.
- Privacy-preserving training (secondary): DP-SGD fine-tuning of the world model to a target ε (e.g., ε ∈ {3, 8} at δ=1e-5) to simulate stronger governance where required. Train/eval protocol otherwise unchanged.
- Logging governance: disable verbose logs and caching; ensure identical instrumentation across variants so measurement does not perturb behavior.

“Verification-like” failure definitions and checkers (added, non-invasive; Conjecture)
Define checks that catch internal inconsistencies or rule violations during rollouts; compute them from trajectories without influencing the model.
- Environment invariants
  - Atari: Lives ∈ Z≥0 and non-increasing; reward increments bounded (clip to known game ranges); screen pixel values in [0, 255]; action from valid set.
  - Crafter: Inventory counts ∈ Z≥0; prerequisites for crafted items must be met; health/energy ∈ [0, 1].
  - Failure if the model-predicted rollout violates any invariant; report rate per 1k steps.
- No-op and commutativity checks
  - No-op invariance (where available): repeated NOOP should keep state within a small L2 pixel delta; failure if drift exceeds threshold.
  - Commutativity of disjoint actions (Crafter-only proxies): performing two independent operations in swapped order yields the same inventory delta; failure if not equal (on sampled independent pairs).
- Self-consistency under resampling
  - From the same state, sample K model rollouts; compute disagreement on invariant-satisfying status or on next high-level predicate (e.g., “has tool X”). Failure if majority self-consistency contradicts environment or oscillates across samples.
- Aggregate Verification-like failure rate = fraction of steps (or episodes) with ≥1 violation; also report breakdown by check type.

Baselines and datasets
- Baselines (iso-architecture, seeds, retained-token budgets): (a) original hierarchical model, (b) flat transformer ablation (no hierarchy), and (c) hierarchical with uncertainty/causal ablations as in the paper [heirarchical-transformer.pdf:3].
- Constraint variants: each baseline evaluated (i) unconstrained, (ii) offline int8, and (iii) DP-SGD (secondary).
- Datasets: Atari 100k and Crafter with identical preprocessing, rollout horizons, evaluation seeds, and ablation grid as the baseline report [heirarchical-transformer.pdf:3].

Evaluation metrics (additions alongside the paper’s)
- Primary (original): rollout compute cost, rollout error (token-/image-level, error vs horizon), final RL performance [heirarchical-transformer.pdf:3].
- New: Verification-like failure rate (overall and by checker), no-op drift, self-consistency entropy; report per-game medians and IQRs.
- Efficiency context: latency, throughput, and peak memory (reported alongside compute cost; not a substitute) [heirarchical-transformer.pdf:3].

Statistical analysis plan
- Design: paired within-game comparison across seeds for each baseline vs its constrained counterpart.
- Binary failure outcomes (per episode)
  - McNemar’s test on paired episode-level failure/no-failure between unconstrained vs constrained. Report risk difference (RD) and 95% CIs via Wilson score; FDR correction across games/checks.
- Failure rates (per-step or per-episode proportions)
  - Paired Wilcoxon signed-rank on per-seed, per-game failure rates; report Cliff’s delta and BCa bootstrap 95% CIs. FDR across games/check types.
- Degradation slope under horizon
  - Mixed-effects logistic regression: Fail ~ β0 + β1·Constraint + β2·Horizon + β3·Constraint×Horizon + (1|Game) + (1|Seed). β3 tests if constrained models degrade faster with horizon. Report 95% CIs and partial R^2.
- Non-inferiority framing (pre-registered)
  - Margin: failure-rate increase ≤ +1% absolute and RL score ≤ 2% relative degradation. Use TOST for non-inferiority on both; if passed, test efficiency superiority on compute cost.
- Robustness gap summary
  - Area under failure-rate vs horizon curve (AUFHC). Compare AUFHC via paired bootstrap; CI for ΔAUFHC.

Implementation notes (to keep results comparable; Conjecture)
- Quantization: post-training static int8 for linear projections and attention matmuls (LLM.int8 or equivalent kernel); calibrate on a held-out training shard; no retraining. Keep retained-token budget, rollout depth, and context identical across conditions.
- Offline constraint: disable all network calls, retrieval, or remote logging; pin RNG seeds; keep identical data feeds and batching to avoid confounds.
- DP-SGD (optional): fine-tune the pre-trained world model with a noise multiplier achieving ε ∈ {3, 8}; keep optimizer/schedule length equal; do not change data or seeds.
- Invariant checkers: implement as offline scripts over logged trajectories; do not feed signals back into training/eval loops.

Three concrete, falsifiable experiments
1) Offline int8 vs unconstrained (primary)
- Setup: Evaluate the hierarchical baseline with and without offline int8 quantization; Atari 100k and Crafter; identical seeds, rollout depth, retained-token budget.
- Metrics: Verification-like failure rate (overall and by checker), AUFHC, rollout error vs horizon, RL performance, rollout compute cost [heirarchical-transformer.pdf:3].
- Hypotheses: Quantization increases failure rate slightly at long horizons; compute cost decreases. Conjecture.
- Pass/fail: McNemar RD CI excludes 0 for failure/no-failure or Wilcoxon p < 0.05 for failure-rate increase after FDR; or non-inferiority holds (Δfailure ≤ +1% abs) with efficiency superiority.

2) Governance-induced budget cap
- Setup: Emulate on-device memory limit by reducing retained-token budget by 25% under offline constraint; compare to unconstrained at original budget (and to a matched unconstrained run at the reduced budget to separate budget vs quantization effects).
- Metrics: Failure rate, AUFHC, RL score, compute cost [heirarchical-transformer.pdf:3].
- Hypotheses: Budget cap increases verification failures, especially no-op invariance and inventory constraints (Crafter). Conjecture.
- Pass/fail: Significant β3 < 0 (worse degradation) in mixed-effects model; or AUFHC difference CI excludes 0.

3) DP-SGD fine-tune vs unconstrained
- Setup: Fine-tune the baseline with DP-SGD to ε ∈ {3, 8}; evaluate as above.
- Metrics: Failure rates, RL score, rollout error, compute cost [heirarchical-transformer.pdf:3].
- Hypotheses: Strong privacy (smaller ε) increases failure rates and rollout error more than weak privacy; compute unchanged. Conjecture.
- Pass/fail: Monotonic trend in failure rates across ε with Cuzick’s test for trend; TOST non-inferiority at ε=8 but not at ε=3.

Reporting
- Present original metrics first (as in the paper) [heirarchical-transformer.pdf:3], then add Verification-like outcomes with CIs and corrected p-values.
- Provide per-game breakdowns and aggregate medians; include failure-type histograms and horizon curves; release invariant-check code.

Why this remains comparable
- You retain the paper’s datasets, ablation grid, primary metrics, seeds, and compute accounting; the only change is the governance constraint at deployment (and an optional DP fine-tune arm), so any differences in Verification-like failures are attributable to the constraint rather than a shift in evaluation protocol [heirarchical-transformer.pdf:3].

Literature anchors
- Abadi et al., 2016. Deep Learning with Differential Privacy. Introduces DP-SGD; use as reference for ε/δ accounting. https://arxiv.org/abs/1607.00133
- Dettmers et al., 2022. LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale. Practical post-training int8 quantization for transformer inference. https://arxiv.org/abs/2208.07339

Citations
- [heirarchical-transformer.pdf:3] 4. Hypotheses and Evaluation; the paper evaluates on Atari 100k and Crafter and reports rollout compute cost, rollout error, and final RL performance; ablations include hierarchical masking, causal guidance, and uncertainty-based masking — User-provided PDF
- Abadi et al., 2016. Deep Learning with Differential Privacy — https://arxiv.org/abs/1607.00133
- Dettmers et al., 2022. LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale — https://arxiv.org/abs/2208.07339

Key takeaways
- Evaluate offline, quantized inference and (optionally) DP-SGD as privacy/governance constraints without changing datasets, seeds, or core metrics [heirarchical-transformer.pdf:3].
- Define Verification-like failures via invariant violations, no-op/commutativity checks, and self-consistency; analyze paired gaps with McNemar/Wilcoxon and mixed-effects slopes over horizon.
- Use non-inferiority margins to determine if privacy constraints preserve reliability while reporting any efficiency gains.