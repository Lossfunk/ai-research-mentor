Below is an evaluation extension that adds efficiency metrics—latency, memory, and throughput—without changing the original accuracy-focused protocol. Where the attached paper is explicit (rollout compute cost, rollout error, final RL performance; benchmarks: Atari 100k and Crafter), I cite it. Any additional operational detail is marked Conjecture and chosen to preserve comparability while enabling robust efficiency measurement.

Scope and invariants from the original evaluation
- Keep the original primary outcomes: rollout compute cost, rollout error, and final RL performance [heirarchical-transformer.pdf:3].
- Keep the same benchmarks (Atari 100k and Crafter) and ablation framing (masking and guidance components) [heirarchical-transformer.pdf:3].
- Do not change training data, rollout depth, retained-token budget, or model capacity when adding efficiency instrumentation; measure efficiency on the same runs used for accuracy to avoid run-to-run confounds [heirarchical-transformer.pdf:3]. Conjecture.

New efficiency metrics (definitions and how to measure)
- Latency (in milliseconds; lower is better)
  - Training iteration latency: mean wall-clock time per optimization step (after warm-up).
  - Model-rollout latency: mean time per imagined step at fixed rollout depth and retained-token budget.
  - Planning latency per decision: end-to-end time from observation encoding to chosen action when planning is used. Conjecture.
  - Measurement: use synchronized GPU timers (e.g., CUDA events with torch.cuda.synchronize before/after), discard first K warm-up steps (e.g., K=50), then average over T steps (e.g., T≥500). Conjecture.

- Memory (in GB; lower is better)
  - Peak GPU memory allocated during: (a) forward+backward training iteration; (b) inference rollout at fixed depth; (c) planning step. 
  - Measurement: max of framework-reported allocator statistics (e.g., torch.cuda.max_memory_allocated/reset_peak_memory_stats). Conjecture.

- Throughput (higher is better)
  - Training throughput: tokens/sec or frames/sec processed during world-model updates.
  - Rollout throughput: imagined steps/sec or tokens/sec during planning/evaluation at fixed depth/budget.
  - Report both raw and normalized by hardware (e.g., per-GPU throughput for multi-GPU). Conjecture.

- Retain original “rollout compute cost” for continuity, and show the relationship between FLOPs and measured latency/throughput [heirarchical-transformer.pdf:3]. Conjecture on how FLOPs is computed.

Measurement protocol to preserve comparability
- Hardware/software control: fix GPU model, driver, CUDA/cuDNN, framework version, and mixed-precision setting across all runs; disclose in appendix. Conjecture.
- Workload control: keep batch size, context length, rollout depth, retained-token budget, and sequence lengths fixed when computing efficiency to avoid shape-induced variance [heirarchical-transformer.pdf:3]. Conjecture.
- Randomness and caching: use fixed seeds; enable deterministic kernels where possible; pre-warm data and compilation caches; run under exclusive GPU mode (no co-tenancy). Conjecture.
- Replicates: collect efficiency metrics over the same seeds used for accuracy and report per-seed paired results to allow paired tests. Conjecture.

Reporting
- Per-game (Atari) and per-setting (Crafter) report:
  - Accuracy: rollout error curves and final RL score, as in the paper [heirarchical-transformer.pdf:3].
  - Efficiency: median latency, peak memory, and throughput with 95% CIs across seeds.
- Aggregate across games by median-of-medians for accuracy and efficiency; include interquartile ranges. Conjecture.

Analysis plan to demonstrate statistical separation of accuracy–efficiency trade-offs
- Paired accuracy vs efficiency comparisons
  - For each game/seed pair, compute differences: ΔRL score, Δrollout error AUC, Δlatency, Δpeak memory, Δthroughput.
  - Use paired Wilcoxon signed-rank tests for central tendencies; adjust p-values with Benjamini–Hochberg across games/metrics. Report effect sizes (Cliff’s delta). Conjecture.

- Pareto-front analysis across token budgets and rollout depths
  - Sweep retained-token budgets (e.g., 10%, 20%, 40%) and possibly rollout depths while holding all else fixed; record (error, latency) and (RL score, latency) pairs.
  - Compute dominated hypervolume for each method; compare hypervolumes via bootstrap (10k resamples) with BCa CIs. A significant hypervolume gap supports separation in the joint accuracy–efficiency space. Conjecture.

- Non-inferiority/equivalence testing
  - If the goal is to claim “same accuracy, better efficiency,” run two one-sided tests (TOST) with pre-registered margins:
    - For accuracy: Δacc within ±2% relative RL score or ±2% rollout error AUC.
    - For efficiency: superiority on latency and/or memory (e.g., ≥5% lower latency). 
  - Conclude non-inferiority on accuracy and superiority on efficiency if both tests pass. Conjecture.

- Joint modeling
  - Fit an ANCOVA/regression per method: accuracy ~ α + β·log(latency). Test whether β differs between methods (interaction term) to show different efficiency sensitivities. Validate with residual checks; use robust SEs. Conjecture.

Concrete, falsifiable experiments
1) Fixed-compute evaluation of accuracy vs latency
- Design: Use the paper’s standard setup (same rollout depth and token budget) and collect per-seed RL scores, rollout error AUCs, and rollout latencies [heirarchical-transformer.pdf:3].
- Hypotheses: H1 non-inferiority on RL score within ±2%; H2 lower rollout latency by ≥5%.
- Tests: TOST for RL non-inferiority; paired Wilcoxon for latency with FDR correction. Report effect sizes and 95% CIs. Conjecture.

2) Token-budget sweep and Pareto dominance
- Design: Train/evaluate at three retained-token budgets (10%, 20%, 40%) under identical hyperparameters; record (rollout error AUC, latency) and (RL score, latency) per seed.
- Hypothesis: Method A achieves a larger dominated hypervolume in accuracy–latency space (dominates at low budgets) [heirarchical-transformer.pdf:3].
- Tests: Bootstrap hypervolume difference (10k resamples); significant positive median and CI excluding 0 indicates separation. Conjecture.

3) Memory scaling with context length
- Design: Evaluate inference at context lengths L ∈ {128, 256, 512} while keeping token budget and depth fixed; record peak memory and latency, plus rollout error AUC.
- Hypotheses: Memory scales sub-quadratically vs L if sparsity reduces activations; accuracy remains within ±2% AUC at matched budgets [heirarchical-transformer.pdf:3].
- Tests: Linear fit on log–log memory vs L; compare slopes across methods with ANCOVA; TOST on AUC equivalence within ±2%. Conjecture.

4) End-to-end planning throughput under evaluation-time constraints (optional)
- Design: Cap per-decision latency to a budget (e.g., 50 ms); measure achieved throughput and RL performance under that cap using identical planners.
- Hypothesis: At tight latency budgets, an efficiency-optimized variant preserves more RL performance [heirarchical-transformer.pdf:3].
- Tests: Paired Wilcoxon on RL score under the latency cap; report percentage of decisions exceeding the cap.

Implementation notes (to make metrics reproducible)
- Latency: Use CUDA events around the exact sections: encoder, Transformer rollout, controller/masking, decoder; report both component-level and end-to-end latencies. Synchronize before/after timing. Conjecture.
- Memory: Reset and read peak memory around the same segments; optionally cross-check with external profilers (Nsight Systems). Conjecture.
- Throughput: Count tokens or imagined steps processed over a timed window; report mean and 95% CIs across replicates. Conjecture.
- Mixed precision: Match the paper’s precision; if unspecified, fix to AMP with static loss scale and disclose. Conjecture.

Why this preserves comparability
- You retain all original accuracy metrics and datasets unchanged [heirarchical-transformer.pdf:3], add orthogonal efficiency readouts, and analyze jointly so conclusions reflect both correctness and cost.
- Using paired, per-seed measurements and fixed workloads ties efficiency differences directly to the same stochastic realizations underlying accuracy, enabling stronger causal attribution. Conjecture.

Literature anchors
- MLPerf Inference Benchmark: methodology for standardized latency/throughput measurement and reporting. https://arxiv.org/abs/1911.02549
- Efficient Transformers: A Survey (definitions and practices for measuring efficiency in attention-based models). https://arxiv.org/abs/2009.06732

Citations
- [heirarchical-transformer.pdf:3] The paper’s evaluation focuses on rollout compute cost, rollout error, final RL performance, and ablations; benchmarks include Atari 100k and Crafter; it hypothesizes improved computational efficiency and rollout stability with SHI. 

Sources
- Sparse Hierarchical Imagination for Efficient Transformer World Models — [heirarchical-transformer.pdf:3]
- Reddi et al., 2020. MLPerf Inference Benchmark — https://arxiv.org/abs/1911.02549
- Tay et al., 2020. Efficient Transformers: A Survey — https://arxiv.org/abs/2009.06732

Key takeaways
- Add latency, memory, and throughput metrics at training, rollout, and planning levels while holding workloads constant to preserve comparability with the original evaluation [heirarchical-transformer.pdf:3].
- Use paired nonparametric tests, equivalence testing, and Pareto-front hypervolume to demonstrate statistical separation in the joint accuracy–efficiency space. Conjecture.