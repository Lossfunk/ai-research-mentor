Below is a compact, evidence-grounded stress-test plan to probe robustness of the attached Hierarchical Transformer on collaborative code and long-document tasks outside its original world-model domain. I anchor the ablations and efficiency reporting to the paper’s emphasis on hierarchical masking/sparsification and compute reporting [heirarchical-transformer.pdf:2–3], and I suggest sparse-attention baselines inspired by Sparse/Routing Transformers referenced in the text [heirarchical-transformer.pdf:2].

Scope and framing
- Goal: Quantify robustness gaps—performance deltas between clean and perturbed inputs—on (a) repository-level collaborative code tasks and (b) long-document QA/summarization. 
- Architectural levers to test: level-specific sparsification and hierarchical masking, plus toggles for causal-graph guidance and uncertainty-based masking, as reported in the original work’s ablations [heirarchical-transformer.pdf:3], and sparse attention design choices [heirarchical-transformer.pdf:2].
- Report both task performance and efficiency (compute cost) as the original paper does for rollouts [heirarchical-transformer.pdf:3], adapted to inference throughput and memory in long-context settings.

Synthetic perturbations (parameterized “knobs”)
Collaborative code (multi-file, multi-turn)
- Cross-file dependency depth: Introduce call chains spanning 2–6 files; optionally interleave with interface/abstract base classes. Vary depth and fan-out.
- Distractor code: Add dead code or near-duplicate functions/classes sharing names/syntax cues across files; vary distractor-to-signal ratio.
- Identifier collisions: Reuse function/variable names across modules with different semantics; optionally shadow in nested scopes.
- Structural reordering: Shuffle module/file order, move key definitions across chunk boundaries; add circular imports guarded by try/except.
- Comment/review noise: Inject realistic code-review threads, inline comments, and commit history with conflicting guidance; vary comment density and sentiment polarity.
- Diff/patch realism: Provide PR-style unified diffs with unrelated hunks; mask some context lines or introduce line-number drift.
- Surface noise: Add formatting churn, docstring verbosity, and mixed tabs/spaces; simulate merge conflict markers.
- Length and density sweeps: Scale repository size (0.5× to 4×), file count, and tokens-per-file; create length-position sweeps for crucial references near chunk boundaries.

Long-document (QA, citation-grounded summarization, multi-hop reading)
- Distractor sections: Insert semantically similar but irrelevant passages; vary lexical overlap and TF-IDF similarity to the question.
- Boundary shifts: Move key facts to chunk starts/ends or split across chunks; insert page breaks mid-sentence.
- Cross-reference perturbations: Renumber figures/tables/equations; mismatch captions; split references/footnotes across pages.
- Structural shuffles: Reorder sections (e.g., Methods before Results), move appendices into main text.
- Paraphrase/abbreviation pressure: Replace terms with domain synonyms/abbreviations; add OCR-like character noise at controlled rates.
- Multi-hop hardness: Increase hops (2→5) and spread supporting evidence across distant sections.

Evaluation metrics
Task correctness
- Code: pass@k on generated patches (k ∈ {1,5}); unit-test success; build success; semantic diff correctness (AST-equivalent patch rate).
- Long-doc: Exact Match/F1 for extractive QA; ROUGE/BERTScore for summarization; citation grounding (precision/recall of evidence spans).
Localization and attribution
- Evidence retrieval hit rate@k; position-normalized success by chunk-relative location (begin/middle/end) to detect “lost-in-the-middle” patterns.
- Cross-chunk dependency success: proportion of questions/edits requiring information across ≥2 chunks that are solved correctly.
Efficiency and stability (aligned with “compute cost” reporting)
- Tokens processed, peak memory, latency, and throughput versus context length; max stable context length without degradation [heirarchical-transformer.pdf:3].
Robustness gap summaries
- Δ clean vs perturbed: absolute and relative drops in the above metrics.
- Area under robustness curve (AURC): integrate metric over perturbation levels.
- Length sensitivity slopes: performance change per log2 increase in context length.
Calibration (if probabilistic outputs available)
- ECE, Brier score; selective risk (accuracy at coverage q).

Concrete, falsifiable experiments
1) Cross-chunk dependency stress (code)
- Hypothesis: With level-specific sparsification and hierarchical masking, accuracy on repo-level tasks will degrade sublinearly with cross-file dependency depth compared to dense or non-hierarchical baselines [heirarchical-transformer.pdf:2–3].
- Design: Create synthetic repos where solving a task requires following a call chain of length L across files (L=1..6), with and without distractors. 
- Variables: L, distractor ratio, boundary placement (key symbol near chunk boundary vs center).
- Metrics: pass@k, test success, evidence localization hit rate; Δ vs clean; throughput and memory [heirarchical-transformer.pdf:3].
- Expected outcome: A measurable robustness slope in both models; smaller slope (less degradation) for the hierarchical variant if sparsification preserves salient global links [heirarchical-transformer.pdf:2]. If performance collapses at chunk boundaries, that indicates boundary sensitivity.

2) Boundary-shift and “lost-in-the-middle” probe (long-doc)
- Hypothesis: Moving key facts to chunk boundaries causes larger performance drops than moving them within-chunk; hierarchical masking may mitigate but not eliminate boundary effects [heirarchical-transformer.pdf:2].
- Design: For each item, create 5 variants placing evidence at positions uniformly across the context (start, early, middle, late, end), plus a split-evidence version across two adjacent chunks.
- Variables: Position, chunk size, degree of paraphrase.
- Metrics: QA EM/F1, evidence precision/recall, position-conditioned success; Δ vs clean middle-position control.
- Expected outcome: Position-dependent accuracy curve with a trough at the middle or boundary; the magnitude of the trough quantifies robustness gaps.

3) Distractor-overlap stress with sparsification (both domains)
- Hypothesis: Level-specific sparsification reduces distraction more than dense local attention by filtering irrelevant tokens at higher levels [heirarchical-transformer.pdf:2].
- Design: Inject semantically similar but irrelevant passages/functions at controlled overlap levels (cosine similarity bins).
- Variables: Overlap bin, number of distractors, sparsification/summary depth (if configurable).
- Metrics: Task accuracy, evidence precision (penalize citing distractors), AURC over overlap; compute cost [heirarchical-transformer.pdf:3].
- Expected outcome: Graceful degradation under rising distractor load for the hierarchical model relative to baselines.

4) Ablations on hierarchical components (both domains)
- Hypothesis: Removing hierarchical masking, causal-graph guidance, or uncertainty-based masking increases sensitivity to perturbations [heirarchical-transformer.pdf:3].
- Design: Evaluate the same perturbation grid with each component toggled off.
- Variables: Component on/off, perturbation type/level.
- Metrics: ΔAURC, Δ length-sensitivity slope, compute/perf trade-offs [heirarchical-transformer.pdf:3].
- Expected outcome: Identifiable contributions of each component to robustness; e.g., uncertainty-based masking helps under high distractor load.

Baselines and controls
- Sparse-attention comparators: BigBird-like or Routing Transformer-like attention patterns as motivated by the paper’s related work [heirarchical-transformer.pdf:2].
- Dense transformer with sliding-window or global tokens (matched parameters).
- Truncation control: a strong short-context model restricted to a fixed window to quantify long-context lift.
- Keep tokenization, chunk size, and context length matched across models.

Statistical analysis plan
- Design: Within-item repeated-measures. For each base item (repo or document), generate multiple perturbation levels. Randomize order. Evaluate with ≥5 seeds.
- Primary analyses:
  - Paired comparisons: Compute per-item Δ (perturbed − clean); report mean Δ with 95% BCa bootstrap CIs; use paired bootstrap for EM/F1 and pass@k.
  - Mixed-effects regression: Metric ~ Model × PerturbationLevel + Length + Position + (1|Item) + (1|Repo/Doc), with by-item random slopes for perturbation. Inspect Model×Perturbation interactions to quantify robustness gaps.
  - Non-inferiority on efficiency: Predefine margin (e.g., no more than 5% perf drop for 2× length) and test using TOST, alongside compute cost reporting [heirarchical-transformer.pdf:3].
- Multiple comparisons: Control FDR (Benjamini–Hochberg) across perturbation families.
- Power: Pilot 50–100 items to estimate Δ and variance; target 80% power to detect Δ=3–5 points, adjusting item count per domain.
- Reliability: Report seed sensitivity; include calibration metrics. Release perturbation generators and item IDs for reproducibility.
- Error taxonomy: Sample 100 failures per condition; label boundary errors, cross-chunk misses, distractor attraction, and parsing/formatting issues.

Reporting and artifacts
- Curves: Accuracy vs length; AURC by perturbation; position-conditioned accuracy; compute/perf frontiers [heirarchical-transformer.pdf:3].
- Ablation table: Component toggles vs ΔAURC and efficiency [heirarchical-transformer.pdf:3].
- Robustness cards: One-page per model summarizing gaps across code and long-doc tasks.

Why this is aligned with the model’s claims
- The plan explicitly targets failure modes where hierarchical sparsification and masking could help (global structure, cross-chunk dependencies) [heirarchical-transformer.pdf:2], while quantifying compute/performance trade-offs emphasized in the original work [heirarchical-transformer.pdf:3]. Including sparse-attention baselines is natural given the related work cited (Sparse/Routing Transformers) [heirarchical-transformer.pdf:2].

Optional datasets/tools to instantiate the plan
- Collaborative code: SWE-bench or synthetic repo harness with unit tests; evaluate pass@k patches and test outcomes.
- Long-doc: SCROLLS-style tasks or custom QA with gold evidence spans; include position sweeps and distractor injections.

Literature anchors (for context on long-context and sparse-attention baselines)
- Lost in the Middle: demonstrates position-dependent degradation in long contexts; useful for boundary-shift probes.
- BigBird: canonical sparse attention baseline for long sequences.

Key takeaways
- Use parameterized perturbations to probe cross-chunk dependencies, distractors, and boundary sensitivity.
- Measure both correctness and efficiency to surface trade-offs the original work highlights [heirarchical-transformer.pdf:3].
- Use paired analyses and mixed-effects models to quantify robustness gaps and isolate contributions of hierarchical components [heirarchical-transformer.pdf:3].

Citations
- [heirarchical-transformer.pdf:2] …“level-specific sparsification policies.” Also references Sparse Transformers and Routing Transformers as related work.
- [heirarchical-transformer.pdf:3] …“report rollout compute cost, rollout error, and final RL performance. Additional ablation studies will investigate the impact of hierarchical masking, causal graph guidance, and uncertainty-based masking…”

External literature anchors
- Lost in the Middle: How Language Models Use Long Context — https://arxiv.org/abs/2307.03172
- Big Bird: Transformers for Longer Sequences — https://arxiv.org/abs/2007.14062