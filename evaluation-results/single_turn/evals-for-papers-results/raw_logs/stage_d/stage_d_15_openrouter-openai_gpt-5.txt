Below is a concise plan to replace hand-designed hierarchy with automatically learned structure (e.g., clustering-derived segments) and evaluate stability and coverage against the paper’s approach while preserving its primary evaluation protocol and ablation framing.

Design: learned structure in place of hand-designed hierarchy
- Segment units
  - Choose atomic units to cluster: per time step (frames/latent states) for Atari/Crafter, or per sentence/paragraph for text variants.
- Representation and clustering
  - Use fixed embeddings from the world model’s intermediate features (e.g., last shared block) and run a lightweight clustering/segmentation algorithm:
    - Offline: constrained k-means or hierarchical agglomerative clustering (HAC) with a contiguity constraint to produce contiguous segments; select k by a pre-registered rule (e.g., k ≈ T/Ltarget or via silhouette/BIC on a held-out shard).
    - Online/streaming: incremental clustering with segment “stickiness” (penalty on boundary moves) to limit churn.
  - Two-level tree: top-level contiguous segments from clustering; within each, a second-level split by peak change-points (e.g., kernelized mean shift or difference in embedding velocity).
- Integration into the model
  - Replace learned hierarchy discovery with clustering-derived segments to:
    - Build attention masks: full attention within segments; cross-segment via fixed summary/memory tokens (unchanged count vs baseline to match compute).
    - Create summary tokens from segment-pooled embeddings (mean or attention-pooled). No other architectural changes.
- Controls for comparability
  - Keep parameter count, retained-token budget, rollout depth, optimizer/schedule, context length, seeds, and ablation toggles identical to the paper’s protocol to ensure apples-to-apples comparisons on rollout compute cost, rollout error, and final performance [heirarchical-transformer.pdf:3].

Evaluation steps
1) Conditions and datasets
- Datasets and protocol: same as the paper (Atari 100k and Crafter), with identical preprocessing and evaluation harness [heirarchical-transformer.pdf:3].
- Methods:
  - Baseline: paper’s hierarchy (hand-designed mechanism).
  - Learned: clustering-derived hierarchy as above.
  - Flat: no hierarchy (iso-capacity).
  - Paper’s ablations on both hierarchy variants: hierarchical masking off, causal guidance off, uncertainty masking off [heirarchical-transformer.pdf:3].

2) Primary metrics (to preserve comparability)
- Rollout compute cost: identical FLOPs accounting per rollout step/sample; report per-episode medians with CIs [heirarchical-transformer.pdf:3].
- Rollout error: teacher-forced error vs horizon curves and aggregate AUC; per-step NLL for text variants if applicable [heirarchical-transformer.pdf:3].
- Final performance: standard task score used in the paper (e.g., RL returns for Atari/Crafter) [heirarchical-transformer.pdf:3].

3) Stability metrics (added for hierarchy quality)
- Boundary churn: fraction of boundaries that move (≥1-step shift) between adjacent windows or across seeds; report churn/100 steps.
- Hierarchy consistency across seeds: Adjusted Rand Index (ARI) and Variation of Information (VI) between segmentations of the same trajectory with different random seeds.
- Edit/oscillation rate: rate of retained-token set flips at segment borders (A→B→A reversions per 1k steps).
- Horizon robustness: slope of rollout error vs horizon and its interaction with method; steeper slopes indicate less stable long-horizon reasoning.

4) Coverage metrics (added for hierarchy quality)
- Event coverage (Atari/Crafter): recall of event-bearing steps (life loss, reward spikes, inventory changes) within top-level segments or among retained tokens; precision to control over-coverage.
- State-transition coverage: recall of predicate changes (e.g., “has tool X”) captured by summary tokens within a fixed lag L.
- Saliency coverage (if text-like): NDCG@k between sentence-level saliency from retained tokens and a heuristic/human saliency reference.

5) Behavioral agreement and regressions
- Token/action agreement: top-1 agreement and symmetric KL between predictions from learned vs hand-designed hierarchy, per step.
- Regression rates: fraction of steps where the hand-designed hierarchy is correct but learned is incorrect (and vice versa) against environment ground truth; report severity weighting by future error AUC.

Statistical tests
- Paired comparisons (per game, per seed)
  - Wilcoxon signed-rank for differences in rollout compute cost, rollout error AUC, final performance, churn rate, and coverage (recall/precision); report Cliff’s delta and BCa bootstrap 95% CIs; Benjamini–Hochberg FDR across metrics/games.
- Clustering stability
  - ARI and VI differences tested via paired Wilcoxon; permutation test on ARI where distributional assumptions are dubious.
- Agreement/regressions
  - McNemar’s test for paired correctness (hand-designed vs learned) to quantify regression and anti-regression rates; report risk difference and Wilson 95% CIs.
- Horizon robustness
  - Mixed-effects regression: Error ~ β0 + β1·Method + β2·Horizon + β3·Method×Horizon + (1|Game) + (1|Seed). β3 tests if learned hierarchy changes degradation with horizon; report 95% CIs.
- Non-inferiority framing (pre-registered)
  - Margins: rollout error ≤ +2% relative; compute cost within ±2%; final performance not worse than −2% relative. Two one-sided tests (TOST). If non-inferiority holds, optionally test superiority on stability/coverage.

Error analysis (diagnostics to explain gaps)
- Over-/under-segmentation: plot segment length distributions; correlate local ΔNLL spikes with boundaries to detect harmful cuts.
- Missed events: where event coverage fails; inspect embedding trajectories to see if cluster granularity is too coarse.
- Boundary sensitivity: sensitivity analysis over k (or penalty hyperparameters) to see stability–coverage trade-offs; map to changes in churn and event recall.
- Cross-segment dependency breaks: identify cases where masking reduces necessary cross-segment attention and increases error; quantify the share of such cases.

Three concrete, falsifiable experiments
1) Non-inferiority on core metrics with stability/coverage audit
- Setup: Run hand-designed vs learned hierarchy on Atari 100k and Crafter with identical seeds, budgets, and ablations [heirarchical-transformer.pdf:3].
- Metrics: rollout compute cost, rollout error AUC, final performance; boundary churn, ARI/VI across seeds; event coverage recall/precision.
- Hypotheses: Learned hierarchy is non-inferior on rollout error and final performance; boundary churn does not exceed +0.5/100 steps; event recall within −2% of baseline (non-inferiority).
- Tests: TOST for core metrics; Wilcoxon for churn and coverage; McNemar for regressions.

2) Horizon robustness and failure localization
- Setup: Evaluate error vs horizon; log event hits/misses around boundaries; compute horizon slopes and error spikes near boundaries.
- Metrics: β3 (Method×Horizon) from mixed-effects model; proportion of ΔNLL spikes within ±d steps of boundaries; severity-weighted regressions.
- Hypotheses: Learned hierarchy does not increase degradation slope (β3 ≈ 0) and does not concentrate error spikes near boundaries beyond baseline.
- Tests: Mixed-effects inference on β3; bootstrap CI for excess spike rate at boundaries.

3) Stability under perturbations
- Setup: Re-run both methods with small input perturbations (seed changes, stochastic data order, minor observation noise); compute ARI/VI stability, churn, and downstream error changes.
- Metrics: ΔARI/ΔVI, Δchurn, Δerror AUC between perturbed and unperturbed runs.
- Hypotheses: Learned hierarchy maintains ARI ≥ 0.8 between runs and Δerror AUC within +2% relative to baseline.
- Tests: Paired Wilcoxon on stability metrics; TOST on Δerror AUC.

Reporting
- Present the paper’s primary metrics first—rollout compute cost, rollout error, final performance—then stability and coverage outcomes with CIs and FDR-adjusted p-values [heirarchical-transformer.pdf:3].
- Include per-game breakdowns and horizon-wise curves; open-source segmentation code and evaluation scripts.

Literature anchors
- Meilă, 2007. Comparing clusterings—an information based distance. Defines Variation of Information and properties for clustering stability assessment. https://doi.org/10.1016/j.jmva.2006.11.013
- Hubert and Arabie, 1985. Comparing partitions. Introduces the Adjusted Rand Index for comparing clusterings. https://doi.org/10.1007/BF01908075

Citations
- [heirarchical-transformer.pdf:3] The paper’s protocol reports rollout compute cost, rollout error, and final performance, and ablates hierarchical masking, causal guidance, and uncertainty-based masking.