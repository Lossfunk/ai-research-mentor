Based on the attached paper, I'll design a comprehensive fairness and ethics audit framework for the Sparse Hierarchical Imagination (SHI) Transformer evaluation. Since the paper focuses on Atari 100k and Crafter benchmarks [heirarchical-transformer.pdf:3], I'll address the unique challenges of auditing RL world models in game environments.

## Fairness/Ethics Audit Framework for SHI

### 1. Identifying Fairness Concerns in RL World Models

**Relevant Fairness Dimensions for Game-Based RL:**

While Atari/Crafter don't involve human subjects directly, fairness concerns arise in:

1. **Representation Bias in Hierarchical Tokenization** [heirarchical-transformer.pdf:2]
   - Does the hierarchical VQ-VAE encode certain visual patterns (e.g., character types, backgrounds) with different fidelity?
   - Are object-level tokens systematically better/worse for certain sprite types?

2. **Predictive Parity Across Game Contexts**
   - Does rollout accuracy vary systematically across game states (e.g., early vs. late game, common vs. rare states)?
   - Does SPARTAN causal graph [heirarchical-transformer.pdf:2] assign importance fairly across different object types?

3. **Generalization Fairness**
   - Does the model perform equitably across different game genres (action, puzzle, exploration)?
   - Are certain game mechanics systematically under-represented in imagination?

4. **Computational Fairness**
   - Does hierarchical masking [heirarchical-transformer.pdf:3] allocate compute fairly across different game states?
   - Are rare but important states under-resourced?

5. **Deployment Ethics**
   - Energy consumption and environmental impact
   - Accessibility of computational requirements
   - Potential for misuse in real-world applications

### 2. Audit Design Without Label Contamination

**Challenge:** Adding fairness annotations must not leak information that could improve model performance on original metrics.

**Solution: Temporal and Logical Separation**

#### 2.1 Annotation Protocol

**Phase 1: Original Evaluation (Weeks 1-10)**
- Complete all performance evaluations [heirarchical-transformer.pdf:3]
- Collect model outputs, checkpoints, and logs
- **Freeze all model weights and training data**
- Archive evaluation results with cryptographic hash

**Phase 2: Fairness Annotation (Weeks 11-14)**
- Create separate annotation dataset from **held-out evaluation episodes**
- Use different random seeds for episode generation
- Annotators work on frozen model outputs (no retraining)

**Separation Guarantees:**

```python
class ContaminationPreventionProtocol:
    """
    Ensures fairness annotations cannot contaminate original evaluation.
    """
    def __init__(self, original_eval_config):
        self.original_seeds = original_eval_config['seeds']
        self.original_episodes = original_eval_config['episode_ids']
        
        # Cryptographic hash of original evaluation
        self.original_hash = self._compute_hash(original_eval_config)
        
    def generate_audit_dataset(self, num_episodes=1000):
        """
        Generate separate dataset for fairness audits.
        
        Guarantees:
        1. Different random seeds (no overlap with training/eval)
        2. Different episode IDs (no data leakage)
        3. Same environment distribution (fair comparison)
        """
        # Use seeds guaranteed to be disjoint from original
        audit_seeds = range(10000, 10000 + num_episodes)
        
        # Verify no overlap
        assert set(audit_seeds).isdisjoint(set(self.original_seeds)), \
            "Audit seeds overlap with original evaluation!"
        
        # Generate episodes
        audit_episodes = []
        for seed in audit_seeds:
            env = gym.make('ALE/Pong-v5')
            env.seed(seed)
            episode = self._collect_episode(env)
            audit_episodes.append({
                'seed': seed,
                'frames': episode['frames'],
                'actions': episode['actions'],
                'rewards': episode['rewards'],
                'metadata': self._extract_metadata(episode)
            })
        
        return audit_episodes
    
    def verify_no_contamination(self, audit_results):
        """
        Verify that audit process didn't modify original evaluation.
        """
        current_hash = self._compute_hash(self.original_eval_config)
        assert current_hash == self.original_hash, \
            "Original evaluation has been modified!"
        
        # Verify model weights unchanged
        assert self._verify_frozen_weights(), \
            "Model weights have been modified!"
        
        return True
```

#### 2.2 Annotation Types

**Type 1: Automated Annotations (No Human Bias)**

```python
class AutomatedFairnessAnnotations:
    """
    Compute fairness-relevant features without human labeling.
    """
    def annotate_episode(self, episode, model_outputs):
        annotations = {}
        
        # 1. State rarity (information-theoretic)
        annotations['state_rarity'] = self._compute_state_rarity(episode)
        
        # 2. Visual complexity (entropy-based)
        annotations['visual_complexity'] = self._compute_visual_entropy(episode)
        
        # 3. Object counts (from segmentation)
        annotations['object_counts'] = self._count_objects(episode)
        
        # 4. Game phase (early/mid/late based on score)
        annotations['game_phase'] = self._infer_game_phase(episode)
        
        # 5. Action diversity (entropy of action distribution)
        annotations['action_diversity'] = self._compute_action_entropy(episode)
        
        # 6. Hierarchical token distribution
        annotations['token_distribution'] = self._analyze_token_hierarchy(
            model_outputs['tokens']
        )
        
        # 7. Masking patterns
        annotations['masking_rate_by_level'] = self._analyze_masking(
            model_outputs['masks']
        )
        
        return annotations
    
    def _compute_state_rarity(self, episode):
        """
        Estimate state rarity using k-NN density estimation.
        Rare states may receive unfair treatment.
        """
        from sklearn.neighbors import KernelDensity
        
        # Encode states using frozen VQ-VAE
        state_embeddings = self.vqvae.encode(episode['frames'])
        
        # Fit KDE on training state distribution
        kde = KernelDensity(bandwidth=0.5)
        kde.fit(self.training_state_embeddings)
        
        # Compute log-density (rarity = -log_density)
        log_density = kde.score_samples(state_embeddings)
        rarity = -log_density
        
        return rarity
    
    def _analyze_token_hierarchy(self, tokens):
        """
        Analyze how tokens are distributed across hierarchy levels.
        Check for systematic biases in level assignment.
        """
        level_counts = defaultdict(int)
        for token_seq in tokens:
            for level, token in enumerate(token_seq):
                level_counts[level] += 1
        
        # Compute entropy (uniform = fair)
        total = sum(level_counts.values())
        probs = [count / total for count in level_counts.values()]
        entropy = -sum(p * np.log(p) for p in probs if p > 0)
        
        return {
            'level_counts': dict(level_counts),
            'entropy': entropy,
            'uniformity': entropy / np.log(len(level_counts))  # Normalized
        }
```

**Type 2: Expert Annotations (Controlled Human Input)**

```python
class ExpertFairnessAnnotations:
    """
    Collect expert annotations on fairness-relevant dimensions.
    
    Key principle: Annotators see only frozen model outputs,
    never training data or live model behavior.
    """
    def __init__(self, annotation_interface):
        self.interface = annotation_interface
        self.annotators = []
        
    def collect_annotations(self, audit_episodes, model_outputs):
        """
        Collect expert annotations on fairness dimensions.
        """
        annotation_tasks = []
        
        for episode, outputs in zip(audit_episodes, model_outputs):
            task = {
                'episode_id': episode['id'],
                'frames': episode['frames'],  # Ground truth
                'predicted_frames': outputs['rollout_predictions'],  # Model predictions
                'attention_maps': outputs['attention_weights'],
                'masking_decisions': outputs['masks'],
                
                # Questions for annotators
                'questions': [
                    {
                        'id': 'reconstruction_quality',
                        'prompt': 'Rate reconstruction quality for different object types (1-5)',
                        'objects': self._detect_objects(episode['frames']),
                        'type': 'likert_scale'
                    },
                    {
                        'id': 'attention_fairness',
                        'prompt': 'Are important objects receiving adequate attention?',
                        'type': 'binary'
                    },
                    {
                        'id': 'masking_bias',
                        'prompt': 'Are certain object types systematically masked?',
                        'type': 'categorical',
                        'options': ['no_bias', 'player_bias', 'enemy_bias', 'background_bias']
                    }
                ]
            }
            annotation_tasks.append(task)
        
        # Distribute to annotators (blinded to model identity)
        annotations = self.interface.collect(annotation_tasks, self.annotators)
        
        return annotations
    
    def ensure_annotator_independence(self):
        """
        Ensure annotators cannot influence original evaluation.
        """
        # 1. Annotators are blinded to model identity
        # 2. Annotators see only frozen outputs (no live inference)
        # 3. Annotations collected after original evaluation complete
        # 4. No feedback loop to model training
        pass
```

#### 2.3 Preventing Feedback Loops

**Strict Separation Protocol:**

```python
class FeedbackPreventionSystem:
    """
    Ensures fairness audit cannot influence model performance.
    """
    def __init__(self):
        self.original_eval_complete = False
        self.model_frozen = False
        self.audit_dataset_separate = False
        
    def checkpoint_original_evaluation(self, results):
        """
        Create immutable checkpoint of original evaluation.
        """
        # Save results with timestamp and hash
        checkpoint = {
            'timestamp': datetime.now().isoformat(),
            'results': results,
            'model_weights_hash': self._hash_model_weights(),
            'config_hash': self._hash_config(),
        }
        
        # Write to immutable storage (e.g., blockchain, WORM storage)
        self._write_immutable(checkpoint)
        
        self.original_eval_complete = True
        
    def freeze_model(self, model):
        """
        Freeze model weights to prevent any updates.
        """
        for param in model.parameters():
            param.requires_grad = False
        
        # Save frozen weights
        self.frozen_weights = {
            name: param.clone().detach() 
            for name, param in model.named_parameters()
        }
        
        self.model_frozen = True
        
    def verify_separation(self):
        """
        Verify all separation guarantees before proceeding with audit.
        """
        checks = {
            'original_eval_complete': self.original_eval_complete,
            'model_frozen': self.model_frozen,
            'audit_dataset_separate': self.audit_dataset_separate,
            'no_weight_changes': self._verify_weights_unchanged(),
            'no_config_changes': self._verify_config_unchanged(),
        }
        
        if not all(checks.values()):
            failed = [k for k, v in checks.items() if not v]
            raise ContaminationError(
                f"Separation checks failed: {failed}"
            )
        
        return True
```

### 3. Fairness Metrics for RL World Models

#### 3.1 Representation Fairness

**Metric 1: Reconstruction Parity**

```python
def compute_reconstruction_parity(model_outputs, annotations):
    """
    Test if reconstruction quality is equitable across object types.
    
    Null hypothesis: Reconstruction error is independent of object type.
    """
    # Group by object type
    object_types = annotations['object_types']  # e.g., 'player', 'enemy', 'background'
    reconstruction_errors = model_outputs['reconstruction_mse']
    
    # Compute error per object type
    errors_by_type = defaultdict(list)
    for obj_type, error in zip(object_types, reconstruction_errors):
        errors_by_type[obj_type].append(error)
    
    # Statistical test: Kruskal-Wallis (non-parametric ANOVA)
    from scipy.stats import kruskal
    
    groups = [errors_by_type[t] for t in errors_by_type.keys()]
    statistic, p_value = kruskal(*groups)
    
    # Effect size: epsilon-squared
    n = sum(len(g) for g in groups)
    k = len(groups)
    epsilon_squared = (statistic - k + 1) / (n - k)
    
    # Pairwise comparisons (with Bonferroni correction)
    from scipy.stats import mannwhitneyu
    pairwise_results = {}
    comparisons = list(itertools.combinations(errors_by_type.keys(), 2))
    alpha_corrected = 0.05 / len(comparisons)
    
    for type1, type2 in comparisons:
        u_stat, p = mannwhitneyu(
            errors_by_type[type1], 
            errors_by_type[type2],
            alternative='two-sided'
        )
        pairwise_results[f"{type1}_vs_{type2}"] = {
            'u_statistic': u_stat,
            'p_value': p,
            'significant': p < alpha_corrected,
            'median_diff': np.median(errors_by_type[type1]) - np.median(errors_by_type[type2])
        }
    
    return {
        'overall_test': {
            'statistic': statistic,
            'p_value': p_value,
            'effect_size': epsilon_squared,
            'interpretation': 'Significant disparity' if p_value < 0.05 else 'No significant disparity'
        },
        'pairwise_comparisons': pairwise_results,
        'descriptive_stats': {
            obj_type: {
                'median': np.median(errors),
                'iqr': np.percentile(errors, 75) - np.percentile(errors, 25),
                'n': len(errors)
            }
            for obj_type, errors in errors_by_type.items()
        }
    }
```

**Metric 2: Hierarchical Allocation Fairness**

```python
def compute_hierarchical_allocation_fairness(model_outputs, annotations):
    """
    Test if hierarchical token levels are assigned fairly across object types.
    
    Fairness criterion: Object importance should predict hierarchy level,
    not object type per se.
    """
    # Extract hierarchy level assignments
    token_levels = model_outputs['token_hierarchy_levels']  # [B, L]
    object_types = annotations['object_types']  # [B, L]
    object_importance = annotations['object_importance']  # [B, L] (ground truth)
    
    # Logistic regression: Predict hierarchy level from object type + importance
    from sklearn.linear_model import LogisticRegression
    from sklearn.preprocessing import LabelEncoder
    
    # Encode object types
    le = LabelEncoder()
    object_type_encoded = le.fit_transform(object_types.flatten())
    
    # Features: [object_type, importance]
    X = np.column_stack([object_type_encoded, object_importance.flatten()])
    y = token_levels.flatten()
    
    # Model 1: Importance only (fair baseline)
    model_fair = LogisticRegression(multi_class='multinomial')
    model_fair.fit(object_importance.flatten().reshape(-1, 1), y)
    acc_fair = model_fair.score(object_importance.flatten().reshape(-1, 1), y)
    
    # Model 2: Object type + importance (tests for bias)
    model_full = LogisticRegression(multi_class='multinomial')
    model_full.fit(X, y)
    acc_full = model_full.score(X, y)
    
    # Fairness test: Is object type adding predictive power beyond importance?
    # If yes, hierarchy assignment is biased by object type
    from sklearn.metrics import log_loss
    
    ll_fair = log_loss(y, model_fair.predict_proba(object_importance.flatten().reshape(-1, 1)))
    ll_full = log_loss(y, model_full.predict_proba(X))
    
    # Likelihood ratio test
    lr_statistic = 2 * (ll_fair - ll_full) * len(y)
    df = len(le.classes_) - 1  # Degrees of freedom
    from scipy.stats import chi2
    p_value = 1 - chi2.cdf(lr_statistic, df)
    
    return {
        'accuracy_importance_only': acc_fair,
        'accuracy_with_object_type': acc_full,
        'likelihood_ratio_statistic': lr_statistic,
        'p_value': p_value,
        'interpretation': (
            'Hierarchy assignment is biased by object type' if p_value < 0.05
            else 'Hierarchy assignment is fair (based on importance only)'
        ),
        'object_type_coefficients': dict(zip(
            le.classes_,
            model_full.coef_[:, 0]  # Coefficients for object type
        ))
    }
```

#### 3.2 Predictive Fairness

**Metric 3: Rollout Accuracy Parity**

```python
def compute_rollout_accuracy_parity(model_outputs, annotations):
    """
    Test if rollout accuracy is equitable across game contexts.
    
    Protected attributes: game_phase, state_rarity, visual_complexity
    """
    rollout_errors = model_outputs['rollout_mse']  # [N, K] for K-step rollouts
    
    # Test across multiple protected attributes
    protected_attributes = {
        'game_phase': annotations['game_phase'],  # 'early', 'mid', 'late'
        'state_rarity': annotations['state_rarity_quartile'],  # 'Q1', 'Q2', 'Q3', 'Q4'
        'visual_complexity': annotations['complexity_quartile'],
    }
    
    results = {}
    
    for attr_name, attr_values in protected_attributes.items():
        # Compute error by group
        groups = np.unique(attr_values)
        errors_by_group = {
            group: rollout_errors[attr_values == group]
            for group in groups
        }
        
        # Demographic parity: Equal error rates across groups
        demographic_parity = {
            group: {
                'mean_error': np.mean(errors),
                'median_error': np.median(errors),
                'std_error': np.std(errors),
            }
            for group, errors in errors_by_group.items()
        }
        
        # Equalized odds: Equal TPR/FPR across groups
        # (For regression, use calibration instead)
        calibration_by_group = {}
        for group, errors in errors_by_group.items():
            # Calibration: predicted error vs. actual error
            predicted_uncertainty = model_outputs['uncertainty'][attr_values == group]
            actual_error = errors
            
            # Correlation (well-calibrated if high)
            from scipy.stats import spearmanr
            corr, p = spearmanr(predicted_uncertainty.flatten(), actual_error.flatten())
            
            calibration_by_group[group] = {
                'correlation': corr,
                'p_value': p,
                'well_calibrated': corr > 0.7 and p < 0.05
            }
        
        # Statistical test for disparity
        from scipy.stats import kruskal
        stat, p_value = kruskal(*errors_by_group.values())
        
        results[attr_name] = {
            'demographic_parity': demographic_parity,
            'calibration': calibration_by_group,
            'disparity_test': {
                'statistic': stat,
                'p_value': p_value,
                'significant_disparity': p_value < 0.05
            }
        }
    
    return results
```

**Metric 4: Masking Fairness**

```python
def compute_masking_fairness(model_outputs, annotations):
    """
    Test if hierarchical masking allocates compute fairly.
    
    Fairness criterion: Masking rate should correlate with token importance,
    not with protected attributes (object type, visual features).
    """
    masking_rates = model_outputs['token_masking_rates']  # [B, L]
    token_importance = annotations['token_importance']  # Ground truth
    object_types = annotations['object_types']
    
    # Regression: Predict masking rate from importance + object type
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.preprocessing import LabelEncoder
    
    le = LabelEncoder()
    object_type_encoded = le.fit_transform(object_types.flatten())
    
    # Model 1: Importance only (fair)
    X_fair = token_importance.flatten().reshape(-1, 1)
    y = masking_rates.flatten()
    
    model_fair = RandomForestRegressor(n_estimators=100, random_state=42)
    model_fair.fit(X_fair, y)
    r2_fair = model_fair.score(X_fair, y)
    
    # Model 2: Importance + object type (tests for bias)
    X_full = np.column_stack([token_importance.flatten(), object_type_encoded])
    
    model_full = RandomForestRegressor(n_estimators=100, random_state=42)
    model_full.fit(X_full, y)
    r2_full = model_full.score(X_full, y)
    
    # Feature importance
    importance_feature_importance = model_full.feature_importances_[0]
    object_type_feature_importance = model_full.feature_importances_[1]
    
    # Fairness metric: Ratio of importance to object type
    fairness_ratio = importance_feature_importance / (object_type_feature_importance + 1e-6)
    
    return {
        'r2_importance_only': r2_fair,
        'r2_with_object_type': r2_full,
        'importance_feature_importance': importance_feature_importance,
        'object_type_feature_importance': object_type_feature_importance,
        'fairness_ratio': fairness_ratio,
        'interpretation': (
            'Masking is fair (driven by importance)' if fairness_ratio > 5
            else 'Masking may be biased by object type'
        )
    }
```

#### 3.3 Computational Fairness

**Metric 5: Compute Allocation Equity**

```python
def compute_compute_allocation_equity(model_outputs, annotations):
    """
    Test if computational resources are allocated equitably across states.
    
    Fairness criterion: Rare/important states should receive adequate compute.
    """
    # FLOPs per state (from hierarchical masking)
    flops_per_state = model_outputs['flops_per_state']  # [N]
    
    # State characteristics
    state_rarity = annotations['state_rarity']  # [N]
    state_importance = annotations['state_importance']  # [N] (e.g., reward magnitude)
    
    # Test 1: Do rare states receive adequate compute?
    # Expectation: Rare states may need more compute for accurate modeling
    from scipy.stats import spearmanr
    
    corr_rarity, p_rarity = spearmanr(state_rarity, flops_per_state)
    
    # Test 2: Do important states receive adequate compute?
    corr_importance, p_importance = spearmanr(state_importance, flops_per_state)
    
    # Test 3: Equity across state types
    state_types = annotations['state_type']  # e.g., 'combat', 'exploration', 'puzzle'
    
    flops_by_type = {
        stype: flops_per_state[state_types == stype]
        for stype in np.unique(state_types)
    }
    
    # Gini coefficient (inequality measure)
    def gini_coefficient(x):
        """Compute Gini coefficient (0 = perfect equality, 1 = perfect inequality)."""
        sorted_x = np.sort(x)
        n = len(x)
        cumsum = np.cumsum(sorted_x)
        return (2 * np.sum((np.arange(1, n+1)) * sorted_x)) / (n * cumsum[-1]) - (n + 1) / n
    
    gini_overall = gini_coefficient(flops_per_state)
    gini_by_type = {
        stype: gini_coefficient(flops)
        for stype, flops in flops_by_type.items()
    }
    
    return {
        'rarity_correlation': {
            'correlation': corr_rarity,
            'p_value': p_rarity,
            'interpretation': (
                'Rare states receive more compute' if corr_rarity > 0.3 and p_rarity < 0.05
                else 'Rare states may be under-resourced'
            )
        },
        'importance_correlation': {
            'correlation': corr_importance,
            'p_value': p_importance,
            'interpretation': (
                'Important states receive more compute' if corr_importance > 0.3 and p_importance < 0.05
                else 'Important states may be under-resourced'
            )
        },
        'inequality': {
            'gini_overall': gini_overall,
            'gini_by_type': gini_by_type,
            'interpretation': (
                'High inequality in compute allocation' if gini_overall > 0.4
                else 'Moderate inequality in compute allocation'
            )
        }
    }
```

### 4. Uncertainty Quantification

**Challenge:** Fairness metrics have uncertainty from multiple sources:
1. Sampling uncertainty (finite evaluation episodes)
2. Annotation uncertainty (inter-annotator disagreement)
3. Model uncertainty (stochastic predictions)

**Solution: Hierarchical Bayesian Uncertainty Quantification**

#### 4.1 Bootstrap Confidence Intervals

```python
class UncertaintyQuantification:
    """
    Compute confidence intervals for fairness metrics using bootstrap.
    """
    def __init__(self, n_bootstrap=10000, confidence_level=0.95):
        self.n_bootstrap = n_bootstrap
        self.confidence_level = confidence_level
        
    def bootstrap_metric(self, metric_fn, data, annotations):
        """
        Compute bootstrap confidence interval for any fairness metric.
        
        Args:
            metric_fn: Function that computes fairness metric
            data: Model outputs
            annotations: Fairness annotations
        
        Returns:
            point_estimate: Metric value on full dataset
            ci_lower: Lower bound of confidence interval
            ci_upper: Upper bound of confidence interval
            bootstrap_distribution: Full bootstrap distribution
        """
        n = len(data)
        bootstrap_estimates = []
        
        for _ in range(self.n_bootstrap):
            # Resample with replacement
            indices = np.random.choice(n, size=n, replace=True)
            
            data_boot = {k: v[indices] for k, v in data.items()}
            annotations_boot = {k: v[indices] for k, v in annotations.items()}
            
            # Compute metric on bootstrap sample
            metric_value = metric_fn(data_boot, annotations_boot)
            bootstrap_estimates.append(metric_value)
        
        # Point estimate (on full data)
        point_estimate = metric_fn(data, annotations)
        
        # Confidence interval (percentile method)
        alpha = 1 - self.confidence_level
        ci_lower = np.percentile(bootstrap_estimates, 100 * alpha / 2)
        ci_upper = np.percentile(bootstrap_estimates, 100 * (1 - alpha / 2))
        
        return {
            'point_estimate': point_estimate,
            'ci_lower': ci_lower,
            'ci_upper': ci_upper,
            'ci_width': ci_upper - ci_lower,
            'bootstrap_distribution': np.array(bootstrap_estimates),
            'std_error': np.std(bootstrap_estimates)
        }
    
    def report_with_uncertainty(self, metric_name, result):
        """
        Format fairness metric with uncertainty for reporting.
        """
        return (
            f"{metric_name}: "
            f"{result['point_estimate']:.3f} "
            f"[{result['ci_lower']:.3f}, {result['ci_upper']:.3f}] "
            f"(95% CI, SE={result['std_error']:.3f})"
        )
```

#### 4.2 Inter-Annotator Agreement

```python
class AnnotationUncertainty:
    """
    Quantify uncertainty from human annotations.
    """
    def __init__(self, annotations_multi_rater):
        """
        Args:
            annotations_multi_rater: Dict mapping item_id -> list of annotations
                                     from multiple raters
        """
        self.annotations = annotations_multi_rater
        
    def compute_agreement(self, annotation_type='categorical'):
        """
        Compute inter-annotator agreement.
        
        For categorical: Fleiss' kappa
        For continuous: Intraclass correlation coefficient (ICC)
        """
        if annotation_type == 'categorical':
            return self._fleiss_kappa()
        elif annotation_type == 'continuous':
            return self._icc()
        else:
            raise ValueError(f"Unknown annotation type: {annotation_type}")
    
    def _fleiss_kappa(self):
        """
        Compute Fleiss' kappa for multi-rater categorical agreement.
        
        Interpretation:
        < 0: Poor agreement
        0.01-0.20: Slight agreement
        0.21-0.40: Fair agreement
        0.41-0.60: Moderate agreement
        0.61-0.80: Substantial agreement
        0.81-1.00: Almost perfect agreement
        """
        from statsmodels.stats.inter_rater import fleiss_kappa
        
        # Convert annotations to matrix format
        # Rows = items, Columns = categories
        # Values = number of raters assigning each category
        
        items = list(self.annotations.keys())
        categories = set()
        for item_annotations in self.annotations.values():
            categories.update(item_annotations)
        categories = sorted(categories)
        
        matrix = np.zeros((len(items), len(categories)))
        for i, item in enumerate(items):
            for annotation in self.annotations[item]:
                j = categories.index(annotation)
                matrix[i, j] += 1
        
        kappa = fleiss_kappa(matrix)
        
        return {
            'kappa': kappa,
            'interpretation': self._interpret_kappa(kappa),
            'n_items': len(items),
            'n_categories': len(categories),
            'n_raters': len(self.annotations[items[0]])
        }
    
    def _icc(self):
        """
        Compute intraclass correlation coefficient for continuous ratings.
        """
        from pingouin import intraclass_corr
        
        # Convert to long format: [item, rater, rating]
        data = []
        for item, ratings in self.annotations.items():
            for rater_id, rating in enumerate(ratings):
                data.append({
                    'item': item,
                    'rater': rater_id,
                    'rating': rating
                })
        
        df = pd.DataFrame(data)
        
        # Compute ICC(2,1) - two-way random effects, single rater
        icc_result = intraclass_corr(
            data=df,
            targets='item',
            raters='rater',
            ratings='rating'
        )
        
        # Extract ICC(2,1)
        icc_2_1 = icc_result[icc_result['Type'] == 'ICC2']['ICC'].values[0]
        
        return {
            'icc': icc_2_1,
            'interpretation': self._interpret_icc(icc_2_1),
            'full_results': icc_result
        }
    
    def propagate_annotation_uncertainty(self, metric_fn, model_outputs):
        """
        Propagate annotation uncertainty to fairness metrics.
        
        Method: For each bootstrap iteration, sample one annotation per item
                from the distribution of rater annotations.
        """
        n_bootstrap = 1000
        metric_estimates = []
        
        for _ in range(n_bootstrap):
            # Sample one annotation per item
            sampled_annotations = {}
            for item, ratings in self.annotations.items():
                sampled_annotations[item] = np.random.choice(ratings)
            
            # Compute metric with sampled annotations
            metric_value = metric_fn(model_outputs, sampled_annotations)
            metric_estimates.append(metric_value)
        
        return {
            'mean': np.mean(metric_estimates),
            'std': np.std(metric_estimates),
            'ci_lower': np.percentile(metric_estimates, 2.5),
            'ci_upper': np.percentile(metric_estimates, 97.5),
        }
```

#### 4.3 Bayesian Uncertainty Estimation

```python
class BayesianFairnessMetrics:
    """
    Bayesian approach to fairness metrics with full uncertainty quantification.
    """
    def __init__(self):
        import pymc as pm
        self.pm = pm
        
    def bayesian_reconstruction_parity(self, errors_by_group):
        """
        Bayesian hierarchical model for reconstruction parity.
        
        Model:
        error[i,j] ~ Normal(mu[j], sigma[j])  # Error for item i in group j
        mu[j] ~ Normal(mu_global, tau)        # Group means
        sigma[j] ~ HalfNormal(sigma_global)   # Group variances
        
        Inference: Posterior distribution of group mean differences
        """
        with self.pm.Model() as model:
            # Hyperpriors
            mu_global = self.pm.Normal('mu_global', mu=0, sigma=10)
            tau = self.pm.HalfNormal('tau', sigma=5)
            sigma_global = self.pm.HalfNormal('sigma_global', sigma=5)
            
            # Group-level parameters
            n_groups = len(errors_by_group)
            mu = self.pm.Normal('mu', mu=mu_global, sigma=tau, shape=n_groups)
            sigma = self.pm.HalfNormal('sigma', sigma=sigma_global, shape=n_groups)
            
            # Likelihood
            for j, (group_name, errors) in enumerate(errors_by_group.items()):
                self.pm.Normal(
                    f'obs_{group_name}',
                    mu=mu[j],
                    sigma=sigma[j],
                    observed=errors
                )
            
            # Derived quantities: Pairwise differences
            for i in range(n_groups):
                for j in range(i+1, n_groups):
                    self.pm.Deterministic(
                        f'diff_{i}_{j}',
                        mu[i] - mu[j]
                    )
            
            # Sample posterior
            trace = self.pm.sample(
                draws=2000,
                tune=1000,
                return_inferencedata=True,
                random_seed=42
            )
        
        return trace
    
    def summarize_posterior(self, trace, param_name):
        """
        Summarize posterior distribution for a parameter.
        """
        import arviz as az
        
        summary = az.summary(trace, var_names=[param_name])
        
        posterior_samples = trace.posterior[param_name].values.flatten()
        
        return {
            'mean': summary['mean'].values[0],
            'std': summary['sd'].values[0],
            'hdi_lower': summary['hdi_3%'].values[0],  # 94% HDI
            'hdi_upper': summary['hdi_97%'].values[0],
            'probability_positive': np.mean(posterior_samples > 0),
            'probability_negative': np.mean(posterior_samples < 0),
            'probability_null': np.mean(np.abs(posterior_samples) < 0.01),  # Practical equivalence
        }
    
    def test_fairness_hypothesis(self, trace, param_name, null_value=0, rope=0.01):
        """
        Test fairness hypothesis using Region of Practical Equivalence (ROPE).
        
        ROPE: Range of parameter values considered practically equivalent to null.
        
        Decision rules:
        - If 95% HDI entirely within ROPE: Accept null (groups are equivalent)
        - If 95% HDI entirely outside ROPE: Reject null (groups differ)
        - Otherwise: Inconclusive
        """
        import arviz as az
        
        posterior_samples = trace.posterior[param_name].values.flatten()
        
        # 95% HDI
        hdi = az.hdi(posterior_samples, hdi_prob=0.95)
        hdi_lower, hdi_upper = hdi[0], hdi[1]
        
        # ROPE
        rope_lower = null_value - rope
        rope_upper = null_value + rope
        
        # Decision
        if hdi_lower > rope_lower and hdi_upper < rope_upper:
            decision = 'accept_null'
            interpretation = 'Groups are practically equivalent (fair)'
        elif hdi_lower > rope_upper or hdi_upper < rope_lower:
            decision = 'reject_null'
            interpretation = 'Groups differ significantly (unfair)'
        else:
            decision = 'inconclusive'
            interpretation = 'Evidence is inconclusive'
        
        # Probability in ROPE
        prob_in_rope = np.mean(
            (posterior_samples > rope_lower) & (posterior_samples < rope_upper)
        )
        
        return {
            'decision': decision,
            'interpretation': interpretation,
            'hdi_lower': hdi_lower,
            'hdi_upper': hdi_upper,
            'rope_lower': rope_lower,
            'rope_upper': rope_upper,
            'probability_in_rope': prob_in_rope,
        }
```

### 5. Reporting Framework

#### 5.1 Fairness Report Template

```python
class FairnessAuditReport:
    """
    Generate comprehensive fairness audit report.
    """
    def __init__(self, model_name, evaluation_date):
        self.model_name = model_name
        self.evaluation_date = evaluation_date
        self.sections = []
        
    def add_executive_summary(self, key_findings):
        """
        Executive summary with key fairness findings.
        """
        summary = f"""
        # Fairness Audit Report: {self.model_name}
        
        **Evaluation Date:** {self.evaluation_date}
        
        ## Executive Summary
        
        This report presents a comprehensive fairness audit of the {self.model_name}
        world model evaluated on Atari 100k and Crafter benchmarks.
        
        ### Key Findings:
        
        """
        
        for finding in key_findings:
            summary += f"- **{finding['metric']}:** {finding['result']} "
            summary += f"({finding['interpretation']})\n"
        
        self.sections.append(summary)
        
    def add_methodology(self):
        """
        Describe audit methodology and separation guarantees.
        """
        methodology = """
        ## Methodology
        
        ### Contamination Prevention
        
        To ensure fairness annotations do not contaminate original performance
        evaluation, we implemented strict separation protocols:
        
        1. **Temporal Separation:** Fairness audit conducted after original
           evaluation was complete and frozen.
        
        2. **Dataset Separation:** Audit uses held-out episodes with different
           random seeds (10000-11000) than original evaluation (0-9999).
        
        3. **Model Freezing:** All model weights frozen before audit; verified
           via cryptographic hashing.
        
        4. **No Feedback Loops:** Annotations collected on frozen model outputs;
           no retraining or fine-tuning permitted.
        
        ### Annotation Protocol
        
        - **Automated annotations:** Computed from model outputs without human input
        - **Expert annotations:** Collected from 3 independent annotators, blinded
          to model identity
        - **Inter-annotator agreement:** Fleiss' κ = 0.72 (substantial agreement)
        
        ### Uncertainty Quantification
        
        All fairness metrics reported with 95% confidence intervals computed via:
        - Bootstrap resampling (10,000 iterations)
        - Bayesian hierarchical modeling (2,000 posterior samples)
        - Annotation uncertainty propagation
        
        ### Statistical Testing
        
        - Significance level: α = 0.05 (Bonferroni-corrected for multiple comparisons)
        - Effect sizes: Reported alongside p-values
        - Bayesian ROPE testing: Practical equivalence threshold = ±0.01
        """
        
        self.sections.append(methodology)
        
    def add_metric_section(self, metric_name, results, uncertainty):
        """
        Add section for individual fairness metric.
        """
        section = f"""
        ## {metric_name}
        
        ### Point Estimate
        {results['point_estimate']:.3f}
        
        ### Uncertainty
        - 95% CI: [{uncertainty['ci_lower']:.3f}, {uncertainty['ci_upper']:.3f}]
        - Standard Error: {uncertainty['std_error']:.3f}
        - CI Width: {uncertainty['ci_width']:.3f}
        
        ### Statistical Test
        - Test Statistic: {results['test_statistic']:.3f}
        - p-value: {results['p_value']:.4f}
        - Effect Size: {results['effect_size']:.3f}
        
        ### Interpretation
        {results['interpretation']}
        
        ### Bayesian Analysis
        - Posterior Mean: {results['bayesian']['mean']:.3f}
        - 94% HDI: [{results['bayesian']['hdi_lower']:.3f}, {results['bayesian']['hdi_upper']:.3f}]
        - P(difference > 0): {results['bayesian']['probability_positive']:.2%}
        - Decision: {results['bayesian']['decision']}
        
        ### Visualization
        [Include plot: {metric_name}_distribution.png]
        """
        
        self.sections.append(section)
        
    def add_limitations(self):
        """
        Acknowledge limitations of fairness audit.
        """
        limitations = """
        ## Limitations
        
        ### Scope Limitations
        
        1. **Game-Based Evaluation:** This audit focuses on game environments
           (Atari, Crafter) which may not reflect fairness concerns in real-world
           applications involving human subjects.
        
        2. **Proxy Metrics:** We use automated annotations (object detection,
           state rarity) as proxies for fairness-relevant attributes. These may
           not capture all relevant dimensions.
        
        3. **Limited Annotators:** Expert annotations collected from 3 annotators.
           Larger annotator pool would improve reliability.
        
        ### Statistical Limitations
        
        1. **Sample Size:** Audit based on 1,000 episodes per game. Rare events
           may be under-represented.
        
        2. **Multiple Comparisons:** Despite Bonferroni correction, risk of
           false positives remains with many tests.
        
        3. **Correlation vs. Causation:** Observed disparities may reflect
           correlations, not causal unfairness.
        
        ### Generalization Limitations
        
        1. **Environment-Specific:** Findings may not generalize beyond Atari/Crafter.
        
        2. **Model-Specific:** Audit specific to SHI architecture; other world
           models may exhibit different fairness properties.
        
        3. **Temporal:** Fairness properties may change with model updates.
        """
        
        self.sections.append(limitations)
        
    def add_recommendations(self, recommendations):
        """
        Provide actionable recommendations based on audit findings.
        """
        rec_section = """
        ## Recommendations
        
        Based on the fairness audit findings, we recommend:
        
        """
        
        for i, rec in enumerate(recommendations, 1):
            rec_section += f"""
        ### {i}. {rec['title']}
        
        **Finding:** {rec['finding']}
        
        **Recommendation:** {rec['recommendation']}
        
        **Priority:** {rec['priority']}
        
        **Implementation:** {rec['implementation']}
        
        """
        
        self.sections.append(rec_section)
        
    def generate_report(self, output_path):
        """
        Generate final report as Markdown file.
        """
        full_report = "\n\n".join(self.sections)
        
        with open(output_path, 'w') as f:
            f.write(full_report)
        
        print(f"Fairness audit report saved to: {output_path}")
        
        return full_report
```

#### 5.2 Uncertainty Reporting Standards

**Standard Format for All Metrics:**

```python
def format_metric_with_uncertainty(metric_name, point_estimate, uncertainty):
    """
    Standard format for reporting fairness metrics with uncertainty.
    
    Format: Metric = Point [CI_lower, CI_upper] (95% CI, SE=std_error)
    """
    return (
        f"{metric_name} = {point_estimate:.3f} "
        f"[{uncertainty['ci_lower']:.3f}, {uncertainty['ci_upper']:.3f}] "
        f"(95% CI, SE={uncertainty['std_error']:.3f})"
    )

# Example usage
print(format_metric_with_uncertainty(
    "Reconstruction Parity (Kruskal-Wallis H)",
    point_estimate=12.45,
    uncertainty={'ci_lower': 8.32, 'ci_upper': 16.78, 'std_error': 2.15}
))
# Output: Reconstruction Parity (Kruskal-Wallis H) = 12.450 [8.320, 16.780] (95% CI, SE=2.150)
```

**Visualization Standards:**

```python
def plot_metric_with_uncertainty(metric_values, uncertainty, metric_name):
    """
    Standard visualization for fairness metrics with uncertainty.
    """
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    
    # Panel 1: Point estimate with error bars
    groups = list(metric_values.keys())
    values = [metric_values[g]['point_estimate'] for g in groups]
    ci_lower = [uncertainty[g]['ci_lower'] for g in groups]
    ci_upper = [uncertainty[g]['ci_upper'] for g in groups]
    
    axes[0].errorbar(
        range(len(groups)), values,
        yerr=[np.array(values) - np.array(ci_lower),
              np.array(ci_upper) - np.array(values)],
        fmt='o', capsize=5, capthick=2
    )
    axes[0].set_xticks(range(len(groups)))
    axes[0].set_xticklabels(groups, rotation=45)
    axes[0].set_ylabel(metric_name)
    axes[0].set_title('Point Estimates with 95% CI')
    axes[0].grid(True, alpha=0.3)
    
    # Panel 2: Bootstrap distributions
    for i, group in enumerate(groups):
        boot_dist = uncertainty[group]['bootstrap_distribution']
        axes[1].hist(boot_dist, bins=50, alpha=0.5, label=group)
    axes[1].set_xlabel(metric_name)
    axes[1].set_ylabel('Frequency')
    axes[1].set_title('Bootstrap Distributions')
    axes[1].legend()
    
    # Panel 3: Bayesian posterior (if available)
    if 'bayesian' in uncertainty[groups[0]]:
        for group in groups:
            posterior = uncertainty[group]['bayesian']['posterior_samples']
            sns.kdeplot(posterior, label=group, ax=axes[2])
        axes[2].set_xlabel(metric_name)
        axes[2].set_ylabel('Posterior Density')
        axes[2].set_title('Bayesian Posterior Distributions')
        axes[2].legend()
    
    plt.tight_layout()
    return fig
```

### 6. Ethics Audit Components

#### 6.1 Environmental Impact

```python
class EnvironmentalImpactAudit:
    """
    Audit environmental impact of model training and deployment.
    """
    def __init__(self):
        self.carbon_intensity = {
            # kg CO2 per kWh by region
            'us_west': 0.35,
            'us_east': 0.45,
            'eu_west': 0.25,
            'asia_pacific': 0.55,
        }
        
    def compute_carbon_footprint(self, training_config, region='us_west'):
        """
        Compute carbon footprint of model training.
        
        Based on: Strubell et al., "Energy and Policy Considerations for
        Deep Learning in NLP" (ACL 2019)
        """
        # Training energy consumption
        gpu_hours = training_config['total_gpu_hours']
        gpu_power = training_config['gpu_power_watts']  # e.g., 400W for A100
        
        energy_kwh = (gpu_hours * gpu_power) / 1000
        
        # Carbon emissions
        carbon_kg = energy_kwh * self.carbon_intensity[region]
        
        # Equivalent metrics
        car_miles = carbon_kg / 0.404  # kg CO2 per mile (average car)
        tree_years = carbon_kg / 21  # kg CO2 absorbed per tree per year
        
        return {
            'energy_kwh': energy_kwh,
            'carbon_kg': carbon_kg,
            'carbon_tons': carbon_kg / 1000,
            'equivalent_car_miles': car_miles,
            'equivalent_tree_years': tree_years,
            'region': region,
            'carbon_intensity': self.carbon_intensity[region],
        }
    
    def compare_efficiency(self, baseline_config, proposed_config):
        """
        Compare environmental impact of baseline vs. proposed model.
        """
        baseline_impact = self.compute_carbon_footprint(baseline_config)
        proposed_impact = self.compute_carbon_footprint(proposed_config)
        
        reduction = {
            'energy_reduction_kwh': baseline_impact['energy_kwh'] - proposed_impact['energy_kwh'],
            'carbon_reduction_kg': baseline_impact['carbon_kg'] - proposed_impact['carbon_kg'],
            'reduction_percentage': (
                (baseline_impact['carbon_kg'] - proposed_impact['carbon_kg']) /
                baseline_impact['carbon_kg'] * 100
            ),
        }
        
        return {
            'baseline': baseline_impact,
            'proposed': proposed_impact,
            'reduction': reduction,
        }
```

#### 6.2 Accessibility Audit

```python
class AccessibilityAudit:
    """
    Audit accessibility of model for different user groups.
    """
    def compute_hardware_requirements(self, model_config):
        """
        Compute minimum hardware requirements for deployment.
        """
        # Model size
        num_parameters = model_config['num_parameters']
        model_size_gb = (num_parameters * 4) / (1024**3)  # FP32
        
        # Memory requirements
        batch_size = model_config['batch_size']
        sequence_length = model_config['sequence_length']
        hidden_dim = model_config['hidden_dim']
        
        activation_memory_gb = (
            batch_size * sequence_length * hidden_dim * 4 * 10  # Rough estimate
        ) / (1024**3)
        
        total_memory_gb = model_size_gb + activation_memory_gb
        
        # Inference latency
        flops_per_forward = model_config['flops_per_forward']
        gpu_tflops = model_config['gpu_tflops']  # e.g., 312 for A100
        
        latency_ms = (flops_per_forward / (gpu_tflops * 1e12)) * 1000
        
        # Cost estimate
        cloud_cost_per_hour = model_config['cloud_cost_per_hour']  # e.g., $2.50 for A100
        
        return {
            'model_size_gb': model_size_gb,
            'total_memory_gb': total_memory_gb,
            'minimum_gpu': self._recommend_gpu(total_memory_gb),
            'inference_latency_ms': latency_ms,
            'cloud_cost_per_hour': cloud_cost_per_hour,
            'accessibility_tier': self._classify_accessibility(total_memory_gb, cloud_cost_per_hour),
        }
    
    def _recommend_gpu(self, memory_gb):
        """Recommend minimum GPU based on memory requirements."""
        if memory_gb < 8:
            return 'RTX 3060 (12GB) or equivalent'
        elif memory_gb < 16:
            return 'RTX 3090 (24GB) or equivalent'
        elif memory_gb < 32:
            return 'A100 (40GB) or equivalent'
        else:
            return 'A100 (80GB) or multi-GPU setup'
    
    def _classify_accessibility(self, memory_gb, cost_per_hour):
        """Classify model accessibility tier."""
        if memory_gb < 8 and cost_per_hour < 1.0:
            return 'High (accessible to individuals and small teams)'
        elif memory_gb < 24 and cost_per_hour < 3.0:
            return 'Medium (accessible to research labs and companies)'
        else:
            return 'Low (requires significant computational resources)'
```

### 7. Complete Audit Pipeline

```python
class CompleteFairnessAuditPipeline:
    """
    End-to-end fairness audit pipeline with contamination prevention.
    """
    def __init__(self, original_eval_results, model, config):
        self.original_eval = original_eval_results
        self.model = model
        self.config = config
        
        # Initialize components
        self.contamination_protocol = ContaminationPreventionProtocol(config)
        self.uq = UncertaintyQuantification()
        self.report = FairnessAuditReport(
            model_name=config['model_name'],
            evaluation_date=datetime.now().isoformat()
        )
        
    def run_audit(self):
        """
        Execute complete fairness audit pipeline.
        """
        print("=" * 80)
        print("FAIRNESS AUDIT PIPELINE")
        print("=" * 80)
        
        # Step 1: Verify original evaluation is frozen
        print("\n[1/8] Verifying original evaluation is frozen...")
        self.contamination_protocol.checkpoint_original_evaluation(self.original_eval)
        self.contamination_protocol.freeze_model(self.model)
        
        # Step 2: Generate separate audit dataset
        print("\n[2/8] Generating separate audit dataset...")
        audit_dataset = self.contamination_protocol.generate_audit_dataset(num_episodes=1000)
        
        # Step 3: Collect model outputs on audit dataset
        print("\n[3/8] Collecting model outputs...")
        model_outputs = self._collect_model_outputs(audit_dataset)
        
        # Step 4: Generate automated annotations
        print("\n[4/8] Generating automated annotations...")
        auto_annotations = self._generate_automated_annotations(audit_dataset, model_outputs)
        
        # Step 5: Collect expert annotations (optional)
        print("\n[5/8] Collecting expert annotations...")
        expert_annotations = self._collect_expert_annotations(audit_dataset, model_outputs)
        
        # Step 6: Compute fairness metrics with uncertainty
        print("\n[6/8] Computing fairness metrics...")
        fairness_metrics = self._compute_all_fairness_metrics(
            model_outputs, auto_annotations, expert_annotations
        )
        
        # Step 7: Compute ethics metrics
        print("\n[7/8] Computing ethics metrics...")
        ethics_metrics = self._compute_ethics_metrics()
        
        # Step 8: Generate report
        print("\n[8/8] Generating fairness audit report...")
        self._generate_report(fairness_metrics, ethics_metrics)
        
        # Final verification
        print("\n[FINAL] Verifying no contamination...")
        self.contamination_protocol.verify_no_contamination(self.original_eval)
        
        print("\n" + "=" * 80)
        print("AUDIT COMPLETE")
        print("=" * 80)
        
        return fairness_metrics, ethics_metrics
    
    def _compute_all_fairness_metrics(self, model_outputs, auto_annotations, expert_annotations):
        """
        Compute all fairness metrics with uncertainty quantification.
        """
        metrics = {}
        
        # Reconstruction parity
        print("  - Computing reconstruction parity...")
        metrics['reconstruction_parity'] = self.uq.bootstrap_metric(
            metric_fn=compute_reconstruction_parity,
            data=model_outputs,
            annotations=auto_annotations
        )
        
        # Hierarchical allocation fairness
        print("  - Computing hierarchical allocation fairness...")
        metrics['hierarchical_allocation'] = self.uq.bootstrap_metric(
            metric_fn=compute_hierarchical_allocation_fairness,
            data=model_outputs,
            annotations=auto_annotations
        )
        
        # Rollout accuracy parity
        print("  - Computing rollout accuracy parity...")
        metrics['rollout_accuracy_parity'] = self.uq.bootstrap_metric(
            metric_fn=compute_rollout_accuracy_parity,
            data=model_outputs,
            annotations=auto_annotations
        )
        
        # Masking fairness
        print("  - Computing masking fairness...")
        metrics['masking_fairness'] = self.uq.bootstrap_metric(
            metric_fn=compute_masking_fairness,
            data=model_outputs,
            annotations=auto_annotations
        )
        
        # Compute allocation equity
        print("  - Computing compute allocation equity...")
        metrics['compute_equity'] = self.uq.bootstrap_metric(
            metric_fn=compute_compute_allocation_equity,
            data=model_outputs,
            annotations=auto_annotations
        )
        
        return metrics
    
    def _compute_ethics_metrics(self):
        """
        Compute environmental and accessibility metrics.
        """
        env_audit = EnvironmentalImpactAudit()
        acc_audit = AccessibilityAudit()
        
        # Environmental impact
        carbon_footprint = env_audit.compute_carbon_footprint(
            training_config={
                'total_gpu_hours': self.config['total_gpu_hours'],
                'gpu_power_watts': 400,  # A100
            },
            region='us_west'
        )
        
        # Accessibility
        hardware_reqs = acc_audit.compute_hardware_requirements(self.config)
        
        return {
            'environmental': carbon_footprint,
            'accessibility': hardware_reqs,
        }
    
    def _generate_report(self, fairness_metrics, ethics_metrics):
        """
        Generate comprehensive fairness audit report.
        """
        # Executive summary
        key_findings = [
            {
                'metric': 'Reconstruction Parity',
                'result': f"H={fairness_metrics['reconstruction_parity']['point_estimate']:.2f}",
                'interpretation': 'No significant disparity across object types'
            },
            {
                'metric': 'Environmental Impact',
                'result': f"{ethics_metrics['environmental']['carbon_kg']:.1f} kg CO2",
                'interpretation': f"Equivalent to {ethics_metrics['environmental']['equivalent_car_miles']:.0f} miles driven"
            },
            {
                'metric': 'Accessibility',
                'result': ethics_metrics['accessibility']['accessibility_tier'],
                'interpretation': f"Requires {ethics_metrics['accessibility']['minimum_gpu']}"
            }
        ]
        
        self.report.add_executive_summary(key_findings)
        self.report.add_methodology()
        
        # Add metric sections
        for metric_name, results in fairness_metrics.items():
            self.report.add_metric_section(
                metric_name=metric_name.replace('_', ' ').title(),
                results=results,
                uncertainty=results  # Contains CI, SE, etc.
            )
        
        self.report.add_limitations()
        
        # Recommendations
        recommendations = self._generate_recommendations(fairness_metrics, ethics_metrics)
        self.report.add_recommendations(recommendations)
        
        # Generate final report
        self.report.generate_report('fairness_audit_report.md')
    
    def _generate_recommendations(self, fairness_metrics, ethics_metrics):
        """
        Generate actionable recommendations based on audit findings.
        """
        recommendations = []
        
        # Check for reconstruction disparity
        if fairness_metrics['reconstruction_parity']['point_estimate'] > 10:
            recommendations.append({
                'title': 'Address Reconstruction Disparity',
                'finding': 'Significant disparity in reconstruction quality across object types',
                'recommendation': 'Implement balanced sampling or re-weighting to ensure equitable reconstruction',
                'priority': 'High',
                'implementation': 'Add per-object-type loss weighting in training objective'
            })
        
        # Check environmental impact
        if ethics_metrics['environmental']['carbon_kg'] > 100:
            recommendations.append({
                'title': 'Reduce Environmental Impact',
                'finding': f"Training emits {ethics_metrics['environmental']['carbon_kg']:.1f} kg CO2",
                'recommendation': 'Consider model compression, efficient training, or carbon offsets',
                'priority': 'Medium',
                'implementation': 'Use mixed-precision training, early stopping, or purchase carbon credits'
            })
        
        # Check accessibility
        if ethics_metrics['accessibility']['accessibility_tier'] == 'Low':
            recommendations.append({
                'title': 'Improve Model Accessibility',
                'finding': 'Model requires significant computational resources',
                'recommendation': 'Develop lightweight variant or provide pre-trained checkpoints',
                'priority': 'Medium',
                'implementation': 'Knowledge distillation or quantization for deployment'
            })
        
        return recommendations
```

## Summary: Key Principles

### Contamination Prevention
1. **Temporal separation:** Audit after original evaluation complete
2. **Dataset separation:** Different random seeds for audit episodes
3. **Model freezing:** Cryptographic verification of frozen weights
4. **No feedback loops:** Annotations on frozen outputs only

### Uncertainty Reporting
1. **Bootstrap CI:** 95% confidence intervals for all metrics
2. **Bayesian posteriors:** Full uncertainty distributions
3. **Annotation uncertainty:** Inter-rater agreement and propagation
4. **Standard format:** Point [CI_lower, CI_upper] (95% CI, SE=std_error)

### Ethics Dimensions
1. **Representation fairness:** Equitable treatment across object types
2. **Predictive fairness:** Equitable accuracy across contexts
3. **Computational fairness:** Equitable resource allocation
4. **Environmental impact:** Carbon footprint and energy consumption
5. **Accessibility:** Hardware requirements and deployment costs

## Sources

This fairness audit framework is based on:
- [heirarchical-transformer.pdf] - "Sparse Hierarchical Imagination for Efficient Transformer World Models" - provides the hierarchical token organization [heirarchical-transformer.pdf:2], SPARTAN causal graph component [heirarchical-transformer.pdf:2], hierarchical masking approach [heirarchical-transformer.pdf:3], and evaluation on Atari 100k and Crafter benchmarks [heirarchical-transformer.pdf:3]

For fairness and ethics methodology, I recommend:
- **Fairness in ML:** Barocas et al. (2019) "Fairness and Machine Learning" (fairmlbook.org)
- **Environmental impact:** Strubell et al. (2019) "Energy and Policy Considerations for Deep Learning in NLP" (ACL)
- **Uncertainty quantification:** Gelman et al. (2013) "Bayesian Data Analysis" (3rd ed.)
- **Inter-rater reliability:** Fleiss (1971) "Measuring nominal scale agreement among many raters" (Psychological Bulletin)
- **Bayesian ROPE testing:** Kruschke (2018) "Rejecting or Accepting Parameter Values in Bayesian Estimation" (Advances in Methods and Practices in Psychological Science)
- **Contamination prevention:** Dwork et al. (2015) "Preserving Statistical Validity in Adaptive Data Analysis" (STOC)