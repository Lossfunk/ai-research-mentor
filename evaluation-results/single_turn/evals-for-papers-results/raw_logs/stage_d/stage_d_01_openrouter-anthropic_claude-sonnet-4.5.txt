Based on my analysis of the attached paper, here's a comprehensive breakdown of the key components, requirements, and risks for reproducing the Sparse Hierarchical Imagination (SHI) training and evaluation pipeline:

## Key Components

### 1. Data Preprocessing [heirarchical-transformer.pdf:2]

**Observation Encoding:**
- Visual observations `xt` are encoded into sequences of discrete latent tokens via a **hierarchical VQ-VAE or token clustering procedure** [heirarchical-transformer.pdf:2]
- Each observation is mapped to a set of latent tokens: `zt = [z¹t, z²t, ..., zᴷt]` [heirarchical-transformer.pdf:2]
- Tokens are organized into **semantic hierarchical levels** (e.g., object-level vs. background tokens) [heirarchical-transformer.pdf:2]

**Input Pipeline:**
- Agent interacts with environment producing visual observations `xt` and actions `at` [heirarchical-transformer.pdf:2]
- Observations are first encoded before being processed by the Transformer

### 2. Hierarchical Architecture Blocks

**Core Components:**

1. **Hierarchical VQ-VAE/Token Encoder** [heirarchical-transformer.pdf:2]
   - Converts raw observations to discrete token sequences
   - Organizes tokens into semantic levels

2. **Transformer World Model** [heirarchical-transformer.pdf:2]
   - Processes token sequences for long-horizon imagination in latent space
   - Maintains hidden state across rollout steps
   - Trained jointly or in alternating phases with SPARTAN [heirarchical-transformer.pdf:2]

3. **SPARTAN Component** [heirarchical-transformer.pdf:2]
   - Learns **causal graphs over object tokens** [heirarchical-transformer.pdf:2]
   - Provides dynamic information about which tokens influence future dynamics
   - Informs masking decisions during imagination [heirarchical-transformer.pdf:2]

4. **Sparsification Mechanisms:**
   - **Level-specific temporal masking** applied during rollout [heirarchical-transformer.pdf:3]
   - **Hierarchical masking** policies [heirarchical-transformer.pdf:3]
   - **Causal graph guidance** from SPARTAN [heirarchical-transformer.pdf:3]
   - **Uncertainty-based masking** [heirarchical-transformer.pdf:3]
   - Memory token mechanism to maintain salient tokens in current rollout context [heirarchical-transformer.pdf:3]

### 3. Training Schedule & Objectives

**Training Approach:**
- **Joint or alternating training** of Transformer and SPARTAN components [heirarchical-transformer.pdf:2]

**Loss Functions:**
- **Image reconstruction loss** applied selectively at subset of rollout steps (checkpoints at t ∈ {t₀, t₅, t₁₀}) [heirarchical-transformer.pdf:3]
- This selective reconstruction encourages semantic consistency while avoiding unnecessary visual detail modeling at intermediate steps [heirarchical-transformer.pdf:3]

**Rollout Pipeline:** [heirarchical-transformer.pdf:3]
1. Given initial observation x₀, encode to z₀
2. Initialize Transformer's hidden state
3. At each rollout step t, apply level-specific temporal masking
4. Generate predictions for future states

## Comparability Requirements

### Baseline Comparisons [heirarchical-transformer.pdf:3]
The paper plans experiments comparing SHI against:
- **DART** [Agarwal et al., 2024]
- **IRIS** [Micheli et al., 2023]
- **STORM** [Zhang et al., 2023]
- **TransDreamer** [Chen et al., 2022]
- **Robine et al. [2023]**

### Evaluation Benchmarks [heirarchical-transformer.pdf:3]

**Primary Metrics:**
1. **Atari 100k benchmark**
   - Final median human-normalized score [heirarchical-transformer.pdf:3]
   
2. **Crafter benchmark**
   - Final median human-normalized score [heirarchical-transformer.pdf:3]

**Performance Metrics to Report:** [heirarchical-transformer.pdf:3]
- Rollout compute cost
- Rollout error
- Final RL performance
- Sample efficiency during policy learning

**Ablation Studies Required:** [heirarchical-transformer.pdf:3]
- Impact of hierarchical masking
- Impact of causal graph guidance
- Impact of uncertainty-based masking

### Critical Comparability Factors

To ensure fair comparison:

1. **Same encoder architecture** - Use identical VQ-VAE/tokenization as baselines (especially IRIS, which this method draws inspiration from)
2. **Identical evaluation protocol** - Use standard Atari 100k (100,000 environment steps) and Crafter protocols
3. **Same random seeds** - Report median scores across multiple seeds
4. **Consistent preprocessing** - Match observation preprocessing (frame stacking, resolution, normalization)
5. **Equivalent model capacity** - Control for total parameter count when comparing efficiency gains

## Leakage Risks Between Pretraining and Evaluation

### Potential Data Leakage Concerns

**1. Benchmark-Specific Tuning**
- **Risk:** Hyperparameters (masking schedules, reconstruction checkpoint times {t₀, t₅, t₁₀}) may be tuned specifically for Atari 100k/Crafter
- **Mitigation:** Document all hyperparameter search procedures; use separate validation environments

**2. Token Hierarchy Design**
- **Risk:** Manual design of semantic levels (object vs. background) may encode domain knowledge specific to evaluation environments
- **Mitigation:** Use unsupervised clustering or learned hierarchies; test on held-out environment families

**3. Reconstruction Checkpoint Selection**
- **Risk:** The specific timesteps {t₀, t₅, t₁₀} for reconstruction loss may be optimized for typical Atari/Crafter episode dynamics
- **Mitigation:** Justify selection based on general principles; test sensitivity to different checkpoint schedules

**4. SPARTAN Causal Graph Pretraining**
- **Risk:** If SPARTAN is pretrained on similar visual domains before fine-tuning on evaluation tasks
- **Mitigation:** Train from scratch on each benchmark; clearly document any transfer learning

**5. Baseline Implementation Choices**
- **Risk:** Using suboptimal baseline implementations while optimizing SHI extensively
- **Mitigation:** Use official baseline codebases (IRIS, DART); match training budgets; report baseline reproduction results

**6. Evaluation Data Contamination**
- **Risk:** Using evaluation environment observations during VQ-VAE training or token clustering
- **Mitigation:** Strictly separate training data collection from evaluation episodes; use fresh environment seeds for evaluation

**7. Human-Normalized Score Calculation**
- **Risk:** Inconsistent human/random baseline scores across papers
- **Mitigation:** Use standardized benchmarks with published baselines; report raw scores alongside normalized scores

### Recommended Safeguards

1. **Pre-register experimental protocol** before running final evaluations
2. **Use separate validation set** for hyperparameter tuning (different games/tasks)
3. **Report all attempted configurations**, not just best results
4. **Provide ablation removing each novel component** to isolate contributions
5. **Test on additional benchmarks** beyond those used during development (e.g., DMControl, if developed on Atari)
6. **Document computational budget** for both method and baselines
7. **Release code and checkpoints** for independent verification

## Missing Implementation Details

The paper appears to be a research proposal rather than a completed study. Critical details not yet specified:

- Specific VQ-VAE architecture and codebook size
- Transformer depth, width, attention heads
- Learning rates, optimizers, batch sizes
- Total training steps/environment interactions
- Exact SPARTAN architecture and training procedure
- Precise masking schedule formulas
- Hardware requirements and wall-clock time

You'll need to either:
1. Wait for the full paper with implementation details
2. Contact authors for clarification
3. Make principled design choices based on cited baselines (especially IRIS [Micheli et al., 2023])

## Sources

This analysis is based entirely on the attached paper:
- [heirarchical-transformer.pdf] - "Sparse Hierarchical Imagination for Efficient Transformer World Models"

For reproduction, I recommend obtaining the referenced baseline papers (IRIS, DART, STORM) and their official implementations to ensure comparability.