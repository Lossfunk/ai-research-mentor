Below is a focused ablation plan to sweep local attention window sizes in your Hierarchical Transformer (Sparse Hierarchical Imagination), with a concrete benchmark suite, minimal code changes, compute budgeting, and failure modes to watch. I align metrics with your evaluation guidance to report rollout compute cost, rollout error, and final RL performance [heirarchical-transformer.pdf:3]. I also leverage the model’s level-specific sparsification to structure per-level window sweeps efficiently [heirarchical-transformer.pdf:2].

Benchmark suite
- Long-context synthetic and language tasks
  - Long Range Arena (LRA): listops, text, retrieval, pathfinder, image; stress-tests locality vs long-range and is standard for efficient attention [Tay et al., 2021]. Use at least text, retrieval, and pathfinder to probe different locality regimes.
- World-model rollouts (to match your evaluation)
  - Choose 1–2 lightweight RL environments for rollout error vs compute (e.g., a small control suite or simplified Atari subset) so you can report rollout compute cost, rollout error, and final RL return as in your plan [heirarchical-transformer.pdf:3]. Keep task difficulty modest to finish sweeps quickly.
- Optional long-document QA/summarization
  - One SCROLLS-style long-doc task or a simple “needle-in-a-haystack” retrieval to specifically detect long-context degradation; keep to ≤1 dataset to stay within budget.

Minimal code changes
- Expose per-level local windows
  - Add a list window_size_per_level = [w_L0, w_L1, …] in the model config. Ensure the default reproduces current behavior (backward-compatible).
- Sliding-window mask
  - In the attention module, replace full mask with a banded mask for each token i over [i−w/2, i+w/2]. If you use chunking, align window stride to chunk size; add a small overlap (e.g., 1/4 window) to mitigate boundary artifacts.
- Global tokens
  - Preserve global attention for the memory token and any summary tokens so they can connect across windows, which your method relies on for retaining salience in rollouts [heirarchical-transformer.pdf:3].
- Relative position bias/clipping
  - Clip or bucket relative positions to the local window span to avoid mismatches between mask and bias.
- Per-level windows
  - Because higher levels are sparser, allow larger windows at upper levels (lower token count amortizes cost) [heirarchical-transformer.pdf:2]. Keep level-specific sparsification unchanged to isolate the effect of windows.

Compute budget and sweep design
- Window grid
  - Start with three configs per level: Small, Medium, Large. Example 3-level model: 
    - S: [64, 64, 64]
    - M: [128, 64, 64]
    - L: [256, 128, 64]
  - Optionally add an “XL” only at top level: [128, 128, 256] to test long-range propagation through sparse higher levels [heirarchical-transformer.pdf:2].
- Seeds and steps
  - 3 seeds per config; 50k–100k steps for LRA tasks; for RL rollouts, run to a fixed token budget or wall-clock cap and report curves (compute vs error vs return) as your evaluation notes [heirarchical-transformer.pdf:3].
- Hardware/time (estimate; scale as needed)
  - For a ~150M-parameter model at 4K tokens: local attention cost per layer scales ~O(n·w). Doubling w roughly doubles attention FLOPs and memory per layer.
  - Expect per-config training wall-clock on 1×A100 40GB: S ≈ 6–8 h; M ≈ 10–14 h; L ≈ 16–24 h, 3 seeds → 1–3 GPU-days per config. Adjust sequence length/Batch to fit VRAM; profile first 1k steps to set batch size.
- Report efficiency
  - Log tokens/s, peak VRAM, and per-batch FLOPs alongside rollout error and final return to align with your plan [heirarchical-transformer.pdf:3]. Also track attention span histograms to confirm the effective receptive field matches configured windows.

Failure modes to monitor
- Long-context degradation and boundary effects
  - Performance drop as context length increases, and errors concentrated near window boundaries when evidence spans >w. Overlapping windows or larger top-level windows can mitigate.
- Cross-level propagation gaps
  - If upper levels use too small windows, global information may not propagate, harming multi-step imagination; inspect memory-token attention patterns [heirarchical-transformer.pdf:3].
- Over-sparsification interactions
  - Level-specific sparsification plus small windows can prune away necessary tokens; monitor token retention statistics and rollout error spikes [heirarchical-transformer.pdf:2–3].
- Context-length extrapolation
  - If trained at w and evaluated at longer sequences, check if performance drops disproportionately; include a 2×-context stress test.
- Instability under curriculum
  - If you try window schedules (growing w during training), watch for loss spikes when w changes; consider brief LR warmup on each increase.

Three concrete, falsifiable experiments
1) Fixed-level window sweep under constant compute
- Hypothesis: Increasing the base-level window improves LRA retrieval and lowers rollout error up to a point, with diminishing returns beyond M; gains reflected in final RL return [heirarchical-transformer.pdf:3].
- Variables: w_L0 ∈ {64, 128, 256}; hold w_L1=L2=64 fixed; adjust batch size to keep total FLOPs per step constant across settings.
- Metrics: LRA retrieval accuracy; rollout error vs tokens; final RL return; tokens/s and peak VRAM [heirarchical-transformer.pdf:3].
- Expected outcome: 128 > 64; 256 marginal; compute scales roughly with w and memory grows linearly in w.
- Risks/controls: Same random seeds across configs; early-stop by validation plateau; profile FLOPs vs window.

2) Per-level window allocation with fixed cost
- Hypothesis: Allocating larger windows to upper (sparser) levels yields better long-context generalization than concentrating capacity at the base, at similar compute due to fewer tokens at higher levels [heirarchical-transformer.pdf:2].
- Variables: Compare A: [256, 64, 64] vs B: [128, 128, 128] vs C: [128, 128, 256]; match step-time via batch tuning.
- Metrics: LRA pathfinder accuracy; long-document retrieval at 2× training context; rollout error at long horizons; attention diagnostics of memory token [heirarchical-transformer.pdf:3].
- Expected outcome: C ≥ B ≥ A on long-context metrics at comparable compute; boundary errors reduced when top-level window is larger.

3) Boundary stress test with overlapping vs non-overlapping windows
- Hypothesis: Modest window overlap (e.g., 25%) reduces boundary-induced errors with minimal compute overhead.
- Variables: Overlap ∈ {0%, 25%}; fixed w_L0=128; inject test sequences where key evidence straddles window edges; include LRA pathfinder and synthetic “needle across boundary” probes.
- Metrics: Accuracy on edge-straddling probes; change in tokens/s; rollout error spikes near boundaries [heirarchical-transformer.pdf:3].
- Expected outcome: Overlap improves edge-straddling accuracy and smooths rollout error; modest throughput drop.

Practical notes
- Start with a pilot on the smallest grid (S/M) to calibrate VRAM and tokens/s; then scale.
- Keep global attention for the memory token enabled in all configs to preserve salience tracking during imagination [heirarchical-transformer.pdf:3].
- Document any changes to the sparsification policy; ideally hold it constant so effects are attributable to window size [heirarchical-transformer.pdf:2].

Literature anchors
- Simple Local Attentions Remain Competitive for Long-Context Tasks: shows that carefully chosen local windows can match more complex methods on long-context benchmarks, motivating window-size sweeps [P1].
- Long Range Arena: widely-used long-context benchmark; includes diverse tasks that stress window locality and long-range reasoning (Tay et al., 2021).

Citations
- [heirarchical-transformer.pdf:2] Method; level-specific sparsification policies; joint/alternating training with SPARTAN — attached PDF.
- [heirarchical-transformer.pdf:3] Hypotheses and Evaluation; report rollout compute cost, rollout error, and final RL performance; memory token for salience; imagination — attached PDF.
- [P1] Simple Local Attentions Remain Competitive for Long-Context Tasks — http://arxiv.org/abs/2112.07210v2
- [LRA] Long Range Arena: A Benchmark for Efficient Transformers — https://arxiv.org/abs/2011.04006

Sources
- Attached: heirarchical-transformer.pdf (Sparse Hierarchical Imagination), pages 2–3.
- Tay et al., 2021. Long Range Arena: A Benchmark for Efficient Transformers. https://arxiv.org/abs/2011.04006
- Zaheer et al., 2021; Tay et al., 2021; related efficient attention literature; specifically used: Katharopoulos et al. 2021 “Simple Local Attentions Remain Competitive for Long-Context Tasks.” http://arxiv.org/abs/2112.07210v2 [P1]