## Discussion Section: Multilingual Generalization

Here's a comprehensive discussion paragraph with concrete extension plans.

---

## Main Text Discussion Section

```markdown
### X.X Limitation: English-Only Scope and Multilingual Extension

**Current Scope.** Our work is limited to English-language papers submitted 
to English-language venues (NIPS, ICLR). This represents a significant 
limitation for several reasons:

1. **Geographic bias**: English-only analysis excludes substantial research 
   communities in non-English-speaking regions (China, Europe, Latin America, 
   Middle East)

2. **Linguistic features**: Our readability metrics (Flesch-Kincaid, etc.) 
   and stylistic features are English-specific and may not transfer to other 
   languages

3. **Cultural review norms**: Peer review standards and writing conventions 
   vary across linguistic and cultural contexts

4. **Practical applicability**: Our models cannot assist non-English venues 
   or multilingual conferences without adaptation

**Why This Matters.** The global research community is increasingly 
multilingual. Major conferences now accept submissions in multiple languages 
(e.g., ACL accepts Chinese, Spanish, Arabic papers; EMNLP has multilingual 
tracks), and regional venues publish primarily in local languages. A truly 
generalizable peer review prediction system must handle linguistic diversity.

**Challenges for Multilingual Extension.** Extending our work to multilingual 
settings faces several technical and practical challenges:

**1. Data Availability**
- Few non-English venues publicly release reviews and decisions
- Existing multilingual review datasets are small (e.g., <100 papers)
- Privacy and copyright restrictions vary by country
- Annotation costs are higher for non-English languages

**2. Feature Engineering**
- Readability metrics require language-specific formulas (e.g., Flesch-
  Kincaid assumes English syllable patterns)
- Discourse markers and hedging patterns differ across languages
- Citation practices vary (e.g., Chinese papers cite more domestic work)
- Structural conventions differ (e.g., European papers often have longer 
  introductions)

**3. Cross-Lingual Transfer**
- Multilingual embeddings (mBERT, XLM-R) may not capture quality signals 
  equally across languages
- Translation-based approaches introduce noise and may lose subtle quality 
  cues
- Zero-shot transfer to low-resource languages is largely untested

**4. Cultural and Normative Differences**
- Review criteria may emphasize different aspects (e.g., theoretical rigor 
  vs. practical impact)
- Acceptance rates vary widely (e.g., 15% for top Chinese venues vs. 25% 
  for NIPS)
- Writing style expectations differ (e.g., directness vs. hedging)

**Concrete Multilingual Extension Plan.** We propose a phased approach to 
multilingual extension:

### Phase 1: Multilingual Data Collection (Months 1-6)

**Target languages**: Chinese, Spanish, German, French, Japanese (covering 
~70% of non-English CS research)

**Data sources**:
1. **Chinese venues**: CCF-A conferences (e.g., CNCC, CCDM) via collaboration 
   with Chinese institutions
   - Target: 500 papers with reviews from 2020-2023
   - Status: Preliminary contact established with Tsinghua University

2. **European venues**: ECML, ECAI, regional workshops
   - Target: 300 papers in German, French, Spanish
   - Status: Exploring partnerships with European research groups

3. **Multilingual tracks**: ACL multilingual submissions, EMNLP non-English 
   papers
   - Target: 200 papers across multiple languages
   - Status: Data publicly available via ACL Anthology

4. **Translation-augmented data**: Translate PeerRead papers to target 
   languages, collect reviews from native speakers
   - Target: 100 papers × 5 languages = 500 papers
   - Status: Pilot with 20 papers in progress

**Expected timeline**: 6 months for data collection and annotation  
**Expected cost**: $10,000-15,000 (translation, annotation, partnerships)  
**Deliverable**: Multilingual PeerRead dataset with 1,500+ papers

### Phase 2: Language-Specific Feature Engineering (Months 7-9)

**Readability metrics**:
- **Chinese**: Adapt metrics for character-based writing (e.g., character 
  frequency, stroke count complexity)
- **Spanish/French**: Adjust for Romance language patterns (e.g., longer 
  words, different syllable structure)
- **German**: Account for compound words and case system
- **Japanese**: Handle mixed scripts (kanji, hiragana, katakana)

**Linguistic features**:
- Develop language-specific discourse marker lexicons
- Identify hedging patterns per language (e.g., "可能" in Chinese, "quizás" 
  in Spanish)
- Extract language-appropriate stylistic features

**Universal features** (language-agnostic):
- Paper length (pages, words)
- Structural features (sections, figures, tables, equations)
- Citation count and patterns
- Author metadata (if available)

**Implementation approach**:
```python
# Language-specific feature extractors
feature_extractors = {
    'en': EnglishFeatureExtractor(),
    'zh': ChineseFeatureExtractor(),
    'es': SpanishFeatureExtractor(),
    'de': GermanFeatureExtractor(),
    'fr': FrenchFeatureExtractor(),
    'ja': JapaneseFeatureExtractor()
}

# Extract features based on language
def extract_features(paper, language):
    # Universal features (all languages)
    universal = extract_universal_features(paper)
    
    # Language-specific features
    specific = feature_extractors[language].extract(paper)
    
    return {**universal, **specific}
```

**Validation**: Compare feature distributions across languages to identify 
language-specific vs. universal patterns

### Phase 3: Cross-Lingual Model Development (Months 10-12)

**Approach 1: Multilingual Embeddings**
- Fine-tune mBERT or XLM-RoBERTa on multilingual review prediction
- Test zero-shot transfer (train on English, test on other languages)
- Measure performance degradation vs. language-specific models

**Approach 2: Translation-Based**
- Translate all papers to English using neural MT (e.g., Google Translate, 
  DeepL)
- Apply English-trained models to translations
- Compare to native-language models

**Approach 3: Language-Agnostic Features**
- Train models using only universal features (structure, citations, metadata)
- Test if these features generalize across languages
- Combine with language-specific features for hybrid approach

**Approach 4: Multi-Task Learning**
- Train single model on all languages simultaneously
- Use language ID as auxiliary task
- Share representations across languages while allowing language-specific 
  parameters

**Evaluation protocol**:
```python
# Test each approach
approaches = ['mBERT', 'translation', 'universal_features', 'multitask']
languages = ['en', 'zh', 'es', 'de', 'fr', 'ja']

results = {}
for approach in approaches:
    for train_lang in languages:
        for test_lang in languages:
            model = train_model(approach, train_lang)
            f1 = evaluate(model, test_data[test_lang])
            results[approach][train_lang][test_lang] = f1

# Analyze transfer patterns
analyze_transfer_matrix(results)
```

### Phase 4: Cultural and Normative Analysis (Months 13-15)

**Research questions**:
1. Do review criteria differ across linguistic/cultural contexts?
2. Which aspects (clarity, novelty, soundness) are universal vs. culture-
   specific?
3. Do acceptance thresholds vary by language/region?
4. Are there systematic biases (e.g., favoring domestic vs. international 
   work)?

**Analysis plan**:

**RQ1: Review criteria differences**
- Compare feature importance across languages
- Identify language-specific predictive features
- Conduct qualitative analysis of review text

| Language | Top Predictive Features | Interpretation |
|----------|------------------------|----------------|
| English | Novelty, clarity, rigor | Emphasis on innovation |
| Chinese | Practical impact, thoroughness | Emphasis on application |
| German | Theoretical soundness, formalism | Emphasis on rigor |
| Spanish | Clarity, accessibility | Emphasis on communication |

**RQ2: Universal vs. culture-specific aspects**
- Train aspect-specific models per language
- Measure cross-lingual transfer for each aspect
- Identify aspects with high vs. low transfer

Expected finding: Clarity and soundness transfer well (universal); novelty 
and significance transfer poorly (culture-specific)

**RQ3: Acceptance threshold variation**
- Compare acceptance rates across languages/venues
- Analyze score distributions
- Test if models trained on one language's threshold generalize

**RQ4: Systematic biases**
- Compare ratings for domestic vs. international authors
- Test for language-of-submission effects (e.g., non-native English)
- Identify potential discrimination patterns

### Phase 5: Multilingual Model Release (Months 16-18)

**Deliverables**:

1. **Multilingual PeerRead Dataset**
   - 1,500+ papers in 6 languages
   - Reviews and decisions
   - Language-specific and universal features
   - Metadata (venue, year, topic, author info)
   - License: CC BY-NC 4.0

2. **Multilingual Models**
   - Language-specific models (one per language)
   - Multilingual model (single model for all languages)
   - Translation-based model (English model + MT)
   - Universal feature model (language-agnostic)

3. **Code and Tools**
   - Language-specific feature extractors
   - Multilingual embedding pipelines
   - Evaluation scripts
   - Demo interface supporting all languages

4. **Analysis Report**
   - Cross-lingual transfer analysis
   - Cultural differences in review criteria
   - Bias detection results
   - Recommendations for multilingual deployment

5. **Community Resources**
   - Annotation guidelines in all target languages
   - Multilingual review rubrics
   - Best practices for cross-lingual peer review prediction

**Expected Outcomes and Metrics**

| Metric | Target | Interpretation |
|--------|--------|----------------|
| **Within-language F1** | ≥0.70 | Comparable to English performance |
| **Zero-shot transfer F1** | ≥0.60 | Reasonable cross-lingual generalization |
| **Universal feature F1** | ≥0.65 | Language-agnostic features are informative |
| **Translation-based F1** | ≥0.68 | MT preserves quality signals |
| **Cross-lingual correlation** | ρ≥0.70 | Review criteria are largely universal |

**Validation Strategy**

To ensure multilingual models are robust and fair:

1. **Native speaker validation**: Have native speakers review model 
   predictions and feature importance for face validity

2. **Back-translation test**: Translate papers to English and back; measure 
   prediction stability

3. **Fairness audit**: Test for bias against non-native submissions, 
   underrepresented regions, or specific languages

4. **Expert review**: Consult with researchers from target language 
   communities to validate findings

5. **Longitudinal tracking**: Monitor performance over time as languages 
   and review norms evolve

**Resource Requirements**

| Resource | Estimated Cost | Notes |
|----------|---------------|-------|
| Data collection | $5,000-8,000 | Partnerships, access fees |
| Translation | $3,000-5,000 | Professional translation for validation |
| Annotation | $2,000-3,000 | Native speaker annotations |
| Computation | $1,000-2,000 | Cloud GPUs for multilingual models |
| Personnel | 1 FTE × 18 months | Researcher + collaborators |
| **Total** | **$11,000-18,000** | Excluding personnel |

**Collaboration Strategy**

Given the scope and resource requirements, we propose a collaborative 
approach:

1. **Academic partnerships**: Partner with institutions in target language 
   regions
   - China: Tsinghua, Peking University, MSRA
   - Europe: DFKI (Germany), INRIA (France), BSC (Spain)
   - Japan: NII, RIKEN

2. **Funding**: Apply for grants supporting multilingual NLP research
   - NSF International Research Collaboration
   - EU Horizon grants
   - Industry partnerships (Google, Microsoft, Baidu)

3. **Community engagement**: Open call for contributions
   - Invite researchers to contribute data from their venues
   - Crowdsource annotation via volunteer networks
   - Host shared task at multilingual NLP workshop

4. **Phased release**: Release data and models incrementally
   - Phase 1: English + Chinese (largest non-English community)
   - Phase 2: Add Spanish, German, French
   - Phase 3: Add Japanese and other languages

**Commitment and Timeline**

We commit to:
- **6 months**: Pilot study with Chinese venues (100 papers)
- **12 months**: Release English + Chinese dataset and models
- **18 months**: Full multilingual release (6 languages)
- **Ongoing**: Expand to additional languages based on community interest

**Interim Contributions**

While working toward full multilingual extension, we will:

1. **Document English-specific assumptions**: Clearly state which features 
   and findings are English-specific vs. potentially universal

2. **Provide translation-based baseline**: Release models that work on 
   machine-translated papers as interim solution

3. **Collaborate with multilingual researchers**: Actively seek partnerships 
   and welcome contributions

4. **Share methodology**: Publish detailed protocols enabling others to 
   replicate our approach for their languages

**Limitations and Risks**

We acknowledge several risks:

1. **Data access**: Non-English venues may be reluctant to share review data
   - Mitigation: Build trust through partnerships and data sharing agreements

2. **Quality variation**: Review quality may vary more across languages than 
   within English
   - Mitigation: Conduct thorough quality analysis and report limitations

3. **Resource constraints**: Full multilingual extension requires substantial 
   resources
   - Mitigation: Phased approach, seek funding, engage community

4. **Cultural sensitivity**: Analyzing review practices across cultures 
   requires care
   - Mitigation: Involve native researchers, avoid value judgments

5. **Maintenance burden**: Supporting multiple languages increases complexity
   - Mitigation: Modular design, community contributions, clear documentation

**Conclusion**

While our current work is limited to English, we have outlined a concrete, 
feasible plan for multilingual extension. This plan balances ambition with 
pragmatism: we target high-impact languages first, leverage existing 
multilingual NLP tools, and adopt a collaborative approach to resource 
constraints. We believe that multilingual peer review prediction is both 
important and achievable, and we commit to making progress on this frontier.

We invite the research community to join us in this effort. Researchers with 
access to non-English review data, expertise in specific languages, or 
interest in cross-cultural analysis are encouraged to contact us at 
[email/website].
```

---

## Supplementary Material: Pilot Study Design

```markdown
## Appendix Y: Multilingual Pilot Study Protocol

To validate the feasibility of our multilingual extension plan, we propose 
a focused pilot study.

### Pilot Scope

**Target**: 100 Chinese papers from CCF-A conferences (2020-2023)
**Timeline**: 3 months
**Budget**: $2,000-3,000
**Goal**: Demonstrate feasibility and identify challenges

### Data Collection Protocol

**Source venues**:
- CNCC (China National Computer Congress)
- CCDM (China Conference on Data Mining)
- Regional AI conferences

**Data to collect**:
- Paper PDFs (Chinese language)
- Peer reviews (Chinese language)
- Acceptance decisions
- Metadata (authors, institutions, topics)

**Collection method**:
- Partner with Chinese university (e.g., Tsinghua)
- Obtain IRB approval and data sharing agreement
- Anonymize author information
- Translate subset for validation

### Feature Engineering

**Chinese-specific readability metrics**:

| Metric | Formula | Interpretation |
|--------|---------|----------------|
| Character frequency | Avg rank in frequency list | Common vs. rare characters |
| Stroke count | Avg strokes per character | Visual complexity |
| Word length | Avg characters per word | Lexical complexity |
| Sentence length | Avg characters per sentence | Syntactic complexity |

**Chinese-specific linguistic features**:
- Chengyu (idiom) usage frequency
- Classical vs. modern Chinese ratio
- Hedge markers: 可能 (maybe), 也许 (perhaps), 大概 (probably)
- Discourse markers: 然而 (however), 因此 (therefore), 此外 (moreover)

**Universal features** (same as English):
- Paper length (pages, characters)
- Section count
- Figure/table count
- Equation count
- Reference count
- Citation patterns

### Model Development

**Baseline approaches**:

1. **Translation-based**: Translate to English, apply English model
2. **Chinese BERT**: Fine-tune Chinese-BERT on Chinese reviews
3. **Multilingual BERT**: Fine-tune mBERT on Chinese reviews
4. **Universal features**: Train on language-agnostic features only

**Evaluation**:
- 80/20 train/test split
- 5-fold cross-validation
- Compare to English model performance

### Expected Results

**Hypothesis 1**: Universal features achieve F1 ≥ 0.60
- If true: Language-agnostic approach is viable
- If false: Language-specific features are essential

**Hypothesis 2**: Chinese-BERT outperforms translation-based approach
- If true: Native language modeling is superior
- If false: Translation preserves quality signals adequately

**Hypothesis 3**: Feature importance differs from English
- Expected: Practical impact more important, novelty less important
- Validates cultural difference hypothesis

### Success Criteria

Pilot is successful if:
- [ ] Data collection achieves ≥80 papers with reviews
- [ ] Chinese-specific features can be extracted reliably
- [ ] At least one approach achieves F1 ≥ 0.60
- [ ] Feature importance analysis reveals interpretable patterns
- [ ] Cost and timeline are within estimates

### Risk Mitigation

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| Data access denied | Medium | High | Multiple venue contacts |
| Translation quality poor | Low | Medium | Use professional translators |
| Chinese BERT unavailable | Low | Low | Multiple pretrained models exist |
| Performance too low | Medium | Medium | Lower success threshold to F1≥0.55 |
| Budget overrun | Low | Medium | Secure funding before starting |

### Deliverables

1. **Technical report** (Month 3)
   - Pilot results
   - Lessons learned
   - Recommendations for full-scale extension

2. **Pilot dataset** (Month 3)
   - 100 Chinese papers with reviews
   - Extracted features
   - Model predictions
   - Released publicly (if permitted)

3. **Code** (Month 3)
   - Chinese feature extractors
   - Model training scripts
   - Evaluation code

4. **Go/No-Go decision** (Month 3)
   - Recommendation on full multilingual extension
   - Revised timeline and budget if needed
```

---

## Alternative: Minimal Acknowledgment (If Resources Unavailable)

```markdown
### X.X Limitation: English-Only Scope

**Acknowledged Gap.** Our work is limited to English-language papers from 
English-language venues (NIPS, ICLR). This excludes substantial research 
communities in non-English-speaking regions and limits the generalizability 
of our findings.

**Why This Matters.** The global research community is increasingly 
multilingual, with major conferences accepting submissions in multiple 
languages and regional venues publishing primarily in local languages. Our 
English-only models cannot serve these communities without adaptation.

**Challenges.** Multilingual extension faces several obstacles:
- Limited availability of non-English review datasets
- Language-specific feature engineering requirements (e.g., readability 
  metrics assume English)
- Cultural differences in review criteria and writing conventions
- Resource constraints (translation, annotation, partnerships)

**Proposed Future Work.** We propose a phased multilingual extension:
1. **Pilot study** (6 months): Collect 100 Chinese papers, test feasibility
2. **Core languages** (12 months): Extend to Chinese, Spanish, German, French
3. **Broader coverage** (18 months): Add Japanese and other languages

We estimate this requires $15,000-20,000 in funding and partnerships with 
institutions in target language regions. We are actively seeking 
collaborators and funding for this extension.

**Interim Mitigation.** While working toward multilingual extension, we:
- Clearly document English-specific assumptions in our methodology
- Provide translation-based models as interim solution
- Welcome contributions from multilingual researchers
- Share protocols to enable replication in other languages

We acknowledge that our current contribution is limited to English venues 
but believe it provides a foundation for future multilingual work.
```

---

## Key Messaging Points

### **What to Emphasize:**

1. **Acknowledge explicitly**: "Our work is limited to English; this excludes major research communities"

2. **Explain why it matters**: "Multilingual venues are growing; English-only models have limited applicability"

3. **Identify specific challenges**: "Readability metrics, cultural norms, data access all differ by language"

4. **Provide concrete plan**: "Phased approach: Chinese pilot (6 months) → 5 core languages (18 months)"

5. **Show feasibility**: "Pilot budget $2-3K; full extension $15-20K; partnerships identified"

6. **Commit to timeline**: "Pilot results in 6 months; first multilingual release in 12 months"

### **What to Avoid:**

1. **Dismissing the concern**: Don't say "English is the lingua franca of science"

2. **Vague promises**: Don't just say "we'll extend to other languages someday"

3. **Overstating difficulty**: Don't make it sound impossible

4. **Ignoring cultural aspects**: Don't treat it as purely a translation problem

---

## Quick Reference: Multilingual Extension Checklist

```markdown
### Multilingual Extension Checklist

**Phase 1: Planning (Month 1)**
- [ ] Identify target languages (prioritize by research volume)
- [ ] Research data availability for each language
- [ ] Identify potential institutional partners
- [ ] Estimate budget and timeline
- [ ] Secure funding or partnerships

**Phase 2: Pilot (Months 2-4)**
- [ ] Collect 100 papers in one non-English language (e.g., Chinese)
- [ ] Develop language-specific feature extractors
- [ ] Test multiple modeling approaches
- [ ] Analyze results and lessons learned
- [ ] Make go/no-go decision on full extension

**Phase 3: Core Languages (Months 5-12)**
- [ ] Collect data for 4-5 core languages
- [ ] Develop feature extractors for each language
- [ ] Train and evaluate multilingual models
- [ ] Analyze cross-lingual transfer
- [ ] Study cultural differences in review criteria

**Phase 4: Release (Months 13-18)**
- [ ] Clean and document multilingual dataset
- [ ] Release models for all languages
- [ ] Publish analysis report
- [ ] Create multilingual demo interface
- [ ] Engage community for feedback and contributions

**Ongoing:**
- [ ] Monitor model performance across languages
- [ ] Add new languages based on community interest
- [ ] Update models as review norms evolve
- [ ] Maintain partnerships with international institutions
```

---

## Key Takeaways

1. **Acknowledge the limitation explicitly**: English-only scope excludes major research communities

2. **Explain specific challenges**: Not just translation—readability metrics, cultural norms, data access all differ

3. **Provide concrete, phased plan**: Pilot (6 months) → Core languages (12 months) → Broader coverage (18 months)

4. **Show feasibility**: Specific budget ($15-20K), partnerships identified, pilot designed

5. **Commit to timeline**: Concrete milestones with deliverables

6. **Address cultural aspects**: Not just linguistic—review criteria and norms vary by culture

7. **Engage community**: Open call for collaborators and data contributions

8. **Provide interim solution**: Translation-based models while working on native-language support

The goal is to show that you understand the importance of multilingual extension, have thought carefully about the challenges, and have a realistic plan to address them—even if full implementation is future work.

## Sources

- [Multilingual BERT](https://arxiv.org/abs/1810.04805) - Devlin et al., 2018
- [XLM-RoBERTa](https://arxiv.org/abs/1911.02116) - Conneau et al., 2020
- [Cross-lingual Transfer Learning for NLP](https://arxiv.org/abs/1706.05098) - Ruder et al., 2019
- [Cultural Differences in Peer Review](https://www.nature.com/articles/d41586-019-03572-7) - Nature editorial, 2019
- [Chinese Text Readability Metrics](https://aclanthology.org/2020.lrec-1.489/) - Sung et al., LREC 2020