Below is concise, paste-ready discussion text, followed by suggested appendices (plots and subsample analyses) and three concrete, falsifiable experiments to directly probe overfitting and leakage risks.

Proposed discussion text
While our acceptance classifier obtains strong cross-validated performance, we acknowledge the risk of overfitting and spurious correlations (e.g., venue/year- or topic-specific artifacts) that can inflate accuracy under random document splits. To assess this risk, we will append diagnostics that stress-test generalization across time, venues, and topics; quantify probability calibration; and probe sensitivity to potential confounds. Specifically, we will report (i) temporal hold-out performance (train on earlier years, test on later years), (ii) leave-one-group-out evaluations by venue and by topic cluster, (iii) matched-subsample analyses that control for year, venue, topic, and length, and (iv) permutation tests establishing a null performance distribution. We will also include learning curves (train vs. validation) and reliability diagrams with Brier score and expected calibration error (ECE) to detect classic overfitting and miscalibration. These diagnostics follow established recommendations for guarding against leakage and for evaluating calibrated probabilities in supervised learning [1][2]. Collectively, they enable us to distinguish genuine signal from dataset- or split-specific artifacts and to quantify uncertainty via stratified bootstrap confidence intervals.

Appendix materials to add
- Appendix Fig. A1: Learning curves (train and validation AUC/PR-AUC vs. training set size) with 95% CIs. Overfitting would manifest as widening train–validation gaps and non-converging validation performance at larger sample sizes.
- Appendix Fig. A2: Temporal generalization. Train on ≤Y0 submissions, test on >Y0 by year; plot AUC/PR-AUC vs. test year with CIs. A large drop vs. random splits suggests time-dependent shortcuts [1].
- Appendix Fig. A3: Leave-one-group-out performance by venue and topic cluster (groups defined by submission venue; and by clustering abstracts via embeddings). Consistent drops in left-out groups indicate reliance on group-specific artifacts [1].
- Appendix Fig. A4: Reliability diagrams (with isotonic and Platt post-calibration overlays) and summary metrics (Brier, ECE). Miscalibration, especially overconfidence at extremes, is consistent with overfitting [2].
- Appendix Fig. A5: Feature stability and ablation. (a) Stability of top features/SHAP importances across folds; (b) AUC change when ablating metadata-like proxies (year tokens, length bins, citation count proxies) if present.
- Appendix Fig. A6: Permutation test. Null distribution of AUC under label shuffling; mark observed AUC and p-value.
- Appendix Table A1: Matched-subsample results. Exact/propensity-matched pairs by year×venue×topic×length; report AUC/PR-AUC relative to full-sample results.

Subsample and robustness analyses to append
- Temporal hold-outs: Train on early years; test on later years (rolling-origin and fixed cut). Compare to random-split baseline [1].
- Group-aware CV: Leave-one-venue-out and leave-one-topic-cluster-out; report average and worst-group metrics with CIs [1].
- Matched subsamples: Within year×venue×topic×length strata, balance acceptance rates and re-train/evaluate; report whether performance persists after controlling confounds.
- Permutation and negative controls: (a) Label permutation test; (b) Negative-control target such as predicting year from features to check for strong time signals that could leak into acceptance prediction [1].
- Calibration and uncertainty: Reliability diagrams, Brier/ECE overall and by group; bootstrap CIs for all key metrics [2].

Three concrete, falsifiable experiments
1) Temporal generalization test
- Hypothesis: If the model overfits to time-specific patterns, its performance will drop on future-year hold-outs compared to random splits.
- Design: Fix a cut year Y0. Train on ≤Y0; test on Y0+1 and Y0+2 (repeat for several Y0). Metrics: AUC, PR-AUC with 95% bootstrap CIs.
- Expected outcome: A non-trivial drop vs. random splits indicates time-dependent shortcuts; stability across years supports temporal robustness [1].

2) Leave-one-group-out by venue and topic
- Hypothesis: If the model exploits venue/topic artifacts, leaving that group out will reduce performance on that group relative to pooled k-fold CV.
- Design: Define groups as venues and as topic clusters (e.g., k-means on sentence-embedding vectors of abstracts). For each group, train on others, test on held-out group. Metrics: Group AUC/PR-AUC; report mean and worst-group performance with CIs.
- Expected outcome: Significant drops on held-out groups indicate overfitting to group-specific signals [1].

3) Matched-subsample robustness
- Hypothesis: If performance is driven by confounds (year, venue, topic, length), then within matched strata the accuracy will attenuate towards chance.
- Design: Construct matched pairs (accepted vs. rejected) within exact strata year×venue×topic×length (or propensity-score bins). Train/evaluate only on matched data. Metrics: AUC/PR-AUC, delta vs. full dataset; Brier/ECE for calibration.
- Expected outcome: Persistence of above-chance performance after matching suggests signal beyond confounds; strong attenuation suggests reliance on those confounds.

Optional additional checks
- Permutation test: Estimate null distribution of AUC under label shuffling; compute p-value for observed AUC [1].
- Calibration stress test: Reliability diagrams pre- and post-calibration (Platt and isotonic) with Brier/ECE; overconfident probabilities that improve markedly after calibration are consistent with overfitting [2].
- Learning curve ablation: Plot learning curves with and without metadata-like proxies (e.g., year tokens/length bins); divergence implicates these features as shortcuts.

Citations
[1] Kapoor, S., & Narayanan, A. (2023). Leakage and the reproducibility crisis in machine-learning-based science — https://ncbi.nlm.nih.gov/pmc/articles/PMC10499856/
[2] Niculescu-Mizil, A., & Caruana, R. (2005). Predicting Good Probabilities with Supervised Learning — https://www.cs.cornell.edu/~alexn/papers/calibration.icml05.crc.rev3.pdf

Key takeaways
- Acknowledge the risk explicitly, and then point to diagnostics that are sensitive to temporal and group-specific leakage, plus calibration and null-model checks.
- Provide plots and tables in the appendix so reviewers can see whether performance persists under stricter splits and controls, and whether predicted probabilities are well-calibrated.