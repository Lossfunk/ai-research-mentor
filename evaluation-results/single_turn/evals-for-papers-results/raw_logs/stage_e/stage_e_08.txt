Rebuttal paragraph: We agree that text classifiers can overfit to superficial lexical cues; to address this directly in PeerRead, we will (i) benchmark against shallow baselines (title-only, bag-of-words, and review-length) to quantify the contribution of surface features, (ii) stress-test our models by masking the top label-correlated unigrams/bigrams and re-evaluating, (iii) test cross-year and cross-venue generalization to reduce the chance of venue-specific lexical shortcuts, and (iv) use counterfactual analyses—minimally editing reviews to flip verdict or sentiment while holding topical content constant—to verify that predictions change for content-relevant reasons. We will also report influence-based attributions to identify which training reviews most drive each prediction and whether these contain review-worthy evidence (claims, justification, comparisons). Counterfactual evaluation and augmentation are established tools for probing and reducing shortcut reliance in NLP; we will adopt them to stress-test our models and report any residual brittleness honestly [P1], [P7].

Concrete, falsifiable diagnostics/experiments:
1) Shallow baselines and length controls
- Hypothesis: If our model relies on review-worthy content (not shortcuts), it will substantially outperform title-only, bag-of-words, and review-length baselines on acceptance prediction.
- Variables: Model type (BOW, length-only, title-only, our model), input fields (title vs full review), and train/test splits.
- Metrics: Accuracy/F1; Δ vs BOW; calibration.
- Expected outcome: Our model retains a statistically significant margin over shallow baselines; small (<5%) performance change when controlling for length bins.

2) Lexical ablation stress test
- Hypothesis: Removing the top-k label-correlated tokens (by mutual information or χ²) from inputs yields only a modest drop for our model if it uses broader review content.
- Variables: k ∈ {50, 100, 500}; masking strategy (mask vs synonym substitution); model type.
- Metrics: Δ accuracy/F1 vs unmasked; robustness curve as k increases.
- Expected outcome: Our model degrades less than BOW under masking; predictions remain stable for content-preserving synonym substitutions.

3) Counterfactual evaluation (and optional augmentation)
- Hypothesis: Minimal edits that flip verdict-relevant content (e.g., changing evidence strength or comparative statements) will flip our model’s prediction; edits to superficial tokens (e.g., venue names, hedges) will not. Training with counterfactuals reduces shortcut use further.
- Variables: Edit type (content-relevant vs superficial), proportion of counterfactuals used in training (0%, 10%, 50%).
- Metrics: Flip rate on content edits vs superficial edits; OOD generalization to a held-out year/venue; robustness gap before/after augmentation.
- Expected outcome: Higher flip rate for content edits than superficial edits; improved OOD robustness with counterfactual augmentation, consistent with prior CDA findings [P1], [P7].

4) Cross-venue and cross-year generalization
- Hypothesis: If the model relies on universal review content cues (e.g., evidence, comparisons), performance will transfer across venues/years better than BOW or title-only.
- Variables: Train venue/year → Test venue/year matrix; model type.
- Metrics: Transfer accuracy/F1; relative drop from in-domain to out-of-domain.
- Expected outcome: Our model shows smaller OOD drops than shallow baselines.

5) Training-instance influence and example-level attribution
- Hypothesis: The most influential training reviews for a given prediction contain substantive justifications (e.g., novelty/rigor assessments) rather than isolated keywords.
- Variables: Attribution method (influence functions; gradient-based attributions); sampling of predicted positives/negatives.
- Metrics: Human-coded presence of review-worthy rationales among top-attributed spans/instances; inter-annotator agreement.
- Expected outcome: Attributed evidence aligns with review content; isolated cue reliance is rare and identified as failure cases.

6) Behavioral test suite of invariances and perturbations
- Hypothesis: The model should be invariant to superficial rewrites (e.g., paraphrases, hedge insertion) and sensitive to meaning-preserving but verdict-changing edits.
- Variables: Perturbation templates (paraphrase, hedge insertion, superficial token swaps; content-rationale edits).
- Metrics: Invariance/sensitivity rates; error typology.
- Expected outcome: High invariance to superficial edits; sensitivity to content edits.

Literature anchors:
- Counterfactually‑Augmented SNLI Training Data Does Not Yield Better Generalization Than Unaugmented Data (cautions and best practices for using counterfactuals to reduce shortcuts) [P1].
- CORE: A Retrieve‑then‑Edit Framework for Counterfactual Data Generation (practical framework we can adapt for PeerRead counterfactual evaluation/augmentation) [P7].

Citations
- [P1] Counterfactually-Augmented SNLI Training Data Does Not Yield Better Generalization Than Unaugmented Data — http://arxiv.org/abs/2010.04762v1
- [P7] CORE: A Retrieve-then-Edit Framework for Counterfactual Data Generation — http://arxiv.org/abs/2210.04873v2

Key takeaways: We will directly test and report the extent of shortcut use via shallow baselines, lexical ablations, cross-domain generalization, counterfactual edits, influence-based attributions, and behavioral tests. These analyses, grounded in prior work on counterfactual probing/augmentation, can substantiate that our models capture review-worthy content—or precisely characterize any remaining shortcut dependence.