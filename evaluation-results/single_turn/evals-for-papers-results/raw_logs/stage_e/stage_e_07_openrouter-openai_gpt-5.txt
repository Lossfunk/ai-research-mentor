Thanks for flagging the missing LLM baselines. We agree this is a gap. Our current study focuses on supervised models trained on PeerRead; we did not include modern instruction-tuned LLMs that can perform rubric-based scoring via prompting. We will remedy this with a principled, reproducible evaluation that (a) fairly compares zero/few-shot LLMs to supervised baselines under strict anti-leakage controls, (b) reports uncertainty and decontamination checks, and (c) emphasizes generalization across venues and years.

Planned LLM inclusion (succinct and principled)
- Models: GPT-4/4o (API), and open LLMs (e.g., Llama-3 70B Instruct) for reproducibility; we will cap context to the same sections as baselines (title+abstract+intro) to equalize token budgets.
- Prompting protocol:
  - Zero-shot rubric prompting that mirrors PeerRead aspect definitions; ordinal response required (with label mapping).
  - Few-shot in-context examples (k=8) sampled from the training fold; identical seeds across models; temperature=0 for determinism.
  - Optional rationale generation for analysis only; scores are the primary output.
- Evaluation:
  - Metrics: Kendall’s tau (ranking) and RMSE per aspect; expected calibration error (ECE) on discretized outputs; retained-performance ratio (RPR = OOD/ID) for cross-venue/year tests.
  - Confidence intervals via bootstrap; pre-registered non-inferiority margins (e.g., Δtau ≤ 0.02 vs. best supervised baseline).
- Anti-leakage and decontamination:
  - Use earliest submission-like versions (OpenReview original or arXiv v1); mask venue names, acknowledgments, “Accepted at …,” arXiv comments, and reference sections.
  - Post-hoc contamination probes: nearest-neighbor search on public corpora for exact title/large n-gram matches; report results and re-run on post-cutoff papers (e.g., newer years) not likely in LLM pretraining.
- Reporting: Release prompts, few-shot exemplars, masking scripts, and model/version hashes; include cost and latency.

Concrete, falsifiable experiments
1) Zero-shot and few-shot LLM vs. supervised baselines
- Design: Compare zero-shot and 8-shot LLMs to our best text-only and features+text baselines on ACL→ICLR and cross-year splits (title+abstract+intro only).
- Metrics: Kendall’s tau, RMSE, ECE, and RPR with 95% CIs.
- Criteria: If LLM tau ≥ baseline tau − 0.02 and RPR ≥ 0.90, we will include LLMs as competitive baselines; if not, we document failure modes and maintain supervised baselines as primary.

2) Prompt and section ablation
- Design: Evaluate LLMs under three inputs: abstract-only, abstract+intro, and full paper; with and without rationales; with rubric-only vs. rubric+exemplar prompts.
- Metrics: Δtau/ΔRMSE and variance across domains.
- Criteria: Adopt the smallest input and simplest prompt that achieves ≥0.90 RPR and within 0.01 of full-prompt tau; otherwise, report section/prompt sensitivity as a limitation.

3) Decontamination stress test
- Design: Two target sets: (a) papers from years posted after documented LLM pretraining cutoffs; (b) papers whose titles/long n-grams are not found by web search/NN retrieval. Re-evaluate zero-/few-shot LLMs.
- Metrics: Tau/RMSE deltas vs. main results.
- Criteria: If performance drops >0.03 tau on decontaminated sets, we will qualify all LLM comparisons accordingly and prioritize newer-year evaluations in the main text.

4) Domain-transfer head-to-head
- Design: Train supervised models on ACL-YY; evaluate on ICLR-YY and ACL-YY+1. Run zero-/few-shot LLMs on the same splits.
- Metrics: RPR and absolute tau; calibration gap ID→OOD.
- Criteria: If LLMs achieve higher RPR with comparable ID tau (Δ≤0.02), we will recommend LLMs for OOD use-cases; if not, we emphasize domain-specific fine-tuning.

How we will frame it in the paper
- We acknowledge that omission of LLMs limits external validity. We will add zero- and few-shot LLM comparisons under strict masking and decontamination, report calibrated uncertainty and OOD retention, and release all prompts and scripts for replication. If LLMs outperform or transfer better, we will revise conclusions accordingly; if they do not, we will document where and why they fall short to guide future work.

Literature anchors
- Brown et al., 2020. Language Models are Few-Shot Learners — https://arxiv.org/abs/2005.14165
- Liang et al., 2022. Holistic Evaluation of Language Models (HELM) — https://arxiv.org/abs/2211.09110

Sources
- Brown, T. B., et al. (2020). Language Models are Few-Shot Learners — https://arxiv.org/abs/2005.14165
- Liang, P., et al. (2022). Holistic Evaluation of Language Models — https://arxiv.org/abs/2211.09110