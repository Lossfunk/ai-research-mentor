Here’s a concise way to frame the modest gains in aspect-score prediction and a concrete contingency plan that reviewers will view as rigorous and realistic.

How to frame the limitations
- Label and task ceiling: Aspect scores are subjective and noisy; multiple studies show substantial inconsistency in peer-review outcomes, setting a hard ceiling on learnable signal even with strong models [2][3]. This likely caps achievable improvements beyond simple baselines on PeerRead’s aspect labels [1].
- Data and rubric constraints: PeerRead’s aspect annotations are sparse and vary across venues/years; the rubric semantics (e.g., “soundness,” “originality”) are coarse and may not align cleanly with text-only signals available at submission time [1].
- Confounding and shortcuts: Non-substantive features (length, sectioning, citation count proxies) can correlate with scores and inflate apparent gains; conversely, true causal signals (e.g., methodology rigor) are under-specified in the inputs [1].
- Generalization gaps: Models trained on one venue/year often do not transfer to others due to shifting rubrics, norms, and writing styles, which can compress headline performance when evaluated properly out-of-domain [1].
- Evaluation mismatch: Treating ordinal aspect scores as pointwise regression can be suboptimal; modest deltas may reflect metric-task mismatch rather than model inability [1].

Contingency plan for future work
- Re-center the objective on reliability and robustness
  - Establish upper bounds from human agreement (multi-review subsets) and target “closing the gap to human IAA” rather than absolute MAE/RMSE gains [2][3].
  - Report uncertainty-aware metrics (prediction intervals, calibration error) and rank-based measures (Kendall’s tau) more aligned with review use-cases [1].
- Improve supervision quality
  - Apply multi-annotator label models (e.g., Dawid–Skene/GLAD) on papers with multiple reviews to infer de-noised aspect targets; compare to single-score training [2][3].
  - Add weak evidence labels from review comments (e.g., sentences critiquing novelty or clarity) to support multi-task learning with rationales [1].
- Reduce confounding and leakage
  - Predefine and ablate non-textual correlates (length, reference count, figure tables); require robustness to their removal to count as a “real” gain [1].
- Reframe the prediction problem
  - Prefer pairwise within-venue/year ranking (paper A > B on originality) over absolute scores; optimize and evaluate for ranking stability [1].
  - Explore multi-task setups (predict overall score + aspects jointly) and intermediate-task transfer (e.g., argument quality detection) as suggested by prior work [4][5].
- Strengthen evaluation design
  - Emphasize out-of-domain tests (train on ACL-YY, test on ACL-YY+1/ICLR-YY) with pre-registered non-inferiority margins; report fail/pass clearly [1].
- Shift deliverables if gains remain modest
  - Pivot to reviewer-assist tools: aspect-evidence extraction and uncertainty-calibrated triage summaries instead of raw score regression; measure human usefulness in small user studies [1].
  - Deliver improved resources: cleaned labels (with confidence), de-identified rationale spans, and robust baselines for reproducibility [1].

Falsifiable experiments to include
1) Human-agreement upper bound and non-inferiority
- Data: Papers with ≥2 reviews (ACL/ICLR subsets).
- Method: Compute inter-reviewer agreement per aspect (Spearman/Kendall, quadratic-weighted kappa). Train models; evaluate on held-out with bootstrapped CIs.
- Criteria: Pre-register a non-inferiority margin; declare success only if model–human gap shrinks by ≥20% vs prior baselines, or model Kendall’s tau is within 0.05 of inter-reviewer tau [2][3].

2) Label denoising vs. single-score training
- Method: Fit a multi-annotator model (Dawid–Skene/GLAD) to infer latent aspect scores; train the same architecture on (a) raw single scores and (b) denoised labels.
- Outcomes: Change in RMSE and calibration (ECE) on gold test; robustness when non-textual features are ablated.
- Criteria: Reject denoising if ΔRMSE < 0.01 and ΔECE < 1% with overlapping 95% CIs; otherwise adopt denoising [2][3].

3) Pairwise ranking vs. regression
- Method: Construct within-venue/year pairwise constraints from review score differences; train a pairwise ranker; compare to pointwise regression.
- Outcomes: Kendall’s tau and top-k precision for identifying the top 20% papers by each aspect.
- Criteria: Accept ranking if tau improves by ≥0.05 and top-20% precision by ≥5 points on out-of-domain tests [1].

4) Confounding control and causal sensitivity
- Method: Evaluate models with and without non-textual features (length, refs, figures) and with text masked for certain sections (e.g., abstract only vs. full).
- Outcomes: Performance deltas; SHAP or influence functions to quantify reliance on confounders.
- Criteria: If removing non-textual features drops >50% of the gain, gains are deemed non-substantive and not claimed [1].

5) Cross-venue/year generalization
- Method: Train on ACL-YY, test on ACL-YY+1 and ICLR-YY; repeat reversed.
- Outcomes: Relative performance retention (RPR) = OOD score / ID score.
- Criteria: Target RPR ≥ 0.9; if < 0.8, prioritize domain adaptation or calibration over new architectures [1].

6) Reviewer-assist utility study (contingency)
- Method: With 10–15 volunteer reviewers, compare baseline reviewing vs. assistance with aspect evidence spans and calibrated score ranges.
- Outcomes: Time-to-first-decision, inter-reviewer agreement, perceived usefulness.
- Criteria: Success if review time drops ≥10% without lowering agreement (kappa non-inferiority, margin 0.02) [2][3].

How to write it in the paper
- State the ceiling candidly and tie it to documented review inconsistency; position results as bounded by label noise rather than purely model capacity [2][3].
- Commit to reliability-first evaluation (human upper bounds, OOD tests, calibration) and to moving from absolute scores to ranking and assistance if progress stalls [1].
- Pre-register thresholds for adopting/abandoning directions (e.g., non-inferiority margins, RPR targets) to avoid hindsight bias.

Literature anchors
- A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications (for task definitions, baselines, and data constraints) [1].
- Evidence of inconsistency in peer review (NeurIPS consistency experiments), motivating reliability ceilings and uncertainty-aware evaluation [2][3].
- Follow-on modeling directions in multi-task and transfer for score prediction [4][5].

Citations
[1] A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications — https://aclanthology.org/N18-1149.pdf
[2] Inconsistency in Conference Peer Review: Revisiting the 2014 NeurIPS Experiment — https://arxiv.org/abs/2109.09774
[3] Has the Machine Learning Review Process Become More Arbitrary as the Field Has Grown? The NeurIPS 2021 Consistency Experiment — https://arxiv.org/abs/2306.03262
[4] Multi-task Peer-Review Score Prediction — https://aclanthology.org/2020.sdp-1.14.pdf
[5] Intermediate-Task Transfer Learning for Peer Review Score Prediction — https://aclanthology.org/2023.ijcnlp-srw.6.pdf

Key takeaway
- Frame modest gains as a consequence of noisy, subjective targets and domain shift rather than solely model limits; adopt reliability-first evaluation, denoise labels, shift to ranking, and have a clear pivot to reviewer-assistive tools if non-inferiority and OOD criteria are not met [1][2][3].