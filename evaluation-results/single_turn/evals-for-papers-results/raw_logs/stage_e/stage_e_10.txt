Limitations and multilingual extension plan. Our current findings are derived from English-only experiments and may not directly generalize to multilingual settings, where script diversity, morphological complexity, tokenization coverage, and domain/cultural variation can alter error profiles and calibration. To mitigate this risk, we will: (i) evaluate zero-shot and few-shot transfer on standardized cross-lingual benchmarks (e.g., XTREME, XNLI) across a typologically and script-diverse set of 10–15 languages (e.g., Arabic, Chinese, Hindi, Spanish, Swahili); (ii) audit subword vocabulary coverage per language and update the tokenizer to a shared multilingual SentencePiece model if coverage gaps exceed a pre-specified threshold; (iii) add language-adaptive continued pretraining on unlabeled corpora for target languages, followed by lightweight supervised finetuning with 1–5k labeled examples per language where feasible; (iv) incorporate translation-based augmentation (forward/back-translation) to bootstrap low-resource supervision; (v) track per-language accuracy/F1, calibration, and fairness/parity metrics, with targeted error analyses for scripts and morphology; and (vi) release the evaluation protocol, code, and multilingual test suites to support reproducibility and community auditing.

Concrete, falsifiable experiments
1) Zero-shot cross-lingual generalization
- Hypothesis: The English-trained model exhibits larger performance drops on morphologically rich and non-Latin-script languages than on morphologically simpler Latin-script languages.
- Setup: Train on English only; evaluate zero-shot on XTREME tasks (e.g., XNLI, TyDiQA subsets).
- Metrics: Accuracy/F1 by language; calibration error; performance gap vs English.
- Expected outcome: Statistically significant drops on languages with lower tokenizer coverage and non-Latin scripts.

2) Tokenizer coverage ablation
- Hypothesis: Replacing the English-only tokenizer with a multilingual tokenizer reduces the average cross-lingual performance gap by at least 3–5 points on XTREME/XNLI.
- Setup: Compare models with (a) English-only tokenizer vs (b) multilingual SentencePiece trained on a balanced multilingual corpus; keep all other training factors fixed.
- Metrics: Subword coverage per language; downstream accuracy/F1; error-type distribution.
- Expected outcome: Improved coverage correlates with higher per-language performance and reduced variance across scripts.

3) Language-adaptive continued pretraining (LAPT)
- Hypothesis: LAPT on unlabeled target-language corpora narrows the zero-shot gap to within 2–3 points of few-shot results without requiring large supervised datasets.
- Setup: From an English-trained base, continue masked-LM or text-to-text pretraining on unlabeled corpora for target languages; then evaluate zero-shot and with 100–1k labeled examples per language.
- Metrics: Downstream task metrics; data-efficiency curves; perplexity changes vs gains.
- Expected outcome: Consistent improvements over the English-only baseline, with diminishing returns beyond modest LAPT steps.

Literature anchors
- XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization — https://arxiv.org/abs/2003.11080
- XNLI: Evaluating Cross-lingual Sentence Representations — https://arxiv.org/abs/1809.05053

Sources
- Hu et al., 2020, XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization — https://arxiv.org/abs/2003.11080
- Conneau et al., 2018, XNLI: Evaluating Cross-lingual Sentence Representations — https://arxiv.org/abs/1809.05053