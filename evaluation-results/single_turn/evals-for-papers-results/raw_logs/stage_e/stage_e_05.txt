Thank you for raising the concern about domain transferability. We agree this is central to assessing the external validity of aspect score models. Cross-domain performance often degrades due to shifts in discourse conventions, topical coverage, and even label semantics; similar brittleness has been observed in aspect-based sentiment analysis and coherence modeling, as well as when transferring transformers across specialized domains [P1, P3, P4]. We will therefore expand our evaluation and analysis to directly test—and, where possible, improve—cross-venue and cross-year generalization.

What we will add (new experiments and ablations)

1) Cross-venue generalization (leave-one-venue-out)
- Design: Train on all but one venue (e.g., ACL/NeurIPS/ICLR subsets) and test on the held-out venue; rotate across venues.
- Metrics: Pearson/Spearman/Kendall-τ correlation with human aspect ratings; MAE; per-aspect breakdown; paired bootstrap tests for significance.
- Analysis: Report performance gaps vs. in-venue baselines; error slices by paper length, subfield keywords, and year. This directly quantifies transfer and mirrors cross-domain setups used in prior work on discourse quality and specialized domains [P3, P4].

2) Temporal shift robustness (cross-year)
- Design: Train on earlier years within a venue, test on later years; repeat across venues.
- Metrics: Same as above; plus relative performance drop vs. in-time test.
- Analysis: Tests sensitivity to evolving writing conventions and reviewing norms; helps disentangle temporal vs. venue shift.

3) Causal- and domain-generalization baselines
- Design: Add domain-generalization baselines to reduce spurious venue-specific correlations:
  - Sample reweighting or causal adjustment methods that minimize dependence on venue/style covariates [P2].
  - Domain-adversarial training (feature invariance to venue) and group DRO using venue as group label.
- Metrics: Same as above; also measure stability across venues (variance of performance across held-out venues).
- Analysis: Tests whether enforcing invariance improves transfer, consistent with domain generalization findings [P2, P3].

4) Representation pretraining and adaptation studies
- Design: Compare base encoder vs. continued pretraining on scientific text or venue-proximal corpora (domain-adaptive pretraining) and/or multi-domain pretraining before supervised fine-tuning; include a no-adaptation control.
- Metrics: Cross-venue scores; cross-year robustness; compare gains/losses per aspect.
- Analysis: Prior cross-domain studies in specialized text suggest that representation learning choices materially affect transferability [P3]; we will quantify this effect for aspect scoring.

5) Spurious-cue and content-vs-style ablations
- Design: Evaluate models when (i) venue names and boilerplate (e.g., section headers) are masked; (ii) superficial formatting cues removed; (iii) sentence order permuted to probe reliance on coherence signals.
- Metrics: Δ performance relative to unmodified text, per aspect.
- Analysis: Identifies reliance on venue-specific stylistic artifacts vs. actual content quality; coherence perturbations are motivated by cross-domain coherence work [P4].

6) Label/covariate shift diagnostics and corrections
- Design: Estimate venue separability with a domain discriminator; correlate domain distance with transfer drop. Examine label marginal differences across venues; apply importance weighting or temperature scaling across domains as needed.
- Metrics: H-divergence/ROC-AUC of domain discriminator; correlation with performance drop; before/after correction performance.
- Analysis: Sheds light on whether covariate vs. label shift drives failures; aligns with causal adjustment perspectives [P2].

7) Multi-task vs. per-aspect heads
- Design: Compare a shared encoder with (i) a single multi-task head vs. (ii) separate per-venue adapters vs. (iii) per-aspect independent heads.
- Metrics: Cross-venue performance for each aspect; parameter efficiency vs. generalization trade-offs.
- Analysis: Tests whether aspect-specific or venue-adaptive heads help retain transferability, consistent with findings that type-system differences and annotation schemes can hinder transfer [P3].

What these additions will clarify
- If performance falls substantially in leave-one-venue-out but improves under causal/domain-generalization baselines or domain-adaptive pretraining, we will be able to attribute failures to spurious venue features and correctable representation gaps [P2, P3]. If performance remains stable under spurious-cue ablations and coherence perturbations, it would suggest the model captures content-relevant signals rather than stylistic shortcuts [P4]. Finally, diagnostics linking domain distance to transfer degradation will make the limits of external validity explicit and reproducible [P3].

Concrete, falsifiable experiments (summary)
- Exp 1: Leave-one-venue-out generalization. Hypothesis: Models trained on venues A+B generalize to venue C with <10% relative drop in correlation vs. in-venue validation. Variables: training venues; held-out venue; model architecture. Metrics: Pearson/Spearman/Kendall-τ; MAE; significance via paired bootstrap. Expected outcome: A measurable but bounded drop; gaps reduced by domain-generalization baselines [P2, P3].
- Exp 2: Domain-adversarial and causal reweighting. Hypothesis: Adding a venue-invariance penalty or causal adjustment improves average held-out venue correlation by ≥2–3 points without harming in-venue performance. Variables: adversarial strength, reweighting scheme. Metrics: Cross-venue correlations; variance across venues. Expected outcome: Reduced performance variance and improved worst-venue performance [P2].
- Exp 3: Spurious-cue ablations and coherence perturbations. Hypothesis: Masking venue-specific tokens and removing formatting yields ≤1-point drop if the model relies on content rather than superficial cues; randomizing sentence order reduces “clarity” prediction by ≥3 points if coherence matters. Variables: masking policy; perturbation intensity. Metrics: Δ correlation per aspect. Expected outcome: Identify aspects sensitive to coherence (consistent with coherence transfer literature) [P4].
- Exp 4: Representation adaptation. Hypothesis: Continued pretraining on scientific/venue-proximal text improves cross-venue correlation by ≥2 points vs. no adaptation. Variables: pretraining corpus; steps. Metrics: Cross-venue correlations; catastrophic forgetting on original venues. Expected outcome: Gains consistent with cross-domain transfer in specialized text [P3].

Commitments for the camera-ready (if paper is accepted)
- Add the cross-venue and cross-year protocols to main results, with full per-aspect tables.
- Include domain-generalization baselines and spurious-cue/coherence ablations.
- Release code and splits to enable replication of transfer results.

We hope this addresses the concern by making transferability an explicit, measurable target and by adopting established cross-domain methodologies from neighboring NLP tasks [P1, P2, P3, P4, P6].

Citations
- [P1] Cross-domain Aspect Category Transfer and Detection via Traceable Heterogeneous Graph Representation Learning — http://arxiv.org/abs/1908.11610v1
- [P2] Domain Generalization via Causal Adjustment for Cross-Domain Sentiment Analysis — http://arxiv.org/abs/2402.14536v1
- [P3] Cross-Domain Generalization and Knowledge Transfer in Transformers Trained on Legal Data — http://arxiv.org/abs/2112.07870v1
- [P4] A Cross-Domain Transferable Neural Coherence Model — http://arxiv.org/abs/1905.11912v2
- [P6] Bidirectional Generative Framework for Cross-domain Aspect-based Sentiment Analysis — http://arxiv.org/abs/2305.09509v1

Sources
- He et al., 2018. A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications — http://arxiv.org/abs/1804.09635v1 (context for dataset; used to motivate evaluation design)
- Works cited as [P#] above.