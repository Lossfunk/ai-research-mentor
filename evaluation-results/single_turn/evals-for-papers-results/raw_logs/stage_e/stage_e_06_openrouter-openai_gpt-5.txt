Here’s language you can use to (a) articulate why underpowered labeling for newer venues limits generalizability, and (b) reassure reviewers with a concrete, falsifiable follow-up plan.

Why underpowered labeling for newer venues hurts generalizability
- Coverage and shift mismatch: PeerRead’s gold labels come from a small set of older conference-years (e.g., ACL 2016; ICLR 2017), while newer venues/years have sparse or no gold aspect labels. Models trained/validated on older venues may overfit venue-specific rubrics, writing norms, or topic mix and fail under distribution shift to newer venues [Kang et al., 2018].
- Label-construction drift: Aspect rubrics and reviewer practices evolve. Mapping older aspect definitions onto newer venues can introduce construct mismatch, reducing external validity even when the surface label names coincide [Kang et al., 2018].
- High-variance evaluation: Low-N labels in newer venues inflate uncertainty (wide CIs), making apparent gains fragile to resampling and hyperparameter choices; this undermines strong claims about transfer.
- Heuristic negatives amplify risk: Where newer venues lack decisions, reliance on arXiv “probably-rejected” heuristics introduces unknown error rates (late acceptances, alternative venues), biasing both training and evaluation toward venue-specific artifacts rather than portable signals (see PeerRead’s construction notes) [Kang et al., 2018; PeerRead GitHub].

Primary follow-up experiment (prospective target-venue benchmark; pre-registered)
Goal: Directly measure cross-venue transfer to a newer venue with adequate statistical power.

- Data: Randomly sample 300–500 submissions from a recent year of the target newer venue. Obtain gold overall and aspect scores via existing public reviews (e.g., OpenReview) or a small-scale expert re-annotation aligned to that venue’s rubric. Document any rubric mapping differences.
- Models/conditions:
  1) Source-only model: trained on PeerRead’s older venues.
  2) Source-only + input constraints: section-limited (title+abstract+intro) and masked venue/metadata to reduce leakage.
  3) Lightweight adaptation: fine-tune on 50–100 labeled target examples (few-shot) with early stopping.
- Splits/metrics: Evaluate on a held-out target test set. Report Kendall’s tau (ranking) and RMSE for aspects, plus calibration (ECE). Also report retained-performance ratio (RPR = OOD/ID).
- Falsifiable criteria (decided a priori):
  - Acceptable transfer: RPR ≥ 0.90 for tau and RMSE degradation ≤ 0.01 relative to in-domain baselines.
  - If 1) fails but 2) succeeds, conclude input-side constraints improve transferability.
  - If 3) improves tau by ≥0.03 with ≤0.01 ID loss, recommend minimal adaptation for deployment; otherwise, limit claims to in-venue reliability.

Two additional analyses to further reassure reviewers
1) Leave-one-venue-out (LOVO) generalization with importance weighting
- Design: Treat each existing PeerRead venue-year as a “pseudo-new” domain in rotation. Train on all others and test on the held-out domain. Additionally, learn a density-ratio reweighting (e.g., CORAL/whitening or shallow propensity model on text embeddings) to reduce covariate shift and re-evaluate.
- Outcomes: Tau/RMSE and RPR with 95% CIs across folds; gain from reweighting.
- Criteria: If median RPR < 0.85 without reweighting but ≥0.90 with reweighting, attribute prior transfer gaps to covariate shift and adopt weighting in the primary newer-venue experiment.

2) Venue/metadata leakage and section sensitivity ablation
- Design: Retrain source-only models after masking venue names, arXiv IDs/comments, acknowledgments, and reference sections; also train on title+abstract+intro only (fixed token budget). Evaluate on the newer venue.
- Outcomes: Δtau/ΔRMSE and RPR changes relative to unmasked/full-text inputs.
- Criteria: If masking or section-limiting improves newer-venue tau by ≥0.02 (CI excludes 0) or raises RPR to ≥0.90, make these constraints the default and revise claims accordingly.

How to frame this in the discussion
- Explicitly acknowledge that sparse or heuristic labels for newer venues limit what can be claimed about cross-venue generalization today, because both label semantics and input distributions shift over time.
- Commit to the prospective target-venue benchmark with pre-registered thresholds and to LOVO + importance-weighted diagnostics. Emphasize that if criteria are not met, you will confine claims to in-venue settings and report calibrated uncertainty for OOD use.
- Release the newer-venue annotations, rubric mapping, and masking scripts to facilitate replication.

Literature anchors
- Kang, Ammar, Schwartz, Cohen (2018). A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications — for dataset scope, venues, and labeling process. https://aclanthology.org/N18-1149.pdf
- Gulrajani, Lopez-Paz (2021). In Search of Lost Domain Generalization — for principled OOD evaluation and simple yet competitive DG baselines (e.g., ERM vs. reweighting). https://arxiv.org/abs/2007.01434

Sources
- Kang, D., Ammar, W., Schwartz, R., & Cohen, W. W. (2018). A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications — https://aclanthology.org/N18-1149.pdf
- PeerRead GitHub repository (construction details and caveats) — https://github.com/allenai/PeerRead
- Gulrajani, I., & Lopez-Paz, D. (2021). In Search of Lost Domain Generalization — https://arxiv.org/abs/2007.01434