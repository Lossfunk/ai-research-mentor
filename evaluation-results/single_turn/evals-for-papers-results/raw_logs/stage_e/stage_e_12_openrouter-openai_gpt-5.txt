Discussion text (for the paper)
We acknowledge that the acceptance classifier may overfit venue/year-specific artifacts and train–validation idiosyncrasies. To quantify and mitigate this risk, we will (i) report learning dynamics and calibration, (ii) evaluate on strictly out-of-domain splits (leave-one-venue-out and time-based train≤t → test t+1), and (iii) probe sensitivity to input masking and model complexity. If performance materially decreases under anti-leakage masking or on OOD splits, we will qualify claims accordingly, prioritize masked, section-limited inputs, and adopt early stopping and regularization tuned on OOD validation. We will also report variability across seeds and subsamples; if high variance persists, we will temper conclusions and recommend minimal target-domain adaptation.

Appendix diagnostics and subsample analyses to include
- Learning curves: Train vs. validation performance and gap as a function of training size; bias–variance trend across model capacity/regularization.
- OOD retention plots: In-domain vs. OOD performance with retained-performance ratio (RPR = OOD/ID) across model size and regularization; include 95% CIs (bootstrap).
- Temporal generalization: Train on ≤year t, test on t+1; line plot of AUC/F1 and τ/RMSE across years.
- Leave-one-venue-out: Violin/box plots of per-fold metrics with CIs; error distribution across venues.
- Calibration: Reliability diagrams with Expected Calibration Error (ECE) and Brier score, in-domain and OOD.
- Masking/ablation sensitivity: Bar chart of metric deltas when masking venue strings, acknowledgments, reference sections, and when restricting to title+abstract+intro.
- Seed/subsample stability: Error bars over 5–10 random seeds; curves at 25/50/75% training subsets to assess fragility.
- Slice robustness: Metrics by document length quintile, topic clusters, and reviewer-disagreement strata.
- Permutation sanity check: Performance with shuffled labels (should be at chance); feature permutation importance to detect reliance on spurious cues.
- Few-shot target adaptation: OOD performance before vs. after fine-tuning on 50–100 labeled target examples to quantify domain gap and adaptation efficacy.