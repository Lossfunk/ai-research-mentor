Replicating PeerRead results on newer conference cycles is limited by distribution shift and evolving review practices. Topic mix, writing style, paper length, and disclosure norms (e.g., code/data statements, broader-impact or reproducibility checklists) have changed, and many venues have revised guidelines and scoring rubrics; as a result, both the input distribution and the target labels may differ from those in legacy PeerRead, making cross-year comparisons noisy and potentially overstating generalization. In particular, acceptance thresholds and aspect definitions can drift, scale anchors may be reworded, and policy changes (e.g., checklist adoption, ethics reviews) can introduce step changes that models trained on older data cannot anticipate. Our claims should therefore be read as applying primarily to the historical snapshot PeerRead captures. To reduce ambiguity, we will (i) report year- and venue-stratified results with cross-year calibration and retained-performance ratios, (ii) provide a rubric crosswalk and re-annotate a stratified sample under current guidelines to measure label drift, (iii) use earliest submission versions and strict masking to limit leakage from meta-information that changed over time, and (iv) include covariate/label-shift diagnostics and reweighting baselines. If performance degrades materially on recent cycles or under the updated rubric, we will qualify conclusions and treat adaptation to current guidelines as a required step rather than an assumption of robustness.