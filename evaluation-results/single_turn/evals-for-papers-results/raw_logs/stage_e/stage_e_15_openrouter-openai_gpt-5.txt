Discussion addendum: annotation disagreement and transparency
We acknowledge that the current release underreports how often annotators disagree and how that uncertainty propagates to models. Because aspects (e.g., clarity, originality) are partly subjective and venue rubrics evolve, collapsing multiple judgments to a single label can obscure real ambiguity and conflate rater noise with signal. To address this, we will treat disagreement as a first-class property of the dataset rather than a nuisance: we will release per-item label distributions and rater counts, compare principled aggregators (e.g., Dawid–Skene, MACE) to majority vote, and evaluate models with targets that preserve uncertainty (soft labels), following prior work showing non-expert labels can be reliable when carefully aggregated and that rater-competence modeling reduces noise.

Concrete follow-up analyses for the appendix
- Descriptive transparency
  - Per-aspect, per-venue-year histograms of label distributions; proportion of items with ≥3 raters; time-on-task/attention-check pass rates.
  - Inter-annotator agreement with uncertainty: Krippendorff’s alpha and (weighted) kappa per aspect with bootstrap CIs; disagreement entropy by venue/year.
- Aggregation audits
  - Compare majority vote, Dawid–Skene, MACE, and mean-of-scores on a stratified expert-adjudicated subset; report correlations and calibration of each aggregator.
  - Rater competence estimates and item difficulty (variance components via mixed-effects or many-facet Rasch); identify systematically lenient/harsh raters.
- Disagreement-aware evaluation
  - Stratify model performance by disagreement bins (low/medium/high): report AUC/F1 (acceptance), Kendall’s tau/RMSE (aspects), and calibration (ECE/Brier) per bin.
  - Correlate human disagreement with model predictive uncertainty (entropy/variance); plot reliability diagrams conditioned on disagreement.
  - Counterfactual training: hard vs. soft labels (mean/variance) using heteroscedastic losses; report ID/OOD performance and calibration deltas.
- Robustness and drift
  - Year-by-year trend of disagreement and IAA to detect rubric drift; sensitivity of model performance to excluding only the highest-disagreement items vs. down-weighting them.
  - Venue/topic slice analyses: whether disagreement predicts error spikes in specific subfields or venues.
- Reproducibility artifacts
  - Release raw annotations, aggregator outputs, and code to reproduce all metrics; pre-register thresholds for when disagreement leads us to qualify claims (e.g., if high-disagreement items drive >30% of errors, we adopt soft-label training and report stratified metrics as primary).

Sources
- Snow, R., O’Connor, B., Jurafsky, D., & Ng, A. Y. (2008). Cheap and Fast—but is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks — https://aclanthology.org/D08-1027.pdf
- Hovy, D., Berg-Kirkpatrick, T., Vaswani, A., & Hovy, E. (2013). Learning Whom to Trust with MACE — https://aclanthology.org/N13-1132.pdf