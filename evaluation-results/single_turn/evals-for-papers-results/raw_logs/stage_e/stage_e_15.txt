Response to reviewer concern on transparency of annotation disagreement in PeerRead
We appreciate the concern and agree that aggregated labels can mask meaningful variation in annotator judgments. To increase transparency, we will document and analyze annotation disagreement explicitly rather than only reporting majority-vote labels, in line with recent work showing that disagreement carries useful signal and that relying solely on a single “gold” label can obscure it [P5]. We will also structure analyses to separate sources of disagreement across label components when applicable, following decomposition-based recommendations [P3]. Where annotator metadata are available, we will quantify how disagreement patterns vary across annotator subgroups and report these results descriptively; we will not infer causality from such analyses [P1].

What we will report in the revised paper
- Annotation protocol: number of annotators per item, instructions, aggregation rule (e.g., majority vote), tie-breaking policy, and any quality-control filters applied.
- Inter-annotator agreement (IAA): percent agreement, Cohen’s kappa and Krippendorff’s alpha with 95% CIs; confusion matrices; class-conditional agreement and prevalence-bias adjusted agreement.
- Disagreement distributions: per-item soft label distributions and their entropy; histograms and summary statistics; heatmaps of annotator-by-annotator confusion.
- Stratification: agreement/disagreement by venue, year, subfield, and label class to reveal where disagreement is concentrated [P3].
- Annotator effects: variance components from a mixed-effects model (item and annotator random intercepts) to quantify how much variance is attributable to items vs. annotators; if available, subgroup contrasts reported descriptively (e.g., pairwise kappas by subgroup) [P1].
- Sensitivity to aggregation: downstream results recomputed under multiple aggregation schemes (majority vote, Dawid–Skene, simple average soft labels) to show robustness of findings to label consolidation choices [P5].

Concrete follow-up analyses for the appendix
- A1. Agreement tables and plots: full tables of kappa/alpha with CIs; confusion matrices; agreement by class; entropy histograms and kernel density plots.
- A2. Stratified disagreement: agreement metrics and entropy by venue/year/field/label; figure panels showing strata with the highest disagreement [P3].
- A3. Annotator-effect modeling: mixed-effects logistic (or ordinal) models quantifying annotator vs. item variance; caterpillar plots of annotator random effects; pairwise kappas across annotator subgroups when metadata are available [P1].
- A4. Soft labels vs. hard labels: re-run core models using per-item soft label distributions; report accuracy, macro-F1, Brier score, and calibration error; include reliability diagrams [P5].
- A5. Aggregation sensitivity: recompute main results under majority vote, Dawid–Skene, and soft-label training; report deltas and bootstrap CIs for key metrics [P5].
- A6. High- vs. low-disagreement robustness: split test data by entropy quantiles and compare performance; include CIs and effect sizes.
- A7. Re-annotation audit (if raw annotator labels are unavailable): for a stratified sample (e.g., n=200 items), collect 5 independent annotations; report full A1–A6 on this audit set to estimate current disagreement and its impact.

Experiments (falsifiable, to include in the appendix)
1) Soft-label training improves calibration without harming accuracy
- Hypothesis: Training with soft labels (per-item annotator distributions) yields lower Brier score and expected calibration error (ECE) vs. hard majority labels, with comparable accuracy [P5].
- Design: Train identical models under (i) hard majority, (ii) soft labels, (iii) label smoothing baselines. Evaluate on the same test set.
- Metrics: Accuracy, macro-F1, Brier, ECE; statistical tests with paired bootstrap.
- Expected outcome: Soft-label models reduce Brier/ECE; accuracy differences are small. If not observed, we will report a negative result.

2) Aggregation choice changes downstream conclusions for borderline items
- Hypothesis: Results on high-disagreement items are sensitive to the aggregation method (majority vs. Dawid–Skene vs. soft), while low-disagreement items are robust [P5].
- Design: Partition test items by entropy tertile; compute model metrics under each aggregation scheme.
- Metrics: Per-tertile accuracy/F1; absolute metric deltas across aggregation schemes; interaction term in regression of performance on scheme × tertile.
- Expected outcome: Larger deltas in the high-entropy tertile; small deltas in the low-entropy tertile.

3) Decomposition clarifies where disagreement arises
- Hypothesis: Disagreement is concentrated in specific label components (e.g., borderline acceptability vs. topicality), and decomposing labels increases IAA within components [P3].
- Design: Map labels into components (task-specific decomposition); compute per-component IAA and entropy; compare to overall.
- Metrics: Component vs. overall kappa/alpha; entropy reduction; likelihood-ratio test for better model fit with decomposed labels.
- Expected outcome: Higher IAA within components; clearer confusion patterns that align with component boundaries.

4) Annotator subgroup differences are detectable and bounded
- Hypothesis: Pairwise agreement differs across annotator subgroups, but item-level entropy remains the primary driver of disagreement [P1].
- Design: If subgroup metadata exist, compute within- and between-subgroup kappas and fit a mixed-effects model with subgroup random effects.
- Metrics: Kappa differences with CIs; variance decomposition (item vs. annotator vs. subgroup).
- Expected outcome: Statistically significant but smaller subgroup variance than item variance; we will report effect sizes and CIs without causal claims.

Practical commitments
- We will release per-item soft label distributions (or the re-annotation audit set) and the code to reproduce all analyses.
- The main text will summarize key IAA and sensitivity findings; full details, plots, and ablation tables will be placed in the appendix.

Citations
- [P1] Everyone's Voice Matters: Quantifying Annotation Disagreement Using Demographic Information — http://arxiv.org/abs/2301.05036v1
- [P3] A Decomposition-Based Approach for Evaluating and Analyzing Inter-Annotator Disagreement — http://arxiv.org/abs/2206.05446v2
- [P5] Leveraging Annotator Disagreement for Text Classification — http://arxiv.org/abs/2409.17577v1