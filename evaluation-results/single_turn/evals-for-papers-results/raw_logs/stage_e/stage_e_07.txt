Thank you for flagging the missing comparisons to large language models (LLMs). You are right: without LLM baselines, our results do not fully reflect the current state of the art on PeerRead tasks. Our original focus was on widely used, reproducible pre-LLM baselines; however, we agree that principled LLM evaluations are needed for a fair and contemporary comparison.

Planned inclusion (next revision):
- Tasks and splits: We will evaluate on the same PeerRead tasks/splits (e.g., acceptance prediction, aspect/consistency labeling) to ensure comparability with our existing results [Kang et al., 2018].
- Models: (i) API LLM (GPT-4) and (ii) open-source LLMs (e.g., Llama-family models at small/medium scales) to balance performance, cost, and reproducibility.
- Protocol: Standardized prompting (zero-shot and k-shot with k∈{2,4,8}), temperature 0, prompt templates chosen on a held-out dev set only, and reporting of token usage/cost. We will release prompts, evaluation code, and hyperparameters.
- Metrics and statistics: ROC-AUC for acceptance prediction; macro-F1 for multi-class labels; calibration (ECE/Brier); bootstrap 95% CIs and paired significance tests.
- Robustness and fairness: Prompt-sensitivity ablations; in-context-length scaling; stratified analyses by venue/year/field; and a venue/year-out generalization check to partially mitigate potential pretraining leakage. We will explicitly caveat residual leakage risk inherent to LLM pretraining on public web text.

Three concrete, falsifiable experiments:
1) Zero- and few-shot LLM baselines vs. our best non-LLM model
   - Hypothesis: Few-shot GPT-4 and mid-size open LLMs outperform our non-LLM baselines on acceptance prediction and aspect labeling.
   - Variables: Model type (GPT-4 vs. open LLMs), shots (0/2/4/8), prompt template.
   - Metrics: ROC-AUC (acceptance), macro-F1 (labels), ECE; report 95% CIs and paired tests.
   - Expected outcome: Few-shot LLMs improve over non-LLM baselines; zero-shot may be competitive but weaker than few-shot.

2) In-context scaling and prompt sensitivity
   - Hypothesis: Performance increases sublinearly with more shots and is moderately sensitive to template choice.
   - Variables: Shots (0–16), 3 prompt templates per task.
   - Metrics: Performance gain per additional shot; variance across templates; cost-performance trade-off curve.
   - Expected outcome: Diminishing returns after 8 shots; template-induced variance <3–5 F1 points with a robust template.

3) Lightweight fine-tuning vs. prompting on open LLMs
   - Hypothesis: Parameter-efficient tuning (LoRA) on training splits yields better generalization to unseen venues/years than few-shot prompting at similar or lower inference cost.
   - Variables: Tuning regime (LoRA vs. prompt-only), model size (7–13B), venue/year-out evaluation.
   - Metrics: Macro-F1/ROC-AUC, calibration, inference latency/cost; significance testing.
   - Expected outcome: LoRA improves venue/year-out performance and calibration, with transparent computational cost.

We will include these results in the next revision and release all prompts, code, and evaluation scripts to facilitate replication.

Citations
- [1] A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications — https://aclanthology.org/N18-1149.pdf
- [2] GPT-4 Technical Report — https://cdn.openai.com/papers/gpt-4.pdf