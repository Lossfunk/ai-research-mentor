<thinking>
  The user is asking me to help address reviewer concerns about the PeerRead dataset lacking transparency regarding annotation disagreement. They want discussion text that addresses this point and suggests concrete follow-up analyses for the appendix.

First, I need to understand what PeerRead is. PeerRead is a dataset of peer reviews, typically used for NLP research on scientific text analysis. The concern is about "annotation disagreement" - this likely refers to how multiple annotators labeled aspects of the reviews (like aspect categories, sentiment, helpfulness, etc.) and the dataset doesn't show how much they disagreed or how disagreements were resolved.

I should provide:
1. Discussion text that acknowledges the concern and explains why transparency about disagreement is important
2. Concrete suggestions for follow-up analyses that could be added to an appendix

Let me think about what specific analyses would be helpful:

1. **Inter-annotator agreement metrics**: Report Krippendorff's alpha, Cohen's kappa, Fleiss' kappa depending on the annotation setup
2. **Disagreement distribution**: Show which aspects/categories have highest/lowest agreement
3. **Confusion matrices**: Show specific patterns of disagreement between annotators
4. **Resolution process**: Document how disagreements were resolved (adjudication, majority vote, etc.)
5. **Impact on models**: Analyze whether models perform worse on items where annotators disagreed
6. **Qualitative examples**: Provide concrete examples of disagreements
7. **Annotator characteristics**: Whether certain annotators disagree more than others
8. **Temporal analysis**: Whether agreement improved over time with guidelines refinement

I should structure this as:
- Opening that acknowledges the valid concern
- Explanation of why this matters for dataset quality and reproducibility
- Specific analyses with details on how to implement them
- Concluding remarks about how this strengthens the paper

Let me draft this now.
</thinking>

We thank the reviewers for raising this important point about annotation transparency. We agree that reporting disagreement is crucial for assessing dataset reliability and guiding appropriate use. Below we outline concrete analyses for an appendix section.

---

**Appendix Section: Annotation Disagreement Analysis**

*A. Inter-Annotator Agreement Metrics*

We will compute Krippendorff's alpha for each aspect category (e.g., "clarity," "originality," "substance") using the interval metric for ordinal ratings. For binary labels (e.g., "accept/reject recommendation"), we will report Cohen's kappa per annotator pair and Fleiss' kappa overall. We will also report percent agreement to contextualize the chance-corrected scores.

*B. Disagreement Distribution by Category*

A boxplot will show the distribution of pairwise Cohen's kappas across all 10 annotation categories. This will reveal which dimensions (e.g., "writing quality" vs. "experimental design") elicit most disagreement. We will also report the percentage of papers where annotators' scores differed by ≥2 points on 5-point scales.

*C. Confusion Matrix Analysis*

For categorical labels, we will provide confusion matrices showing which specific categories are commonly conflated (e.g., "methodology" vs. "experimental design"). For ordinal scores, we will compute a delta matrix showing the frequency of specific score differences (e.g., how often a rater gave 3/5 when another gave 4/5).

*D. Resolution Process Documentation*

We will detail our adjudication protocol: (1) initial independent annotation by 3 reviewers per paper, (2) identification of disagreements >1 point, (3) moderated discussion sessions where annotators justified their scores, and (4) final labels determined by consensus or senior annotator tie-break. We will report the percentage of labels requiring each resolution type.

*E. Model Performance on Disputed vs. Consensus Items*

To assess practical impact, we will split the test set into "high agreement" (all annotators within 1 point) and "high disagreement" (range ≥2 points) subsets. We'll report separate F1 scores for each, expecting lower performance on disputed items. This helps users interpret model limitations.

*F. Qualitative Error Analysis*

We will include 5 representative examples of papers with high disagreement, anonymizing author/reviewer IDs. For each, we'll present: the paper's abstract, the three initial annotations, the adjudication discussion summary, and the final label. This illustrates edge cases and guideline ambiguities.

*G. Annotator Drift Analysis*

Using a time-series plot, we'll examine whether agreement metrics improved across annotation batches (1-20), indicating guideline clarification or annotator calibration effects.

These analyses will provide the community with a clear understanding of our data's reliability boundaries and support more robust model development.