<thinking>
  The user wants a research plan for human-centered evaluation of AI legal assistants. This is a complex request that requires careful consideration of:

1. Research methodology (human-centered evaluation)
2. Specific steps for IRB/ethics approval
3. Concrete milestones
4. The domain-specific context of legal AI

I need to structure this comprehensively but practically. Let me think about what a good research plan would include:

**Key Components:**
- Research questions/objectives
- Study design (methodology)
- Participants/recruitment
- Data collection methods
- IRB/ethics considerations
- Milestones and timeline
- Risk mitigation
- Dissemination plan

**Human-centered evaluation specifics:**
- Need to involve actual users (lawyers, paralegals, clients)
- Should assess usability, trust, fairness, impact on work
- Should consider both quantitative and qualitative measures
- Need to address real-world usage scenarios

**IRB/Ethics specifics for legal AI:**
- Confidentiality of legal information is paramount
- Attorney-client privilege concerns
- Potential for AI to give wrong legal advice - risk to participants
- Power dynamics (access to justice issues)
- Bias and fairness concerns
- Data security requirements

**Milestones:**
- Should be concrete, measurable, time-bound
- Cover preparation, execution, analysis, dissemination

Let me draft a comprehensive plan that addresses all these aspects.

Structure:
1. Executive Summary/Overview
2. Research Questions & Objectives
3. Study Design
4. Participants & Recruitment
5. IRB & Ethics Framework
6. Data Collection & Instruments
7. Milestones & Timeline
8. Risk Mitigation
9. Dissemination & Impact

I should make this practical and actionable, with specific details about IRB steps and concrete milestones.
</thinking>

# Research Plan: Human-Centered Evaluation of AI Legal Assistants

## 1. Research Overview & Objectives

**Primary Research Question**: How do AI legal assistants impact practitioner effectiveness, client outcomes, and access to justice while maintaining ethical and professional standards?

**Specific Objectives**:
- Assess usability and trust in AI-assisted legal workflows
- Measure impact on legal decision quality and efficiency
- Identify bias, fairness, and accountability gaps
- Evaluate effects on attorney-client relationships
- Develop evidence-based guidelines for responsible deployment

---

## 2. Study Design: Mixed-Methods Comparative Evaluation

**Design**: 3-phase longitudinal study with counterbalanced crossover design

- **Phase 1 (Lab-based)**: Controlled tasks with law students and junior associates (n=40) to establish baseline metrics
- **Phase 2 (Field Study)**: 12-week embedded deployment with practicing attorneys (n=25) at partner law firms
- **Phase 3 (Community Study)**: 8-week pilot with legal aid organizations serving self-represented litigants (n=30)

**Conditions**: 
- Control: Traditional legal research/workflow tools
- Treatment: AI legal assistant (e.g., retrieval-augmented generation system with case law database)

---

## 3. IRB & Ethics Framework: Step-by-Step Compliance

### **Pre-Submission Requirements**

**Step 1: Jurisdiction-Specific Legal Ethics Review**
- Submit protocol for review by state bar association's ethics committee
- Obtain formal opinion on attorney-client privilege preservation
- Consult with legal ethics scholars on unauthorized practice of law (UPL) boundaries

**Step 2: Data Security Protocol Development**
- Establish isolated, encrypted research environment (SOC 2 Type II compliant)
- Implement "no real client data" rule: use synthetic but realistic case vignettes
- Create data use agreements with partner firms specifying:
  - No client-identifiable information enters study systems
  - All prompts/de-identified at source
  - Right to withdraw data without penalty

**Step 3: Risk-Benefit Analysis Documentation**
- **Minimal Risk Category Justification**: Participants are legal professionals evaluating tools, not receiving legal advice
- **Potential Risk**: AI-generated errors could influence professional judgment
- **Mitigation**: Mandatory "AI suggestion review" training; expert oversight panel; all outputs marked as "for research purposes only"

### **IRB Application Components**

**Exempt vs. Expedited Determination**: 
- File for **Expedited Review** (Category 7: Educational settings) rather than exempt status due to:
  - Professional vulnerability of junior attorneys
  - Indirect impact on client services in field study
  - Collection of video/audio screen recordings

**Required Documentation**:
1. **Informed Consent (Layered Approach)**:
   - Tier 1: General study overview for all participants
   - Tier 2: Detailed technical monitoring disclosure for screen recording subset
   - Tier 3: Firm-level MOU addressing partnership dynamics and non-retaliation clauses

2. **Privacy & Confidentiality Plan**:
   - Certificate of Confidentiality application (NIH) for Phase 3 (vulnerable populations)
   - De-identification schema: Replace firm names with codes; scrub geographic identifiers; aggregate performance data
   - Data retention: Destroy raw recordings within 90 days; retain only coded transcripts

3. **Vulnerable Population Protections** (Phase 3):
   - Community advisory board with legal aid clients
   - Simplified consent forms (6th-grade reading level)
   - Compensation that doesn't coerce participation ($50 gift card, not cash)
   - On-site social worker support during interviews

### **Post-Approval Monitoring**
- Quarterly IRB progress reports highlighting any adverse events
- **Adverse Event Definition**: Participant reports relying on AI output in actual case; breach of data security; participant distress during interview
- Pre-planned stopping rules: 3+ adverse events triggers full study pause

---

## 4. Participants & Recruitment Strategy

| Phase | Population | Recruitment Method | Compensation | Sample Size |
|-------|------------|-------------------|--------------|-------------|
| 1 | Law students, 2nd-3rd year | Email listservs, clinic directors | $25/hr | 40 |
| 2 | Associates (0-5 yrs exp) at mid-size firms | Bar association partnerships, professional networks | $500 stipend + CLE credit | 25 |
| 3 | Legal aid clients (pro se litigants) | Court self-help centers, community orgs | $50 gift card | 30 |

**Inclusion Criteria**: 
- Phase 2: Must have malpractice insurance; partner approval
- Phase 3: Active case in housing or family court; English or Spanish speaking

---

## 5. Data Collection Instruments & Metrics

### **Quantitative Measures**
- **Task Performance**: Time-to-completion, error rate (expert-coded), citation accuracy
- **System Usability Scale (SUS)**: Administered bi-weekly
- **Trust Scale**: Adapted from AI Trust Inventory (6-point Likert)
- **Cognitive Load**: NASA-TLX after each task
- **Algorithmic Fairness Perceptions**: Custom 15-item scale measuring bias detection

### **Qualitative Methods**
- **Think-Aloud Protocols**: 20 sessions (screen recording + audio)
- **Semi-Structured Interviews**: 45 min at weeks 4, 8, 12
- **Diary Studies**: Weekly voice memos (5 min) on "critical incidents"
- **Focus Groups**: 3 groups of 8 participants (Phase 2) on workflow integration

### **Client Outcome Proxies** (Phase 3)
- Document quality scores (rubric-based)
- Court acceptance rates of AI-assisted filings
- Participant self-efficacy scales (pre/post)

---

## 6. Concrete Milestones & Timeline (12-Month Study)

### **Months 1-2: Setup & Approval**
- **Milestone 1.1**: IRB protocol submitted (Day 15)
- **Milestone 1.2**: Legal ethics opinion obtained (Day 30)
- **Milestone 1.3**: Data security infrastructure audited (Day 45)
- **Milestone 1.4**: IRB approval received (Day 60)
- **Deliverable**: Approved protocol, signed MOUs with 3 partner firms

### **Months 3-4: Phase 1 Execution**
- **Milestone 2.1**: 40 participants recruited and consented (Day 75)
- **Milestone 2.2**: Baseline usability testing complete (Day 90)
- **Milestone 2.3**: Counterbalanced tasks administered (Day 105)
- **Deliverable**: Preliminary usability report; refined instruments

### **Months 5-7: Phase 2 Field Deployment**
- **Milestone 3.1**: 25 attorneys onboarded and trained (Day 120)
- **Milestone 3.2**: Week 4 checkpoint: 80% diary study compliance (Day 135)
- **Milestone 3.3**: Mid-point interviews complete (Day 150)
- **Milestone 3.4**: Week 12 data collection closed (Day 180)
- **Deliverable**: De-identified dataset; weekly compliance dashboards

### **Months 8-9: Phase 3 Community Study**
- **Milestone 4.1**: Community advisory board convened (Day 185)
- **Milestone 4.2**: 30 participants recruited via trusted partners (Day 200)
- **Milestone 4.3**: Post-intervention self-efficacy data collected (Day 225)
- **Deliverable**: Comparative analysis across all three phases

### **Months 10-12: Analysis & Dissemination**
- **Milestone 5.1**: Quantitative analysis complete (Day 240)
- **Milestone 5.2**: Thematic saturation reached in qualitative coding (Day 255)
- **Milestone 5.3**: Preliminary findings shared with participant advisory panel (Day 270)
- **Milestone 5.4**: Manuscript submitted to peer-reviewed journal (Day 300)
- **Milestone 5.5**: Practitioner guidelines published (Day 330)
- **Deliverable**: 2 publications; open dataset; ethics framework toolkit

---

## 7. Risk Mitigation Matrix

| Risk Level | Scenario | Mitigation | Contingency |
|------------|----------|------------|-------------|
| **High** | AI suggests incorrect legal precedent | All outputs reviewed by supervising attorney; disclaimer labels; no real case use | Immediate system audit; pause if pattern detected |
| **High** | Data breach of de-identified case vignettes | Air-gapped environment; no cloud storage; 2FA required | Incident response plan; IRB notification within 24hrs |
| **Medium** | Participant feels pressured by firm to join | Firm MOU includes non-retaliation; independent consent process | Ethics hotline for participants; allow anonymous withdrawal |
| **Medium** | Coercion in legal aid population | Community partner facilitates consent; non-cash compensation | Social worker advocates for participant concerns |
| **Low** | Technical failure during field study | 24/7 helpdesk; backup manual processes | Extend study timeline; exclude failed sessions from analysis |

---

## 8. Dissemination & Impact Plan

**Academic Outputs**:
- Submit to *Journal of Empirical Legal Studies* (quantitative) and *Harvard Journal of Law & Technology* (policy)
- Present at FAT* Conference and Legal Tech Conference

**Practice-Facing**:
- **Plain-language ethics guidelines** for legal AI vendors (Creative Commons license)
- **CLE module** on AI oversight for bar associations
- **Open dataset** of de-identified usability metrics on Harvard Dataverse

**Community**:
- Return of findings to Phase 3 participants via community meeting
- Policy brief for state access to justice commissions

---

## 9. Budget Snapshot (Key Items)

- IRB fees & ethics consultation: $5,000
- Data security infrastructure: $15,000
- Participant compensation: $28,500
- Research assistant (50% FTE): $30,000
- Transcription & coding software: $3,000
- **Total**: ~$81,500 (excludes PI salary)

---

## 10. Success Criteria

- **Scientific**: 80% retention rate; 3+ significant findings on trust/bias dimensions
- **Ethical**: Zero unaddressed adverse events; 100% IRB protocol compliance
- **Impact**: Guidelines adopted by at least 1 state bar; dataset accessed by 5+ research teams

This plan balances methodological rigor with ethical safeguards tailored to the legal profession's unique confidentiality and professional responsibility requirements.