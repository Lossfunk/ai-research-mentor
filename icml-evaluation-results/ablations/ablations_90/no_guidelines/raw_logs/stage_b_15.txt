<thinking>
  The user is asking about how to check if their research idea on auditing AI hiring platforms is novel and how to scope it responsibly. This is a common concern in research - wanting to ensure you're not just replicating existing work. Let me think about how to help them systematically approach this.

Key aspects to cover:
1. How to conduct a thorough literature review
2. Specific search strategies for this topic
3. How to identify gaps and opportunities for novelty
4. How to scope the project appropriately
5. What aspects might still be novel despite the topic being "hot"

I should provide concrete, actionable steps rather than just general advice. The user seems to be in the early stages of research planning, so I should be encouraging but also realistic.

Let me structure my response around:
- Understanding the current landscape
- Identifying potential novelty dimensions
- Scoping strategies
- Practical next steps

I should mention specific databases, search terms, and frameworks they can use. I should also help them think about different angles - technical, social, legal, etc.

The topic of auditing AI hiring platforms is indeed active, but there are still many unexplored dimensions. I need to help them see how to find their niche.

Let me avoid being too generic and give specific examples relevant to AI hiring systems.
</thinking>

Your concern is valid—AI hiring is a well-studied area, but there are still meaningful gaps. Here’s a systematic way to assess novelty and scope:

### 1. **Map the Existing Audit Landscape**
First, conduct a structured literature sweep beyond just "AI hiring bias." Search for:
- **Specific audit targets**: Resume screeners, video interview analyzers, gamified assessments, chatbot recruiters, background check aggregators
- **Audit methodologies**: Shadow audits (creating fake profiles), reverse engineering, adversarial testing, fairness metric benchmarking, stakeholder interviews
- **Key papers**: Look for *Procaccia & Tucker (2020)*, *Raghavan et al. (2020)*, *Ajunwa (2021)*, *Bogen & Rieke (2018)*, and recent FAccT/AAAI conference proceedings

**Pro tip**: Use Google Scholar with queries like:
```
"audit" "hiring" "platform" -"financial" -"medical"
"algorithmic impact assessment" recruitment
"fairness" "applicant tracking system" "evaluation"
```

### 2. **Identify Novelty Dimensions**
Even if the core idea isn’t new, these angles might be:

| Dimension | Overdone | Potentially Novel |
|-----------|----------|-------------------|
| **Population** | US-based, tech roles | Gig workers, global south, public sector, non-English languages |
| **Metric** | Demographic parity, equal opportunity | Candidate experience, accessibility (ADA compliance), economic opportunity shift |
| **Method** | Synthetic data audits | Longitudinal real-world deployment, participatory audits with job seekers, legal compliance checks (e.g., NYC LL 144) |
| **System type** | Resume parsers | Multi-stage pipelines, vendor black boxes, "human-in-the-loop" interfaces |
| **Outcome** | Binary hire/no-hire | Salary negotiation, promotion trajectories, ghosting rates |

### 3. **Scope Responsibly with a "Minimum Viable Audit"**
Define boundaries to avoid boiling the ocean:
- **Single platform, single feature**: Audit one vendor’s video interview "tone analysis" rather than the whole system
- **Geographic constraint**: Focus on one city’s public sector hiring (e.g., NYC’s AEDT law compliance)
- **Time-bound**: Audit before/after a major algorithm update or policy change
- **Stakeholder-limited**: Partner with 2-3 advocacy groups, not entire industry

### 4. **Check for "Audit Fatigue" Signals**
Contact authors of recent papers and ask: *"What did you wish you could’ve studied but couldn’t?"* This reveals live gaps. Also scan:
- Recent EEOC litigation dockets (what’s being missed?)
- Vendor release notes (what new features lack scrutiny?)
- Subreddit r/recruitinghell (what complaints are unaddressed?)

### 5. **Pre-Registration & Contribution Statement**
Write a one-page pre-registration doc answering:
- *What specific claim am I testing?* (e.g., "Vendor X’s system disadvantages candidates with non-linear career paths")
- *What data would disprove novelty?* (If I find X, I’ll pivot to Y)
- *Who benefits from this audit?* (If answer is vague, scope is too broad)

**Bottom line**: The topic is crowded, but *most audits are snapshots*. Longitudinal, participatory, or jurisdiction-specific audits that engage job seekers as co-investigators remain underexplored. If you can articulate *whose problem* your audit solves that prior work hasn’t, you’re likely in novel territory.