<thinking>
  The user is asking for a rebuttal paragraph to address reviewer concerns that PeerRead models learn spurious lexical cues instead of review-worthy content. They want diagnostics or post-hoc analyses to support the response.

First, I need to understand what PeerRead is. PeerRead is a dataset and task for predicting the accept/reject decision of scientific papers based on their text (abstracts, reviews, etc.). The concern is that models might be learning superficial lexical patterns (like specific words or phrases) rather than deeper semantic content that actually matters for review decisions.

This is a common concern in NLP - models picking up on spurious correlations rather than true signal. The reviewers are essentially saying "your model is just learning to detect certain keywords that happen to correlate with acceptance, not actually understanding what makes a paper good."

To address this, I need to:
1. Acknowledge the concern (it's valid)
2. Propose specific diagnostic analyses that could test whether the model is using spurious cues
3. Suggest post-hoc interpretability methods
4. Potentially propose new experiments or controls

Let me think about specific diagnostics:

1. **Lexical ablation studies**: Remove or mask suspected spurious tokens and see if performance drops
2. **Control tasks**: Train on shuffled or randomized text to see if model still learns
3. **Attention analysis**: Visualize what parts of text the model attends to
4. **Feature importance**: Use LIME/SHAP to identify important features
5. **Adversarial testing**: Construct examples that have spurious cues but opposite labels
6. **Correlation analysis**: Check correlation between specific lexical features and predictions vs ground truth
7. **Domain shift evaluation**: Test on different venues/time periods to see if lexical patterns generalize
8. **Human evaluation**: Have humans evaluate papers with/without spurious cues that model predicts differently
9. **Input perturbation**: Systematically replace suspected spurious words with synonyms and observe prediction changes
10. **Probing classifiers**: Train simple linear models on lexical features alone to see how much they can predict

I should structure the rebuttal to be:
- Professional and constructive
- Acknowledge the validity of the concern
- Propose concrete, actionable analyses
- Show that these analyses would actually address the concern
- Be specific about what each analysis would reveal

Let me draft this:

Start by acknowledging the concern is important and valid. Then propose a multi-pronged approach:

1. **Lexical ablation and perturbation analysis** - systematically remove or replace high-frequency tokens that correlate with acceptance/rejection to measure performance degradation
2. **Control experiments** - train on bag-of-words vs full text, or on shuffled text, to isolate lexical cue effects
3. **Interpretability analysis** - use attention mechanisms and feature attribution methods (LIME/SHAP) to visualize what models actually use
4. **Adversarial evaluation** - construct synthetic examples that contain spurious cues but opposite semantic content
5. **Temporal/venue generalization** - test on held-out venues or time periods to check if learned patterns are robust
6. **Human baseline comparison** - compare model's focus areas with human reviewer emphasis

I should make this sound like a confident, well-thought-out response that shows the authors understand the concern and have concrete plans to address it.

Let me write the paragraph now.
</thinking>

We appreciate the reviewers' valid concern regarding potential reliance on spurious lexical cues. To directly address this, we propose a comprehensive diagnostic suite that disentangles lexical artifacts from semantic understanding:

**First**, we will conduct systematic **lexical ablation studies** where we identify and mask high-frequency tokens that correlate with acceptance (e.g., "novel," "state-of-the-art") using frequency-controlled perturbations. By measuring performance degradation when these tokens are replaced with semantic equivalents (e.g., "new" for "novel") versus random substitutions, we can quantify whether the model exploits surface-level patterns or robust semantic features. We'll also perform **input perturbation tests** by injecting spurious cue words into rejected papers and vice versa, tracking prediction flipping rates.

**Second**, we'll implement **control task experiments** training our architecture on bag-of-words representations and shuffled text where syntactic structure is destroyed but lexical statistics preserved. If performance remains high on these degraded inputs, it would confirm spurious learning; significant drops would validate that our model leverages meaningful compositionality.

**Third**, we will apply **post-hoc interpretability methods** including attention visualization and SHAP value analysis to identify which textual spans most influence predictions. We'll specifically compare attribution maps between original and lexically-perturbed examples to detect instability indicative of cue exploitation. These will be validated against **human reviewer annotations** from the PeerRead corpus to align model focus with actual review commentary.

**Finally**, we'll perform **cross-venue generalization tests** training on one conference and evaluating on others (e.g., train on ICLR, test on NeurIPS). Spurious lexical patterns are venue-specific, while review-worthy content generalizes; robust cross-venue performance would strongly argue against superficial cue learning. These analyses will be included in the final manuscript to provide transparent evidence of model behavior.