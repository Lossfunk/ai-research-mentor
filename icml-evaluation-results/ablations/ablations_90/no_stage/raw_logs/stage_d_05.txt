<thinking>
  <thinking>
The user wants a two-phase replication plan with specific technical requirements. This is a concrete proposal that needs critical evaluation. Let me assess:

1. Re-implementing Hierarchical Transformer baseline - This is reasonable if the paper is solid and reproducible
2. Low-parameter alternative via grouped pooling - This could work but needs scrutiny
3. Power analysis - Good practice, but often done poorly
4. Avoiding double-counting correlated improvements - Critical methodological concern

Key issues to address:
- Is the hierarchical transformer actually a strong baseline worth replicating?
- Grouped pooling as "low-parameter alternative" sounds vague - what's the actual hypothesis?
- Power analysis for deep learning is often meaningless without proper effect size estimates
- The double-counting issue suggests they might be planning multiple comparisons without correction

I need to be direct about weaknesses while giving a clear path forward.
</thinking>

This plan has merit but needs significant refinement before you waste time on implementation.

**The core issue:** "Grouped pooling" is too vague to be a research contribution. If you mean replacing hierarchical attention with something like grouped convolutions, what's your actual hypothesis? That hierarchy is overparameterized? That local patterns suffice? Be specific.

**Power analysis in deep learning is mostly theater** unless you have pilot data. Effect sizes in transformers vary wildly with initialization and hyperparameters. Instead, run the smallest experiment that could falsify your hypothesis (e.g., 3 seeds, 2 datasets) and use that to estimate variance.

**Double-counting:** This suggests you're planning multiple metrics without correction. Pick ONE primary metric before you start. Everything else is exploratory. If you must report multiple, use Bonferroni or control FDR.

**Revised plan:**

**Phase 1:** Re-implement the baseline, but first verify the original code runs and results are reproducible. If not, pick a different paper. Spend 2 weeks max here.

**Phase 2:** Define your alternative *mechanistically*. Example: "Replace hierarchical attention with grouped mean-pooling (k=4 groups) followed by lightweight cross-group attention." This is testable. Run ablations on group size vs. performance.

**Skip formal power analysis.** Instead, establish a variance baseline in Phase 1, then use that to determine seed count for Phase 2. Report confidence intervals, not p-values.

**One metric.** Choose between accuracy, F1, or whatever the original paper used. Stick to it.
</thinking>