<thinking>
  <thinking>
The user wants a benchmarking plan for multilingual fact-checking with limited GPU resources. This is a concrete, specific request. I need to:
1. Give my honest assessment of the core challenges
2. Provide a focused, realistic plan given GPU constraints
3. Include expected outputs and sanity checks
4. Keep it concise and actionable

Key constraints: limited GPU access means I should prioritize smaller models, efficient evaluation, and avoid full fine-tuning. Multilingual fact-checking needs representative languages and datasets.

My assessment: This is feasible but requires ruthless prioritization. The biggest risk is scope creep - trying to evaluate too many models/languages and getting no clear signal.
</thinking>

This is feasible but requires ruthless prioritization. With limited GPUs, you can't evaluate everything, so you must choose models and languages strategically.

**Core Plan (2-3 week timeline):**

**Models:** Pick 3-4 open-weights models that fit your VRAM: Llama-3-8B, Mistral-7B, Gemma-7B, and one multilingual specialist like BLOOM-7B. Skip the 70B models entirely - they'll eat your GPU time for marginal gains.

**Languages:** Select 3 languages with very different characteristics: English (high-resource), Spanish (medium), and one low-resource language like Swahili or Hindi. This gives you coverage without exploding your evaluation matrix.

**Dataset:** Use CLAIMBUSTER or MultiFC for English/Spanish. For your third language, you'll likely need to translate 200-300 claims manually or use a small parallel subset. Don't build a new dataset - that's a separate project.

**Evaluation Protocol:**
- Zero-shot prompting only. No fine-tuning with your GPU constraints.
- Use a simple prompt: "Is this claim true/false/unverified? Explain in one sentence."
- Score with accuracy and macro-F1. Track "refusal rate" (when models say they can't verify).

**Expected Outputs:**
- Table: Model Ã— Language performance (accuracy, F1, refusal rate)
- Error analysis: 20 examples where all models failed vs. where they disagreed
- Cost analysis: GPU hours per model-language pair

**Sanity Checks:**
- Does performance correlate with model size? If not, your evaluation is broken.
- Are refusal rates >30%? Your prompt is too hard or claims are unverifiable.
- Is English performance >15% better than other languages? That's expected, but if it's >40%, your non-English data has issues.

**What to skip:** Full fine-tuning, human evaluation, or building custom multilingual datasets. Focus on getting clean numbers from existing resources.
</thinking>