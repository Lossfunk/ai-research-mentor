<thinking>
  <thinking>
The user wants a plan for privacy-preserving mental health chatbots using only open-source tools. They want experiments, constraints, and checkpoints. This is a specific request for a structured plan, so I should provide concrete, actionable guidance.

Key considerations:
- Privacy-preserving means data minimization, local processing, encryption, anonymization
- Mental health domain is high-stakes: requires safety, accuracy, empathy
- Open-source only: no proprietary APIs, must self-host
- Need experiments to validate privacy claims and therapeutic utility
- Need constraints (technical, legal, ethical)
- Need checkpoints for progress

I should be critical about what's actually feasible and what are common pitfalls. The biggest risks are: false sense of privacy, inadequate safety measures, and using open-source as a checkbox without understanding limitations.

Let me structure this concisely but with substance.
</thinking>

This is a high-risk project where "privacy-preserving" and "mental health" create serious tensions. Here's a focused plan:

**Core Constraints (Non-Negotiable)**
- All data stays on-device or in user-controlled infrastructure (no third-party APIs)
- No fine-tuning on real patient data without IRB and legal review
- Must implement safety guardrails that exceed standard content filters
- Open-source models only: Llama 2/3, Mistral, etc. (no GPT-4 API)

**Key Experiments (Prioritize #1, #2)**
1. **Privacy Leakage Test**: Prompt the chatbot 100x to reveal its system prompt, training data snippets, or prior conversation history. Measure success rate. If >0%, architecture fails.
2. **Safety Efficacy**: Create 50 crisis scenarios (suicidal ideation, self-harm, abuse). Test if bot responds with appropriate resources vs. harmful advice. Need >95% correct escalation.
3. **Utility vs. Baseline**: Compare to non-private baseline on 200 therapy-relevant prompts (empathy, CBT techniques, reflection quality). If gap >15%, privacy gains aren't worth utility loss.

**Checkpoints (Kill Criteria)**
- **Week 2**: If you can't run a 7B model locally with <5s latency, scope is too ambitious.
- **Week 4**: If Experiment #1 shows any data leakage, stop and redesign architecture. Don't proceed to safety testing.
- **Week 6**: If Experiment #2 shows <90% safe handling of crisis scenarios, this cannot be deployed for real users. Period.

**Critical Warning**: Most "privacy-preserving" chatbots fail Experiment #1 because developers underestimate prompt injection and model memorization. If you're using LoRA fine-tuning on any conversational data, you're likely creating a privacy risk, not solving one. Start with RAG over local documents before considering fine-tuning.
</thinking>