<thinking>
  <thinking>
The user wants me to design an intervention experiment for a Hierarchical Transformer, replacing hierarchical pooling with flat mean pooling. They want specific details about baselines, datasets, metrics, and statistical tests.

Key points to address:
1. The intervention: replacing hierarchical pooling with flat mean pooling
2. Baselines: need to specify what to compare against
3. Datasets: which ones are appropriate
4. Metrics: primary and secondary
5. Statistical tests: which ones to use

I need to be critical and direct. Let me think about whether this is a good idea and what the pitfalls might be.

First, hierarchical pooling in transformers typically aggregates information at different levels (e.g., token -> segment -> document). Flat mean pooling would just average everything at once. This could lose important structural information.

My honest assessment: This is likely a bad idea for most tasks that benefit from hierarchy, but could be interesting for specific cases where hierarchy doesn't matter. I need to be direct about this.

I should specify:
- Baselines: original hierarchical model, other pooling variants
- Datasets: ones where hierarchy matters vs. ones where it might not
- Metrics: standard task metrics plus efficiency metrics
- Statistical tests: appropriate for comparing multiple models

Let me structure this concisely but critically.
</thinking>

This is a weak intervention for most tasks. Hierarchical pooling exists for a reason—it preserves structural information that flat pooling destroys. You'll likely see performance drops, especially on tasks requiring document-level understanding.

**Baselines:**
- Original hierarchical transformer (your main comparison)
- Flat max pooling (not just mean)
- Weighted flat pooling (learned attention over all tokens)
- Standard non-hierarchical transformer (e.g., BERT) for reference

**Datasets (split by hierarchy sensitivity):**
- *Hierarchy matters*: SciDocs (document classification), Hyperpartisan News Detection, Multi-News summarization
- *Hierarchy ambiguous*: GLUE (average across tasks), IMDB sentiment
- *Control*: Randomly shuffled versions of hierarchical datasets to test if structure is actually used

**Metrics:**
- **Primary**: Task-specific (F1, ROUGE, accuracy)
- **Secondary**: 
  - Parameter count and FLOPs (flat pooling should be slightly cheaper)
  - Attention pattern analysis: measure how much attention weight concentrates at hierarchical boundaries vs. flat model
  - Ablate on document length: plot performance vs. input length to see where hierarchy helps most

**Statistical Tests:**
- Paired bootstrap resampling (10k samples) for each dataset—gives confidence intervals on performance differences
- Friedman test across multiple datasets if comparing >2 variants, followed by Nemenyi post-hoc
- Report effect sizes (Cohen's d), not just p-values

**Critical warning**: If you don't see significant drops on hierarchy-sensitive tasks, your original hierarchical pooling was probably broken. Check that first.
</thinking>