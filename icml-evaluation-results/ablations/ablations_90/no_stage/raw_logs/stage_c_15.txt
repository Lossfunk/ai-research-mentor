<thinking>
  <thinking>
The user wants a research plan for human-centered evaluation of AI legal assistants. This is a specific request with context, so I should provide concrete, actionable feedback. I need to:
1. Assess what's missing or weak in typical approaches
2. Give specific IRB/ethics considerations
3. Provide concrete milestones, not vague phases
4. Keep it focused and critical where needed

Key issues I often see: vague recruitment plans, inadequate risk assessment for legal contexts, weak baseline comparisons, and ethics plans that wouldn't pass IRB scrutiny. I should address these directly.
</thinking>

This research plan needs to be tighter, especially on ethics. Here's a concrete framework:

**IRB/Ethics (Critical Path)**
- **Risk classification**: This is likely "minimal risk" but could be "more than minimal" if you're giving legal advice. Clarify: are you evaluating *outputs* or *advice*? The latter requires legal review board + IRB.
- **Recruitment**: "Law students" is too vague. Specify: 1L vs 3L (different skill levels), compensation ($30-50/hr is standard), and how you'll avoid coercion (no recruiting from your own class).
- **Data handling**: Legal queries contain privileged information. You need: (1) simulated cases only, or (2) actual IRB protocol for handling potentially privileged data with a data safety monitoring board.
- **Deception**: If you're not telling participants it's AI-generated, that's deception. Requires full board review and debriefing protocol.

**Milestones (8-week timeline)**
- Week 1: Finalize IRB protocol (not "submit" - finalize). Include legal ethics review letter.
- Week 2: Pilot test case scenarios with 2 lawyers. Revise based on realism check.
- Week 3-4: Recruit 24 participants (power analysis: 12 per group for 80% power, medium effect). Run sessions.
- Week 5: Transcribe and code for accuracy, usefulness, trust markers. Use established frameworks (e.g., NASA-TLX for cognitive load).
- Week 6: Comparative analysis against baseline (human lawyer, not just "no AI"). This is crucial - without a human baseline, you can't claim anything about AI performance.
- Week 7: Draft results. Check for any inadvertent legal advice given during study - document and mitigate.
- Week 8: IRB continuing review submission if needed.

**What's Missing**
You didn't specify your evaluation metrics. "Human-centered" is meaningless without operationalization. Are you measuring: task completion time? accuracy of legal analysis? user trust? Pick 2-3 primary outcomes, not a kitchen sink.

Also, your plan needs a legal consultant on the research team. IRBs will flag this immediately if you're in a law school or working with real legal scenarios.
</thinking>