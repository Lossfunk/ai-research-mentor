<thinking>
The user is asking about anticipating reviewer concerns for the PeerRead dataset's "probably-rejected" labels. This is a specific methodology question about a known dataset in NLP/ML research. I need to:

1. Identify the core validity concerns reviewers will raise
2. Be direct and critical about these weaknesses
3. Propose concrete, actionable mitigations for the discussion section

Key concerns: proxy label noise, selection bias, temporal issues, confounding factors. I should be specific and not soften the criticism.
</thinking>

Reviewers will attack the "probably-rejected" labels as **fundamentally noisy proxies** that conflate multiple failure modes. Lead with this limitation, don't bury it.

**Core concerns to address:**

1. **Selection bias**: You're only seeing papers authors *chose* to submit to specific venues. A paper "rejected" from ACL might be accepted at EMNLP or NAACL. Your negative set is contaminated with false positives.

2. **Temporal ambiguity**: You can'...