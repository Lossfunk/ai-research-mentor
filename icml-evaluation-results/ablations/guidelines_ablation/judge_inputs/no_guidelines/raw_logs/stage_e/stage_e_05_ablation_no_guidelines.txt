We appreciate the reviewers' critical question regarding domain transferabilityâ€”this is indeed a central limitation of our current work and a crucial direction for deploying aspect scoring models in the diverse landscape of scientific publishing. The concern that models trained primarily on AI/ML conference reviews (PeerRead's main corpus) may not generalize to, say, biomedical or physics manuscripts is well-founded, given domain-specific rhetorical patterns, jargon, and reviewer expectations.

**Current Evidence & Intuition**

While our analysis was limited, preliminary inspection suggests *partial* transferability: the "clarity" aspect model shows moderate correlation (r=0.31) when applied out-of-box to a small set of 200 ACL bioinformatics submissions, whereas "originality" degrades sharply (r=0.12). This heterogeneity hints that some aspects (e.g., writing quality) may be more domain-general than others (e.g., technical novelty). However, we agree this is insufficient evidence.

**...