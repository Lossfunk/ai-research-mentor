# Two-Phase Replication Plan: Hierarchical Transformer & Low-Parameter Alternative

Since no specific model was attached, I'll outline a general replicable framework based on prominent hierarchical transformers (e.g., H-Transformer-1D, Hourglass, Swin). Adjust specifics to match your target paper.

---

## **Phase 1: Baseline Re-Implementation**

### **1.1 Model Specification & Diagnosis**
First, **clearly define the baseline architecture**:

- **Hierarchy mechanism**: Identify the downsampling/upsampling strategy (e.g., pooling, strided convolutions, attention subsampling)
- **Token merging**: How are sequences shortened at each level? (e.g., local attention windows, pooling after blocks)
- **Skip connections**: Identify residual pathways and cross-scale connections
- **Positional encoding**: How is position handled across scales?
- **Task heads**: Classification, generation, or dense prediction?

**Action**: Create a **parameter budget table** before coding:
| Component | Params (tar...