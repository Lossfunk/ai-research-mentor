{
  "scenario": {
    "scenario_id": "nlp_negative_results",
    "topic": "cross-lingual transfer learning",
    "persona": "NLP researcher with unexpected negative results",
    "constraints": "transfer to low-resource languages failed; deciding whether to pivot or investigate further",
    "extra": {
      "stage": "D"
    }
  },
  "system_id": "openrouter:openai/gpt-5",
  "stop_reason": "goal_reached",
  "stopped_by_student": true,
  "error": null,
  "turn_count": 7,
  "elapsed_seconds": 518.3631660938263,
  "transcript": [
    {
      "role": "user",
      "content": "Hey, I'm NLP researcher with unexpected negative results. I want to get started in research in cross-lingual transfer learning. Constraints: transfer to low-resource languages failed; deciding whether to pivot or investigate further. How should I start?",
      "turn": 0
    },
    {
      "role": "assistant",
      "content": "Here’s a pragmatic way to decide whether to push further or pivot, and a minimal-but-strong path to get traction in cross‑lingual transfer for low‑resource languages.\n\n1) Do a 10–14 day diagnosis sprint before committing\n- Verify the basics\n  - Task and labels: identical label space, casing/diacritics policy, Unicode normalization (NFC), tokenization boundaries. Check for class imbalance/label shift.\n  - Seeds and variance: run 5 seeds; if variance is high, you may be overfitting or under-resourced.\n- Tokenizer and script coverage\n  - Report per-language: percent of characters unseen, average subword length, proportion of long subword chains (>8 pieces), and unknown rate. If poor, expect transfer to fail.\n- Domain and data health\n  - Are source/target domains mismatched? Compute perplexity of a target-language LM on source vs target text. Large gaps signal domain shift.\n  - Sample-size curve: train on 0, 32, 128, 512, 2k target labels to see if you’re on a steep part of the curve.\n- Quick translation sanity checks\n  - Translate-test: target→English using a strong MT (e.g., NLLB-200, M2M-100) + an English model. This is an upper-bound sanity check.\n  - Translate-train: English→target of labeled data to fine-tune a target model. If this beats your zero-shot, your alignment is the problem, not the task.\n- Alignment probes\n  - Sentence retrieval with LaBSE/LASER3: target–English retrieval accuracy on mined parallel sentences.\n  - BLI/word translation on a small bilingual lexicon if available.\n\nGo/no-go from the sprint\n- If translate-test baselines are strong but multilingual zero-shot lags by a wide margin (>10 F1/accuracy) and DAPT/TAPT doesn’t close it by ≥3–5 points, consider pivoting or reframing the task.\n- If a small amount of target supervision (≤500 labels) yields steep gains or DAPT/TAPT gives ≥3 points, it’s worth investing further.\n\n2) Establish minimal strong baselines (replicable and competitive)\n- Multilingual PLM fine-tuning\n  - Model: XLM-R base/large or mDeBERTa-v3 base; for generative tasks, mT5-base or ByT5-small if scripts are poorly covered.\n  - Language-adaptive pretraining (continued MLM) on unlabeled target text (50k–100k steps, batch 256, lr 1e-4, sequence 512).\n  - Fine-tune with lr 1e-5–2e-5, bs 32–64, early stopping, 5 seeds.\n- Parameter-efficient variants\n  - Adapters or LoRA with language-specific modules (one per target). Use MAD-X-style: shared task adapter + language adapter.\n- Translation-based baselines\n  - Translate-test pipeline + strong English task model.\n  - Translate-train to create synthetic target data; filter with quality scores and round-trip consistency.\n- Label-efficient upper bound\n  - Few-shot: annotate 100–500 target examples; try self-training (pseudo-labeling with confidence thresholding) and consistency under translation.\n\n3) Common failure modes and quick fixes\n- Tokenizer harms transfer\n  - If subword fragmentation is high: try byte/char models (ByT5, CANINE/Charformer) or augment the vocab by retraining SentencePiece with target text and reinitializing new embeddings from the mean of similar tokens.\n- Script/orthography issues\n  - Normalize Unicode, standardize diacritics, implement consistent transliteration if relevant; ensure evaluation uses the same normalization pipeline.\n- Morphology and OOV-heavy domains\n  - Use morphological segmentation or subword regularization (SentencePiece sampling) during fine-tuning; add copy-augmentation for proper nouns (esp. NER).\n- Domain shift\n  - Do domain-adaptive pretraining (DAPT) on target-domain monolingual text and task-adaptive pretraining (TAPT) on unlabeled task inputs in target.\n- Weak cross-lingual alignment\n  - Contrastive fine-tuning with parallel or mined sentence pairs; distill from a strong English teacher via MT; apply invariance regularizers (e.g., R-Drop) across translation pairs.\n\n4) Small, high-yield data tactics\n- Active learning: uncertainty or diversity sampling to label 200–1k examples; typically +3–8 F1 when starting from low baselines.\n- Back-translation and round-trip paraphrasing to augment target text.\n- Lexicon/NER-specific: gazetteers, transliteration variants, copy noise augmentation.\n\n5) Evaluation and analysis you can do fast\n- Break down errors by token frequency, subword length, named entities, and morphological richness.\n- Calibration: ECE or confidence–accuracy curves per language; poor calibration is common in transfer.\n- Compare multi-source training: add typologically close donors; measure gains vs distant donors.\n\n6) A concrete 4-week plan\n- Week 1: Data/normalization audit, tokenizer stats, 5-seed English and zero-shot baselines, translate-test baseline.\n- Week 2: DAPT/TAPT for 1–2 days on target monolingual; fine-tune with adapters/LoRA; run translate-train. Build sample-size curve up to 512 labels (crowd or internal).\n- Week 3: Contrastive alignment with mined bitext (LaBSE/LASER3), self-training, and consistency under translation. Add typologically close source languages.\n- Week 4: Error analysis, ablations (adapters vs full FT; with/without DAPT; byte/char model if fragmentation is high). Decide using go/no-go criteria.\n\n7) When to pivot and viable pivots\n- Pivot if:\n  - Translate-test >> multilingual zero-shot and remains so after DAPT/adapters/self-training.\n  - Tokenization is fundamentally hostile and byte/char models underperform due to compute or latency constraints.\n  - Domain shift dominates and unlabeled target text is scarce.\n- Pivot options:\n  - Reframe to few-shot cross-lingual learning with active learning and distillation.\n  - Switch tasks where transfer is more stable (e.g., retrieval/semantic similarity with multilingual embeddings).\n  - Focus on building/curating high-quality small datasets for the target language; publish a negative-results + best-practices paper (venues: ACL workshops like AmericasNLP, LoResMT, Masakhane-related workshops).\n\n8) Implementation notes\n- Unlabeled data: OSCAR/CC100/Wikipedia or community corpora; match domain where possible.\n- Tools: LaBSE/LASER3 for mining; SimAlign for word-level projection; NLLB-200/M2M-100 for MT.\n- Ethics: for indigenous/low-resource languages, coordinate with communities; document consent and data provenance.\n\nIf you share the specific task, target language(s), data size, and compute budget, I can propose a tailored minimal baseline and ablation plan, including expected deltas and code-ready recipes.",
      "turn": 1
    },
    {
      "role": "user",
      "content": "This diagnosis sprint is exactly what I need, but I'm worried about the tokenizer; my target languages (Quechua and Guarani) are showing massive fragmentation on XLM-R. Between ByT5 and vocabulary expansion with SentencePiece, which typically yields a better 'bang for the buck' when compute for long sequences is limited?",
      "turn": 1
    },
    {
      "role": "assistant",
      "content": "Short answer: With limited compute for long sequences, vocabulary expansion + a short burst of continued pretraining (MLM) on Quechua/Guarani almost always gives better bang-for-buck than switching to ByT5. Use ByT5-small as a diagnostic/backup, not your primary path.\n\nWhy\n- ByT5 strengths: zero OOV, solid robustness on poorly covered scripts. Cost: 3–6x longer sequences than subwords → quadratic attention cost, lower throughput, stricter memory limits. Good if you can shorten contexts or your tasks are short.\n- Vocab expansion strengths: preserves XLM-R’s learned knowledge and alignment; fixes the worst fragmentation; needs only tens of GPU-hours for continued MLM to learn new embeddings. Typically +2–6 F1/acc over the fragmented baseline on NER/CLS, often at near-baseline inference cost.\n\nA minimal, high-yield recipe for vocab expansion\n1) Data\n- Collect 50–200MB unlabeled text per language (OSCAR/CC/Wikipedia/parallel sides; match domain).\n2) Train a new SentencePiece (unigram) that favors your targets\n- Train on mix: 30–50% Quechua + 30–50% Guarani + a small multilingual slice (to keep English/Spanish intact).\n- Settings: vocab_size ~300k–320k; character_coverage 1.0 (Latin + diacritics); model_type unigram; enable subword regularization later during FT.\n3) Initialize from XLM-R\n- Remap all tokens shared with the old vocab.\n- For new tokens: initialize embedding as the length-weighted average of the old-token embeddings obtained by segmenting the same string with the original XLM-R tokenizer; fall back to N(0,0.02) if coverage <80%.\n- Resize the LM head the same way. Freeze new rows for the first 1–2k steps to stabilize (optional but helps).\n4) Short continued pretraining (MLM)\n- Mix: 50% Quechua/Guarani, 50% multilingual; seq len 256–512.\n- Hyperparams: AdamW lr 1e-4, warmup 1k, total 50k–100k steps, batch 256–512 sequences, mask 15%, span masking if supported. Gradient checkpointing if needed.\n5) Fine-tune\n- Start with adapters/LoRA or full FT; 5 seeds; subword regularization on; evaluate per-language.\n\nWhat to expect and when to pivot\n- If your current XLM-R has extreme fragmentation (avg subword length >3.5; ≥10% tokens with >8 pieces), the above usually recovers most of the loss without increasing inference cost.\n- If after 30–50k MLM steps you don’t get ≥3 points over baseline on a dev set, try: more target mix (up to 70%), a slightly larger vocab, or byte/char fallback (below).\n\nUse ByT5-small as a fast diagnostic or for short-input tasks\n- Do a 1–2 day smoke test: byt5-small, max_len 256–384, batch so that tokens/sec ≈ XLM-R at 512. If it’s ≥3 points better than your fragmented XLM-R while keeping acceptable latency, consider it for:\n  - Short-context classification, slot/NER with short sentences.\n  - Translate-test pipelines (MT to English → strong English model) where sequence lengths stay short.\n- Caveats: For longer contexts (doc-level, QA), throughput/memory usually becomes the bottleneck before accuracy wins justify it.\n\nMiddle-ground fallbacks (cheap)\n- Subword regularization during FT (SentencePiece sampling) and slight LR increase often help morphology-heavy targets.\n- Unsupervised morpheme segmentation (e.g., Morfessor) to mine 5–10k frequent morphemes; add them to the vocab before MLM.\n- If re-SPM is hard in your stack: keep the old tokenizer and add 10–20k “added tokens” for very frequent words/morphemes, then do 30–50k MLM focusing on learning those rows. It’s weaker than re-SPM but cheap and sometimes enough.\n\nDecision rule\n- If you can’t afford >2x sequence length at inference: prioritize vocab expansion + short MLM.\n- If your tasks are short-input and ByT5-small wins by ≥3 points in a day’s test and latency is acceptable: consider ByT5 for that task; otherwise stick with expanded XLM-R.\n\nIf you share your task type, typical input lengths, and a rough GPU budget, I can suggest concrete hyperparameters (batch sizes, exact step counts) and a 3-day A/B protocol to pick a winner.",
      "turn": 2
    },
    {
      "role": "user",
      "content": "This recipe for vocab expansion feels much more sustainable than a full pivot, but I'm worried about the initialization step: when remapping shared tokens, how should I handle the 'displaced' weights if the new SentencePiece unigram model significantly changes the subword boundaries for common Spanish/English anchor terms?",
      "turn": 2
    },
    {
      "role": "assistant",
      "content": "Great question. You’re right to worry—retraining SentencePiece can shift English/Spanish boundaries and erode your “anchor” knowledge. There are two practical paths, depending on how much you want to change the tokenizer.\n\nPreferred: keep the original XLM-R tokenizer and add tokens\n- Why: zero anchor drift. Old tokens stay exactly as-is; you only gain new pieces for Quechua/Guarani.\n- How:\n  - Mine 10–30k high-frequency target strings (words/morphemes), with correct normalization (NFC) and SentencePiece “▁” for word-initial variants (e.g., include both ▁morpheme and morpheme if it can occur inside words).\n  - tokenizer.add_tokens(new_tokens); model.resize_token_embeddings(len(tokenizer)).\n  - Initialize each new embedding as the average of the old embeddings of the same string tokenized by the original tokenizer (length-weighted). Tie or copy to the LM head if applicable.\n  - Do 30–50k steps of continued MLM with a 50–70% Quechua/Guarani mix. Optionally freeze the new rows for the first 1–2k steps, then unfreeze.\n- What you get: a big drop in fragmentation for target languages with essentially no change to English/Spanish segmentation or performance.\n\nIf you must retrain SentencePiece (new model): minimize damage and handle “displaced” weights\n- Copy what you can\n  - For tokens whose exact string exists in both vocabs, copy embeddings (and LM head rows) verbatim.\n- Initialize new tokens from old pieces\n  - For each new token t, segment its string with the old tokenizer: t -> [p1, p2, …]. Initialize E_new[t] as a length- or logprob-weighted average of E_old[pi]. Handle word-initial “▁” consistently.\n- Protect anchor knowledge during continued pretraining\n  - Data mix: include 30–50% English/Spanish in MLM batches to keep the model fluent in anchors while it learns new pieces.\n  - L2-SP regularization: add an L2 penalty toward the original weights, at least on shared-token embeddings and early transformer layers (coeff ~1e-3 to 1e-4). This curbs drift without freezing learning.\n  - Short retention/distillation: for 10–20k steps, run",
      "turn": 3
    },
    {
      "role": "user",
      "content": "The averaging initialization for the 'preferred' path makes sense, but since Quechua and Guarani are highly agglutinative, should I prioritize adding sub-word morphemes over full words to the vocabulary to maximize coverage? Also, is there a specific heuristic for the 'length-weighted' average calculation you'd recommend?",
      "turn": 3
    },
    {
      "role": "assistant",
      "content": "Short answer\n- Yes: for agglutinative Quechua/Guarani, prioritize productive morphemes and frequent stems over whole words. Keep a small budget for ultra‑frequent closed‑class words and NE forms.\n- Use a simple, stable init: a length‑weighted average of the original pieces with norm matching. It’s robust and easy to implement.\n\nWhat to add (and how to pick it)\n- Target mix\n  - 60–75% productive morphemes (suffixes/prefixes/clitics) and frequent stems.\n  - 15–30% very frequent full wordforms (function words, auxiliaries, pronouns, high‑freq NE surface forms).\n  - 5–10% common bundles (stem+suffix) if suffix-only tokens aren’t reliably used by your tokenizer.\n- Caveat on “inside-word” tokens\n  - If your tokenizer stack truly supports added tokens inside words (HF fast tokenizers can, if you add tokens with appropriate flags), add both forms: word‑initial (with ▁) and non‑initial (without).\n  - If not, suffix-only tokens may not fire. In that case, bias toward stems and frequent stem+suffix bundles. You can still add a few pure suffixes for the translate‑train side where spacing/transliteration sometimes exposes them.\n- Candidate generation and ranking (cheap, effective)\n  1) Get candidates:\n     - Run unsupervised morphological segmentation (e.g., Morfessor) on each language to propose morphemes + stems.\n     - Also mine frequent n‑gram character substrings that align to contiguous runs of current tokens.\n  2) Score by marginal coverage gain:\n     - For candidate s, scan current tokenizations. Each time s matches a contiguous run of k existing subwords, its gain is (k−1). Sum over corpus: Gain(s) = Σ((k−1) × occurrences).\n     - Multiply by type productivity: number of distinct word types containing s.\n     - Rank by Gain × log(1 + type_count). This favors pieces that collapse long chains across many words.\n  3) Filters:\n     - Appear in ≥200–500 distinct types and ≥2–5k total occurrences (tune to corpus size).\n     - Keep orthographic variants you actually see (with/without hyphen; nasalized vowels in Guarani; ejectives/apostrophes in Quechua).\n  4) Budget: start with 10k–20k added tokens per language. Recompute fragmentation stats; if avg pieces/word is still high (>3.0), add another 5–10k.\n\nConcrete hints for Q/Qa and Gn\n- Quechua: prioritize common suffixes and clitics like -kuna (pl), -cha (dim), -qa (topic), -ta/-man/-paq (case), evidentials -mi/-shi/-cha, possessive/person endings, productive derivation (-na, -y, -yku). Include stem variants with apostrophes and glottal stops (k’, q’, ch’). Add both ▁kuna and kuna if inside-word use is supported.\n- Guarani: include nasalized vowels and diacritics (ã, ẽ, ĩ, õ, ũ, y); common affixes like -rã, -ha, -va, -gua, -re/-pe/-gui (cases), person markers, negation circumfix components nd…i (consider bundles for discontinuous morphology at surface level). Handle hyphenation variants if they appear in your corpora.\n\nInitialization heuristic (length‑weighted average)\n- Let old tokenizer segment the new token string t (strip the leading ▁ when measuring length):\n  - t → [p1, p2, …, pm]\n- Compute weights:\n  - wi = len_chars(pi_without_▁)^α\n  - α in [0.5, 1.0]; α = 0.7 works well in practice.\n  - Optional: if t starts a word (has ▁), add a small boost to the first piece (e.g., w1 ← 1.2 × w1).\n- Initialize embedding:\n  - E_new(t) = (Σ wi E_old(pi)) / (Σ wi)\n  - Renormalize the vector to have the same L2 norm as the mean norm of its source pieces (or the global mean embedding norm). This stabilizes early training.\n- LM head: copy E_new(t) or reproject through the same tying used by the base model.\n- Training tip: freeze new rows for 1–2k steps, then unfreeze; do 30–50k steps of continued MLM with 50–70% target text.\n\nPractical decision rules\n- If your tokenizer cannot reliably apply inside‑word added tokens, prioritize stems and a handful of high‑yield suffix bundles; revisit full re‑SPM if suffix coverage remains poor.\n- After adding 10–20k tokens, recheck:\n  - Avg subword length target: ≤2.5\n  - Share of tokens with chains ≥8 pieces: ≤1–2%\n  - If you don’t meet these after one round, add more morphemes/stems before considering ByT5.\n\nOptional refinements (cheap)\n- Mix weighting with old SPM likelihoods if available: wi = len^0.7 × exp(−λ · rank(pi|t)) with small λ (e.g., 0.1). Often similar to pure length weighting.\n- Keep a 10–15% token budget for language‑specific numerals/date formats and common NE substrings to reduce copy errors in NER/slot tasks.\n\nIf you share a few corpus stats (avg pieces/word, tail of the chain-length distribution, and whether inside‑word added tokens work in your stack), I can propose an exact token budget and a ranked list construction recipe you can implement in a day.",
      "turn": 4
    },
    {
      "role": "user",
      "content": "The Morfessor-based ranking for Gain × log(type_count) is a brilliant way to quantify 'productivity'—I'll implement that today. One quick check: if I'm using a RoBERTa-base architecture where the tokenizer is a Byte-level BPE, does the 'inside-word' token support typically require manual modification of the merges.txt, or is adding them to the vocabulary via the HF `add_tokens` method sufficient to ensure they trigger during the greedy match?",
      "turn": 4
    },
    {
      "role": "assistant",
      "content": "Short answer\n- For RoBERTa’s byte-level BPE, just calling tokenizer.add_tokens is not sufficient to make new pieces appear inside words during greedy BPE. The merges (merges.txt) determine which byte sequences are formed. To reliably get inside‑word pieces, you need new merge rules, i.e., retrain/modify merges.txt.\n- You can add word‑initial variants (with the leading space marker “Ġ”) via add_tokens and they will fire at word boundaries, but true inside‑word suffix/prefix tokens won’t reliably trigger without merges.\n\nPractical options, from safest to most invasive\n1) Cheap/safe: add only word‑initial forms; focus on stems and bundles\n- Add both “Ġstem” and “stem” if you want both word‑initial and inside‑word coverage, but expect only the “Ġstem” to consistently trigger.\n- Prioritize stems and a few high‑yield bundles (stem+suffix) rather than pure suffixes, since suffix‑only tokens won’t fire inside words without merges.\n- This improves fragmentation at word boundaries with zero risk to English/Spanish anchors.\n\n2) Reliable inside‑word support: retrain BBPE merges\n- Train a new byte-level BPE on a mix of Quechua/Guarani + a slice of English/Spanish to preserve anchors.\n- Keep vocab size similar; export vocab.json and merges.txt.\n- Remap embeddings:\n  - Copy shared tokens (exact strings, including “Ġ” variants).\n  - Initialize new tokens from old decomposition (length‑weighted average), and continue MLM 30–50k steps with 50–70% target text.\n- This is the robust way to get productive morphemes to appear inside words.\n\n3) Advanced hack (use with caution): AddedToken matching inside words\n- HF “fast” tokenizers can match AddedToken patterns before BPE if you register them as AddedToken(single_word=False). This can make an added token fire even inside a longer word.\n- Caveats:\n  - It’s exact string matching; easy to over‑match or conflict with normalization.\n  - Large added-token lists can slow tokenization and create unexpected spans that weren’t seen during pretraining.\n  - Offset mappings and detokenization can get tricky.\n\nMinimal example of the hack\n- Only try this for a small set of very high‑yield morphemes; test thoroughly.\n\nfrom transformers import AddedToken\n\n# For word-initial variant (leading-space marker in RoBERTa is \"Ġ\")\ntok_str_initial = \"Ġkuna\"   # example stem/morpheme at word start\ntok_str_inside  = \"kuna\"     # inside-word variant\n\ntok.add_tokens([\n    AddedToken(tok_str_initial, single_word=False, lstrip=False, rstrip=False, normalized=False),\n    AddedToken(tok_str_inside,  single_word=False, lstrip=False, rstrip=False, normalized=False),\n])\n\n# Verify it actually fires\ns = \"Paykuna chaykunawan\"\nprint(tok.convert_ids_to_tokens(tok(s).input_ids))\n\nDecision rule\n- If you truly need productive suffixes to trigger inside words, plan on retraining BPE merges (or switch to a SentencePiece unigram tokenizer where adding pieces is more flexible).\n- If you want a low‑risk, fast win without touching merges, add only word‑initial forms and focus on stems/bundles; combine with continued MLM.\n\nQuick sanity checks\n- After any change, verify firing:\n  - Does the new token appear as a single token in tokenization of words where it should?\n  - Measure avg pieces/word and share of chains ≥8 pieces before/after.\n- Guard anchors:\n  - Keep 30–50% English/Spanish in MLM batches to preserve anchor knowledge.\n  - Consider a light L2 penalty toward original embeddings on shared tokens during continued pretraining.",
      "turn": 5
    },
    {
      "role": "user",
      "content": "The 'AddedToken' hack is tempting for a quick test, but retraining the merges feels like the only way to truly respect the morphology. If I retrain the BBPE merges on a mix of languages, should I keep the original RoBERTa base vocabulary size (50,265) or is there a 'sweet spot' for the total vocab size when adding two highly agglutinative languages?",
      "turn": 5
    },
    {
      "role": "assistant",
      "content": "Recommendation in one line: stay close to RoBERTa’s 50k, and only bump to ~60–65k if your tokenization audit still shows heavy fragmentation after retraining merges on a Q/Gn‑oversampled mix. Going much beyond ~70k usually gives diminishing returns and bloats params without fixing the core issue.\n\nWhy this “sweet spot”\n- Sequence length dominates cost; vocab size mainly adds embedding/LM‑head params. Each +10k vocab adds ~7.7M embedding params (+ another ~7.7M in the tied LM head during MLM), i.e., ~12% extra params for RoBERTa‑base. You get only modest further reductions in pieces/word beyond ~60–65k for Latin‑script agglutinative languages.\n- For BBPE, corpus mix matters more than raw vocab size. Oversampling Q/Gn during BPE training produces the key inside‑word merges you want; a modest size increase just lets those merges “fit” without evicting English/Spanish anchors.\n\nA quick, low-cost selection protocol (1–2 days)\n1) Train BBPE merges on the same mixed corpus with three sizes:\n   - 50,265 (baseline size)\n   - ~60,000–65,000\n   - ~70,000 (upper bound)\n   Mix: 30–40% Spanish/English, 60–70% Quechua/Guarani; oversample Q/Gn 3–5× relative to their raw web frequency.\n2) Offline tokenization audit (no model training):\n   - For Q/Gn: avg pieces/word, 95th percentile chain length, share of tokens with ≥8 pieces.\n   - For ES/EN anchors: percent of top‑20k tokens preserved as single tokens (exact string match, including “Ġ” forms), and overall overlap with old vocab.\n3) Pick the smallest size that hits targets:\n   - Q/Gn: avg pieces/word ≤ 2.5; ≥8‑piece chains ≤ 1–2%.\n   - ES/EN: ≥85–90% of frequent anchors preserved.\n   In practice, the jump from 50k→60–65k plus Q/Gn oversampling is enough; only go to ~70k if you’re still missing the targets.\n\nTraining tips to preserve anchors and speed convergence\n- Keep normalization and specials identical to RoBERTa (byte‑level, “Ġ” space marker, same special tokens).\n- Initialization:\n  - Copy embeddings for exact‑match tokens.\n  - For new tokens, initialize from old decomposition (length‑weighted averaging) and norm‑match; do the same for the LM head.\n- Retention during MLM:\n  - Include 30–50% ES/EN in batches.\n  - Optional L2‑SP toward the original weights on shared tokens and early layers (λ ~1e‑3 to 1e‑4).\n- Budget: 30–50k MLM steps at seq len 256–512 with 50–70% Q/Gn suffices to stabilize new merges.\n\nIf you want to squeeze more out of 50k without growing vocab\n- Heavily oversample Q/Gn in BPE training (e.g., 5×) and include morphology‑rich text. This often yields most of the win even at 50k, at the cost of a small increase in ES/EN fragmentation.\n- Validate with the same tokenization metrics; if still above targets, then move to ~60–65k.\n\nRule of thumb\n- Start at 50,265. If after retraining merges (with Q/Gn oversampling) avg pieces/word is still >2.7 or long‑chain tail is stubborn, try 60–65k. Avoid >70k unless you’ve verified a clear drop in fragmentation and you can afford the ~15–25M extra params during MLM.",
      "turn": 6
    },
    {
      "role": "user",
      "content": "This protocol is exactly what I needed to stop spinning my wheels. One final detail: when initializing new tokens via length-weighted averaging of their old BBPE decompositions, should I apply a small amount of Gaussian noise to break symmetry, or is the norm-matching sufficient to prevent gradient signal collapse during the first few MLM steps?",
      "turn": 6
    },
    {
      "role": "assistant",
      "content": "Short answer: Norm-matching is usually enough. Add tiny Gaussian noise only as “insurance,” and keep it very small; otherwise you just slow convergence.\n\nPractical rule\n- Default: no noise. Length-weighted averaging + norm-matching + a short continued-MLM run is stable in practice.\n- Add tiny noise only when many new tokens share nearly identical decompositions (e.g., word-initial vs inside-word variants or morphemes that decompose to the same 2–3 pieces), to avoid them starting with near-duplicate vectors.\n\nIf you add noise, use a very small, direction-only jitter\n- Scale: about 5–10% of the original BERT/RoBERTa init std (0.02), i.e., σ ≈ 0.001–0.002, or equivalently ~0.5–1% of the target vector norm.\n- Make the noise mostly orthogonal to the mean so you perturb direction, not length:\n  - u = E_new / ||E_new||\n  - ε ~ N(0, σ^2 I)\n  - ε⊥ = ε − (ε·u) u\n  - E_new ← E_new + ε⊥\n  - Renormalize to the target norm (either the average norm of source pieces or the global embedding mean norm).\n\nMinimal init recipe\n1) Compute length-weighted average of old BBPE pieces.\n2) Match the L2 norm (to source-piece mean or global mean).\n3) Optional: add ε⊥ with σ = 0.001–0.002 and renorm again.\n4) Copy/tie LM-head rows accordingly.\n\nTraining stabilizers that matter more than noise\n- Freeze new rows for 1–2k steps, then unfreeze; or use a 0.5× LR on new rows for the first 5–10k steps.\n- Keep 30–50% EN/ES in MLM batches to preserve anchors.\n- Optional L2-SP toward original weights on shared tokens (λ ~1e-3 to 1e-4).\n\nDiagnostics\n- If gradients on new rows are near-zero after a few hundred steps, your mask rate may be too low or the new tokens aren’t firing; increase mask rate on target text or verify tokenization. If they’re large and unstable, your noise was too big—drop σ or skip noise.\n\nBottom line: start with norm-matched averages (no noise). If you see many near-duplicate inits or sluggish divergence between closely related new tokens, add a tiny orthogonal jitter (σ ≈ 0.001–0.002) and keep the early freeze/low-LR schedule.",
      "turn": 7
    },
    {
      "role": "user",
      "content": "The orthogonal jitter for direction-only perturbation is a brilliant way to handle the near-duplicate initialization problem. I have a solid implementation plan now, from the Morfessor-based ranking to the norm-matched embedding surgery, and I'm ready to move into the execution phase.",
      "turn": 7
    }
  ]
}