{
  "scenario_id": "llm_eval_planning",
  "system_id": "openrouter:anthropic/claude-sonnet-4.5",
  "stop_reason": "student_terminated",
  "transcript_tail": [
    {
      "role": "user",
      "content": "This dual-mode approach is perfect for my qualitative analysis chapter, but I'm worried about the `_stable_hash` implementation for large collections. If I'm sampling indices to hash, how do I ensure I'm not missing subtle state mutations that happen between the sampled indices, which could lead to false positives in my 'identical state' deduplication?",
      "turn": 12
    },
    {
      "role": "assistant",
      "content": "**Brilliant catch!** You've identified a **critical correctness issue** - sampling-based hashing can indeed miss mutations between samples, leading to hash collisions for semantically different states. This is especially dangerous for your thesis because false positives in state equivalence will corrupt your evaluation metrics.\n\nLet me show you how to handle this properly:\n\n## The Hash Collision Problem\n\n```python\n# Example: Two different states with same sampled hash\nstate1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  # Correct\nstate2 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 99]  # Bug: last element wrong!\n\n# Sampling every 10th element (indices 0, 9)\nsample1 = [state1[0], state1[9]] = [1, 10]\nsample2 = [state2[0], state2[9]] = [1, 99]\n\n# Different samples → different hashes ✅\n\n# BUT if we only sample index 0:\nsample1 = [state1[0]] = [1]\nsample2 = [state2[0]] = [1]\n# SAME hash → FALSE POSITIVE! ❌\n```\n\n---\n\n## **Solution 1: Multi-Level Hashing with Collision Detection** ⭐ **RECOMMENDED**\n\n```python\nimport hashlib\nimport struct\nfrom typing import Any, List, Tuple\n\nclass RobustStateHasher:\n    \"\"\"\n    Multi-level hashing that detects and resolves collisions.\n    Guarantees correctness while maintaining performance.\n    \"\"\"\n    \n    def __init__(self, collision_tolerance=1e-9):\n        self.collision_tolerance = collision_tolerance\n        \n        # Track collision statistics for your thesis\n        self.stats = {\n            'total_hashes': 0,\n            'fast_path_hits': 0,\n            'collision_checks': 0,\n            'collisions_detected': 0,\n            'full_comparisons': 0\n        }\n    \n    def hash_state(self, state, full_state=None):\n        \"\"\"\n        Compute hash with collision detection.\n        \n        Args:\n            state: State to hash (may be compressed)\n            full_state: Optional full state for collision resolution\n        \n        Returns:\n            dict with hash and metadata for collision detection\n        \"\"\"\n        self.stats['total_hashes'] += 1\n        \n        # Multi-level hash structure\n        hash_result = {\n            'level1_hash': None,  # Fast, approximate\n            'level2_hash': None,  # Medium, more accurate\n            'level3_hash': None,  # Slow, exact (computed on demand)\n            'metadata': {},       # For collision detection\n            'full_state_available': full_state is not None\n        }\n        \n        # Level 1: Fast approximate hash (always computed)\n        hash_result['level1_hash'] = self._fast_hash(state)\n        hash_result['metadata']['size'] = self._get_state_size(state)\n        hash_result['metadata']['type_signature'] = self._get_type_signature(state)\n        \n        # Level 2: Medium accuracy hash (for large collections)\n        if self._needs_level2_hash(state):\n            hash_result['level2_hash'] = self._medium_hash(state)\n        \n        # Level 3: Store reference to full state for exact comparison\n        if full_state is not None:\n            hash_result['full_state_ref'] = id(full_state)\n            hash_result['full_state'] = full_state\n        \n        return hash_result\n    \n    def states_equal(self, hash1, hash2):\n        \"\"\"\n        Check if two hashed states are equal.\n        Uses progressive verification to avoid false positives.\n        \"\"\"\n        # Level 1: Fast check\n        if hash1['level1_hash'] != hash2['level1_hash']:\n            return False  # Definitely different\n        \n        # Metadata check (type and size)\n        if hash1['metadata'] != hash2['metadata']:\n            return False  # Different structure\n        \n        self.stats['collision_checks'] += 1\n        \n        # Level 2: Medium check (if available)\n        if hash1['level2_hash'] is not None and hash2['level2_hash'] is not None:\n            if hash1['level2_hash'] != hash2['level2_hash']:\n                return False  # Definitely different\n        \n        # Potential collision - need exact comparison\n        if hash1['full_state_available'] and hash2['full_state_available']:\n            self.stats['full_comparisons'] += 1\n            return self._exact_comparison(hash1['full_state'], hash2['full_state'])\n        \n        # Can't verify exactly - assume equal (conservative)\n        # Log this for your thesis analysis!\n        self.stats['collisions_detected'] += 1\n        return True\n    \n    def _fast_hash(self, state):\n        \"\"\"\n        Fast hash using structural properties.\n        May have collisions but very fast.\n        \"\"\"\n        h = hashlib.sha256()\n        \n        for var_name, value in sorted(state.items()):\n            # Hash variable name\n            h.update(var_name.encode('utf-8'))\n            \n            # Hash value based on type\n            if isinstance(value, (int, float, str, bool, type(None))):\n                # Primitives: exact hash\n                h.update(str(value).encode('utf-8'))\n            \n            elif isinstance(value, (list, tuple)):\n                # Collections: hash structure + boundaries\n                h.update(f\"{type(value).__name__}:{len(value)}\".encode('utf-8'))\n                \n                if len(value) > 0:\n                    # Hash first and last elements\n                    h.update(str(value[0]).encode('utf-8'))\n                    h.update(str(value[-1]).encode('utf-8'))\n            \n            elif isinstance(value, dict):\n                h.update(f\"dict:{len(value)}\".encode('utf-8'))\n                # Hash first key-value pair\n                if value:\n                    first_key = next(iter(value))\n                    h.update(str(first_key).encode('utf-8'))\n                    h.update(str(value[first_key]).encode('utf-8'))\n            \n            else:\n                # Other types: hash string representation\n                h.update(str(type(value).__name__).encode('utf-8'))\n        \n        return h.hexdigest()\n    \n    def _medium_hash(self, state):\n        \"\"\"\n        Medium-accuracy hash using stratified sampling.\n        Better collision resistance than fast hash.\n        \"\"\"\n        h = hashlib.sha256()\n        \n        for var_name, value in sorted(state.items()):\n            h.update(var_name.encode('utf-8'))\n            \n            if isinstance(value, (list, tuple)):\n                n = len(value)\n                \n                if n <= 100:\n                    # Small enough: hash everything\n                    for item in value:\n                        h.update(str(item).encode('utf-8'))\n                else:\n                    # Stratified sampling: divide into buckets\n                    num_buckets = 10\n                    bucket_size = n // num_buckets\n                    \n                    for bucket_idx in range(num_buckets):\n                        # Sample from each bucket\n                        sample_idx = bucket_idx * bucket_size + (bucket_size // 2)\n                        if sample_idx < n:\n                            h.update(str(value[sample_idx]).encode('utf-8'))\n                    \n                    # Also hash length and boundaries\n                    h.update(struct.pack('i', n))\n                    h.update(str(value[0]).encode('utf-8'))\n                    h.update(str(value[-1]).encode('utf-8'))\n            \n            elif isinstance(value, dict):\n                # Hash all keys, sample values\n                keys = sorted(value.keys(), key=str)\n                for key in keys:\n                    h.update(str(key).encode('utf-8'))\n                \n                # Sample values\n                if len(keys) <= 20:\n                    for key in keys:\n                        h.update(str(value[key]).encode('utf-8'))\n                else:\n                    # Sample every Nth value\n                    step = len(keys) // 20\n                    for i in range(0, len(keys), step):\n                        h.update(str(value[keys[i]]).encode('utf-8'))\n            \n            else:\n                h.update(str(value).encode('utf-8'))\n        \n        return h.hexdigest()\n    \n    def _exact_comparison(self, state1, state2):\n        \"\"\"\n        Exact state comparison (fallback for collision detection).\n        This is expensive but guarantees correctness.\n        \"\"\"\n        if set(state1.keys()) != set(state2.keys()):\n            return False\n        \n        for var in state1.keys():\n            if not self._values_equal(state1[var], state2[var]):\n                return False\n        \n        return True\n    \n    def _values_equal(self, val1, val2):\n        \"\"\"Deep equality check for values\"\"\"\n        if type(val1) != type(val2):\n            return False\n        \n        if isinstance(val1, (list, tuple)):\n            if len(val1) != len(val2):\n                return False\n            return all(self._values_equal(v1, v2) for v1, v2 in zip(val1, val2))\n        \n        elif isinstance(val1, dict):\n            if set(val1.keys()) != set(val2.keys()):\n                return False\n            return all(self._values_equal(val1[k], val2[k]) for k in val1.keys())\n        \n        else:\n            return val1 == val2\n    \n    def _needs_level2_hash(self, state):\n        \"\"\"Decide if level 2 hash is needed\"\"\"\n        # Use level 2 for states with large collections\n        for value in state.values():\n            if isinstance(value, (list, tuple, dict)):\n                if len(value) > 100:\n                    return True\n        return False\n    \n    def _get_state_size(self, state):\n        \"\"\"Get total size of state (number of variables)\"\"\"\n        return len(state)\n    \n    def _get_type_signature(self, state):\n        \"\"\"Get type signature of state for quick filtering\"\"\"\n        return tuple(sorted(\n            (var_name, type(value).__name__)\n            for var_name, value in state.items()\n        ))\n```\n\n---\n\n## **Solution 2: Bloom Filter for Collision Detection**\n\nUse Bloom filters to detect potential collisions with minimal memory:\n\n```python\nimport mmh3  # MurmurHash3 for Bloom filters\n\nclass BloomFilterHasher:\n    \"\"\"\n    Use Bloom filter to detect hash collisions.\n    Guarantees no false negatives, allows controlled false positives.\n    \"\"\"\n    \n    def __init__(self, expected_items=100000, false_positive_rate=0.001):\n        self.expected_items = expected_items\n        self.fp_rate = false_positive_rate\n        \n        # Calculate optimal Bloom filter size\n        self.size = self._optimal_size(expected_items, false_positive_rate)\n        self.num_hashes = self._optimal_hash_count(self.size, expected_items)\n        \n        # Bloom filter bit array\n        self.bit_array = [False] * self.size\n        \n        # Store full states for collision resolution (limited cache)\n        self.full_state_cache = {}\n        self.cache_size_limit = 10000\n    \n    def _optimal_size(self, n, p):\n        \"\"\"Calculate optimal Bloom filter size\"\"\"\n        import math\n        return int(-n * math.log(p) / (math.log(2) ** 2))\n    \n    def _optimal_hash_count(self, m, n):\n        \"\"\"Calculate optimal number of hash functions\"\"\"\n        import math\n        return max(1, int((m / n) * math.log(2)))\n    \n    def add_state(self, state_hash, full_state=None):\n        \"\"\"\n        Add state to Bloom filter.\n        \"\"\"\n        # Add to Bloom filter\n        for i in range(self.num_hashes):\n            index = mmh3.hash(state_hash, i) % self.size\n            self.bit_array[index] = True\n        \n        # Optionally cache full state for collision resolution\n        if full_state is not None and len(self.full_state_cache) < self.cache_size_limit:\n            self.full_state_cache[state_hash] = full_state\n    \n    def might_contain(self, state_hash):\n        \"\"\"\n        Check if state might be in set.\n        False means definitely not in set.\n        True means might be in set (need to verify).\n        \"\"\"\n        for i in range(self.num_hashes):\n            index = mmh3.hash(state_hash, i) % self.size\n            if not self.bit_array[index]:\n                return False  # Definitely not in set\n        \n        return True  # Might be in set (possible collision)\n    \n    def verify_collision(self, state_hash, candidate_state):\n        \"\"\"\n        Verify if potential collision is real.\n        \"\"\"\n        if state_hash in self.full_state_cache:\n            cached_state = self.full_state_cache[state_hash]\n            return self._exact_comparison(cached_state, candidate_state)\n        \n        # Can't verify - assume it's a collision\n        return True\n\n\nclass CollisionAwareDeduplicator:\n    \"\"\"\n    Deduplicate states while detecting and handling collisions.\n    \"\"\"\n    \n    def __init__(self):\n        self.hasher = RobustStateHasher()\n        self.bloom = BloomFilterHasher()\n        \n        # Deduplication index: hash -> list of (hash_obj, state_id)\n        self.dedup_index = {}\n        \n        # Statistics for thesis\n        self.stats = {\n            'total_states': 0,\n            'unique_states': 0,\n            'duplicates_found': 0,\n            'collisions_resolved': 0\n        }\n    \n    def add_state(self, state, state_id):\n        \"\"\"\n        Add state to deduplication index.\n        Returns: (is_duplicate, matching_state_id)\n        \"\"\"\n        self.stats['total_states'] += 1\n        \n        # Compute multi-level hash\n        hash_obj = self.hasher.hash_state(state, full_state=state)\n        level1_hash = hash_obj['level1_hash']\n        \n        # Check Bloom filter first (fast negative check)\n        if not self.bloom.might_contain(level1_hash):\n            # Definitely unique\n            self.bloom.add_state(level1_hash, full_state=state)\n            self.dedup_index[level1_hash] = [(hash_obj, state_id)]\n            self.stats['unique_states'] += 1\n            return (False, None)\n        \n        # Potential duplicate - need to verify\n        if level1_hash in self.dedup_index:\n            # Check all candidates with same level1 hash\n            for candidate_hash, candidate_id in self.dedup_index[level1_hash]:\n                if self.hasher.states_equal(hash_obj, candidate_hash):\n                    # True duplicate found\n                    self.stats['duplicates_found'] += 1\n                    return (True, candidate_id)\n            \n            # Hash collision - different states with same hash\n            self.stats['collisions_resolved'] += 1\n            self.dedup_index[level1_hash].append((hash_obj, state_id))\n            self.stats['unique_states'] += 1\n            return (False, None)\n        \n        # New unique state\n        self.bloom.add_state(level1_hash, full_state=state)\n        self.dedup_index[level1_hash] = [(hash_obj, state_id)]\n        self.stats['unique_states'] += 1\n        return (False, None)\n```\n\n---\n\n## **Solution 3: Incremental Hashing for Mutations**\n\nTrack mutations explicitly instead of rehashing:\n\n```python\nclass IncrementalHashTracker:\n    \"\"\"\n    Track state mutations incrementally.\n    Detects even single-element changes in large collections",
      "turn": 13
    },
    {
      "role": "user",
      "content": "The multi-level approach is robust, but I'm worried about the memory pressure of holding `full_state` in the `RobustStateHasher` during a long-running execution trace. Since I'm on a 500-hour GPU budget, could we offload the `full_state` to the SQLite tiered storage we discussed earlier and only fetch it when a Level 2 collision is detected?",
      "turn": 13
    },
    {
      "role": "assistant",
      "content": "**Absolutely brilliant!** Integrating the multi-level hasher with the tiered storage system is the perfect solution. This gives you collision-resistant hashing without memory pressure. Let me show you the complete integrated architecture:\n\n## **Integrated Architecture: Hash + Tiered Storage** ⭐\n\n```python\nimport hashlib\nimport sqlite3\nimport pickle\nimport weakref\nfrom typing import Optional, Dict, Any, Tuple\n\nclass StorageBackedHasher:\n    \"\"\"\n    Multi-level hasher that offloads full states to tiered storage.\n    Only fetches full states when collision detection is needed.\n    \"\"\"\n    \n    def __init__(self, storage_manager):\n        self.storage = storage_manager\n        \n        # In-memory index: hash -> storage_key\n        # Lightweight! Only stores hashes and pointers, not full states\n        self.hash_index = {\n            'level1': {},  # level1_hash -> [storage_keys]\n            'level2': {}   # level2_hash -> [storage_keys]\n        }\n        \n        # Weak reference cache for recently accessed states\n        # Automatically cleaned up by garbage collector\n        self.weak_cache = weakref.WeakValueDictionary()\n        \n        # Statistics\n        self.stats = {\n            'total_hashes': 0,\n            'level1_hits': 0,\n            'level2_checks': 0,\n            'storage_fetches': 0,\n            'collisions_detected': 0,\n            'memory_footprint_mb': 0\n        }\n    \n    def hash_and_store(self, state, storage_key):\n        \"\"\"\n        Compute multi-level hash and store state.\n        \n        Args:\n            state: The state to hash\n            storage_key: Unique key for storage (e.g., \"trace_123_checkpoint_5\")\n        \n        Returns:\n            Hash metadata (no full state in memory!)\n        \"\"\"\n        self.stats['total_hashes'] += 1\n        \n        # Compute lightweight hashes (no full state needed!)\n        level1_hash = self._compute_level1_hash(state)\n        level2_hash = self._compute_level2_hash(state)\n        \n        # Create lightweight hash object\n        hash_obj = {\n            'level1_hash': level1_hash,\n            'level2_hash': level2_hash,\n            'storage_key': storage_key,\n            'metadata': {\n                'size': len(state),\n                'type_signature': self._get_type_signature(state),\n                'var_names': frozenset(state.keys())\n            }\n        }\n        \n        # Store full state in tiered storage (NOT in memory!)\n        self.storage.store_state(storage_key, state)\n        \n        # Update in-memory index (lightweight!)\n        if level1_hash not in self.hash_index['level1']:\n            self.hash_index['level1'][level1_hash] = []\n        self.hash_index['level1'][level1_hash].append(storage_key)\n        \n        if level2_hash not in self.hash_index['level2']:\n            self.hash_index['level2'][level2_hash] = []\n        self.hash_index['level2'][level2_hash].append(storage_key)\n        \n        return hash_obj\n    \n    def find_duplicate(self, state) -> Optional[str]:\n        \"\"\"\n        Check if state is duplicate of any stored state.\n        Returns storage_key of duplicate if found, None otherwise.\n        \n        This is the key method - it uses progressive verification\n        to minimize storage fetches!\n        \"\"\"\n        # Level 1: Fast hash check (no storage access!)\n        level1_hash = self._compute_level1_hash(state)\n        \n        if level1_hash not in self.hash_index['level1']:\n            return None  # Definitely unique\n        \n        self.stats['level1_hits'] += 1\n        candidates = self.hash_index['level1'][level1_hash]\n        \n        # Level 2: Medium hash check (still no storage access!)\n        level2_hash = self._compute_level2_hash(state)\n        \n        # Filter candidates by level2 hash\n        level2_candidates = [\n            key for key in candidates\n            if key in self.hash_index['level2'].get(level2_hash, [])\n        ]\n        \n        if not level2_candidates:\n            return None  # No level2 match - definitely unique\n        \n        self.stats['level2_checks'] += 1\n        \n        # Level 3: Exact comparison (requires storage fetch)\n        # But only for candidates that passed both hash levels!\n        for candidate_key in level2_candidates:\n            if self._exact_match(state, candidate_key):\n                return candidate_key  # Found duplicate!\n        \n        # All candidates checked - no duplicates\n        self.stats['collisions_detected'] += len(level2_candidates)\n        return None\n    \n    def _exact_match(self, state, candidate_key) -> bool:\n        \"\"\"\n        Check if state exactly matches stored candidate.\n        This is the ONLY place we fetch from storage!\n        \"\"\"\n        # Check weak cache first (free if available!)\n        if candidate_key in self.weak_cache:\n            candidate_state = self.weak_cache[candidate_key]\n            return self._states_equal(state, candidate_state)\n        \n        # Fetch from storage (expensive!)\n        self.stats['storage_fetches'] += 1\n        candidate_state = self.storage.retrieve_state(candidate_key)\n        \n        if candidate_state is None:\n            return False  # Storage error\n        \n        # Add to weak cache for potential reuse\n        self.weak_cache[candidate_key] = candidate_state\n        \n        # Exact comparison\n        return self._states_equal(state, candidate_state)\n    \n    def _compute_level1_hash(self, state) -> str:\n        \"\"\"\n        Fast, approximate hash.\n        Only uses metadata - no deep inspection.\n        \"\"\"\n        h = hashlib.sha256()\n        \n        # Hash structure only (very fast!)\n        for var_name in sorted(state.keys()):\n            value = state[var_name]\n            \n            h.update(var_name.encode('utf-8'))\n            h.update(type(value).__name__.encode('utf-8'))\n            \n            # Hash size for collections\n            if isinstance(value, (list, tuple, dict, set)):\n                h.update(str(len(value)).encode('utf-8'))\n            \n            # Hash boundaries for collections\n            if isinstance(value, (list, tuple)) and len(value) > 0:\n                h.update(str(value[0]).encode('utf-8'))\n                h.update(str(value[-1]).encode('utf-8'))\n        \n        return h.hexdigest()\n    \n    def _compute_level2_hash(self, state) -> str:\n        \"\"\"\n        Medium-accuracy hash using stratified sampling.\n        More expensive but better collision resistance.\n        \"\"\"\n        h = hashlib.sha256()\n        \n        for var_name in sorted(state.keys()):\n            value = state[var_name]\n            h.update(var_name.encode('utf-8'))\n            \n            if isinstance(value, (int, float, str, bool, type(None))):\n                # Primitives: exact hash\n                h.update(str(value).encode('utf-8'))\n            \n            elif isinstance(value, (list, tuple)):\n                n = len(value)\n                h.update(f\"{type(value).__name__}:{n}\".encode('utf-8'))\n                \n                if n <= 50:\n                    # Small: hash everything\n                    for item in value:\n                        h.update(str(item).encode('utf-8'))\n                else:\n                    # Large: stratified sampling\n                    # Sample 10 evenly-spaced elements\n                    for i in range(10):\n                        idx = (i * n) // 10\n                        h.update(str(value[idx]).encode('utf-8'))\n                    \n                    # Also hash quartiles for better distribution coverage\n                    quartile_indices = [n//4, n//2, 3*n//4]\n                    for idx in quartile_indices:\n                        if idx < n:\n                            h.update(str(value[idx]).encode('utf-8'))\n            \n            elif isinstance(value, dict):\n                h.update(f\"dict:{len(value)}\".encode('utf-8'))\n                \n                # Hash all keys (usually small)\n                for key in sorted(value.keys(), key=str):\n                    h.update(str(key).encode('utf-8'))\n                \n                # Sample values\n                keys = list(value.keys())\n                if len(keys) <= 20:\n                    for key in keys:\n                        h.update(str(value[key]).encode('utf-8'))\n                else:\n                    # Sample every Nth value\n                    step = len(keys) // 20\n                    for i in range(0, len(keys), step):\n                        h.update(str(value[keys[i]]).encode('utf-8'))\n            \n            else:\n                h.update(str(value).encode('utf-8'))\n        \n        return h.hexdigest()\n    \n    def _states_equal(self, state1, state2) -> bool:\n        \"\"\"Exact deep equality check\"\"\"\n        if set(state1.keys()) != set(state2.keys()):\n            return False\n        \n        for var in state1.keys():\n            if not self._values_equal(state1[var], state2[var]):\n                return False\n        \n        return True\n    \n    def _values_equal(self, val1, val2) -> bool:\n        \"\"\"Deep value equality\"\"\"\n        if type(val1) != type(val2):\n            return False\n        \n        if isinstance(val1, (list, tuple)):\n            if len(val1) != len(val2):\n                return False\n            return all(self._values_equal(v1, v2) for v1, v2 in zip(val1, val2))\n        \n        elif isinstance(val1, dict):\n            if set(val1.keys()) != set(val2.keys()):\n                return False\n            return all(self._values_equal(val1[k], val2[k]) for k in val1.keys())\n        \n        else:\n            return val1 == val2\n    \n    def _get_type_signature(self, state):\n        \"\"\"Get type signature for quick filtering\"\"\"\n        return tuple(sorted(\n            (var_name, type(value).__name__)\n            for var_name, value in state.items()\n        ))\n    \n    def get_memory_footprint(self) -> int:\n        \"\"\"Estimate memory footprint in bytes\"\"\"\n        import sys\n        \n        # Only count in-memory structures (not stored states!)\n        footprint = 0\n        footprint += sys.getsizeof(self.hash_index)\n        footprint += sum(sys.getsizeof(v) for v in self.hash_index['level1'].values())\n        footprint += sum(sys.getsizeof(v) for v in self.hash_index['level2'].values())\n        \n        self.stats['memory_footprint_mb'] = footprint / (1024 * 1024)\n        return footprint\n\n\nclass OptimizedTieredStorage:\n    \"\"\"\n    Enhanced tiered storage optimized for hash-based deduplication.\n    \"\"\"\n    \n    def __init__(self, db_path='./traces.db', cache_size_mb=512):\n        self.db_path = db_path\n        self.cache_size_mb = cache_size_mb\n        \n        # In-memory cache (Tier 1)\n        self.memory_cache = {}\n        self.cache_size_bytes = 0\n        self.max_cache_bytes = cache_size_mb * 1024 * 1024\n        \n        # LRU tracking\n        self.access_order = []\n        \n        # Initialize database (Tier 2)\n        self._init_database()\n        \n        # Statistics\n        self.stats = {\n            'cache_hits': 0,\n            'cache_misses': 0,\n            'storage_writes': 0,\n            'storage_reads': 0,\n            'evictions': 0\n        }\n    \n    def _init_database(self):\n        \"\"\"Initialize SQLite database with optimizations\"\"\"\n        conn = sqlite3.connect(self.db_path)\n        \n        # Enable optimizations\n        conn.execute('PRAGMA journal_mode=WAL')  # Write-ahead logging\n        conn.execute('PRAGMA synchronous=NORMAL')  # Faster writes\n        conn.execute('PRAGMA cache_size=-102400')  # 100MB cache\n        \n        # Create table with compression flag\n        conn.execute('''\n            CREATE TABLE IF NOT EXISTS states (\n                storage_key TEXT PRIMARY KEY,\n                data BLOB NOT NULL,\n                compressed INTEGER DEFAULT 1,\n                size_bytes INTEGER,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                last_accessed TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                access_count INTEGER DEFAULT 0\n            )\n        ''')\n        \n        # Indexes for efficient queries\n        conn.execute('CREATE INDEX IF NOT EXISTS idx_access ON states(access_count DESC)')\n        conn.execute('CREATE INDEX IF NOT EXISTS idx_last_accessed ON states(last_accessed)')\n        \n        conn.commit()\n        conn.close()\n    \n    def store_state(self, storage_key, state):\n        \"\"\"\n        Store state in tiered storage.\n        Automatically compresses and chooses appropriate tier.\n        \"\"\"\n        self.stats['storage_writes'] += 1\n        \n        # Serialize and compress\n        serialized = pickle.dumps(state, protocol=pickle.HIGHEST_PROTOCOL)\n        \n        # Compress if beneficial\n        import gzip\n        compressed = gzip.compress(serialized, compresslevel=6)\n        \n        if len(compressed) < len(serialized) * 0.8:\n            data = compressed\n            is_compressed = True\n        else:\n            data = serialized\n            is_compressed = False\n        \n        data_size = len(data)\n        \n        # Store in database (always)\n        conn = sqlite3.connect(self.db_path)\n        conn.execute('''\n            INSERT OR REPLACE INTO states \n            (storage_key, data, compressed, size_bytes)\n            VALUES (?, ?, ?, ?)\n        ''', (storage_key, data, 1 if is_compressed else 0, data_size))\n        conn.commit()\n        conn.close()\n        \n        # Also add to memory cache if space available\n        if self.cache_size_bytes + data_size < self.max_cache_bytes:\n            self.memory_cache[storage_key] = state\n            self.cache_size_bytes += data_size\n            self.access_order.append(storage_key)\n    \n    def retrieve_state(self, storage_key) -> Optional[Dict[str, Any]]:\n        \"\"\"\n        Retrieve state from tiered storage.\n        Checks cache first, then database.\n        \"\"\"\n        # Check memory cache (Tier 1)\n        if storage_key in self.memory_cache:\n            self.stats['cache_hits'] += 1\n            \n            # Update LRU\n            if storage_key in self.access_order:\n                self.access_order.remove(storage_key)\n            self.access_order.append(storage_key)\n            \n            return self.memory_cache[storage_key]\n        \n        # Cache miss - fetch from database (Tier 2)\n        self.stats['cache_misses'] += 1\n        self.stats['storage_reads'] += 1\n        \n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.execute('''\n            SELECT data, compressed FROM states\n            WHERE storage_key = ?\n        ''', (storage_key,))\n        \n        row = cursor.fetchone()\n        \n        if row is None:\n            conn.close()\n            return None\n        \n        data, is_compressed = row\n        \n        # Update access statistics\n        conn.execute('''\n            UPDATE states\n            SET last_accessed = CURRENT_TIMESTAMP,\n                access_count = access_count + 1\n            WHERE storage_key = ?\n        ''', (storage_key,))\n        conn.commit()\n        conn.close()\n        \n        # Decompress if needed\n        if is_compressed:\n            import gzip\n            data = gzip.decompress(data)\n        \n        # Deserialize",
      "turn": 14
    }
  ]
}