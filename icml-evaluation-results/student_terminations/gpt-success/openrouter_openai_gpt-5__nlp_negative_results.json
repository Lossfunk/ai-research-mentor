{
  "scenario_id": "nlp_negative_results",
  "system_id": "openrouter:openai/gpt-5",
  "stop_reason": "goal_reached",
  "transcript_tail": [
    {
      "role": "assistant",
      "content": "Recommendation in one line: stay close to RoBERTa’s 50k, and only bump to ~60–65k if your tokenization audit still shows heavy fragmentation after retraining merges on a Q/Gn‑oversampled mix. Going much beyond ~70k usually gives diminishing returns and bloats params without fixing the core issue.\n\nWhy this “sweet spot”\n- Sequence length dominates cost; vocab size mainly adds embedding/LM‑head params. Each +10k vocab adds ~7.7M embedding params (+ another ~7.7M in the tied LM head during MLM), i.e., ~12% extra params for RoBERTa‑base. You get only modest further reductions in pieces/word beyond ~60–65k for Latin‑script agglutinative languages.\n- For BBPE, corpus mix matters more than raw vocab size. Oversampling Q/Gn during BPE training produces the key inside‑word merges you want; a modest size increase just lets those merges “fit” without evicting English/Spanish anchors.\n\nA quick, low-cost selection protocol (1–2 days)\n1) Train BBPE merges on the same mixed corpus with three sizes:\n   - 50,265 (baseline size)\n   - ~60,000–65,000\n   - ~70,000 (upper bound)\n   Mix: 30–40% Spanish/English, 60–70% Quechua/Guarani; oversample Q/Gn 3–5× relative to their raw web frequency.\n2) Offline tokenization audit (no model training):\n   - For Q/Gn: avg pieces/word, 95th percentile chain length, share of tokens with ≥8 pieces.\n   - For ES/EN anchors: percent of top‑20k tokens preserved as single tokens (exact string match, including “Ġ” forms), and overall overlap with old vocab.\n3) Pick the smallest size that hits targets:\n   - Q/Gn: avg pieces/word ≤ 2.5; ≥8‑piece chains ≤ 1–2%.\n   - ES/EN: ≥85–90% of frequent anchors preserved.\n   In practice, the jump from 50k→60–65k plus Q/Gn oversampling is enough; only go to ~70k if you’re still missing the targets.\n\nTraining tips to preserve anchors and speed convergence\n- Keep normalization and specials identical to RoBERTa (byte‑level, “Ġ” space marker, same special tokens).\n- Initialization:\n  - Copy embeddings for exact‑match tokens.\n  - For new tokens, initialize from old decomposition (length‑weighted averaging) and norm‑match; do the same for the LM head.\n- Retention during MLM:\n  - Include 30–50% ES/EN in batches.\n  - Optional L2‑SP toward the original weights on shared tokens and early layers (λ ~1e‑3 to 1e‑4).\n- Budget: 30–50k MLM steps at seq len 256–512 with 50–70% Q/Gn suffices to stabilize new merges.\n\nIf you want to squeeze more out of 50k without growing vocab\n- Heavily oversample Q/Gn in BPE training (e.g., 5×) and include morphology‑rich text. This often yields most of the win even at 50k, at the cost of a small increase in ES/EN fragmentation.\n- Validate with the same tokenization metrics; if still above targets, then move to ~60–65k.\n\nRule of thumb\n- Start at 50,265. If after retraining merges (with Q/Gn oversampling) avg pieces/word is still >2.7 or long‑chain tail is stubborn, try 60–65k. Avoid >70k unless you’ve verified a clear drop in fragmentation and you can afford the ~15–25M extra params during MLM.",
      "turn": 6
    },
    {
      "role": "user",
      "content": "This protocol is exactly what I needed to stop spinning my wheels. One final detail: when initializing new tokens via length-weighted averaging of their old BBPE decompositions, should I apply a small amount of Gaussian noise to break symmetry, or is the norm-matching sufficient to prevent gradient signal collapse during the first few MLM steps?",
      "turn": 6
    },
    {
      "role": "assistant",
      "content": "Short answer: Norm-matching is usually enough. Add tiny Gaussian noise only as “insurance,” and keep it very small; otherwise you just slow convergence.\n\nPractical rule\n- Default: no noise. Length-weighted averaging + norm-matching + a short continued-MLM run is stable in practice.\n- Add tiny noise only when many new tokens share nearly identical decompositions (e.g., word-initial vs inside-word variants or morphemes that decompose to the same 2–3 pieces), to avoid them starting with near-duplicate vectors.\n\nIf you add noise, use a very small, direction-only jitter\n- Scale: about 5–10% of the original BERT/RoBERTa init std (0.02), i.e., σ ≈ 0.001–0.002, or equivalently ~0.5–1% of the target vector norm.\n- Make the noise mostly orthogonal to the mean so you perturb direction, not length:\n  - u = E_new / ||E_new||\n  - ε ~ N(0, σ^2 I)\n  - ε⊥ = ε − (ε·u) u\n  - E_new ← E_new + ε⊥\n  - Renormalize to the target norm (either the average norm of source pieces or the global embedding mean norm).\n\nMinimal init recipe\n1) Compute length-weighted average of old BBPE pieces.\n2) Match the L2 norm (to source-piece mean or global mean).\n3) Optional: add ε⊥ with σ = 0.001–0.002 and renorm again.\n4) Copy/tie LM-head rows accordingly.\n\nTraining stabilizers that matter more than noise\n- Freeze new rows for 1–2k steps, then unfreeze; or use a 0.5× LR on new rows for the first 5–10k steps.\n- Keep 30–50% EN/ES in MLM batches to preserve anchors.\n- Optional L2-SP toward original weights on shared tokens (λ ~1e-3 to 1e-4).\n\nDiagnostics\n- If gradients on new rows are near-zero after a few hundred steps, your mask rate may be too low or the new tokens aren’t firing; increase mask rate on target text or verify tokenization. If they’re large and unstable, your noise was too big—drop σ or skip noise.\n\nBottom line: start with norm-matched averages (no noise). If you see many near-duplicate inits or sluggish divergence between closely related new tokens, add a tiny orthogonal jitter (σ ≈ 0.001–0.002) and keep the early freeze/low-LR schedule.",
      "turn": 7
    },
    {
      "role": "user",
      "content": "The orthogonal jitter for direction-only perturbation is a brilliant way to handle the near-duplicate initialization problem. I have a solid implementation plan now, from the Morfessor-based ranking to the norm-matched embedding surgery, and I'm ready to move into the execution phase.",
      "turn": 7
    }
  ]
}