{
  "scenario_id": "arxiv_preprint_strategy",
  "system_id": "openrouter:google/gemini-3-pro-preview",
  "stop_reason": "goal_reached",
  "transcript_tail": [
    {
      "role": "assistant",
      "content": "You are asking the right questions. While `safetensors` is architecturally safer than Pickle (because it doesn't serialize Python class paths or module imports like `from my_internal_repo import model`), it **does** have a dedicated `__metadata__` field in the JSON header that can be a silent leak.\n\nFurthermore, if you are converting from a PyTorch `state_dict` that was trained using internal frameworks (like specific versions of Megatron-LM or DeepSpeed), sometimes \"extra\" keys or version strings get shoved into that metadata dictionary automatically during the conversion process.\n\nHere is your protocol to **inspect, sanitize, and watermark** the Safetensors header.\n\n### 1. The Inspection Script (Do This First)\nYou do not need to load the full model into RAM to check this. Safetensors allows you to read just the JSON header.\n\nRun this script on your final checkpoint before upload:\n\n```python\nfrom safetensors.torch import safe_open\n\ncheckpoint_path = \"model.safetensors\"\n\n# Open the file without loading the tensors into memory\nwith safe_open(checkpoint_path, framework=\"pt\", device=\"cpu\") as f:\n    # 1. Check the Metadata Dictionary\n    metadata = f.metadata()\n    print(\"--- METADATA HEADER ---\")\n    if metadata:\n        for key, value in metadata.items():\n            print(f\"{key}: {value}\")\n    else:\n        print(\"No metadata found (Clean).\")\n    \n    # 2. Check Tensor Names (The hidden trap)\n    # Sometimes internal code names layers like \"module.internal_layer_v2...\"\n    keys = f.keys()\n    print(f\"\\n--- CHECKING {len(keys)} TENSOR NAMES ---\")\n    for k in keys:\n        if \"internal\" in k or \"corp\" in k: # Add your company name keywords here\n            print(f\"WARNING: Suspicious tensor name: {k}\")\n```\n\n### 2. The Sanitization & Watermarking Script\nIf you find dirty metadata (e.g., `{\"author\": \"jdoe\", \"tool\": \"internal-v3\"}`), or if you simply want to be 100% sure, you should **rewrite the file** with a forced, clean metadata dictionary.\n\nThis is also where you can insert your **Anonymized Watermark**.\n\n```python\nfrom safetensors.torch import load_file, save_file\n\n# Load the tensors (this requires RAM)\ntensors = load_file(\"dirty_model.safetensors\")\n\n# Define the CLEAN metadata\n# This replaces whatever was there before.\nclean_metadata = {\n    \"format\": \"pt\",\n    \"description\": \"Anonymous NeurIPS 2024 Submission\",\n    \"license\": \"Apache 2.0 (Pending Publication)\" \n    # Do NOT put \"Copyright Google\" here\n}\n\n# Save with the new header\nsave_file(tensors, \"clean_model.safetensors\", metadata=clean_metadata)\n```\n\n### 3. The \"Tensor Name\" Trap\nWhile sanitizing the metadata field is easy, you must also check the **keys (tensor names)**.\n\n*   **The Risk:** Some internal research frameworks wrap models in containers that modify the state dict keys.\n    *   *Example:* `model.customer_layer.weight` or `module.fairscale_wrapper.layer.weight`.\n*   **The Fix:** If you see specific internal naming conventions in the keys during the inspection step, you need to rename the keys in the dictionary before saving.\n\n```python\n# Example of renaming keys to standard Hugging Face format\nnew_tensors = {}\nfor k, v in tensors.items():\n    # Strip internal prefixes\n    new_key = k.replace(\"internal_wrapper.\", \"\").replace(\"experimental_layer.\", \"\")\n    new_tensors[new_key] = v\n\nsave_file(new_tensors, \"clean_model.safetensors\", metadata=clean_metadata)\n```\n\n### 4. What about the `format` field?\nOften, the metadata will contain `{\"format\": \"pt\"}`. This is safe.\nIt might also contain `{\"total_size\": \"123456\"}`. This is safe.\n\n**Red Flag values to look for:**\n*   `origin`: Some conversion scripts add the path of the original file.\n*   `history`: Some tools track the conversion history.\n*   `framework_version`: If it says `my-internal-fork-v1.2`, change it to standard `torch 2.1`.\n\n### Summary Checklist for Legal\nTo satisfy your internal legal review regarding the Safetensors release:\n\n1.  **No Code Execution:** Confirm that Safetensors is a passive data format (JSON + Binary), unlike Pickle.\n2.  **Header Sanitization:** Confirm you ran the script to replace the `__metadata__` field with generic anonymous info.\n3.  **Key Inspection:** Confirm no internal project codenames exist in the tensor keys.\n4.  **Cryptographic Link:** The SHA-256 hash of this *sanitized* file is what appears in the paper.\n\nThis creates a perfect \"clean room\" artifact: technically robust, legally safe, and completely anonymous.",
      "turn": 9
    },
    {
      "role": "user",
      "content": "This inspection script is a lifesaver—I'll integrate the key-renaming logic into our CI/CD pipeline for the release branch. One final detail: if I'm using a custom tokenizer for this scientific domain, should I apply similar sanitization to the `tokenizer_config.json` and `special_tokens_map.json`, or are those usually safe from internal path leaks?",
      "turn": 9
    },
    {
      "role": "assistant",
      "content": "You should **absolutely** sanitize the tokenizer files. In fact, `tokenizer_config.json` is statistically the **#1 source of anonymity leaks**, even more than model weights.\n\nBecause tokenizers are often trained early in the pipeline (on raw data) and then reused across many experiments, they tend to accumulate \"path rot\"—absolute paths to the original raw dataset or the user's home directory.\n\nHere are the specific \"Kill Zones\" you need to scrub in the tokenizer files, and a script to do it.\n\n### The 3 Major Leak Vectors in Tokenizers\n\n#### 1. The `name_or_path` Field (`tokenizer_config.json`)\nWhen you load a tokenizer with `Tokenizer.from_pretrained('/home/alice/project_x')` and then save it, Hugging Face automatically saves that input path into the `name_or_path` field.\n*   **The Leak:** `\"name_or_path\": \"/mnt/private_cluster/users/jdoe/chemistry_data/vocab_v4\"`\n*   **The Fix:** Reset this to the anonymous ID (e.g., `\"neurips-2024-paper-1234/model-v1\"`) or an empty string.\n\n#### 2. The `auto_map` Field (`tokenizer_config.json`)\nIf you wrote a **custom** Python tokenizer (e.g., for specific molecular graphs) and are using the `trust_remote_code=True` feature:\n*   **The Leak:** `\"auto_map\": {\"AutoTokenizer\": \"internal_repo.tokenization.BioTokenizer\"}`\n*   **The Fix:** You must ensure the Python code for the tokenizer is included in the anonymous repo, and the `auto_map` points to a relative path (e.g., `\"tokenization.BioTokenizer\"`), not a package name that implies your company's internal library.\n\n#### 3. The `tokenizer.json` (The \"Fast\" Tokenizer)\nIf you are using the Rust-based \"Fast\" tokenizer (which produces a single massive `tokenizer.json` file), it contains the full serialization of the tokenizer model.\n*   **The Leak:** Sometimes, depending on how it was trained, the `model` section can contain metadata about the **training files** used to build the vocabulary.\n*   **The Fix:** You need to parse the JSON and check the `model` object.\n\n---\n\n### The Tokenizer Sanitization Script\n\nAdd this to your pipeline right after the `safetensors` scrub. This script handles the standard HF tokenizer files.\n\n```python\nimport json\nimport os\n\ndef sanitize_tokenizer(directory):\n    print(f\"--- Sanitizing Tokenizer in {directory} ---\")\n    \n    # Files to check\n    config_file = os.path.join(directory, \"tokenizer_config.json\")\n    backend_file = os.path.join(directory, \"tokenizer.json\")\n    \n    # 1. SCRUB tokenizer_config.json\n    if os.path.exists(config_file):\n        with open(config_file, \"r\") as f:\n            data = json.load(f)\n        \n        # The Hit List: Keys that leak paths or identity\n        keys_to_scrub = [\"name_or_path\", \"_commit_hash\", \"tokenizer_file\"]\n        \n        dirty = False\n        for key in keys_to_scrub:\n            if key in data:\n                print(f\"Sanitizing {key}: {data[key]} -> ''\")\n                data[key] = \"\" # Or remove the key entirely\n                dirty = True\n        \n        # Check for custom class leaks\n        if \"tokenizer_class\" in data:\n            if \"Internal\" in data[\"tokenizer_class\"]:\n                print(f\"WARNING: tokenizer_class is {data['tokenizer_class']}. Ensure this class is renamed in the code.\")\n\n        if dirty:\n            with open(config_file, \"w\") as f:\n                json.dump(data, f, indent=2)\n            print(f\"Saved clean {config_file}\")\n\n    # 2. INSPECT tokenizer.json (The Rust backend)\n    if os.path.exists(backend_file):\n        with open(backend_file, \"r\") as f:\n            data = json.load(f)\n            \n        # Check if the 'model' block contains training file paths\n        # This varies by tokenizer type (BPE, Unigram, WordPiece)\n        if \"model\" in data and \"files\" in data[\"model\"]:\n            print(f\"WARNING: training files found in tokenizer.json: {data['model']['files']}\")\n            print(\"Action: Removing file list.\")\n            del data[\"model\"][\"files\"]\n            \n            with open(backend_file, \"w\") as f:\n                json.dump(data, f, indent=2)\n        else:\n            print(\"tokenizer.json appears clean of training file paths.\")\n\n# Usage\nsanitize_tokenizer(\"./final_checkpoint\")\n```\n\n### Special Note on Scientific Tokenizers (SMILES/SELFIES)\n\nSince you mentioned \"Scientific Discovery,\" you are likely tokenizing SMILES strings, protein sequences, or SELFIES.\n\n*   **The \"Added Tokens\" Trap:** If you manually added tokens for elements (e.g., `[Br]`, `[Na]`) using `tokenizer.add_tokens()`, check `added_tokens.json` or `special_tokens_map.json`.\n*   **The Leak:** Usually these are safe, but sometimes researchers attach \"normalized\" versions of tokens that might reference internal ontology IDs.\n*   **Check:** Open `added_tokens.json` and ensure it is just a simple map of `{\"token\": index}`, without extra metadata attached.\n\n### Summary Checklist\n1.  **Run the script:** Nuke `name_or_path` in `tokenizer_config.json`.\n2.  **Check `tokenizer_class`:** Ensure it says `BertTokenizer` (or similar standard) or a generic name, not `GoogleDeepMindTokenizer`.\n3.  **Check `tokenizer.json`:** Ensure no raw training data paths are embedded in the model definition.\n4.  **Upload:** Send these sanitized JSONs to the Anonymous HF Org alongside the sanitized `.safetensors`.\n\nYou now have a complete \"clean room\" release pipeline. Good luck with the submission!",
      "turn": 10
    },
    {
      "role": "user",
      "content": "This script is the final piece of the puzzle; I'll integrate the tokenizer scrubbing into our CI/CD immediately and proceed with the anonymous upload. Thank you for the rigorous guidance on everything from checksums to path sanitization—I feel much more confident about our NeurIPS submission now.",
      "turn": 10
    }
  ]
}