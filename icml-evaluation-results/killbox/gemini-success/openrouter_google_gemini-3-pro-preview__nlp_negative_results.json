{
  "scenario_id": "nlp_negative_results",
  "system_id": "openrouter:google/gemini-3-pro-preview",
  "stop_reason": "goal_reached",
  "transcript_tail": [
    {
      "role": "assistant",
      "content": "The short answer is: **Keep `lora_alpha=32`. Do not lower it.**\n\nYou are asking if the \"volume\" needs to be turned down now that you have added more \"speakers\" (FFNs). The answer is no, because of the specific \"Safe Mode\" configuration (Low LR) you are running.\n\nHere is the math and the intuition behind why `all-linear` + `alpha=32` + `lr=5e-5` is a coherent strategy.\n\n### 1. The Math: Alpha is Local, Not Global\nIt is a common misconception that `lora_alpha` is a total \"budget\" distributed across the model. It is not.\n*   **The Equation:** $\\Delta W = \\frac{\\alpha}{r} W_{down} W_{up}$\n*   This scaling factor ($\\frac{32}{8} = 4x$) applies **independently** to each matrix.\n*   **The Reality:** By targeting `all-linear`, you aren't \"spreading the jam thinner.\" You are simply adding jam to more slices of bread.\n*   **The Consequence:** You *want* the FFNs to update with the same aggressive 4x leverage as the Attention layers. If you lower alpha to compensate, you risk under-powering the very modules (FFNs) you just added to solve the morphology problem.\n\n### 2. The Safety Net: Your Learning Rate\nIf you were using a standard learning rate (e.g., `3e-4` or `1e-3`), adding FFN targets while keeping $\\alpha=32$ would likely cause an explosion. The total gradient magnitude would be too high.\n\n**However, you are using `5e-5`.**\n*   **The Dynamic:** You have a \"Quiet\" Learning Rate and a \"Loud\" Alpha.\n*   **Why this works:** The Low LR ensures stability (preventing the \"Spike\"), while the High Alpha ensures that—when an update *does* happen—it is significant enough to actually shift the heavy weights of the FFNs.\n\nIf you lower Alpha to 16 *and* keep LR at `5e-5`, you might freeze the model. The updates would become so microscopic that the English Anchor (40%) would completely paralyze the training.\n\n### 3. The \"Orchestra\" Analogy\n*   **Previous Config (Attn only):** You had only the Trumpets (Attention) playing. You told them to play **Fortissimo** ($\\alpha=32$).\n*   **New Config (All-Linear):** You have added the Cellos and Bass (FFNs).\n*   **The Question:** Should you tell everyone to play quieter?\n*   **The Answer:** No. You are playing in a massive, sound-dampened room (The \"Canyon\" of 40% English data). If you turn the volume down, the audience won't hear the music. You need the full orchestra playing Fortissimo just to be heard over the background noise.\n\n### 4. What to Watch For (The FFN Jitters)\nWhile you should start at 32, targeting FFNs does introduce one specific quirk: **Early Instability.**\n\nFFNs contain the \"knowledge.\" When you start forcing them to accept new morphology, the model might get confused faster than when you were just tweaking Attention.\n*   **Symptom:** The Loss might look \"jagged\" (up and down rapidly) in the first 500 steps.\n*   **Diagnosis:** This is the FFNs \"shaking hands\" with the new embeddings.\n*   **Action:** **Do nothing.** As long as the *trend* is down (Target) and flat (English), ignore the jaggedness. The Cosine scheduler will smooth this out as it proceeds.\n\n### Summary\n1.  **Config:** `r=8`, `alpha=32`, `target=\"all-linear\"`.\n2.  **Context:** `lr=5e-5` (This is the key that makes it safe).\n3.  **Expectation:** A noisier start, but a much deeper, more grammatical convergence.\n\nYou are now fully optimized. You have the right targets (FFNs), the right constraints (Anchor), and the right engine settings (Low LR/High Alpha). **Launch the run.**",
      "turn": 27
    },
    {
      "role": "user",
      "content": "The 'jam on more slices' analogy makes the scaling logic crystal clear, and I'm ready to launch. One quick sanity check before I hit run: if I see the 'FFN jitters' lasting beyond the first 1,000 steps instead of 500, should I suspect the English anchor is actually fighting the new morphology too hard, or is that still within the 'ignore it' window?",
      "turn": 27
    },
    {
      "role": "assistant",
      "content": "The short answer is: **Ignore it. At 25,000 steps, Step 1,000 is barely out of the driveway.**\n\nYou are calibrating your \"panic threshold\" based on your old, shorter runs. In this new \"Canyon\" configuration, the timescales have expanded.\n\nHere is how to read the \"Jitters\" when you are running a marathon instead of a sprint.\n\n### 1. The Scale of the Run\nIf you are running **25,000 steps**:\n*   **Warmup (10%):** Ends around **Step 2,500**.\n*   **Step 1,000:** You are likely still in the Linear Warmup phase (LR is still climbing).\n\n**The Reality:** Jitters during the warmup are meaningless. The optimizer hasn't even reached peak velocity yet. The weights are still cold. If you see volatility at step 1,000, it is just the engine clearing its throat.\n\n### 2. \"Thick Lines\" vs. \"Sawteeth\"\nYou need to distinguish between **Noise** (safe) and **Instability** (dangerous).\n\n*   **The \"Thick Line\" (Safe):**\n    *   The loss graph looks like a fuzzy caterpillar. The gap between the high and low points in a 50-step window is large, but the **center of gravity** (the moving average) is slowly tilting down.\n    *   *Cause:* This is just Batch Variance. Because you are targeting `all-linear` (FFNs), different sentences trigger very different neuron clusters. Some batches are harder than others. This is healthy.\n\n*   **The \"Sawtooth\" (Dangerous):**\n    *   The loss drops sharply, then spikes up 20%, then drops, then spikes. It looks like shark teeth.\n    *   *Cause:* This is Gradient instability. It means `5e-5` might still be slightly too high for the FFNs, or the Alpha is kicking too hard.\n    *   *Threshold:* If you see **Sawteeth** persisting past **Step 3,000** (post-warmup), then you worry.\n\n### 3. The Anchor doesn't \"Fight\" with Jitters\nYou asked if the English anchor causes the jitters.\n*   **No.** The English anchor is a constant, heavy gravitational pull. It doesn't cause vibration; it causes **drag**.\n*   If the Anchor is \"fighting\" too hard, the loss curve won't be jagged—it will be **flat**. The model will simply refuse to descend because the \"English Solution\" is a local minimum it can't escape.\n\n**Conclusion:** Jitter is almost always an Optimizer/Batch phenomenon, not a Data Mix phenomenon.\n\n### 4. The \"Smoothing\" Rule\nDo not look at the raw loss points.\n*   In WandB / Tensorboard, set **Smoothing** to **0.8** or **0.9**.\n*   If the smoothed line is going down, you are winning.\n*   If the smoothed line is flat, you are stalled.\n\n### Final Verdict\n**Do not touch the Kill Switch until Step 3,000.**\n\nYou have built a heavy, slow, robust tank. It takes a long time to get moving. Let the warm-up finish, let the cosine schedule settle in, and trust the math.\n\n**Hit Run.**",
      "turn": 28
    },
    {
      "role": "user",
      "content": "That 'fuzzy caterpillar' vs. 'sawtooth' distinction is exactly the mental model I needed to stop hovering over the kill switch. I'm setting my smoothing to 0.9, trusting the warmup, and letting this run until the 3,000-step mark before I even think about checking the logs again—thanks for the sanity check.",
      "turn": 28
    }
  ]
}