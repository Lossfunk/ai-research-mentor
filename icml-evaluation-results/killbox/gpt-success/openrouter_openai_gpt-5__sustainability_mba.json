{
  "scenario_id": "sustainability_mba",
  "system_id": "openrouter:openai/gpt-5",
  "stop_reason": "goal_reached",
  "transcript_tail": [
    {
      "role": "assistant",
      "content": "Use a single downloadable archive. When COPY writes a partitioned directory, package it into a .zip (or .tar.gz) on disk and hand that file handle to st.download_button. Don’t load the files into Python memory; write the archive directly in your capped temp dir and, if space is tight, delete source files as you add them to the archive.\n\nPractical pattern\n- Export to a unique directory under your capped temp dir.\n- Size-gate and headroom-check.\n- Zip to a single file on disk (streaming, not in-memory).\n- Offer st.download_button with an open file handle.\n- Clean up directory and archive via your janitor or a “Delete export” button.\n\nExample\nimport os, uuid, time, zipfile, shutil, streamlit as st\n\nTMP_DIR = \"/var/tmp/duckdb_tmp\"  # your capped tmpfs\n\ndef dir_size_bytes(path: str) -> int:\n    total = 0\n    for root, _, files in os.walk(path):\n        for f in files:\n            try:\n                total += os.path.getsize(os.path.join(root, f))\n            except FileNotFoundError:\n                pass\n    return total\n\ndef zip_dir_stream(src_dir: str, zip_path: str, delete_src=True):\n    with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED, compresslevel=6) as zf:\n        for root, _, files in os.walk(src_dir):\n            for f in files:\n                full = os.path.join(root, f)\n                arc = os.path.relpath(full, src_dir)\n                zf.write(full, arc)\n                if delete_src:\n                    try: os.remove(full)\n                    except Exception: pass\n        # remove emptied dirs if requested\n        if delete_src:\n            for root, dirs, _ in os.walk(src_dir, topdown=False):\n                for d in dirs:\n                    p = os.path.join(root, d)\n                    try: os.rmdir(p)\n                    except Exception: pass\n            try: os.rmdir(src_dir)\n            except Exception: pass\n\ndef has_headroom(path=TMP_DIR, need_mb=512):\n    try:\n        u = shutil.disk_usage(path)\n        return (u.free // (1024*1024)) >= need_mb\n    except FileNotFoundError:\n        return False\n\ndef prepare_partitioned_export(con, sql, partition_cols):\n    run_id = uuid.uuid4().hex[:8]\n    out_dir = os.path.join(TMP_DIR, f\"esg_export_{run_id}\")\n    zip_path = out_dir + \".zip\"\n\n    # 1) Check headroom for export (at least 512 MB free, adjust as needed)\n    if not has_headroom(TMP_DIR, 512):\n        st.warning(\"Low temp disk headroom. Narrow your filters or try later.\")\n        return None, None\n\n    # 2) Run COPY into a directory\n    with st.spinner(\"Exporting partitioned Parquet...\"):\n        con.execute(f\"COPY ({sql}) TO ? (FORMAT PARQUET, PARTITION_BY ({', '.join(partition_cols)}), COMPRESSION 'zstd', ROW_GROUP_SIZE 200000);\", [out_dir])\n\n    # Optional: estimate size to ensure we can zip\n    dir_bytes = dir_size_bytes(out_dir)\n    # Reserve roughly the same space for the zip (conservative)\n    free_mb = shutil.disk_usage(TMP_DIR).free // (1024*1024)\n    if free_mb * 1024 * 1024 < dir_bytes * 1.1:\n        st.warning(\"Not enough temp space to package the export. You can download partitions manually from the server, or narrow your filters.\")\n        return out_dir, None\n\n    # 3) Zip in place, deleting sources to keep peak disk low\n    with st.spinner(\"Packaging export (.zip)...\"):\n        zip_dir_stream(out_dir, zip_path, delete_src=True)\n\n    return out_dir, zip_path  # out_dir may be removed already\n\nUsage in Streamlit\ncon = duck_conn()\nsql = \"\"\"\nSELECT org_id, vendor_id, esg_score, tier, year\nFROM read_parquet(? || '/**/*.parquet')\nWHERE org_id IN (SELECT org_id FROM org_filter)\n\"\"\"\n# con.register(\"org_filter\", ...) and set base path param as needed\n\nout_dir, zip_path = prepare_partitioned_export(con, sql, partition_cols=[\"year\", \"tier\"])\nif zip_path and os.path.exists(zip_path):\n    size_mb = os.path.getsize(zip_path) / (1024*1024)\n    st.caption(f\"Archive size: {size_mb:.1f} MB\")\n    with open(zip_path, \"rb\") as fh:\n        st.download_button(\n            \"Download partitioned Parquet (.zip)\",\n            fh,\n            file_name=\"esg_export.zip\",\n            mime=\"application/zip\",\n        )\n\n    # Optional cleanup button (or rely on your janitor)\n    def _cleanup():\n        try: os.remove(zip_path)\n        except Exception: pass\n    st.button(\"Delete export from server\", on_click=_cleanup)\n\nNotes and options\n- Tarball alternative: tar.gz can be faster/smaller for many small files. Use tarfile.open(..., \"w:gz\") similarly and set mime to application/gzip.\n- Space constraints: Zipping duplicates bytes temporarily. The “zip while deleting sources” approach above keeps peak usage near max(dir_size, zip_size) instead of dir_size + zip_size.\n- Very large exports: Consider COPY to an S3 prefix (PARTITION_BY ...) and show a pre-signed URL to the prefix or a downloadable manifest CSV listing the object URIs. That avoids large archives on the VM.\n- Single-file option: Offer a toggle to export “single Parquet file” (no PARTITION_BY) when partitions aren’t required; simpler to download and may be fine for moderate sizes.",
      "turn": 51
    },
    {
      "role": "user",
      "content": "This 'zip-while-deleting' logic is brilliant for staying under my resource cap. If I want to include a 'Data Dictionary' PDF or a README text file inside that same ZIP archive for the users, what's the best way to inject those static files into the `zip_dir_stream` process?",
      "turn": 51
    },
    {
      "role": "assistant",
      "content": "Best approach: extend your zipping step to accept “extra” entries and add them to the archive after (or before) you walk the Parquet output directory. Use zipfile.write for on-disk files (PDF) and writestr for small in-memory text (README). Don’t delete the static source files; they’re shared assets.\n\nDrop-in implementation\n\nimport os, zipfile, shutil\n\ndef _estimate_bytes(path):\n    try:\n        return os.path.getsize(path)\n    except Exception:\n        return 0\n\ndef zip_dir_stream(src_dir: str, zip_path: str, delete_src=True, extras=None):\n    \"\"\"\n    Zip src_dir contents into zip_path.\n    - delete_src: remove files from src_dir after adding to keep peak disk low.\n    - extras: list of dicts describing additional files to inject into the zip.\n        Each item supports:\n          {\"path\": \"/abs/path/DataDictionary.pdf\", \"arc\": \"docs/DataDictionary.pdf\"}\n          {\"bytes\": b\"...\", \"arc\": \"README.txt\"}  # for small text\n    \"\"\"\n    extras = extras or []\n    with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED, compresslevel=6) as zf:\n        # 1) Add Parquet export dir\n        for root, _, files in os.walk(src_dir):\n            for f in files:\n                full = os.path.join(root, f)\n                arc = os.path.relpath(full, src_dir)\n                try:\n                    zf.write(full, arc)\n                except FileNotFoundError:\n                    continue\n                if delete_src:\n                    try: os.remove(full)\n                    except Exception: pass\n        if delete_src:\n            # clean empty dirs\n            for root, dirs, _ in os.walk(src_dir, topdown=False):\n                for d in dirs:\n                    p = os.path.join(root, d)\n                    try: os.rmdir(p)\n                    except Exception: pass\n            try: os.rmdir(src_dir)\n            except Exception: pass\n\n        # 2) Inject extras\n        for e in extras:\n            if \"path\" in e:\n                p = e[\"path\"]\n                arc = e.get(\"arc\") or os.path.basename(p)\n                # Writes from disk in chunks; does not load entire file in RAM\n                zf.write(p, arc)\n            elif \"bytes\" in e:\n                # For small text (README, MANIFEST). Avoid for large binaries.\n                arc = e[\"arc\"]\n                zf.writestr(arc, e[\"bytes\"])\n\nHeadroom check (include extras)\n\ndef calc_export_size_bytes(src_dir, extras):\n    # Size of Parquet dir + extras-on-disk + extras-in-memory\n    total = 0\n    for root, _, files in os.walk(src_dir):\n        for f in files:\n            try:\n                total += os.path.getsize(os.path.join(root, f))\n            except FileNotFoundError:\n                pass\n    for e in extras or []:\n        if \"path\" in e:\n            total += _estimate_bytes(e[\"path\"])\n        elif \"bytes\" in e:\n            total += len(e[\"bytes\"])\n    return total\n\nUsage in your prepare step\n\ndef generate_readme(meta: dict) -> str:\n    # Build a small README dynamically (safe to writestr)\n    lines = [\n        \"ESG Export\",\n        f\"Version: {meta.get('version','n/a')}\",\n        f\"Generated: {meta.get('ts','')}\",\n        f\"Partitions: {', '.join(meta.get('partitions', []))}\",\n        \"\",\n        \"Contents:\",\n        \"- Parquet dataset under ./\",\n        \"- Data Dictionary: docs/DataDictionary.pdf\",\n        \"\",\n        \"Notes:\",\n        \"All numeric columns are stored in Parquet with standard types.\",\n    ]\n    return \"\\n\".join(lines)\n\n# After COPY ... TO out_dir (partitioned)\nextras = [\n    {\"path\": \"/opt/app/static/DataDictionary.pdf\", \"arc\": \"docs/DataDictionary.pdf\"},\n    {\"bytes\": generate_readme({\"version\": version, \"ts\": ts, \"partitions\": [\"year\",\"tier\"]}).encode(\"utf-8\"),\n     \"arc\": \"README.txt\"},\n]\n\nexpected_bytes = calc_export_size_bytes(out_dir, extras)\nfree_bytes = shutil.disk_usage(TMP_DIR).free\nif free_bytes < int(expected_bytes * 1.1):  # conservative\n    st.warning(\"Not enough temp space to package the export with docs. Please narrow filters.\")\n    st.stop()\n\nzip_dir_stream(out_dir, zip_path, delete_src=True, extras=extras)\nwith open(zip_path, \"rb\") as fh:\n    st.download_button(\"Download (.zip with README + Data Dictionary)\", fh, file_name=\"esg_export.zip\", mime=\"application/zip\")\n\nNotes\n- For large documentation files, always use the \"path\" method (zipfile.write) so Python doesn’t load them fully into memory. Reserve writestr for small text like README or MANIFEST.json.\n- Keep arc names stable and user-friendly (e.g., README.txt at the root; docs/DataDictionary.pdf).\n- If you prefer tar.gz, the same pattern works with tarfile and addfile; use a fileobj for large files and add bytes for small text.",
      "turn": 52
    },
    {
      "role": "user",
      "content": "This is exactly what I needed to make the capstone look professional for the final presentation. The `writestr` trick for the dynamic README is a great touch for keeping the export context-aware without extra disk overhead. I'm ready to build this out!",
      "turn": 52
    }
  ]
}