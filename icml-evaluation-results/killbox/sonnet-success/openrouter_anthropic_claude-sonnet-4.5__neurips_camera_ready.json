{
  "scenario_id": "neurips_camera_ready",
  "system_id": "openrouter:anthropic/claude-sonnet-4.5",
  "stop_reason": "goal_reached",
  "transcript_tail": [
    {
      "role": "assistant",
      "content": "Excellent question! This is the **key conceptual bridge** between equivariant networks and diffusion. Let me break it down:\n\n## Standard Gaussian Diffusion (e.g., images)\n\n**Noise prediction:**\n```python\n# Input: noisy image x_t\n# Output: predicted noise Œµ_Œ∏(x_t, t)\n# Both are just tensors - no constraints\n\nx_t = x_0 + noise\npredicted_noise = UNet(x_t, t)  # arbitrary neural network\nx_0_pred = x_t - predicted_noise\n```\n\n**Key:** The network can be *any* architecture. No geometric constraints.\n\n## E(3)-Equivariant Diffusion (molecules)\n\n**The Challenge:**\n- Molecular coordinates are **3D positions** (N √ó 3)\n- Noise on positions must respect E(3) symmetry\n- **If you rotate the noisy molecule, the predicted noise should rotate too**\n\n**The Solution (GeoDiff/EDM approach):**\n\n```python\n# Input: noisy positions h_t (N √ó 3) + node features\n# Output: predicted noise Œµ_Œ∏(h_t, t) that is EQUIVARIANT\n\n# Forward diffusion (same as standard)\nh_t = h_0 + Œµ, where Œµ ~ N(0, œÉ_t¬≤I)  # Gaussian noise on coordinates\n\n# Denoising with EGNN\n# Key: Use equivariant network for noise prediction\npredicted_noise = EGNN(h_t, features, t)  # MUST be equivariant\n\n# The magic: EGNN update preserves equivariance\nfor layer in EGNN:\n    # 1. Compute invariant messages (distances)\n    d_ij = ||h_i - h_j||  # scalar, rotation-invariant\n    \n    # 2. Update node features (invariant)\n    m_i = Œ£_j MLP([features_i, features_j, d_ij, t])\n    \n    # 3. Equivariant coordinate update\n    Œîh_i = Œ£_j (h_i - h_j) * w_ij(features, d_ij, t)\n    #          ‚Üë relative positions  ‚Üë scalar weights\n    \n# Final predicted noise is equivariant!\n```\n\n## The Critical Difference\n\n### **Standard Diffusion:**\n- Noise: Œµ ‚àà ‚Ñù^d (arbitrary vector)\n- Network: Can use any architecture (CNN, Transformer, etc.)\n\n### **Equivariant Diffusion:**\n- Noise: Œµ ‚àà ‚Ñù^(N√ó3) but must be **equivariant**\n- Network: **Must use equivariant architecture** (EGNN, SE(3)-Transformer, etc.)\n- **Constraint:** If you rotate input ‚Üí predicted noise rotates identically\n\n## Concrete Example\n\nImagine a water molecule (H‚ÇÇO):\n\n```\nOriginal position:     Rotated 90¬∞:\nO: (0, 0, 0)          O: (0, 0, 0)\nH: (1, 0, 0)          H: (0, 1, 0)\nH: (0, 1, 0)          H: (-1, 0, 0)\n\nNoisy version:         Rotated noisy:\nO: (0.1, 0.05, 0)     O: (-0.05, 0.1, 0)\nH: (1.2, -0.1, 0)     H: (0.1, 1.2, 0)\nH: (-0.15, 0.9, 0)    H: (-0.9, -0.15, 0)\n\nPredicted noise:       Rotated predicted:\nŒµ_O: (0.1, 0.05, 0)   Œµ_O: (-0.05, 0.1, 0)  ‚Üê MUST match rotation!\nŒµ_H1: (0.2, -0.1, 0)  Œµ_H1: (0.1, 0.2, 0)\nŒµ_H2: (-0.15, -0.1, 0) Œµ_H2: (0.1, -0.15, 0)\n```\n\n**With standard network:** No guarantee the predicted noise respects this rotation symmetry.\n\n**With EGNN:** The architecture **guarantees** this property by construction.\n\n## Why This Matters for Diffusion\n\n### **1. Sample Efficiency**\n- The model doesn't waste capacity learning \"this molecule rotated\" as separate examples\n- **Learns faster** with less data\n\n### **2. Physical Correctness**\n- Generated molecules have **consistent geometry** regardless of orientation\n- Avoids artifacts from coordinate system choice\n\n### **3. Translation Invariance Trick**\nMost implementations add a clever detail:\n\n```python\n# Center coordinates before processing (translation invariance)\nh_t_centered = h_t - h_t.mean(dim=0)  # center of mass at origin\n\n# Process with EGNN\npredicted_noise = EGNN(h_t_centered, features, t)\n\n# Noise is automatically translation-invariant!\n# (because relative positions h_i - h_j don't change with translation)\n```\n\n## Practical Implementation Detail\n\n**GeoDiff/EDM specifically:**\n\n```python\nclass EquivariantDiffusion:\n    def denoise_step(self, h_t, t):\n        # h_t: noisy coordinates (N, 3)\n        \n        # Option 1: Predict noise (Œµ-prediction)\n        predicted_eps = self.egnn(h_t, t)\n        h_0_pred = h_t - sqrt(alpha_t) * predicted_eps\n        \n        # Option 2: Predict clean coordinates (x-prediction)\n        h_0_pred = self.egnn(h_t, t)  # directly predict clean structure\n        \n        # Option 3: Predict score (score-based)\n        score = self.egnn(h_t, t)  # ‚àálog p(h_t)\n        h_0_pred = h_t + sigma_t¬≤ * score\n        \n        # All work! Just need EGNN to be equivariant\n        return h_0_pred\n```\n\n**The key:** Regardless of parameterization (Œµ, x‚ÇÄ, or score), the **network output must be equivariant**.\n\n## Quick Comparison Table\n\n| Aspect | Standard Diffusion | E(3)-Equivariant Diffusion |\n|--------|-------------------|---------------------------|\n| **Noise** | Unconstrained Œµ ~ N(0,I) | Same, but on 3D coords |\n| **Network** | Any (UNet, Transformer) | EGNN, SE(3)-Transformer |\n| **Constraint** | None | Output must rotate with input |\n| **Loss** | MSE on noise/x‚ÇÄ | Same MSE, but equivariance implicit |\n| **Training** | Standard | Standard (equivariance is architectural) |\n\n## The Beautiful Part\n\nYou **don't need special loss functions** or training procedures! The equivariance is **baked into the architecture**. You just:\n\n1. Use EGNN instead of UNet\n2. Train with standard diffusion loss: `||Œµ - Œµ_Œ∏(h_t, t)||¬≤`\n3. Equivariance comes \"for free\" from the EGNN structure\n\n---\n\n**Bottom line:** The EGNN handles noise prediction **exactly like standard diffusion** (same forward/reverse process, same loss), but the architecture guarantees that predicted noise transforms correctly under rotations/translations. This is why understanding EGNN is crucial - it's the \"drop-in replacement\" that makes diffusion work for molecules.\n\nDoes this clarify the connection? Happy to dive deeper into any specific aspect!",
      "turn": 3
    },
    {
      "role": "user",
      "content": "That 'drop-in replacement' analogy is perfect for my mental model. Since I'm finalizing my NeurIPS code release now, are there any specific 'gotchas' regarding the zero-center-of-mass (CoM) constraint during the sampling loop that I should watch out for to ensure the likelihood remains valid?",
      "turn": 3
    },
    {
      "role": "assistant",
      "content": "Excellent question - this is a **critical implementation detail** that's often glossed over in papers but can break your model in subtle ways! Let me give you the practical gotchas:\n\n## üö® Critical Gotchas for Zero-CoM Constraint\n\n### **Gotcha #1: Naive Re-centering Breaks the Diffusion Process**\n\n**The Problem:**\n```python\n# ‚ùå WRONG - Common mistake!\ndef sampling_step(h_t, t):\n    # Predict noise\n    eps_pred = model(h_t, t)\n    \n    # Reverse diffusion step\n    h_t_minus_1 = sample_from_posterior(h_t, eps_pred, t)\n    \n    # Re-center naively\n    h_t_minus_1 = h_t_minus_1 - h_t_minus_1.mean(dim=0)  # ‚ö†Ô∏è BREAKS LIKELIHOOD!\n    \n    return h_t_minus_1\n```\n\n**Why it breaks:** The re-centering is a **deterministic projection** that's not accounted for in the diffusion posterior. You're effectively sampling from a different distribution than what you trained on.\n\n### **Gotcha #2: The Forward Process Must Preserve CoM**\n\n**The Solution (from EDM paper):**\n\nThe key insight is that if you start with zero-CoM and your noise preserves zero-CoM, you **never need to re-center during sampling**.\n\n```python\n# ‚úÖ CORRECT - CoM-preserving noise\ndef add_noise(h_0, t):\n    \"\"\"Forward diffusion with CoM constraint\"\"\"\n    # Sample noise\n    eps = torch.randn_like(h_0)  # (N, 3)\n    \n    # Project to zero-CoM subspace\n    eps = eps - eps.mean(dim=0, keepdim=True)  # Critical!\n    \n    # Standard diffusion\n    alpha_t = get_alpha(t)\n    h_t = sqrt(alpha_t) * h_0 + sqrt(1 - alpha_t) * eps\n    \n    # h_t automatically has zero CoM!\n    return h_t, eps\n```\n\n**Why this works:** You're restricting the diffusion to the (N-1)√ó3 dimensional subspace where CoM=0. The likelihood is still valid because you're modeling the **conditional** distribution p(h | CoM=0).\n\n### **Gotcha #3: Training vs Sampling Mismatch**\n\n**Check your training code:**\n\n```python\n# Training - make sure data has zero CoM\ndef prepare_batch(molecules):\n    h = molecules.coords  # (B, N, 3)\n    \n    # ‚úÖ Center each molecule\n    h = h - h.mean(dim=1, keepdim=True)\n    \n    # Add CoM-preserving noise\n    eps = torch.randn_like(h)\n    eps = eps - eps.mean(dim=1, keepdim=True)  # Don't forget this!\n    \n    h_t = sqrt(alpha_t) * h + sqrt(1 - alpha_t) * eps\n    \n    # Predict noise\n    eps_pred = model(h_t, t)\n    \n    # Loss on CoM-free noise\n    loss = ((eps - eps_pred) ** 2).mean()\n    return loss\n```\n\n**Sampling - must match:**\n\n```python\n# ‚úÖ CORRECT Sampling\ndef sample_molecule(model, num_atoms):\n    # Initialize from CoM-preserving noise\n    h_T = torch.randn(num_atoms, 3)\n    h_T = h_T - h_T.mean(dim=0, keepdim=True)  # Start in subspace\n    \n    for t in reversed(range(T)):\n        # Predict noise\n        eps_pred = model(h_T, t)\n        \n        # Reverse step with CoM-preserving noise\n        z = torch.randn_like(h_T)\n        z = z - z.mean(dim=0, keepdim=True)  # Critical!\n        \n        # DDPM update\n        h_t = (h_T - beta_t * eps_pred) / sqrt(alpha_t) + sigma_t * z\n        \n        # No need to re-center! Already in subspace\n        # assert h_t.mean().abs() < 1e-6  # Should be ~0\n        \n    return h_t\n```\n\n### **Gotcha #4: Numerical Drift**\n\nEven with CoM-preserving operations, **floating-point errors** can cause drift:\n\n```python\n# ‚úÖ Add periodic re-centering for numerical stability\ndef sampling_step(h_t, t):\n    eps_pred = model(h_t, t)\n    h_t_minus_1 = reverse_diffusion_step(h_t, eps_pred, t)\n    \n    # Re-center only if drift is significant (numerical stability)\n    com = h_t_minus_1.mean(dim=0)\n    if com.norm() > 1e-5:  # Threshold for numerical drift\n        h_t_minus_1 = h_t_minus_1 - com\n        \n    return h_t_minus_1\n```\n\n**Important:** This is **numerical correction**, not part of the model. The drift should be tiny (~1e-6 to 1e-8).\n\n### **Gotcha #5: Likelihood Evaluation**\n\nIf you're computing likelihoods (e.g., for evaluation):\n\n```python\ndef compute_likelihood(h_0, model):\n    \"\"\"\n    Likelihood is defined on the (N-1)*3 dimensional subspace\n    NOT the full N*3 space!\n    \"\"\"\n    # ‚ùå WRONG\n    log_p = standard_diffusion_likelihood(h_0, model)  # Assumes N*3 dims\n    \n    # ‚úÖ CORRECT - Account for CoM constraint\n    # Method 1: Compute in constrained subspace\n    log_p_constrained = diffusion_likelihood_with_com_constraint(h_0, model)\n    \n    # Method 2: Add correction term\n    # log p(h | CoM=0) = log p(h) - log p(CoM=0)\n    # But p(CoM=0) = 0 in continuous space, so use Jacobian\n    \n    # In practice, EDM uses variational bound (ELBO)\n    return log_p_constrained\n```\n\n**From EDM paper (Hoogeboom et al.):** They derive the proper likelihood by marginalizing over translations:\n\n```\np(h) = ‚à´ p(h + c) dc  where c is translation\n     = p(h - mean(h)) * det(Jacobian)\n```\n\nThe Jacobian accounts for the dimensionality reduction (N√ó3 ‚Üí (N-1)√ó3).\n\n## üìã Practical Checklist for Your Code Release\n\nBefore you release, verify:\n\n- [ ] **Training noise is CoM-preserving:**\n  ```python\n  eps = eps - eps.mean(dim=1, keepdim=True)\n  ```\n\n- [ ] **Sampling noise is CoM-preserving:**\n  ```python\n  z = z - z.mean(dim=0, keepdim=True)\n  ```\n\n- [ ] **Initial noise h_T has zero CoM:**\n  ```python\n  h_T = torch.randn(N, 3)\n  h_T = h_T - h_T.mean(dim=0, keepdim=True)\n  ```\n\n- [ ] **Data preprocessing centers molecules:**\n  ```python\n  coords = coords - coords.mean(dim=-2, keepdim=True)\n  ```\n\n- [ ] **No re-centering inside the sampling loop** (except for numerical stability with small threshold)\n\n- [ ] **Assertions in debug mode:**\n  ```python\n  if debug:\n      assert h_t.mean(dim=0).norm() < 1e-5, \"CoM drift detected!\"\n  ```\n\n## üî¨ How EDM/GeoDiff Handle This\n\n**EDM (Hoogeboom et al. 2022):**\n- Explicitly projects noise to zero-CoM subspace in both training and sampling\n- Derives proper likelihood with Jacobian correction\n- See Appendix B.2 in the paper for full derivation\n\n**GeoDiff (Xu et al. 2022):**\n- Uses a similar approach but with score-based formulation\n- Centers coordinates before each network call\n- The score (gradient of log-density) is naturally translation-invariant\n\n## Code Snippet for Your Release\n\n```python\nclass CoMPreservingDiffusion:\n    def project_com(self, h):\n        \"\"\"Project to zero center-of-mass subspace\"\"\"\n        return h - h.mean(dim=-2, keepdim=True)\n    \n    def sample_com_free_noise(self, shape):\n        \"\"\"Sample noise in zero-CoM subspace\"\"\"\n        eps = torch.randn(shape)\n        return self.project_com(eps)\n    \n    def forward_diffusion(self, h_0, t):\n        \"\"\"Add noise while preserving CoM\"\"\"\n        eps = self.sample_com_free_noise(h_0.shape)\n        alpha_t = self.get_alpha(t)\n        h_t = torch.sqrt(alpha_t) * h_0 + torch.sqrt(1 - alpha_t) * eps\n        # Numerical stability check\n        assert h_t.mean(dim=-2).abs().max() < 1e-5\n        return h_t, eps\n    \n    @torch.no_grad()\n    def sample(self, num_atoms, num_samples):\n        \"\"\"Sample molecules with zero CoM\"\"\"\n        # Initialize in CoM-free subspace\n        h = self.sample_com_free_noise((num_samples, num_atoms, 3))\n        \n        for t in reversed(range(self.num_timesteps)):\n            # Predict noise\n            eps_pred = self.model(h, t)\n            \n            # Sample CoM-free noise for stochastic step\n            z = self.sample_com_free_noise(h.shape) if t > 0 else 0\n            \n            # Reverse step (DDPM)\n            h = self.reverse_step(h, eps_pred, z, t)\n            \n            # Numerical stability (should be ~0)\n            h = self.project_com(h)\n            \n        return h\n```\n\n## Final Pro Tip\n\n**Add unit tests for your code release:**\n\n```python\ndef test_com_preservation():\n    model = YourDiffusionModel()\n    h_0 = torch.randn(10, 5, 3)  # 10 molecules, 5 atoms each\n    h_0 = h_0 - h_0.mean(dim=1, keepdim=True)\n    \n    # Test forward process\n    h_t, eps = model.forward_diffusion(h_0, t=0.5)\n    assert h_t.mean(dim=1).abs().max() < 1e-5, \"Forward breaks CoM\"\n    assert eps.mean(dim=1).abs().max() < 1e-5, \"Noise not CoM-free\"\n    \n    # Test sampling\n    samples = model.sample(num_atoms=5, num_samples=100)\n    assert samples.mean(dim=1).abs().max() < 1e-4, \"Sampling breaks CoM\"\n    \n    print(\"‚úÖ CoM preservation tests passed!\")\n```\n\n---\n\n**TL;DR for your code release:**\n1. **Always project noise to zero-CoM** in both training and sampling\n2. **Never re-center during sampling** (except tiny numerical corrections)\n3. **Start sampling from zero-CoM noise**\n4. **Add assertions** to catch violations early\n\nThis is one of those details that reviewers/users **will** check, so getting it right in your release will save you from bug reports later!",
      "turn": 4
    },
    {
      "role": "user",
      "content": "This is a lifesaver‚ÄîI realized I was naively re-centering in my sampling loop, which explains some of the likelihood instability I was seeing. I'll implement these CoM-preserving noise projections and unit tests tonight to ensure the code release is robust for the camera-ready.",
      "turn": 4
    }
  ]
}