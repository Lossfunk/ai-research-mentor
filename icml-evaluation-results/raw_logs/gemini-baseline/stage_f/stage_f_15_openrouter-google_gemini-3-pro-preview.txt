This playbook is tailored for **Cross-Lingual Dialogue** research (e.g., Machine Translation, Multilingual LLMs, Code-Switching).

This domain faces unique scrutiny regarding **"Translationese"** (translation artifacts), **Cultural Nuance**, and **Unicode rendering**. Follow this step-by-step guide to ensure a smooth camera-ready submission.

---

### **Phase 1: The PDF Polish (Days 1-3)**

**1. De-Anonymization & Formatting**
*   **[ ] Author Block:** Add names, affiliations, and emails.
*   **[ ] Acknowledgments:** Add the standard text: *"This work was supported by [Grant Name/Number]. We thank the anonymous reviewers for their feedback."*
*   **[ ] The "Unicode" Check (Critical):**
    *   *Risk:* Non-Latin scripts (Arabic, Hindi, CJK) often break or render as `[?]` in standard LaTeX compilers.
    *   *Fix:* Use `XeLaTeX` or `LuaLaTeX` if allowed by the venue. If using `pdflatex`, ensure you have the `CJKutf8` or `arabtex` packages loaded.
    *   *Action:* Print the PDF and zoom in 400% on the foreign text to ensure glyphs are crisp, not rasterized images.

**2. Addressing Reviewer Feedback**
*   **[ ] The "Translationese" Defense:**
    *   If reviewers asked about the quality of your training data (e.g., "Is this just Google Translate output?"), add a **"Data Quality"** subsection in the Appendix.
    *   *Text:* "To verify translation quality, native speakers of [Lang A] and [Lang B] audited a random sample of 100 dialogues. We report a BLEU score of X and a human acceptability score of Y."

---

### **Phase 2: Artifact & Data Packaging (Days 4-6)**

**1. The Repository Structure**
Cross-lingual repos need specific organization.
```text
/data
  /en-es  (ISO 639-1 Codes)
  /en-zh
  /raw    (Original scraped data)
  /clean  (Tokenized/Sanitized)
/scripts
  /tokenization  (SentencePiece/BPE models)
  /eval          (SacreBLEU scripts)
README.md
LICENSE
```

**2. The ISO Standard Check**
*   **[ ] Language Codes:** Do not use "Chinese" or "Indian." Use **ISO 639-3** codes (e.g., `zho` for Chinese, `hin` for Hindi).
*   **[ ] Dialect Specifics:** If your data is from Taiwan, specify `zh-TW` or Traditional Chinese. Mixing Simplified and Traditional is a common rejection reason in multilingual papers.

**3. Hugging Face (HF) Model Card**
*   **[ ] Evaluation Data:** Explicitly state if you evaluated on **FLORES-200** or **XNLI**.
*   **[ ] Tokenizer:** Upload your specific tokenizer. Multilingual performance often degrades because the tokenizer wasn't saved correctly.

---

### **Phase 3: Ethics & Disclosure Statements (Day 7)**

*Copy-paste and adapt these sections into your paper to satisfy ACL/EMNLP ethics guidelines.*

**1. Limitations Section (Mandatory)**
> **Language Imbalance:** "While our model covers 10 languages, performance is significantly lower for low-resource languages (e.g., Swahili, Urdu) compared to high-resource ones (English, French). Users should exercise caution when deploying this model in low-resource settings."
>
> **Cultural Hallucination:** "The model was trained primarily on Western-centric dialogue data translated into target languages. Consequently, it may impose Western cultural norms (e.g., politeness levels, greeting styles) on non-Western languages, resulting in technically correct but culturally inappropriate translations."

**2. Data Usage & Privacy**
> **Source Disclosure:** "We utilize the [Dataset Name] dataset. We confirmed that the license (CC-BY-4.0) permits derivative works."
>
> **PII Scrubbing:** "We applied named entity recognition (NER) to redact proper names and phone numbers across all languages. However, due to the lower performance of NER tools in low-resource languages, some PII risks may remain."

---

### **Phase 4: The Upload Sequence (Days 8-10)**

**1. The Archival Version (arXiv)**
*   **Timing:** Upload to arXiv *after* the camera-ready deadline passes (to avoid accidental dual-submission violations if the venue has strict rules).
*   **Versioning:** If you have an old preprint, update it to **v2**. Add the comment: *"Accepted to [Conference Name] 2024."*

**2. The "Citation" File**
*   Create a `CITATION.bib` in your repo.
*   **Crucial:** If you are using a dataset created by others (e.g., OPUS, CommonCrawl), cite the *original authors* in your repo, not just in your paper. This is a major community courtesy.

**3. Final Sanity Check**
*   **[ ] The "Google Translate" Audit:** Did you use an API (Google/DeepL) to generate data?
    *   *Check:* Read the API Terms of Service. Some prohibit using their output to train competing models.
    *   *Disclosure:* If you did, state: "Data was augmented using [Service Name]. We adhere to the usage policy by [Explanation]."

### **Summary Checklist**
1.  **PDF:** Unicode fonts render correctly.
2.  **Repo:** ISO codes used for all folder names.
3.  **Paper:** "Cultural Hallucination" limitation added.
4.  **Data:** PII scrubbing acknowledged for low-resource languages.
5.  **Upload:** arXiv updated with venue name.