I have generated a **Self-Audit Checklist** tailored to the "Compressible LLM" project we discussed.

These are the most common compliance traps in EMNLP/ACL/NeurIPS submissions for efficiency papers.

### **1. Dataset & License Compliance**
**The Risk:** You used a model (e.g., LLaMA-3) or a dataset (e.g., The Pile, C4) without verifying if your compression method creates a "derivative work" that violates the original license.

| **Risk Flag** | **Why it’s a problem** | **The Fix (Actionable)** |
| :--- | :--- | :--- |
| **"We used the Books3 dataset..."** | **High Risk.** Books3 is currently involved in copyright litigation. Using it may trigger a desk rejection or ethics flag. | **Fix:** Switch to **RedPajama-v2** or **Dolma** (fully licensed stacks). If you *must* use it, explicitly justify it as "legacy reproduction" and acknowledge the controversy. |
| **"We distilled GPT-4..."** | **Medium Risk.** OpenAI's TOS prohibits using model outputs to train competing foundation models. | **Fix:** Clarify that this is for **research analysis only**, not for a commercial product. Verify if you are using a dataset like *ShareGPT* (user-uploaded) vs. direct API scraping (TOS violation). |
| **LLaMA Community License** | LLaMA is *not* Open Source (OSI definition). Calling it "Open Source" in the abstract is factually incorrect. | **Fix:** Refer to it as **"Open Weights"** or **"Publicly Available."** Ensure your compressed model release adheres to the LLaMA Community License (attribution requirements). |

### **2. Human Data & Privacy (PII)**
**The Risk:** Even if you didn't collect new data, the datasets you used for calibration (e.g., for quantization) might contain Personally Identifiable Information (PII).

| **Risk Flag** | **Why it’s a problem** | **The Fix (Actionable)** |
| :--- | :--- | :--- |
| **Social Media Data** | If you used Tweets or Reddit posts for calibration, did you anonymize usernames? | **Fix:** Add a sentence: *"We utilized the anonymized version of [Dataset Name] and performed an additional pass to redact PII using [Tool Name/Presidio]."* |
| **Crowdsourced Eval** | Did you use humans to rate the quality of the compressed model? | **Fix:** You **must** disclose: (1) Hourly wage paid (must be >$15/hr or local living wage), (2) Platform used (e.g., MTurk, Prolific), and (3) IRB approval status or exemption. |

### **3. Compute & Environmental Disclosures**
**The Risk:** Compression papers often claim to be "Green AI" because inference is cheaper, but they hide the massive energy cost of the compression phase (e.g., Neural Architecture Search).

| **Risk Flag** | **Why it’s a problem** | **The Fix (Actionable)** |
| :--- | :--- | :--- |
| **Hidden "Search" Costs** | You claim "10x efficiency" but spent 500 GPU hours finding the pruning mask. | **Fix:** Report **Total Cost of Ownership (TCO)**. <br> *Example:* "While compression required 50 GPU-hours (approx. 10kg CO2eq), the model breaks even after 1M inference requests." |
| **Vague Hardware Specs** | "We ran on NVIDIA GPUs." | **Fix:** Be specific. "All experiments run on 8x NVIDIA A100-80GB." This is crucial for reproducibility. |

---

### **4. Immediate "Text Replacement" Fixes**

**Search your paper for these phrases and replace them to improve compliance:**

*   ❌ **Bad:** "LLaMA-3 is an open-source model."
*   ✅ **Good:** "LLaMA-3 is an open-weights model."
*   ❌ **Bad:** "We show no loss in accuracy." (Overclaiming)
*   ✅ **Good:** "We observe negligible degradation (<1%) on standard benchmarks."
*   ❌ **Bad:** "This method reduces carbon footprint."
*   ✅ **Good:** "This method reduces inference-time energy consumption, though valid compression requires an upfront compute investment of [X] GPU hours."

---

### **How to get a specific review:**
**Paste your Abstract and Ethics Statement below.**
I will parse it and highlight specific sentences that might trigger a "Conditional Accept" or "Reject" from an Ethics Reviewer.