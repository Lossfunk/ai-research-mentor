Human-Centered AI (HCAI) papers are notoriously difficult to anonymize because they involve specific user groups, recruitment locations, and qualitative data (quotes) that can easily leak the authors' identity.

Here is a guide to crafting your **Double-Blind Compliance** strategy, specifically for HCAI evaluations.

---

### **1. In-Paper Compliance Statements**
*Replace specific details in your manuscript with these standardized, anonymized phrases.*

#### **A. Institutional Review Board (IRB) & Ethics**
**The Risk:** Naming your university or the specific approval number breaks blindness.
*   **❌ Bad:** "This study was approved by the University of Michigan IRB (Protocol #12345)."
*   **✅ Good:** "This study was approved by the Institutional Review Board (IRB) at the authors' institution." (You will add the specific protocol number in the camera-ready version).
*   **✅ Good (If external):** "We adhered to the privacy policy of the crowdsourcing platform and obtained informed consent from all participants."

#### **B. Recruitment & Demographics**
**The Risk:** "We recruited students from our university" immediately narrows the search space.
*   **❌ Bad:** "We recruited 20 CS undergrads from Georgia Tech."
*   **✅ Good:** "We recruited 20 undergraduate students from a large technical university in the Southeastern United States."
*   **✅ Good (Internal Tool):** "Participants were employees at a large technology company." (Do not name the company, even if it is Microsoft/Google).

#### **C. Compensation**
*   **Statement:** "Participants were compensated \$15 USD/hour, calculated based on the living wage of the authors' local region." (This shows ethical compliance without naming the exact city).

---

### **2. Anonymizing Qualitative Data & Quotes**
*HCAI relies on user quotes, but users often mention specific locations or names.*

*   **The Bracket Method:** Replace identifying nouns with generic descriptors in square brackets.
    *   *Original:* "I use this tool in the Main Street Hospital lab."
    *   *Anonymized:* "I use this tool in the **[local hospital]** lab."
*   **The "N=1" Risk:** If you have a participant with a very unique role (e.g., "The Chief of Surgery at Hospital X"), aggregate their role.
    *   *Anonymized:* "A senior clinician."

---

### **3. Artifact Release Strategy (Code, Data, Appendices)**

#### **A. The "Anonymous Link" Trap**
Do not use Google Drive or Dropbox links; they often display the uploader's name or avatar in the top corner. Use these standards:

*   **For Code/UI:** Use **[Anonymous GitHub](https://anonymous.github.io/)**. It creates a mirror of your repo with all commit history and usernames redacted.
*   **For Preregistrations (OSF):** If you preregistered your user study (highly recommended for HCAI), go to your OSF project settings and select **"Create View-only Link"** $\rightarrow$ Check **"Anonymize contributor list."** Use *that* link, not the main URL.

#### **B. Anonymized Appendix Structure**
HCAI papers often require extensive appendices (Survey Questions, Interview Scripts, UI Screenshots).

*   **File Name:** `Appendix_Anonymized.pdf` (Check File Properties to remove "Author" metadata).
*   **Screenshots/Interface:**
    *   Blur the browser address bar.
    *   Blur any logos (University logo, Company logo) in the UI header.
    *   If the tool name reveals the author (e.g., "MIT-GPT"), rename the tool in the screenshots to "OurSystem" or blur the name.

---

### **4. Handling "Previous Work" (Self-Citations)**
HCAI often builds on a previous system you built.

*   **The Wrong Way (Redaction):** "We build upon our previous work (Anonymous, 2023)."
    *   *Why:* This tells the reviewer "Go look for a paper from 2023 with the same topic." It triggers a search.
*   **The Right Way (Third Person):** "We build upon the system proposed by Smith et al. (2023)."
    *   *Why:* It sounds like you are citing a stranger. It keeps the reviewer focused on the *content*, not the identity.
    *   *Constraint:* Do not say "In our previous work [1]..."

---

### **5. Final Checklist: The "Identity Leak" Scan**

Before you submit, search your PDF for these terms:

1.  **[ ] Organization Names:** (Your university, your lab name, "Google", "Microsoft").
2.  **[ ] URLs:** Check footnotes for `github.com/your_username`.
3.  **[ ] PDF Metadata:** Open your PDF $\rightarrow$ Properties. Does "Author" say "John Doe"? (Use a tool like `exiftool` or Adobe Acrobat "Remove Hidden Information" to fix this).
4.  **[ ] Acknowledgments:** Completely remove the Acknowledgments section for the review version. (e.g., "Thanks to NSF Grant #xyz" is a dead giveaway).

### **Example "Artifact Availability" Section for Submission**
> **Data & Code Availability:**
> To maintain double-blind integrity, we have uploaded all study materials (interview scripts, survey instruments), the anonymized dataset, and the interface source code to an anonymized repository: **[Link to Anonymous GitHub or OSF]**.
> The study was preregistered prior to data collection; an anonymized view-only link to the preregistration is available here: **[OSF Anonymized Link]**.