This proposal replaces the arbitrary "Fixed-Window" slicing (e.g., every 512 tokens) with a **Dynamic Semantic Segmentation** module. This ensures that the Level 2 Aggregator processes linguistically coherent units (e.g., "The Introduction," "The Methodology") rather than arbitrary fragments.

### 1. The Intervention: Unsupervised Semantic Segmentation

We introduce a pre-processing step that groups sentences into segments before they enter the Hierarchical Transformer.

**Algorithm: Constrained Agglomerative Clustering**
Since text is linear, we cannot use standard K-Means (which ignores order). We use adjacent grouping.

1.  **Atomic Units:** Split document into sentences $S = \{s_1, ..., s_N\}$.
2.  **Fast Encoding:** Encode sentences using a lightweight, frozen Sentence-BERT (`all-MiniLM-L6-v2`) to get vectors $v_1, ..., v_N$.
3.  **Scoring:** Calculate Cosine Similarity between adjacent sentences: $Sim(v_i, v_{i+1})$.
4.  **Segmentation:**
    *   *Thresholding:* Place a segment boundary wherever similarity drops below a threshold $\tau$ (determined via validation set).
    *   *Result:* Variable-length segments $Seg_1, ..., Seg_M$.
5.  **Hierarchical Feed:**
    *   The Level 1 Encoder now processes these variable-length segments (up to max length 512, handling overflow by splitting only if a single semantic segment exceeds the limit).
    *   The Level 2 Aggregator attends to these semantic units.

### 2. Evaluation Metrics

We need to measure if "Learned Structure" is better than "Fixed Structure."

#### A. Boundary Stability (The "Jitter" Test)
**Objective:** A robust structural model should not change its segmentation boundaries due to minor synonym changes.
**Procedure:**
1.  Run segmentation on the original document $D$. Record boundary indices $B_{orig}$.
2.  Perturb $D$ using synonym replacement (e.g., using WordNet) to create $D'$.
3.  Run segmentation on $D'$. Record boundary indices $B_{pert}$.
**Metric:** **WindowDiff** or **Pk Metric**.
*   Standard metrics in text segmentation that measure the probability that two sentences are incorrectly separated or grouped.
*   *Hypothesis:* Semantic Clustering should have lower WindowDiff scores (higher stability) than fixed-window chunking (which shifts entirely if one word is added).

#### B. Segment Purity (Topic Coverage)
**Objective:** Do the learned segments establish coherent topics?
**Procedure:**
1.  Train a standard LDA (Latent Dirichlet Allocation) topic model on the corpus.
2.  For each segment generated by the model, calculate the distribution of topics.
**Metric:** **Topic Entropy**.
*   $H(Seg) = - \sum p(topic) \log p(topic)$.
*   *Target:* We want **Low Entropy** per segment (each segment focuses on one topic) but **High Entropy** for the document (the document covers many topics).
*   *Comparison:* Compare against Fixed-Window chunks (which often bleed topics at the edges).

#### C. Computational Overhead
**Metric:** **Latency Penalty**.
*   Clustering adds $O(N)$ or $O(N^2)$ pre-processing time depending on the algorithm.
*   Report: "Time to First Token" vs. "Total Inference Time."

### 3. Statistical Analysis Plan

To prove the Learned Structure is superior, we perform two specific tests.

#### Test 1: The "Variance Reduction" Test (Levene’s Test)
**Hypothesis:** Fixed windows split semantic concepts across chunks, causing the Level 2 Aggregator's attention weights to be "noisy" or diffuse. Semantic segments should concentrate attention.
*   **Variable:** The Entropy of the Level 2 Attention Distribution.
*   **Test:** **Levene’s Test for Equality of Variances**.
*   **Outcome:** If the Learned Structure has significantly lower variance in attention entropy ($p < 0.05$), it implies the model is more consistently finding "key" segments.

#### Test 2: The "Length Robustness" Regression
**Hypothesis:** Fixed windows fail as documents get longer (signal dilution). Learned segmentation condenses long documents into a manageable number of semantic nodes.
*   **Analysis:** Linear Regression of Performance ($Y$) on Document Length ($X$).
    $$Accuracy = \beta_0 + \beta_1 \cdot \text{Length} + \beta_2 \cdot \text{Method} + \beta_3 \cdot (\text{Length} \times \text{Method})$$
*   **Key Statistic:** $\beta_3$ (Interaction Term).
*   **Success:** If $\beta_3$ is positive for the "Learned" method, it means the learned structure degrades *slower* than the fixed structure as length increases.

### 4. Control Baselines

To ensure the clustering is doing the work (and not just the variable length):

1.  **Baseline 1:** Fixed-Window (Original Paper).
2.  **Baseline 2:** Random Segmentation.
    *   Place boundaries at random sentence indices (matching the *number* of segments from the clustering approach).
    *   *Purpose:* Proves that the *semantic content* of the segment matters, not just the fact that they are variable length.

### 5. Implementation Note: Handling Batching

Variable-length segments break standard tensor rectangularity.
*   **Solution:** Use **Nested Tensors** (PyTorch) or extensive padding/masking.
*   **Reporting:** You must report the **Padding Ratio** (percentage of compute spent on padding tokens) for the Learned approach vs. Fixed approach. High padding ratios might make the Learned approach computationally unviable despite better accuracy.