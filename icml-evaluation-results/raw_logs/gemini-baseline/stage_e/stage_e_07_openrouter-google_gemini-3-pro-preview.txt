Here is a succinct, scientifically grounded response. It validates the reviewer's critique while explaining the methodological nuance (specifically **token limits** and **data contamination**) that makes simply "adding LLMs" non-trivial.

---

### Response to Reviewer
"We acknowledge the reviewerâ€™s valid point regarding the absence of Large Language Model (LLM) baselines. Given the rapid evolution of NLP, comparing our current architectures against pre-trained transformers is a necessary step to contextualize our results.

However, applying standard LLMs to the PeerRead task introduces two specific methodological challenges that we intend to address in a principled follow-up:

1.  **The Long-Document Context Window:** Standard models (e.g., BERT/RoBERTa) are limited to 512 tokens. Scientific papers average 4,000+ tokens, and critical acceptance signals often reside in the Discussion or methodology details, which would be lost to truncation.
2.  **Data Contamination (Test Set Leakage):** Generative models (like GPT-4 or Llama) include public arXiv data in their pre-training corpora. Evaluating them on PeerRead risks measuring 'memorization' rather than 'prediction,' as the model may have already seen the 'test' papers during training.

**Proposed Plan for Future Inclusion:**
To integrate LLMs rigorously, we propose a two-pronged extension:
*   **Encoder-Based:** We will benchmark **Longformer** and **Hierarchical-SciBERT** (specifically designed for scientific text) to handle full-paper context without aggressive truncation.
*   **Generative:** We will utilize open-weights models (e.g., smaller Llama or Mistral variants) where we can inspect the training data cutoff dates to ensure strictly 'unseen' test instances, thereby avoiding the contamination issues inherent in closed-source APIs."