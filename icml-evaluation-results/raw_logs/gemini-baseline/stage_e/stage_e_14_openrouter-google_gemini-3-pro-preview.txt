This is a critical addition. In the current climate of NLP research (with a heavy focus on ethics and fairness), ignoring the "Matthew Effect" (the rich get richer) or the "Native Speaker Bias" is a rejection risk.

Here is a discussion addendum that frames this limitation as an **ethical imperative** rather than just a data flaw, followed by a concrete mitigation plan.

### Discussion Section Addendum

> **"Socioeconomic Bias and Algorithmic Mirroring:**
> We explicitly recognize that the PeerRead dataset mirrors the systemic inequities inherent in the peer review process. Because our training labels (conference acceptance) are generated by human reviewers who may harbor implicit biases regarding institutional prestige, geographic origin, or linguistic fluency, our models risk encoding these biases as proxies for scientific quality.
>
> Specifically, there is a danger that the model learns to prioritize 'prestige markers'—such as high-resource experimental setups or specific rhetorical styles common to elite Western institutions—over the intrinsic logic of the research. Consequently, deploying such a model as an automated filter could amplify the 'Matthew Effect,' systematically disadvantaging under-resourced labs or non-native English speakers. Therefore, we emphasize that this model should be viewed as a diagnostic tool to *study* review patterns, rather than a normative oracle to replace human judgment."

---

### Mitigation Plan for Camera-Ready Version

To demonstrate that you are taking this seriously, outline these three steps you will complete before the final upload:

**1. The "Prestige Correlation" Audit**
*   **The Action:** We will cross-reference the model's false positives (rejected papers predicted as accepted) and false negatives against the **author affiliations** (extracted from the arXiv metadata).
*   **The Goal:** Check if the model is systematically "under-predicting" acceptance for papers from non-Western or lower-ranking institutions. We will report the **Disparate Impact Ratio** in the appendix.

**2. The "Linguistic Fluency" Control**
*   **The Action:** We will compute the correlation between the model’s predicted acceptance probability and standard **perplexity scores** (using a generic pre-trained LM).
*   **The Goal:** If the correlation is extremely high ($r > 0.8$), it suggests the model is acting as a grammar/fluency checker rather than a content evaluator. We will report this correlation to quantify the "Native Speaker Bias."

**3. "Datasheets for Datasets" Documentation**
*   **The Action:** We will include a standardized "Datasheet" (Gebru et al.) in the supplementary material.
*   **The Goal:** This will explicitly document the demographics of the training data (e.g., "predominantly US/European institutions from 2017-2020") to ensure future users understand the socioeconomic scope and limitations of the dataset.