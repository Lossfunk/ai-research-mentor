This is a common critique when applying deep learning (high parameter count) to datasets like PeerRead (low sample count, ~10k papers). Reviewers worry the model is simply memorizing the training set or latching onto transient topic trends rather than learning "quality."

Here is a draft for the Discussion section and a list of specific diagnostic plots to include in the Appendix to prove robustness.

### Discussion Section Text

> **"Model Capacity and Generalization Risks:**
> We acknowledge the reviewers' concern regarding the risk of overfitting, particularly given the high parameter-to-sample ratio inherent in fine-tuning deep language models on the relatively modest PeerRead corpus. While standard regularization techniques (dropout, weight decay, and early stopping based on validation loss) were employed, the potential for the model to memorize training instances or over-index on year-specific topic distributions remains.
>
> To investigate this, we conducted a rigorous post-hoc analysis of the **generalization gap**â€”the divergence between training and validation performance. Our diagnostics (detailed in Appendix C) reveal that while training loss naturally approaches zero, the validation accuracy stabilizes early and tracks the test performance closely, suggesting the model is learning transferable features rather than memorizing noise. Furthermore, our 'Temporal Generalization' analysis demonstrates that models trained on historical data (e.g., 2017) retain predictive power on future unseen years (e.g., 2018), indicating robustness to the temporal concept drift that typically plagues overfitted models."

---

### Proposed Appendix Diagnostics

To back up the text above, you need to generate and reference these three specific visualizations/tables.

#### 1. The "Generalization Gap" Plot (Loss Curves)
Reviewers need to see that you didn't train for too long.
*   **The Plot:** A dual-line chart showing **Training Loss** vs. **Validation Loss** over epochs.
*   **What to look for:** You want to show the exact epoch where Validation Loss stops decreasing (the "Early Stopping" point).
*   **Caption:** *"Figure C1: Learning curves for the SciBERT encoder. The vertical line marks the early stopping point. The stability of validation loss beyond this point indicates the model is not suffering from catastrophic overfitting despite the model capacity."*

#### 2. The "Temporal Holdout" Matrix
This is the strongest defense against "topic overfitting." If you overfit, you learn *this year's* buzzwords. If you generalize, you learn *universal* quality.
*   **The Table:** A grid where Rows = Train Year and Columns = Test Year.
*   **The Metric:** F1 Score.
*   **The Argument:** If the model trained on 2017 performs well on 2018 (off-diagonal elements are strong), it proves the model hasn't overfitted to the specific topics of 2017.

#### 3. The "Sample Size" Ablation (Learning Curve)
Show that the model actually *needs* the data.
*   **The Plot:** Performance (Y-axis) vs. % of Training Data Used (X-axis: 10%, 25%, 50%, 75%, 100%).
*   **The Argument:**
    *   *If the curve is flat:* The model learned a simple heuristic (like "length") instantly and ignored the rest.
    *   *If the curve slopes upward:* The model is actively benefiting from more examples, implying it is learning a complex distribution rather than memorizing a small set of rules.
*   **Caption:** *"Figure C3: Performance as a function of training set size. The monotonic increase in F1 score suggests the model is effectively utilizing the dataset variance to refine its decision boundary, rather than saturating on a small number of memorized features."*