This is a valid critique, as scientific discourse structure varies significantly across cultural and linguistic boundaries. A model trained on standard American/British English academic prose may fail to recognize valid argumentation patterns in other languages or even in English variations used in non-Western venues.

Here is a draft for your Discussion section, followed by a concrete 3-step plan to demonstrate you have thought this through.

### Discussion Section Draft

> **"Generalization to Multilingual Contexts:**
> A significant limitation of the PeerRead dataset is its exclusive focus on English-language venues (ACL, ICLR, NIPS). Consequently, our models risk encoding an Anglocentric bias, potentially overfitting to the specific rhetorical structures and stylistic norms of Western academic English (e.g., the preference for direct active-voice assertions). This raises concerns regarding the model's applicability to multilingual venues or non-English submissions, where argumentation styles and discourse markers may differ fundamentally. While we hypothesize that the latent features of scientific validity (e.g., experimental rigor, logical coherence) are language-agnostic, our current architecture cannot decouple these semantic signals from the surface-level linguistic realization. Therefore, the performance reported here should be interpreted as specific to the Anglophone peer-review ecosystem, and future work is required to validate these findings in cross-lingual settings."

---

### Concrete Multilingual Extension Plan

To show reviewers you have a roadmap, outline this **"Cross-Lingual Transfer Protocol"** in your Future Work or Appendix:

**1. Architecture Shift to Multilingual Encoders**
We propose replacing the current SciBERT/GloVe encoders with **XLM-R (Cross-lingual Language Model - Roberta)** or **mBERT**. These models are pre-trained on 100+ languages and align semantic spaces across languages.
*   *Hypothesis:* If the model learns "scientific logic" rather than "English keywords," the internal representations of a paper in English should align with its translation in French or Chinese.

**2. The "Pseudo-Multilingual" Probe (Data Augmentation)**
Since open-access peer review data in non-English languages is scarce, we will generate a synthetic test set using **Neural Machine Translation (NMT)**.
*   *Method:* Translate a stratified sample of the PeerRead test set (Accepted and Rejected papers) into high-resource languages (e.g., Chinese, Spanish, German) and then back into English, or feed the translated text directly into the XLM-R model.
*   *Metric:* Measure the **invariance of the prediction**. If the model accepts the English version but rejects the Spanish translation of the same paper, it proves the model is over-reliant on language-specific surface features.

**3. Zero-Shot Transfer to Non-English Venues**
We will curate a small evaluation set from open-access non-English journals (e.g., *TAL* for French NLP or *Computaci√≥n y Sistemas*).
*   *Experiment:* Train on PeerRead (English) $\rightarrow$ Inference on Non-English Target.
*   *Goal:* This measures **Zero-Shot Cross-Lingual Transfer**. Success here would demonstrate that the model has learned universal features of scientific quality that transcend the source language.