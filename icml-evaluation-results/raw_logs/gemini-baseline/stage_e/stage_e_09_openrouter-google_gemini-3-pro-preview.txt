Here is a draft for the discussion section and a structured list of metrics for the appendix. This approach frames crowd-sourcing not as a replacement for expert review, but as a **calibration tool** to measure label noise.

### Discussion: Validating Robustness via Crowd-Sourced Calibration

"While our current labels rely on conference outcomes and available review scores, these signals inevitably contain noise from reviewer variance and the 'silver-standard' nature of our rejection heuristic. To rigorously quantify this noise, future iterations of the dataset would benefit from a targeted **'Expert-Crowd' annotation layer**—employing graduate-level annotators to re-evaluate a stratified sample of papers. Unlike the original conference reviewers, these annotators would assess papers in a double-blind setting, strictly isolating content quality from author reputation (the 'prestige bias').

Furthermore, crowd-workers could be tasked specifically with validating the 'Probably Rejected' class by manually searching for subsequent publications of these drafts in other venues. This human-in-the-loop verification would allow us to transform our assumption of rejection from a heuristic into a measured probability, thereby allowing future models to down-weight or filter 'false negatives' (papers rejected by ACL but accepted by AAAI) during training."

***

### Appendix Update: Proposed Validation Metrics

If a crowd-sourcing study were conducted, reviewers would expect these specific metrics in the appendix to prove data quality:

**1. The "Rejection Heuristic" Precision Score**
*   **Definition:** The percentage of papers in the "Probably Rejected" class that human annotators confirm *did not* appear in any comparable venue within 12 months.
*   **Target:** $>85\%$.
*   **Purpose:** Validates the core assumption that "arXiv-only = Rejected."

**2. Reviewer-Crowd Alignment (Cohen’s $\kappa$)**
*   **Definition:** Inter-annotator agreement between the original aspect scores (e.g., Originality) and the crowd’s blind assessment of the same aspect.
*   **Purpose:** If $\kappa$ is low, it indicates the original reviews were highly subjective or influenced by unblinded author names. If $\kappa$ is moderate-to-high, the labels are robust.

**3. The "Readability" Correlation Coefficient**
*   **Definition:** Pearson correlation between the model's hand-engineered readability features (e.g., Flesch-Kincaid) and crowd-rated "Clarity."
*   **Purpose:** Proves that the hand-engineered features are actual proxies for human-perceived quality, not just arbitrary math.

**4. Blind vs. Non-Blind Score Delta**
*   **Definition:** The average difference in scores given by the original (unblinded) reviewers vs. the crowd (blinded) for papers from top-tier institutions.
*   **Purpose:** Quantifies the "Halo Effect" in the training data. A large delta implies the model might be learning institutional prestige rather than paper quality.