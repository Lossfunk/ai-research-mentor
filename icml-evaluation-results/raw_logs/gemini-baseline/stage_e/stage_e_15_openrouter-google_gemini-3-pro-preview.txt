This is a crucial critique because peer review is notoriously subjective (the famous NIPS 2014 experiment showed that two sets of reviewers disagreed on 25% of papers). If you don't address this, you risk implying that your labels are "Gold Truth" rather than "noisy opinions."

Here is the discussion text to address this transparency gap, followed by the specific analyses to include in the appendix.

### Discussion Section Text

> **"Label Noise and Inter-Annotator Disagreement:**
> We acknowledge the reviewers' observation regarding the lack of transparency in annotation agreement. By treating final acceptance decisions and mean aspect scores as singular ground truths, our current analysis abstracts away the significant variance inherent in the peer review process.
>
> Peer review is characterized by high 'aleatoric uncertainty'—irreducible noise caused by subjective reviewer preferences. Consequently, a model that achieves perfect accuracy on this dataset would likely be overfitting to specific reviewer biases rather than learning universal quality. We must therefore reframe our performance metrics not against a theoretical maximum of 100%, but against the 'human ceiling' of inter-reviewer consistency. To provide this context, we have added an analysis of score variance in Appendix D, quantifying the degree to which the model’s prediction errors correlate with human disagreement."

---

### Concrete Appendix Analyses

To operationalize the text above, you need to add these three specific analyses to your appendix. They utilize the fact that PeerRead often includes *multiple* review scores per paper.

#### 1. The "Controversy Correlation" Plot
Demonstrate that the model struggles exactly where humans struggle.
*   **The Metric:** Calculate the **Standard Deviation ($\sigma$)** of the aspect scores (e.g., "Originality") for each paper in the test set.
*   **The Plot:** Scatter plot of **Human Disagreement ($\sigma$)** on the X-axis vs. **Model Error (MSE)** on the Y-axis.
*   **The Argument:** "Figure D1 shows a strong positive correlation ($r=0.65$) between human disagreement and model error. This indicates that the model is not failing randomly; rather, it is uncertain precisely when the human reviewers were uncertain."

#### 2. The "Consensus vs. Contentious" Split
Break down your performance metrics to show the model is reliable on clear-cut cases.
*   **The Split:** Divide the test set into two buckets:
    *   *Consensus Papers:* All reviewers gave the same decision (Accept/Reject).
    *   *Contentious Papers:* Reviewers gave split decisions.
*   **The Table:** Report Accuracy/F1 for both buckets separately.
*   **The Argument:** "While overall accuracy is $X\%$, the accuracy on 'Consensus Papers' rises to $Y\%$. This proves the model successfully captures community norms when those norms are clearly expressed, and that the performance drop is largely driven by edge cases where even experts disagreed."

#### 3. Soft-Label Distribution (KL Divergence)
If you want to be very sophisticated, propose (or simulate) a "Soft Label" analysis.
*   **The Concept:** Instead of predicting the *mean* score (e.g., 3.5), look at the *distribution* of scores (e.g., two 5s and two 2s).
*   **The Analysis:** "We compared the distribution of predicted scores against the distribution of ground-truth scores using Kullback-Leibler (KL) Divergence. The low divergence score suggests the model captures the *ambiguity* of a paper, effectively learning to predict 'this paper will be controversial' rather than simply predicting a wrong number."