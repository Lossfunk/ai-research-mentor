Short discussion
Augmenting PeerRead with crowd-sourced annotations would materially strengthen our robustness claims by (i) turning many single-rater or heuristic labels into multi-rater judgments, enabling direct estimates of label noise and reliability ceilings; (ii) providing venue-agnostic, rubric-aligned aspect scores and pairwise preferences that transfer better across domains; and (iii) attaching evidence (highlighted spans/rationales) and quality flags (parse quality, leakage indicators) that separate substantive signal from artifacts. Concretely, we propose a stratified annotation round (by venue, year, and topic) on 1–2k papers with 3–5 raters per item, collecting: aspect scores with short rationales, pairwise within-venue comparisons, span highlights supporting judgments, parsing-quality ratings, and flags for leakage (e.g., “to appear in…”), plus a small gold set adjudicated by experts. Multi-rater aggregation (Dawid–Skene/GLAD/IRT) will yield uncertainty-aware labels and rater-severity estimates, allowing us to report reliability-adjusted performance bounds and run sensitivity analyses that directly account for noise.

This design improves external validity (cross-venue harmonization via venue-agnostic rubrics and IRT alignment), internal validity (explicit noise estimates, leakage flags, deduplication checks), and interpretability (span-level evidence). It also enables practical use: models can be trained to abstain when crowd uncertainty is high and evaluated relative to empirically estimated ceilings rather than absolute scores.

Metrics to track in an appendix update
- Annotation coverage and QC
  - Number of items and raters per item (mean/median), effective number of raters after QC, annotation time per item, cost per label.
  - Gold-standard accuracy and inter-annotator pass rate on sentinel items.
- Reliability and noise
  - Inter-rater reliability per aspect (Krippendorff’s alpha, ICC, Kendall’s W) with CIs.
  - Estimated false-positive/false-negative rates from manual audits; label uncertainty (entropy/variance) per item.
  - IRT outputs: rater severity/consistency variance; item difficulty; differential item functioning across venues/topics.
- Harmonization and leakage
  - Fraction of items flagged for leakage or low parse quality; performance deltas when excluding flagged items.
  - Scale alignment diagnostics pre/post standardization or IRT mapping (e.g., distribution overlap, mean shifts).
- Model impact
  - Reliability-adjusted performance: fraction of the ceiling achieved (for τ/ρ, MAE) before vs after using aggregated crowd labels.
  - Calibration metrics (ECE/coverage) and changes after training on uncertainty-aware labels.
  - Few-shot transfer curves on newer venues using crowd labels for calibration (performance vs k).
- Robustness and fairness
  - Slice performance and reliability by venue, year, topic, document length, and parse quality, with bootstrap CIs.
  - Stability of feature attributions (rank correlations across venues/years) when trained on crowd-aggregated labels.
- Data health and reproducibility
  - Deduplication rate (near-duplicate clusters) discovered during annotation.
  - Proportion of items with span-level evidence and agreement on evidence overlap (IoU/Jaccard).
  - Versioning: timestamped sources, v1-only compliance rate.

We will release the annotation protocol, rubrics, aggregation code, and per-item uncertainty to support reproducibility and to let users weight or filter labels according to reliability.