We agree that acceptance and aspect models can overfit superficial cues (template phrases, length, boilerplate). To guard against this and demonstrate reliance on review-worthy content, we (i) freeze inputs to arXiv v1 posted before notification and strip venue/status strings and boilerplate, (ii) show that models using only trivial proxies (length, #references, author count) perform near chance, while our full models substantially outperform them, and (iii) run targeted ablations where removing proxy feature groups has small effects but removing or shuffling substantive body text causes large drops. Out-of-domain tests (leave-year/leave-venue) retain a sizable fraction of in-domain performance, which would be unlikely if the model depended on venue-specific templates. Post-hoc attributions concentrate on Methods/Results rather than headers or footers, and high-confidence error audits reveal misses tied to nuanced novelty/impact assessment rather than formatting. Together, these controls and diagnostics suggest the models primarily leverage content relevant to review, not spurious lexical artifacts.

Diagnostics and post-hoc analyses we will include
- Negative-control baselines: Train/evaluate models using only length/#references/#authors/figure-equation counts; report the margin by which full models exceed them.
- Drop-one feature-group ablations: Remove proxy groups (length/readability/reference counts/headers) vs removing substantive text features (body, Methods/Results) and report ΔAUC/Δτ with CIs.
- Content perturbations: Sentence shuffling within sections, header masking, abstract-only vs body-only vs introduction-only; large drops under content shuffling indicate semantic reliance.
- Topic- and length-matched evaluation: Match positives/negatives on topic embeddings and length to reduce confounding; re-report performance.
- Attribution/stability: SHAP/permutation importance for hand features; token/section saliency maps for text encoders; check stability of importance rankings across folds/venues and concentration on substantive sections.
- Partial dependence and monotonicity: Plot acceptance probability vs length/#refs to verify plausible, non-dominant effects.
- Parse-quality sensitivity: Stratify by PDF-to-text quality; rerun after excluding low-quality parses to rule out parsing artifacts.
- Cross-domain generalization: Train on year/venue A, test on B; if performance holds while negative-control models collapse, it argues against shortcutting.
- Sanity checks: Label randomization and feature randomization tests to verify no hidden leakage or degenerate shortcuts.

We will add these results (with code and evidence trails) to the paper to substantiate that improvements stem from review-relevant content rather than spurious lexical cues.