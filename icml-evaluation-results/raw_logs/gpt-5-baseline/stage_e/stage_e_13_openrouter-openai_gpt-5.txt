Our ability to replicate PeerRead analyses on newer conference cycles is constrained by both data drift and moving targets in the review process. On the input side, covariate shift arises from evolving topic mixes, new tracks, template and page-limit changes, increased use of appendices/code, and different arXiv and OpenReview practices that affect what text is available pre-notification; parsing quality and section conventions also change over time. On the label side, acceptance criteria and aspect rubrics evolve (e.g., adoption of reproducibility checklists, ethics/broader-impact sections, artifact badges, rolling review), and committees often recalibrate score distributions or add new guidelines, creating rubric and scale drift that weakens comparability with earlier years. Acceptance-rate fluctuations and area-specific policies induce label shift that can make a fixed threshold or historical baseline misleading. Sparse or embargoed reviews in recent cycles further limit power, and later re-submissions can blur “rejected” outcomes if not tracked longitudinally. Consequently, headline metrics from older cohorts may not generalize without adjustment; results should be interpreted as time- and venue-conditional. To mitigate these limits in ongoing work, we adopt time-aware evaluation (leave-year-out), freeze features to pre-notification versions, harmonize scales within year/venue, report reliability-adjusted and standardized effect sizes, quantify source–target shift, and provide sensitivity analyses (reweighting and few-shot calibration) so that any remaining drift is explicit rather than implicit.