We acknowledge that PeerRead is predominantly English and our models may not generalize to multilingual venues due to linguistic, stylistic, and parsing differences, as well as potential rubric drift across languages. To address this, we will build a multilingual extension along two complementary tracks: (i) translate-to-English, where non-English papers (v1, pre-notification, leakage-stripped) are machine-translated and scored with the English model, and (ii) native multilingual modeling using language-agnostic encoders (e.g., XLM-R/mT5-style) with venue/language random effects to harmonize scales. We will curate a held-out multilingual benchmark starting with 3–5 high-resource languages (e.g., Chinese, Spanish, French, German) using venues with public decisions/aspects where available; each item will include language ID, parse-quality flags, and translation-quality scores. We will run zero-shot tests (train on English only; evaluate on native vs translated inputs), few-shot adaptation (k ∈ {50, 100, 200} labeled target-language papers), and report ordinal and calibration metrics (Kendall’s τ/Spearman’s ρ or AUC, ECE), expressed as a fraction of language-specific reliability ceilings. We will normalize or drop language-sensitive proxies (readability indices, raw length) in favor of section-aware and semantic features; quantify source–target shift; and perform back-translation robustness checks plus human spot audits of translations. Success criteria are: zero-shot retains ≥50–60% of the reliability ceiling and k-shot closes ≥70–80% with well-calibrated uncertainty; if unmet, we will deploy abstention for high-uncertainty languages. We will release the multilingual subset, code, prompts/protocols, and a datasheet documenting language coverage, translation QC, and known failure modes.