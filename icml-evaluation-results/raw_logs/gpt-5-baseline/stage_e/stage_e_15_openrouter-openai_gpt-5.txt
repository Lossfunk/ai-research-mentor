We acknowledge that the current PeerRead release does not make annotation disagreement sufficiently transparent. Many aspect labels are single-rater or sparsely rated, and even multi-rater items can reflect subjective rubrics, venue-specific severity, and year-to-year drift. As a result, “ground truth” is noisy, the achievable ceiling varies by aspect and venue, and model errors can be confounded with rater disagreement. In the camera-ready, we will (i) reframe results relative to venue- and aspect-specific reliability ceilings rather than absolutes, (ii) expose disagreement and rater effects explicitly, and (iii) propagate label uncertainty into evaluation and, where possible, training.

Concrete follow-up analyses for the appendix
- Raw disagreement disclosure
  - Release per-item rating sets (with hashed rater IDs), not just aggregates; report the proportion of single- vs multi-rater items by venue/year/aspect.
  - Plot score distributions and pairwise rater differences; include Bland–Altman plots for continuous/ordinal aspects.

- Reliability estimates and ceilings
  - Compute inter-rater reliability per aspect and venue-year (Krippendorff’s alpha/ICC/Kendall’s W with bootstrap CIs).
  - Report reliability ceilings and re-express model performance as a fraction of these ceilings.

- Rater/venue effects modeling
  - Fit a many-facet Rasch/IRT or Dawid–Skene/GLAD model to estimate rater severity/consistency and item difficulty; provide caterpillar plots of rater effects.
  - Map raw scores to a shared latent scale; compare model performance pre/post alignment.

- Disagreement-aware evaluation
  - Stratify metrics by disagreement quantiles (e.g., label entropy or score variance) to show how performance and calibration change when annotators disagree.
  - Compare majority-vote vs probabilistic aggregation (soft labels) vs adjudicated gold on a small subset; quantify sensitivity of conclusions.

- Error overlap with disagreement
  - Correlate model residuals with item-level disagreement; report the fraction of “errors” occurring on high-disagreement items.
  - Risk–coverage curves that abstain on high-uncertainty or high-disagreement items; quantify error reduction.

- Sources of disagreement
  - Variance decomposition (item vs rater vs venue-year) via mixed-effects models.
  - Slice analysis: disagreement rates by topic, document length, parse quality, and year; include CIs.

- Protocol transparency
  - Publish rubrics/instructions shown to annotators, sentinel item accuracy, and adjudication guidelines.
  - Document missingness and scale differences across venues; provide mapping code used for harmonization.

- Training with uncertainty (optional)
  - Retrain with soft labels or noise-robust losses; report changes in calibration and robustness, especially on high-disagreement items.

These additions make disagreement observable, quantify the reliability limits within which models should be judged, and show how conclusions change under different, explicitly documented aggregation and uncertainty assumptions.