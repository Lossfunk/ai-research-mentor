We acknowledge the risk that the acceptance classifier overfits the training distribution or exploits superficial regularities. To address this directly, we will add diagnostics that separate genuine generalization from memorization and quantify variance. Concretely, we will (i) report learning curves and capacity/regularization sweeps to show the generalization gap and where early stopping is chosen, (ii) provide cross-year/venue holdout results alongside topic- and length-matched subsample evaluations to reduce confounding, (iii) run permutation and feature-shuffle sanity checks to verify collapse to chance under broken supervision, and (iv) include deduplication/leakage audits to ensure no data dependence inflates test performance. We will also include calibration and risk–coverage analyses so that any residual overfitting manifests transparently as miscalibration or poor selective prediction.

Appendix materials we will add
- Train vs validation learning curves and generalization gap; early-stopping checkpoints and nested CV selection.
- Model capacity/regularization sweeps (e.g., depth/C/λ) and performance vs model size; choose operating point at peak validation performance.
- Training set size ablations (performance vs data) to show monotonic gains rather than brittle peaks.
- Cross-year/venue train-on-A test-on-B matrix; topic- and length-matched evaluations within each cell.
- Permutation tests: label shuffling and feature shuffling baselines; negative-control features-only model (length/refs/authors) near chance.
- Deduplication and leakage sensitivity: remove near-duplicates/author-linked items; v1-only freeze and status-string/citation-time censoring; report ΔAUC with CIs.
- Calibration diagnostics: reliability diagrams, Brier score/ECE before and after temperature scaling.
- Risk–coverage curves and confidence histograms to show behavior under selective prediction.
- Parse-quality and document-length stratification; bootstrap CIs and fold-to-fold variance to quantify stability.

These additions make clear where the model genuinely generalizes, where it needs regularization or abstention, and how sensitive results are to data artifacts, directly addressing overfitting concerns.