We recognize that PeerRead reflects the socioeconomic landscape of its source venues: acceptance labels and review text can encode historical advantages tied to institution prestige, country/region, funding access, and language fluency. Even when we avoid explicit author/affiliation features, models can still pick up proxies (email domains, location cues, funding acknowledgments, idioms) and parsing quality can vary by region or template, risking systematic errors. Our results should not be interpreted causally or used for individual screening; they are time- and venue-conditional and may amplify existing inequities if deployed naively. In the camera-ready, we will explicitly document these limits and add a bias audit and mitigation suite so that any residual gaps are measured, bounded, and, where possible, reduced.

Mitigation steps for the camera-ready
- Content-only training and ablations
  - Train/evaluate with author/affiliation/emails/acknowledgments pages and boilerplate removed; report performance deltas and whether group gaps shrink.
  - Ablate features that are strong group proxies (e.g., email domains, geographic terms, funding strings) and show impact on accuracy and gaps.

- Grouped fairness audit (aggregate-only reporting)
  - Derive coarse, public-attribute groups from affiliations: country/region and World Bank income tier, institution prestige quartiles (e.g., CS rankings), and sector (industry vs academia), plus a parse-quality slice.
  - Report per-group metrics with bootstrap CIs: TPR/FNR gaps at the chosen threshold (equal opportunity), AUROC/AP gaps (threshold-free), calibration gaps (ECE/Brier), and risk–coverage curves.

- Matched and controlled analyses
  - Within venue-year and topic bins (semantic embeddings), compare groups matched on topic, document length, and parse quality to reduce confounding; report residual disparities.
  - Propensity reweighting sensitivity analysis to assess how much gaps change when source distributions are equalized.

- Debiasing and robustness
  - Train with group-aware objectives (group DRO) and venue/language random effects; compare to ERM.
  - Adversarial removal of group predictability from representations; quantify reduction in group-classifier accuracy and fairness gaps vs accuracy cost.

- Proxy and attribution checks
  - Measure predictability of group from model inputs/representations; if high, identify contributing tokens/sections via faithful attribution (deletion/insertion) and document them.
  - Re-run models without acknowledgments and with funding/affiliation strings masked; report gap changes.

- Data quality and multilingual pathway
  - Audit parse quality by region/template; rerun after excluding low-quality parses and report gap shifts.
  - For non-English items, evaluate a translate-to-English baseline to test whether fluency/parse penalties drive disparities.

- Governance and transparency
  - Add a datasheet/ethics statement detailing known biases, intended use (research/auditing), and non-goals (no individual triage).
  - Release only aggregated group statistics and auditing code; do not publish per-paper sensitive attributes beyond what is already public.

We will include an appendix with the bias audit tables/plots, ablation deltas, and a short “fairness change log” summarizing which interventions most reduce gaps with minimal accuracy loss. This makes socioeconomic risks explicit and provides concrete, reproducible steps to monitor and mitigate them.