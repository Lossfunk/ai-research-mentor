With 2 hours per week, the key is to pick a narrow safety sub-area, do scoped, repeatable work, and ship tiny artifacts (PRs, evals, notes). Here are three feasible tracks and an 8‑week micro‑plan for each. Pick one.

Track A: Safety evals and red‑teaming (lowest compute, fast impact)
Goal: add or improve a concrete safety evaluation and run it on open models.

What you’ll do
- Maintain tiny JSON/CSV test sets and scripts.
- Run established eval frameworks; file issues/PRs; write short results notes.

8-week plan (2 h/week)
- W1: Setup. Skim a safety eval survey; install one eval harness (e.g., lm-evaluation-harness or OpenAI Evals). Run a tiny task locally on a small open model.
- W2: Pick one failure mode (jailbreaks, prompt injection for tools, toxicity, hallucinated citations). Collect 20–50 clean test prompts; define a clear scoring rule.
- W3: Add the test to the harness (or a minimal standalone script) and open a PR or issue proposing the spec. If new to PRs, start with docs or config fixes labeled “good first issue.”
- W4: Run your eval on 2–3 open models (7B or smaller) via APIs or local; report results with seeds, version, and prompts.
- W5: Improve reliability: add automatic scoring, inter-rater agreement for any manual labels, and confidence intervals. Compare to a baseline defense (simple refusal filter or safety prompt).
- W6: Domain shift: craft a variant set (paraphrased or language-switched prompts) and measure robustness.
- W7: Write a 1–2 page note: task, data, scoring, results, limitations. Share in an issue, a gist, or a forum post.
- W8: Iterate on reviewer feedback; merge PR or publish a small repo.

Where to contribute
- lm-evaluation-harness (add a task or fix metadata), OpenAI Evals (contribute eval specs), guardrails/validators libs, jailbreak/attack repos. Filter for “good first issue” and “help wanted.”

Deliverables
- 1 PR + a small, versioned test set + a short write‑up with reproducible scripts.

Track B: Mechanistic interpretability on tiny transformers
Goal: replicate a known circuit in a toy model and add one tool or ablation.

What you’ll do
- Train or load a 1–2 layer transformer on a toy task (IOI, modular addition).
- Use probing/ablation tools; contribute docs or bugfixes.

8-week plan (2 h/week)
- W1: Install TransformerLens; run a provided tutorial end‑to‑end. Save a working notebook.
- W2: Load a pre-trained tiny model; reproduce a basic result (e.g., identify a head important for a toy task).
- W3: Implement a clean ablation (zeroing a head, path patching) and verify the expected performance drop/recovery.
- W4: Add one visualization or metric (e.g., attention pattern heatmap wrapper, neuron activation histograms). Open a small docs PR if something was unclear.
- W5: Robustness: test whether the same circuit appears across seeds or training curricula; log results.
- W6: Distill your steps into a minimal notebook with comments and assertions.
- W7: Open a PR to examples/docs or file a well‑scoped issue with a proposed fix; share your minimal repro.
- W8: Write a 1–2 page summary: task, circuit, ablation, reproducibility across seeds, open questions.

Deliverables
- 1 polished notebook + 1 small PR (docs/fix/example) + summary note.

Track C: Safety documentation, cards, and dataset audits
Goal: improve transparency for an open model or dataset.

What you’ll do
- Draft or improve a Model Card/Data Card; run simple bias/harms checks.
- Open PRs to repos that lack safety-relevant documentation.

8-week plan (2 h/week)
- W1: Pick a small open model/dataset. Read existing docs. List missing safety sections (intended use, OOD behavior, known harms, eval results).
- W2: Run quick checks: toxicity, stereotypes, privacy leakage tests on small samples; record methodology.
- W3: Draft safety sections (intended use, misuse risks, limitations, evaluation summary).
- W4: Open an issue proposing documentation improvements; align on scope with maintainers.
- W5: Incorporate maintainer feedback; add references and exact eval details (versions, prompts).
- W6: Add a simple, automated check script to the repo (or a separate “evals” folder).
- W7: Finalize PR; ensure reproducibility (requirements, seed, data URLs).
- W8: Publish a short blog/gist summarizing the process and results.

Deliverables
- 1 merged Model/Data Card PR + a tiny script for automated checks.

Minimum setup checklist (1 evening upfront if possible)
- Git + GitHub account; basic PR workflow.
- Python + virtual env; Jupyter/VS Code.
- A place to log results (README + CSVs, or a lightweight tracker).

Reading list (1 paper/post per week, 20–30 min each)
- Concrete Problems in AI Safety (Amodei et al., 2016).
- RLHF/Constitutional AI/RLAIF overview post.
- A recent survey on LLM safety evaluations and jailbreaks.
- A mechanistic interpretability tutorial/overview.
- A short piece on documentation/model cards and responsible deployment.

Good practices (especially with safety work)
- Scope tightly: finish a minimal slice each week (test set of 20 prompts, one ablation, one doc section).
- Reproducibility: fix seeds, record model/version/prompts, keep a results CSV.
- Ethics/dual use: avoid publishing novel, highly effective attack details without coordination; prefer responsible disclosure to maintainers.
- Measure against baselines: e.g., naive refusal, paraphrased prompts, randomized controls.
- Write limits clearly: what your test measures and what it doesn’t.

Choose a track and I’ll propose a concrete target (repo/dataset, exact task, and a checklist for your first 2 sessions). Also share:
- Your Python/Git comfort level.
- Whether you prefer evals, interpretability, or documentation/policy-leaning work.
- Any compute constraints (only CPU/Colab is fine).