You need something finishable in 4 weeks: low compute, clear baselines, measurable contribution. The safest pattern is “reproduce + evaluate + one clean extension.” Pick one of the topics below and commit today.

Five thesis-ready topics (choose one)
1) Robust vision under distribution shift (CIFAR-10-C)
- Problem: How much do common augmentations improve robustness to corruptions?
- Data: CIFAR-10 (train), CIFAR-10-C (test; 19 corruptions × 5 severities).
- Baseline: ResNet-18 (ImageNet-pretrained optional), standard training.
- Extension: Compare RandAugment vs MixUp vs CutMix vs AugMix under a fixed compute budget. Report accuracy and mean Corruption Error (mCE).
- Metrics: Top-1 accuracy on clean and C-sets, mCE, calibration (ECE), mean ± std across 3–5 seeds.
- Why it works: Small models, fast training, strong story with clear plots.

2) Calibration and uncertainty for text classifiers
- Problem: Are small transformers well-calibrated? Can simple methods fix miscalibration?
- Data: AG News or SST-2 for in-domain; OOD: IMDB (for sentiment) or Yahoo Answers (for topic).
- Baseline: DistilBERT fine-tune.
- Extension: Evaluate temperature scaling, label smoothing, MixUp, and ensembling; measure OOD shift.
- Metrics: Accuracy, ECE, Brier score, AUROC for confidence-based rejection; reliability diagrams.
- Why it works: Low compute; strong methodology angle.

3) Post-training compression: accuracy–latency trade-offs
- Problem: How far can we compress without unacceptable accuracy loss?
- Data: CIFAR-10 (ResNet-18) or SST-2 (DistilBERT).
- Baseline: FP32 model.
- Extension: Post-training quantization (8-bit/4-bit; torch.ao or bitsandbytes) and magnitude pruning; optionally knowledge distillation to a smaller student. Measure CPU/GPU latency and memory.
- Metrics: Accuracy vs latency vs model size; Pareto curves.
- Why it works: Practical value; easy to replicate.

4) LLM jailbreak robustness mini-benchmark (eval study)
- Problem: How robust are small open LLMs to jailbreaks and safety prompts?
- Models: TinyLlama, Phi-2, Mistral 7B Instruct (via API if needed; subset to small).
- Baseline: No-defense prompts.
- Extension: Test a small suite (30–60 curated prompts across harm categories), add paraphrases/roleplay variants, and one simple defense (system prompt or refusal filter). Report reproducible rates.
- Metrics: Harmful completion rate, refusal rate, attack success under paraphrase; include annotation protocol and inter-rater agreement.
- Why it works: No training; pure eval + careful methodology.

5) Remote sensing land-cover with multispectral ablations (EuroSAT)
- Problem: Do multispectral bands materially improve over RGB?
- Data: EuroSAT RGB and multispectral (13-band Sentinel-2).
- Baseline: ResNet-18 on RGB.
- Extension: Modify first layer for 13 bands or use a 1×1 conv to project 13→3; band-drop ablations; geographic split to avoid leakage.
- Metrics: Accuracy, macro-F1, confusion matrix; analysis by class; simple Grad-CAM sanity check.
- Why it works: Small, impactful, teaches geoML pitfalls.

Pick based on your constraints
- Minimal compute and quick results: Topics 2 or 4.
- Strong engineering deliverable: Topic 3.
- Classic ML thesis feel (clear plots/ablations): Topic 1.
- Climate/earth angle: Topic 5.

Four-week plan (default; adapt time)
Week 1: Scope, setup, and baseline
- Finalize topic; write a 1-page proposal (template below).
- Assemble dataset; set up repo with environment, data loader, training/eval scripts.
- Run the baseline end-to-end; log results; create first plots/tables.
- Start related work notes (10–15 references).

Week 2: Experiments and ablations
- Pre-register comparisons and metrics; fix seeds.
- Run core ablations (e.g., 2–3 augmentations; 2–3 calibration methods; 2 quantization levels).
- Track compute (GPU hours, hardware) and keep a results CSV.

Week 3: Robustness + analysis
- Evaluate under shift (CIFAR-10-C; OOD dataset; paraphrased jailbreaks; geographic split).
- Add uncertainty or variance estimates (3–5 seeds; CIs).
- Draft Methods and Experiments sections; generate final figures.

Week 4: Write-up and reproducibility
- Write Abstract, Intro, Related Work, Methods, Experiments, Results, Limitations, Conclusion.
- Polish figures/tables; finalize repo (README, scripts, requirements, seed control).
- Do a dry run of all experiments from scratch; freeze versions and results.

One-page proposal template (paste into your doc today)
- Title
- Problem statement (2–3 sentences)
- Hypothesis (what change you expect and why)
- Dataset(s) and splits (with leakage/shift considerations)
- Baselines and methods compared
- Metrics and statistical treatment (mean ± std, CIs, calibration metrics if relevant)
- Compute/resources (hardware, expected runtime)
- Risks and fallback (e.g., if X fails, report Y)
- Timeline (week-by-week)
- Deliverables (repo, report, figures)

Thesis structure checklist
- Abstract: problem, method, key result in 150–200 words.
- Intro: motivation, contributions (bulleted), summary of results.
- Related Work: 1–2 pages situating your question.
- Methods: clear, reproducible description; include baselines and your extension.
- Experiments: datasets, metrics, protocols; tables/figures with captions; report seeds/versions.
- Results: main findings, ablations, robustness; include failure cases.
- Limitations and ethics: scope, threats to validity, potential misuse.
- Conclusion + future work.
- Appendix: hyperparameters, extra plots, code pointers.

Common pitfalls to avoid
- Vague scope; adding features mid-way. Lock your comparison set early.
- No strong baseline. Always include a simple, competitive one.
- Single run results. Use multiple seeds or CIs where possible.
- Leaky splits or unfair comparisons. Match training budget across methods.
- Unreproducible environment. Pin versions; provide scripts.

Tell me:
- Your preferred topic from the list (or niche you want).
- Compute available (CPU only, Colab, single GPU).
- Any constraints from your department (page limit, required sections).
I’ll reply with a concrete, step-by-step checklist for that topic, including exact commands, hyperparameters, and figure/table templates.