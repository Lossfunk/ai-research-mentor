Totally doable. Plenty of impactful AI research is empirical, engineering, or evaluation-driven—not math-heavy.

Ways to do real research with minimal math
- Evaluation/benchmarks: Design tests for safety, robustness, hallucinations, tool-use, or RAG grounding. Run models, compare, and write up protocols.
- Data-centric work: Curate/clean datasets, craft instruction or red-teaming sets, build better splits (avoid leakage), measure the impact.
- Reproducibility studies: Reproduce a paper; tighten baselines, seeds, and reporting. Document pitfalls and fixes.
- Safety red-teaming and defenses: Create a small, well-scoped attack set and test simple defenses (prompting, filters, constraints).
- Tooling/UX: Build small utilities for logging, prompts, error analysis, or annotation; improve docs; add examples to popular repos.
- Efficiency engineering: Quantization/pruning/kv-cache recipes with clear accuracy–latency trade-offs.

Minimal math you’ll need (you likely already have this)
- Averages, standard deviation, confidence intervals (bootstrap or mean ± 1.96·SE).
- Classification metrics (accuracy, precision/recall, F1), calibration (ECE) via sklearn.
- Clear, fair comparisons: same data, budgets, and seeds.

A simple, repeatable research template (4–6 weeks, a few hours/week)
1) Scope a question
- Example: “How robust are small open LLMs to paraphrased jailbreaks, and do simple refusal prompts help?”
- Or: “Which chunking + retrieval combo yields the best grounded answers on a public QA set?”

2) Build a baseline and harness
- Pick 1–2 open models; set up scripts to run evals; log to CSV.
- Freeze versions, seeds, and prompts; write a README.

3) Add one clean extension
- Safety: add a paraphrase/roleplay variant set; test 1–2 defenses.
- RAG: compare chunk sizes, overlap, rerankers; add citation scoring.
- Data: clean labels or add a small, well-curated subset; measure delta.

4) Robustness + reporting
- Test domain shift (different topics/wording/regions).
- Report mean ± std over runs; show failure cases; state limits.

5) Ship
- Push a tidy repo, small dataset, and a 2–4 page write-up or blog with figures and exact commands.

Concrete low-math project ideas (pick one)
- LLM jailbreak mini-benchmark: 50–100 prompts across harm categories + paraphrases; evaluate 3 open models with and without a simple safety prompt; report harmful completion and refusal rates, inter-rater agreement for labels.
- RAG grounding eval: On a small open QA set with source docs, compare chunking and retrieval settings; score answers and citations (LLM-as-judge + spot checks); analyze hallucinations vs retrieval quality.
- Instruction data curation: Start from a public instruction set, clean and de-dup, add 200 high-quality items for one niche; fine-tune a 1–3B model with LoRA and show gains on a targeted eval.
- Post-training compression study: Quantize a small classifier or LLM head to 8/4-bit; plot accuracy vs latency/memory across CPU/GPU; publish a simple recipe.
- Repro + robustness: Reproduce a small vision/NLP paper; add geographic/OOD split and calibration analysis; document gaps and fixes.

Workflow tips to keep it math-light and rigorous
- Baselines first; match training/eval budgets.
- Track everything (seed, versions, prompts, data hashes); save a results.csv.
- Use sklearn and scipy for metrics/CI; avoid hand-deriving anything.
- Prefer plots that tell stories: accuracy vs latency, clean vs shifted accuracy, confidence–accuracy reliability diagrams.
- Write a clear Limitations section (what your test does and doesn’t measure).

Resources that won’t drown you in math
- fast.ai Practical Deep Learning for Coders (hands-on).
- Hugging Face Course (Transformers, datasets, evaluation).
- Papers with Code to find tasks + repos.
- Sklearn and SciPy docs for metrics and simple stats.

If you share:
- Your preferred area (safety evals, RAG, data curation, efficiency, vision, NLP),
- Compute (CPU/Colab/single GPU),
- Time window,
I’ll give you a concrete, step-by-step mini-plan with exact datasets, models, metrics, and a checklist for your first two sessions.