You’ll make the most progress by scoping tiny, finishable projects with clear end states. Optimize for repeatable late‑night sessions, low compute, and shipping small artifacts.

Guiding principles
- Time-box hard: 2 sessions/week, 60–90 minutes each. End every session with a commit and a 5-line “next steps” note.
- Reproduce → small extension: run a known baseline end-to-end, then add one focused change.
- Definition of Done (DoD): a reproducible repo (README + scripts), a results.csv, 1–2 plots/tables, and a 1–2 page write-up with limitations.
- Kill criteria: if blocked >2 sessions (e.g., data/infra), switch to a simpler variant or another track.

Minimal tool stack (all free)
- Colab or Kaggle Notebooks (free GPUs), GitHub, Python, PyTorch or Hugging Face Transformers, scikit-learn, matplotlib/seaborn. Optional: Weights & Biases free tier for tracking.

Pick one of these low-compute project templates (10–15 total hours)
1) LLM safety/hallucination mini-benchmark (no training)
- Question: How robust are 2–3 small open models to paraphrased prompts on a narrow domain?
- Steps: curate 50–100 prompts (benign or non-sensitive safety prompts), define scoring, run models, add one simple defense (system prompt or refusal filter), report harmful completion rate/accuracy + CIs.
- Deliverables: tiny dataset (CSV/JSON), eval script, table with results, short note.

2) CIFAR-10 robustness under corruptions (small training)
- Question: Do AugMix/MixUp/RandAugment improve CIFAR-10-C robustness under equal training time?
- Steps: train ResNet-18 for 20–30 epochs with each augmentation, evaluate on CIFAR-10-C, report clean vs corrupted accuracy, calibration (ECE).
- Deliverables: training/eval scripts, results.csv, accuracy vs corruption plots, short note.

3) RAG grounding eval on a public QA set (no training required)
- Question: Which chunking/retrieval settings reduce hallucinations on a small corpus?
- Steps: build a minimal RAG pipeline (Haystack/LlamaIndex or HF), compare chunk sizes/overlap and a simple reranker, evaluate answer correctness and citation grounding (LLM-as-judge + spot checks).
- Deliverables: reproducible notebook/script, config table vs accuracy/grounding score, failure case examples.

4) Mechanistic interpretability on a toy transformer (tiny models)
- Question: Can you replicate a known head/circuit and test robustness across random seeds?
- Steps: use TransformerLens with a pre-trained small model (e.g., IOI), do head ablations/path patching, repeat across 3 seeds.
- Deliverables: polished notebook, small figure, summary of consistency.

5) Post-training compression trade-offs (engineering)
- Question: How much can you quantize/prune before accuracy drops too far?
- Steps: start with a small classifier (CIFAR-10, ResNet-18), apply 8/4-bit quantization (torch.ao/bitsandbytes), optional magnitude pruning; measure accuracy, latency, and memory.
- Deliverables: Pareto plot (accuracy vs latency/memory), simple recipe in README.

A 6-week night-owl plan (2 sessions/week)
- Week 0 (one setup session)
  - Choose a template above; write a 1-page plan: question, dataset(s), metrics, DoD, risks/fallback.
  - Create a GitHub repo with environment file and an empty results.csv.

- Week 1
  - Session A: Implement baseline end-to-end (run 1 model or 1 training run). Save results, pin versions/seeds.
  - Session B: Automate eval + logging; produce first plot/table; note issues and next steps.

- Week 2
  - Session A: Add your one small extension (e.g., augmentation variant, safety prompt, chunk size change).
  - Session B: Run fair comparison (same budget). Update plots/tables.

- Week 3
  - Session A: Robustness check (paraphrases, CIFAR-10-C, held-out subset). Add basic uncertainty (mean ± std or bootstrap).
  - Session B: Error analysis: 10–20 failure cases with brief labels/notes.

- Week 4
  - Session A: Clean repo, finalize scripts, add seed control and data links. Regenerate results from scratch.
  - Session B: Write a 1–2 page report (Intro, Method, Experiments, Results, Limitations), export final figures.

- Week 5 (optional polish)
  - Share for feedback (HF forums, r/MachineLearning weekly, repo issue templates). Incorporate fixes.
  - If appropriate, open a small PR (eval, docs, bugfix) to an upstream repo you used.

- Week 6 (stretch)
  - Add one extra ablation or a stronger baseline; or prepare a short blog post.

Session template (keeps you moving when tired)
- 10 min: Review last notes; set a single goal for tonight.
- 45–60 min: Execute that one goal.
- 10–15 min: Log results to CSV/W&B; commit; write next-step bullet points.

Definition of Done checklist
- Reproducible: env file, dataset links/hashes, scripts, fixed seeds.
- Fair comparisons: same data and budget; report versions/hardware.
- Results: table + 1–2 plots; mean ± std or CIs; include failure cases.
- Write-up: clear question, method, metrics, results, limitations, ethics note.

Risk management and fallbacks
- If data access or training is flaky: switch to eval-only (Track 1 or 3).
- If compute tight: cut epochs/batch sizes; prefer smaller models; use mixed precision on Colab.
- If scope balloons: lock your comparison set (e.g., 2 methods × 2 settings) and ship.

Quick start (first two sessions, copyable tasks)
- Session 1: Fork/setup repo; pip install; pick dataset/model; run a single baseline or eval on 100 samples; save results.csv with columns [run_id, model, setting, metric, value, seed, timestamp].
- Session 2: Wrap into a function/script; add argparse/config; generate your first plot (matplotlib) and a README with “how to reproduce.”

If you tell me:
- Which template you prefer,
- Your available GPU (Colab/Kaggle vs local),
- Any domain interest (safety, vision, RAG, interpretability, efficiency),
I’ll reply with exact datasets/models, commands, and a step-by-step checklist for your first two sessions.