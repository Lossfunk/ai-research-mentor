Short answer: “LLM tutoring” isn’t novel; “confidence‑calibrated, abstaining, human‑in‑the‑loop tutoring specifically for community colleges with measurable learning gains and equity audits” can be.

What’s been done
- LLM tutors exist (Khanmigo/Duolingo‑style), and classic ITS systems (ALEKS, ASSISTments, Carnegie Learning) are strong baselines in math. Research on LLM calibration/abstention and hallucination reduction is active, but rarely tied to real college courses with outcomes and handoff to human tutors.

Where you can be novel
- Calibration‑first design: The tutor gives an answer + confidence + rationale, and abstains or escalates when uncertain.
- Verified tutoring: Math answers checked by code/unit tests; non‑math grounded in OER (e.g., OpenStax) with citations; model’s claims must be verifiable.
- Skill‑level calibration: Per‑topic/per‑skill calibration curves and thresholds, not one global threshold.
- Human‑in‑the‑loop: Seamless escalation to a tutor center with a summarized transcript when confidence is low; logs support triage and equity monitoring.
- Community‑college fit: Works for developmental math, intro stats, writing/ESL; mobile‑first; accessibility; multilingual.

Baselines to compare
- LLM baselines
  - Uncalibrated LLM (e.g., GPT‑4/Claude) with chain‑of‑thought.
  - Self‑consistency majority vote (k samples) without explicit calibration.
  - RAG‑tutor: LLM grounded on a fixed OER corpus, no abstention.
- Non‑LLM/ITS baselines
  - ALEKS/ASSISTments (math), Carnegie Learning/MATHia if accessible.
  - Writing feedback tools (Grammarly/ETS e‑rater) for composition tasks.
  - Human‑tutor baseline where feasible (e.g., time‑to‑help, correctness of guidance on a sampled set).
Aim to match accuracy/explanation quality while reducing overconfident errors and improving risk‑coverage.

How to implement calibration credibly
- Uncertainty signals: self‑consistency vote fraction; verifier score (unit tests for math, retrieval overlap/citation checks for reading/writing), and model log‑prob proxy.
- Map to probability: temperature scaling or isotonic regression per skill/topic using held‑out data.
- Selective answering: choose thresholds to control target risk (e.g., ≤5% error on answered items); plot risk‑coverage curves.
- Deferral policy: below threshold, offer hints/questions or route to human support with context.

What to measure
- Learning/effectiveness
  - Pre/post gains on aligned items; mastery progression; time‑to‑mastery.
  - Explanation quality (expert rubric) and hint usefulness ratings.
- Calibration/safety
  - ECE/Brier/NLL; overconfident‑wrong rate; risk‑coverage AUC.
  - Hallucination rate; citation coverage for grounded answers.
- Operations/equity
  - Deferral rate and resolution time; tutor center load.
  - Performance and calibration across subgroups (course, ESL status, disability services, device type).
- Adoption/cost
  - Student satisfaction/retention; cost per successful mastery.

Feasibility and risks
- Feasible as a pilot in 1–2 courses (e.g., algebra, intro stats, comp I) within a term.
- Risks: academic integrity (design for hints-first; restrict final answers in graded contexts), privacy (FERPA—minimize data; obtain IRB/consent), accessibility (WCAG compliance, multilingual), equity (avoid worse calibration for certain groups—monitor and adjust).

Minimal viable path (8–10 weeks)
1) Scope: One course unit (e.g., linear equations). Build RAG over OpenStax and a small bank of instructor‑approved solutions.
2) Pipeline: Generator + verifier (math executes code/tests; writing checks grounded facts); self‑consistency (k=5–10); per‑skill calibrator; abstain/deferral logic.
3) Offline eval: 300–500 items with instructor gold labels and explanation rubrics. Report accuracy, ECE, risk‑coverage vs baselines.
4) Pilot: Randomize sections or weeks to calibrated tutor vs uncalibrated RAG tutor; measure learning gains, overconfident‑wrong incidents, and deferrals. Collect student/tutor feedback.
5) Iterate on thresholds, UI for confidence, and handoff workflow.

Bottom line: The idea is only moderately novel in concept, but a calibration‑and‑deferral‑centric tutor tailored to community college contexts, with rigorous calibration metrics and real learning outcomes, is both publishable and useful. Compare against uncalibrated LLMs and established ITS, and make risk‑coverage and overconfident‑wrong reduction the centerpiece of your results.