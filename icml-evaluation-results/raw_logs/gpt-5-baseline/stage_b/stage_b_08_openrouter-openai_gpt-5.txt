Short answer: Moderately original, with manageable risk if you scope it to data cleaning, explanation, and community workflows. Risk rises sharply if you let an LLM give health directives or drive automated interventions.

What exists already
- Citizen-sensing platforms (PurpleAir, Sensor.Community/Luftdaten, OpenAQ) and basic dashboards/alerts.
- NLP on environmental complaints/smell reports and simple rule-based summaries.
- Few proofs-of-concept that fuse free‑text logs with low-cost sensor data using LLMs. This fusion + decision support is not saturated.

Where novelty is credible
- Text–sensor fusion for QA/QC: Use an LLM to extract structured events from logs (wildfire smoke, construction, indoor sources, device maintenance) and improve sensor calibration, drift detection, and anomaly attribution.
- Grounded summaries with numeric reasoning: Retrieval‑augmented, citation‑backed daily/weekly summaries that combine local sensors + official monitors + weather/smoke forecasts, with verified calculations done outside the LLM.
- Low‑resource, multilingual collection: LLMs for on-device transcription, normalization, and de‑duplication of SMS/WhatsApp voice/text reports.
- Triage for agencies/NGOs: Prioritize clusters of issues (schools, senior housing), deduplicate, and generate evidence packets with provenance for action.
- Fairness and trust: Explanations tailored to lay audiences with uncertainty and source attributions; cross‑language consistency checks.

Risk profile (and how to reduce it)
- Health/safety misinformation (high if prescriptive): Do not issue medical advice; frame outputs as informational; escalate to human experts for anything beyond low‑risk tips. Use retrieval‑grounded generation with whitelisted sources (EPA/WHO/local agencies) and automated fact checks.
- Privacy/ethics (medium–high): Logs often include precise locations/timestamps. Get explicit consent, minimize collection, hash IDs, geofence/blur locations, allow deletion, and consider aggregation/differential privacy for public outputs. Comply with GDPR/CCPA where applicable.
- Data quality (medium): Low‑cost sensors drift and over‑respond (e.g., humidity). Calibrate against nearby reference monitors, apply humidity correction, detect device resets/maintenance via text logs, and gate model decisions on QC flags.
- Security/abuse (medium): Poisoning via bogus logs, prompt‑injection in text fields. Sanitize inputs, strip/escape markup, rate‑limit, and use anomaly detection + reputation for users/devices.
- Equity/bias (medium): Uneven sensor density and language access. Evaluate performance and coverage across neighborhoods and languages; invest in outreach and weighting schemes.

How to evaluate it credibly
- Groundedness and correctness: Measure citation coverage and factuality of summaries; numeric checks done with deterministic code, not the LLM.
- QA/QC gains: Reduction in PM2.5 MAE vs reference monitors after text‑assisted QC; improved precision/recall for event attribution (wildfire vs local source).
- Operational value: Faster detection of episodes vs baseline dashboards; de‑duplication rate; time‑to‑action for partner agencies.
- Robustness/fairness: Stress tests with adversarial/noisy logs; consistency across languages; performance by area income/sensor density.
- User impact: Helpfulness/trust ratings; sustained engagement; opt‑in rates given privacy controls.

Minimal viable path (low risk, 6–8 weeks)
1) Pick one city/region with many citizen sensors and accessible reference monitors. Partner with a local NGO/agency.
2) Build a data pipeline: ingest sensor feeds + weather/smoke forecasts + user logs. Do QC and all numeric computations outside the LLM.
3) Train simple classifiers (LLM‑assisted labeling) for event attribution and maintenance detection; validate against known episodes and reference data.
4) Add a retrieval‑grounded summarizer that produces daily briefs with citations and confidence, explicitly non‑prescriptive.
5) Run a pilot with users; log errors, red‑team prompts, and iterate on safety and privacy.

Bottom line
- Originality: Moderate if you focus on text–sensor fusion for QC, attribution, and grounded communication; low if it’s just an AQI chatbot; higher if you show measurable QC/response improvements at community scale.
- Risk: Manageable with grounding, privacy safeguards, and human‑in‑the‑loop; high if you cross into automated health advice or unattended interventions. Define a narrow decision loop and evaluate it rigorously.