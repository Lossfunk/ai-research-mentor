Short answer: The space isn’t “overdone,” but it is noisy. Plenty of vendor “bias audits” exist (often shallow and compliance‑focused), while rigorous, end‑to‑end, independently executed audits that cover validity, multi‑stage pipelines, disability/accessibility, and real‑world impact are rare. You can still be novel if you pick a concrete slice, use strong methods, and align with current regulations.

How to check novelty quickly
- Map the niche
  - Tool type: resume screeners, video/voice interviews, games‑based assessments, coding tests, job‑ad targeting, chatbots/screening Q&A.
  - Setting: specific job family (e.g., customer service, RN, warehouse), geography (NYC LL144 applicability, EU AI Act), candidate population (non‑English, mobile‑only).
- Look for prior audits in your niche
  - Search for “bias audit” or “independent audit” + vendor name; check NYC Local Law 144 public summaries; look for case studies (e.g., Pymetrics/HireVue), EEOC actions, and NGO reports.
  - Scan academic work: resume audit studies, paired‑testing on hiring platforms, and validation studies in I‑O psychology (SIOP, UGESP).
- Novelty rubric (more “yes” = more novel)
  - Multi‑stage, end‑to‑end audit across sourcing → screening → assessment → interview → offer, not a single tool in isolation.
  - Intersectional and small‑group analysis (sex×race, disability, age) with appropriate statistics (e.g., Fisher’s exact when n is small), not just overall adverse‑impact ratio.
  - Validity and job‑relatedness (UGESP/SIOP): content/construct/criterion validity evidence, not just parity metrics.
  - Accessibility and accommodations (ADA/WCAG, non‑English, low literacy, mobile‑only).
  - Time dynamics: drift/retraining, updates, monitoring, and rollback procedures.
  - Transparency and candidate experience: explanations, notice/consent, appeal mechanisms, data rights.
  - Black‑box plus counterfactual methods (paired resumes/controlled attributes) and, where possible, white‑box review of model docs.
If you can credibly cover 3–5 of these in a focused domain, you’re in novel territory.

Scope it responsibly (and feasibly)
- Pick one job family + one or two tool types
  - Example: hourly warehouse roles using resume screener + video interview.
- Define claims up front
  - “We assess adverse impact, validity proxies, and accessibility; we do not claim legal compliance determination for any single employer.”
- Use mixed methods
  - Black‑box paired testing: generate matched résumés/replies that differ on a protected attribute proxy (e.g., name or caregiving gap) to test score sensitivity.
  - Pipeline analysis: measure pass‑through rate disparities across stages; show how small disparities compound.
  - Validity proxies: correlate assessment scores with job‑relevant criteria (where available) or expert rubrics; check calibration and thresholding logic.
  - Documentation review: vendor docs vs regulatory requirements (notice, data retention, human oversight).
  - Accessibility audit: mobile UX, bandwidth, screen reader, language; candidate drop‑off by device/network.
- Provide constructive outputs
  - Vendor scorecards tied to NIST AI RMF and (where applicable) NYC LL144/EU AI Act requirements.
  - Remediation simulations: threshold changes, human‑in‑the‑loop review, or alternative assessments; plot accuracy–fairness trade‑offs.

What to compare against (baselines)
- Human‑only screening baseline (if data available): disparity and consistency vs the AI pipeline.
- Simple heuristic baselines: keyword filters or length‑of‑experience rules matched for selection rate.
- Prior vendor audit metrics: adverse‑impact ratio and demographic parity; show where your methods go beyond (e.g., intersectional, calibration, validity).

Key metrics to report
- Fairness: adverse‑impact ratio, difference in selection rates, equal opportunity (if labels exist), intersectional parity; uncertainty intervals.
- Validity and calibration: correlation with job outcomes or expert ratings; Brier/ECE for calibrated scores; test–retest reliability if applicable.
- Pipeline compounding: disparity at each stage and overall; Shapley‑style attribution of disparity to stages.
- Transparency/privacy: presence of candidate notice, explanation quality, data retention, access/appeal routes.
- Accessibility: completion/drop‑off by device/bandwidth; assistive tech compatibility; language coverage.

Ethical/legal risk (and mitigations)
- Legal exposure: scraping/TOS, CFAA concerns; consult counsel; prefer partnerships with employers/vendors or public APIs/sandboxes.
- PHI/PII and biometrics: if video/voice, consider BIPA‑like regimes; minimize collection; avoid face analysis unless strictly necessary and disclosed.
- Title VII/ADA/ADEA: do not infer sensitive traits; handle accommodations; avoid tests that penalize disabilities or caregiving gaps.
- Research ethics: IRB or equivalent review, informed consent when interacting with real candidates; avoid harming applicant chances (use sandbox or synthetic accounts).
- Vendor retaliation/NDAs: publish methods, not secrets; allow right‑to‑reply; focus on systemic findings.

A credible 8–10 week plan
- Week 1–2: Choose job family + tools; secure partner or sandbox; preregister audit plan; legal/IRB check.
- Week 2–4: Build synthetic, realistic résumé/response generator with controllable attributes; design paired tests; define metrics and power analysis.
- Week 4–6: Run black‑box tests; collect stage‑wise results; perform intersectional and calibration analyses; do accessibility checks; review documentation against NIST RMF/LL144/EU AI Act.
- Week 6–8: Compare to baselines; simulate remediations; draft vendor scorecards and candidate‑facing disclosures; solicit expert review (I‑O psychologist, legal).
- Week 8–10: Produce a public report with methods, code, and limitations; share with stakeholders; plan a follow‑up longitudinal drift audit.

Ways to stand out
- Focus on disability/accessibility and accommodations—rarely done well.
- Do an end‑to‑end pipeline audit (sourcing + screening + assessment) with compounding disparity analysis.
- Evaluate validity/job‑relatedness under UGESP/SIOP principles, not just fairness metrics.
- Release a reusable audit protocol and synthetic testing kit so others can replicate without sensitive data.

Bottom line: Auditing AI hiring isn’t “done” if you make it rigorous, scoped, and aligned with real regulations and I‑O standards. Pick a narrow slice, use strong causal/paired methods and end‑to‑end metrics, and deliver actionable, transparent findings with ethical safeguards. That will be both responsible and novel enough to matter.