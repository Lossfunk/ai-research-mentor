Below is a concise, practical shortlist of strong venues for a 6‑month “compressible LLMs” project, including what they typically expect on ethics and formatting. Exact templates, page limits, and policies change year to year—always confirm against the current CFP before submission.

Quick venue picker
- Primarily a new compression method with broad ML interest: NeurIPS, ICLR, ICML
- Heavy systems/runtime efficiency or hardware co‑design: MLSys
- Language-centric evaluation or contributions for NLP practice: ACL/EMNLP (via ARR)
- Need rolling review and faster turn-around to journal-quality: TMLR
- Mature, extended version with full proofs/engineering: JMLR

Top venues and typical constraints

1) NeurIPS (Neural Information Processing Systems)
- Fit: General ML; model compression (quantization, pruning, distillation), efficient LLM inference/training.
- Reviewing: Double‑blind; ethics review for flagged papers.
- Formatting: NeurIPS LaTeX template (neurips_20xx). Main paper typically ~9 pages; references unlimited; appendix allowed but not guaranteed to be reviewed.
- Ethics/compliance:
  - Reproducibility checklist required.
  - Ethics statement required if applicable (dual‑use, bias/fairness, dataset consent, privacy, environmental impact).
  - Code/data release strongly encouraged at camera‑ready.
- Notes: Datasets and Benchmarks track can be suitable if your contribution is a new compression benchmark or evaluation protocol.

2) ICLR (International Conference on Learning Representations)
- Fit: Methodological advances in representation learning; quantization‑aware training, sparsity, low‑rank/adapter design for LLMs.
- Reviewing: Double‑blind on OpenReview; public discussion.
- Formatting: ICLR LaTeX template (iclr20xx_conference). Main paper commonly ~8 pages; references unlimited; appendices allowed.
- Ethics/compliance:
  - Limitations section mandatory.
  - Ethics/broader‑impact discussion required when relevant.
  - Reproducibility checklist required; code link encouraged (often expected) at submission or camera‑ready.

3) ICML (International Conference on Machine Learning)
- Fit: Broad ML; theoretical/empirical compression methods with strong experimental rigor.
- Reviewing: Double‑blind; ethics review for flagged submissions.
- Formatting: ICML LaTeX template (icml20xx), two‑column. Main paper typically ~8 pages; references unlimited; appendices allowed.
- Ethics/compliance:
  - Reproducibility checklist required.
  - Ethics/negative societal impact discussion when applicable.
  - Code/data release strongly encouraged.

4) MLSys (Conference on Machine Learning and Systems)
- Fit: Systems‑oriented efficiency: GPU/accelerator kernels for quantized ops, KV‑cache compression at scale, compiler/serving/runtime co‑design, memory/latency/throughput tradeoffs.
- Reviewing: Double‑blind; optional artifact evaluation with badges.
- Formatting: MLSys LaTeX template (mlsys20xx), two‑column. Page limit is typically more generous than ICML/ICLR; check current CFP.
- Ethics/compliance:
  - Strong emphasis on reproducible systems evaluation: fixed seeds, hardware specs, throughput/latency, energy.
  - Artifact sharing encouraged; badges available.

5) ACL/EMNLP (via ARR – ACL Rolling Review)
- Fit: NLP‑centric compression of LLMs, quality/faithfulness impacts, instruction‑tuned small models, multilingual/low‑resource compression outcomes.
- Reviewing: Double‑blind via ARR; can commit to ACL/EMNLP/NAACL after reviews.
- Formatting: ACL LaTeX template (acl_latex