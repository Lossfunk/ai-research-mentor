<thinking>
Experienced researcher. They want compliance mapping, not pedagogy. I should be blunt about what *will* get flagged for LM-for-law papers: human data, legal risk, misuse, artifacts. Keep venue-agnostic but aligned with NeurIPS/ICLR/EMNLP/ACL.
</thinking>

**Assessment:** *Language models for law* trigger **heightened ethics scrutiny**. Reviewers look for concrete disclosures, not boilerplate. Missing any of the sections below is a fast reject or ethics flag at major AI venues.

---

## Required ethics & compliance sections (what you must explicitly cover)

### 1. **Human Data & Legal Text Provenance**
- Source of legal data (cases, contracts, statutes)
- Public vs proprietary status; license terms
- Presence of personal data and any de‑identification
- Jurisdictional constraints (e.g., EU court records ≠ US)

**Disclosure checklist:** dataset names, licenses, collection dates, removal requests honored

---

### 2. **Intended Use and Non‑Use (Critical for Law)**
- What the model is *not* suitable for (e.g., legal advice, automated decisions)
- Role of human oversight
- Explicit disclaimer that outputs are not legally binding

Reviewers will penalize vague “assist lawyers” language.

---

### 3. **Risk of Harm & Misuse**
- Hallucinated citations and legal precedent fabrication
- Bias amplification (jurisdiction, demographic, firm size)
- Automation bias in legal decision‑making

You must discuss **plausible real‑world harms**, not abstract ones.

---

### 4. **Evaluation with Human Stakeholders**
- Who evaluated outputs (lawyers, students, annotators)
- Compensation and consent
- Inter‑annotator agreement and disagreement handling

If no legal experts were used, justify why.

---

### 5. **Fairness and Bias**
- Differential performance across jurisdictions, case types, or parties
- Limits of fairness claims given data imbalance

Saying “out of scope” without evidence is a red flag.

---

### 6. **Privacy**
- Memorization risks from sensitive legal texts
- Any red‑teaming or leakage tests performed
- What you did *not* test

---

### 7. **Compute and Resource Disclosure**
- Model size, training/fine‑tuning compute
- Use of proprietary LMs (API dependence must be stated)

---

### 8. **Artifact & Reproducibility Adherence**
- Code availability (or justification if not released)
- Dataset access instructions and restrictions
- Prompt sets, evaluation rubrics, and annotation guidelines

Benchmarks without artifacts are increasingly rejected.

---

### 9. **Legal and Ethical Limitations**
- No claims of compliance with law or regulation
- Explicit statement that the model does not replace legal professionals

---

## Venue expectations (collapsed, but real)
- **NeurIPS / ICLR:** Mandatory ethics + limitations; misuse discussion required  
- **ACL / EMNLP:** Data statements and human annotation ethics are scrutinized hardest  
- **Artifact badges:** increasingly expected for empirical LM papers

**Direction:** Write these sections *before* finalizing experiments. If you want, paste your current ethics/limitations section and I’ll mark exactly what reviewers will say is missing or weak.