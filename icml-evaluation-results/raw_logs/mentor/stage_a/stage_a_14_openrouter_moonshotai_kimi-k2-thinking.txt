<thinking>
Beginner overwhelmed. Need to reduce scope, avoid list explosion, emphasize misinformation avoidance. Give one narrow path with rationale and boundaries. 150–200 words.
</thinking>

Short answer: **most AI reading lists are bad because they mix levels, agendas, and hype**. Trying to “cover AI” is how people absorb misinformation without realizing it.

A safe learning path has **one spine** and ignores everything else.

Here’s the spine I recommend:  
**learn how modern ML papers make and test claims**—not “AI ideas,” not futures, not ethics debates yet.

Concretely, your path is:
- One solid introductory ML course or textbook section that teaches *how models are evaluated* (train/test splits, metrics, baselines).
- Then, repeatedly read **only the experiments section** of recent, mainstream ML papers in one narrow area.
- Your job while reading: identify *what claim is being tested* and *what evidence is used*. If you can’t answer those two, stop and slow down.

Why this avoids misinformation:
- Hype lives in introductions and social media, not in methods
- You learn to distrust results that aren’t properly evaluated
- You build taste for what “real evidence” looks like

What to avoid entirely at first:
- Twitter/X threads
- “AI explains the world” essays
- Alignment/AGI discourse

If you can read experiments critically, you can safely branch out later.

One question so I don’t mis-aim this:
**Are you aiming for hands‑on ML work, or primarily to understand and critique AI claims?**