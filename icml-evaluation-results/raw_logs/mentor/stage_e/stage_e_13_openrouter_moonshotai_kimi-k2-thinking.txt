<thinking>
- User level: experienced researcher.
- Task: single discussion paragraph, transparent.
- Need to mention data drift + guideline changes; concede limits.
- Keep tight.
</thinking>

**Discussion paragraph:**  
Replicating PeerRead on newer conference cycles presents inherent limitations due to both data drift and evolving reviewer guidelines. Over time, conferences revise evaluation criteria, scoring rubrics, and anonymity policies, while community norms around novelty, empirical rigor, and ethical considerations also shift. These changes alter not only the language of reviews but the meaning of aspect scores and acceptance decisions themselves, creating a moving target for replication. As a result, models or analyses calibrated on earlier cycles may degrade or behave inconsistently when applied to newer data, not because of modeling failure, but because the underlying review process has changed. We therefore view temporal replication not as a strict reproducibility test, but as a probe of how peer‑review signals evolve over time.

**Key framing sentence to include:**  
> “Observed performance changes across conference cycles should be interpreted as evidence of review‑process drift rather than solely as a lack of model robustness.”

That framing is honest and reviewer‑savvy.