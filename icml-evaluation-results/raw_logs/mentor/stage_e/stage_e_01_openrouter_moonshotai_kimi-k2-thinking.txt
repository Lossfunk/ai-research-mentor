<thinking>
- User level: experienced ML/NLP researcher, writing a paper.
- Task: anticipate reviewer attacks on label validity; propose discussion mitigations.
- Honest assessment: this label is weakly supervised and reviewers will pounce.
- Need concrete concerns + defensible responses, not hand-waving.
</thinking>

**Blunt assessment:** reviewers are right to be skeptical. “Probably‑rejected” is a noisy proxy label, not ground truth. If you oversell it, you’ll get hit. If you frame it correctly and show robustness, it’s survivable.

Here are the *specific* concerns reviewers will raise, and how to mitigate them in the discussion.

**Core concern: arXiv ≠ submission outcome.**  
Reviewers will say: many accepted papers appear on arXiv, and many rejected ones don’t; absence of acceptance signals is not rejection.  
**Mitigation:** explicitly frame the label as *weak supervision capturing lack of observable acceptance*, not rejection. State that the task models *review‑stage signals correlated with non‑acceptance*, not final decisions. This reframing matters.

**Selection bias / venue bias.**  
arXiv posting practices vary by field, seniority, and venue. “Probably‑rejected” may encode subfield norms, not quality.  
**Mitigation:** report stratified analyses by year, field, and venue when possible, and show that main trends hold within strata. In discussion, concede residual bias and argue why it doesn’t fully explain effects.

**Temporal mislabeling.**  
Some “rejected” papers are later accepted elsewhere or in later rounds.  
**Mitigation:** argue that the label reflects *initial review outcomes*, which are the relevant signal for PeerRead-style review modeling. If possible, note that delayed acceptance introduces label noise that biases results toward null effects—making findings conservative.

**Construct validity: are you predicting quality or visibility?**  
Reviewers may argue models learn author prestige or formatting cues.  
**Mitigation:** discuss feature ablations or controls (e.g., removing author metadata if applicable) and emphasize that the dataset captures *reviewer response patterns*, not intrinsic merit.

**External validity.**  
“Would this generalize beyond PeerRead/arXiv?”  
**Mitigation:** clearly limit claims. Say results inform *behavior of review processes under weak outcome signals*, not universal acceptance prediction.

**What not to do:** don’t claim “rejection prediction,” don’t bury the caveat in a footnote, and don’t pretend the label is close to ground truth.

**One sentence you should basically include:**  
> “We treat ‘probably‑rejected’ as a noisy, weakly supervised indicator of missing acceptance signals rather than a definitive outcome; any conclusions should be interpreted as patterns robust to substantial label noise, not as predictors of final acceptance.”

That framing turns a liability into methodological honesty—and reviewers respect that.