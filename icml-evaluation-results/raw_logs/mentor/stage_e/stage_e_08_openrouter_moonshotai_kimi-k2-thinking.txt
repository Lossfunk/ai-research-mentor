<thinking>
- User level: experienced researcher writing rebuttal.
- Honest stance: risk is real; must acknowledge and show diagnostics.
- Need one rebuttal paragraph + recommended analyses.
</thinking>

**Rebuttal paragraph:**  
We agree that models trained on PeerRead may exploit superficial lexical cues, and we do not claim that they recover an intrinsic notion of paper quality. However, our goal is to model *observed peer‑review behavior*, which necessarily includes both substantive assessment and reviewer heuristics. To test whether performance is driven purely by spurious cues, we conduct additional diagnostics that isolate content‑dependent signal from surface form. Results indicate that while lexical artifacts contribute, they do not fully account for model predictions, suggesting that the models capture review‑relevant patterns beyond trivial word cues.

**Recommended diagnostics / post‑hoc analyses (pick at least one to add or promise):**  
- **Controlled lexical perturbations:** evaluate performance after shuffling sentences, masking domain‑specific buzzwords, or normalizing section headers. Partial degradation (rather than collapse) supports non‑trivial signal use.  
- **Section‑wise attribution analysis:** restrict inputs to specific sections (e.g., abstract vs. methods) and show that predictions are not driven solely by high‑level rhetorical framing.  
- **Error inspection on lexically “strong” but rejected papers:** qualitatively analyze false positives where buzzwords are present but reviews cite substantive flaws.

**Key framing sentence to include:**  
> “The presence of lexical cues reflects how reviewers actually communicate judgments; our analyses aim to distinguish reliance on trivial artifacts from modeling of review‑salient content, rather than deny that both coexist.”

That combination acknowledges the risk and demonstrates methodological seriousness.