<thinking>
- User level: experienced researcher.
- Honest take: lack of disagreement reporting is a real weakness.
- Need discussion text + concrete appendix analyses.
- Keep concise and credible.
</thinking>

**Discussion text:**  
We acknowledge that the current presentation of PeerRead does not make annotation disagreement sufficiently transparent. Aspect scores and acceptance labels aggregate reviewer judgments that may reflect substantial divergence, particularly for borderline submissions. Without explicitly characterizing this disagreement, downstream models risk being interpreted as predicting a single “ground truth” label when the underlying signal is inherently uncertain. Our findings should therefore be read as modeling *central tendencies* of reviewer behavior rather than unanimous judgments.

**Concrete follow‑up analyses for the appendix:**  
We will report **per‑paper reviewer variance and entropy** for each aspect, and stratify model performance by disagreement level to show where predictions are most and least reliable. In addition, we will include **agreement statistics** (e.g., pairwise correlations or Krippendorff’s α where applicable) to distinguish stable aspects from inherently noisy ones. Finally, we will visualize **error rates versus disagreement**, making clear that many “errors” occur in cases with low reviewer consensus.

**Key sentence to include:**  
> “Explicitly surfacing reviewer disagreement reframes model errors as reflections of evaluative ambiguity rather than annotation noise alone.”

That directly addresses the transparency critique and strengthens the paper’s interpretability.