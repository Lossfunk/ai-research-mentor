<thinking>
- User level: experienced researcher responding to reviews.
- Need succinct response; acknowledge gap without committing to rushed experiments.
- Be principled, not defensive.
</thinking>

**Response draft:**  
We agree with the reviewers that comparisons to large language models (LLMs) are an important missing reference point. Our study predates the widespread availability of instruction‑tuned LLMs suitable for controlled evaluation, and we therefore focused on models that allow fine‑grained analysis of features and supervision sources. As a result, our conclusions should be read as characterizing the PeerRead task and labels rather than claiming state‑of‑the‑art performance.

As future work, we plan to include LLM baselines in a *diagnostic* rather than purely competitive setting: (i) evaluating zero‑ and few‑shot LLMs on acceptance and aspect prediction to assess how much signal can be recovered without task‑specific training, and (ii) repeating key ablations with LLM inputs constrained to manuscript text only, to separate language understanding from reviewer‑signal exploitation. This will allow a principled comparison that clarifies whether LLM gains reflect better modeling of scientific discourse or greater sensitivity to dataset artifacts.

We have updated the discussion to reflect this scope and limitation.