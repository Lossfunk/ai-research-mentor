<thinking>
- User level: experienced researcher.
- Need discussion text + concrete diagnostics.
- Overfitting risk is real; must acknowledge and show controls.
- Keep concise.
</thinking>

**Suggested discussion text:**  
We acknowledge the risk that the acceptance classifier may overfit idiosyncrasies of the PeerRead dataset, particularly venue‑specific decision thresholds and reviewer signaling patterns. While held‑out test performance mitigates this concern, it does not rule out overfitting to stable but non‑generalizable correlations. Accordingly, our results should be interpreted as modeling acceptance decisions *within the observed data regime*, rather than as evidence of robust decision prediction under distribution shift.

**Diagnostics / subsample analyses to append (and explicitly reference):**  
Include **learning curves** showing performance saturation with increasing training data; early saturation supports limited capacity rather than memorization. Add **year‑ or venue‑held‑out evaluations** to visualize performance drops under mild distribution shift. Finally, report **performance stratified by score margin** (clear accepts/rejects vs. borderline cases); overfitting typically manifests as inflated accuracy on easy cases with little gain on ambiguous ones.

**Key sentence to include:**  
> “Appendix analyses demonstrate that gains concentrate in low‑ambiguity regimes and degrade under temporal or venue shifts, suggesting limited memorization but constrained generalization.”

That acknowledges the risk while showing you’ve interrogated it seriously.