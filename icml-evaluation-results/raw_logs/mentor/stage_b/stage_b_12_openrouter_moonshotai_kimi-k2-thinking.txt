<thinking>
User is mid-level applied research. Need verdict: not novel in headline; potential novelty in calibration + pedagogy + population. Baseline: strong non-LLM tutors and uncalibrated LLM. Avoid long lists.
</thinking>

**Headline-level, this is not novel.** LLM tutoring, uncertainty estimation, and confidence calibration have all been explored. Reviewers will assume “yet another LLM tutor” unless you make the *calibration* and the *population* do real scientific work.

Where novelty *could* exist is very specific: **calibration that changes pedagogical behavior for underprepared students**. If your system only outputs a confidence score or says “I’m not sure,” that’s incremental. If calibrated uncertainty *causally improves learning outcomes*—by triggering fallback explanations, human escalation, or different scaffolding—that’s more interesting, especially in community college contexts that are under-studied.

The biggest risk: this collapses into an HCI/ed‑tech deployment with weak controls. Reviewers will ask whether gains come from novelty, extra attention, or the model itself.

**Baseline choice matters more than the model.**  
The baseline you *must* beat is not another fancy LLM—it’s:
- the *same* LLM tutor **without calibration**, using identical prompts and content.
If you don’t show that calibration changes outcomes, the paper dies.

A secondary baseline (if you can manage it) is a **simple rule‑based tutor** with scripted uncertainty (“I don’t know, here’s a hint”). Many such systems perform surprisingly well.

Do *not* lead with benchmark accuracy. Lead with learning outcomes, misuse reduction, or trust calibration.

Before you proceed: are you aiming for an ML venue, learning sciences, or deployment? The answer changes what “novel” even means here.