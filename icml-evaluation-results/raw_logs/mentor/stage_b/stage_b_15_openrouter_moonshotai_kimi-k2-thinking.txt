<thinking>
User is thoughtful, applied ethics/ML. Needs reassurance but honest: area is crowded; novelty via scope, access, method. Explain how to check novelty and define responsible scope. No tools.
</thinking>

**Your instinct is right: AI hiring audits are crowded—but that doesn’t mean the idea is dead.**  
It *is* overdone at the headline level. Many papers audit résumé screeners or job ads for bias using public proxies. Reviewers now expect more than “we tested platform X and found bias Y.”

To check novelty **properly**, don’t start with papers—start with *claims*. Ask: *What does this platform explicitly claim to do fairly or accurately?* Novel audits test **unstated or untested assumptions**, not generic fairness metrics. If your audit only reproduces known disparities, it’s incremental.

Then check **access realism**. Most published audits rely on synthetic résumés and black‑box probing because they lack internal access. If that’s your plan too, your scope must be modest and precise. Overclaiming (“this proves discrimination”) is the fastest way to get rejected—or criticized ethically.

Responsible scope comes from three constraints:
- **Audit what users actually experience**, not internal intent.
- **Match claims to evidence**: black‑box methods → limited conclusions.
- **Anticipate harm**: publishing exploit paths or sensitive vulnerabilities can backfire.

Where real novelty still exists:
- Longitudinal audits (systems change over time; most studies are static).
- Intersectional or role‑specific effects that platforms ignore.
- Auditing *feedback loops* (how rejection shapes future candidates), not just initial screening.

Before you proceed, write one sentence answering:  
*“This audit reveals a failure mode that prior hiring audits could not detect because ___.”*  
If you can’t fill that blank cleanly, the idea needs sharpening—not abandoning, but refocusing.