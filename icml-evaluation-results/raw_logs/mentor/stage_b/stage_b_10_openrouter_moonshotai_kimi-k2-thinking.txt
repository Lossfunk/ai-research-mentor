<thinking>
User is mid-level, asking about prior work + ethics. Need clear verdict: lots of prior work; ethical risk is substantial; viability depends on framing. Lead with assessment, then specifics, then direction.
</thinking>

**Yes, there is substantial prior work—and the ethical risk is high.**  
Self‑supervised learning on clinical text (including nursing and shift notes) is *not* new. It sits squarely in clinical NLP: representation learning from EHR notes, masked‑language modeling, contrastive objectives, etc. Reviewers will immediately place you next to work from MIMIC‑III/IV–style corpora and ask what’s different.

**Where novelty might still exist:** shift notes are unusually temporal, informal, and action‑oriented. If your SSL objective explicitly models **handoff continuity, uncertainty, or decision transitions across shifts**, that’s more interesting than “we pretrained a model on notes.”

**Ethical and practical risk (this is the bigger issue):**
- **Privacy leakage**: self‑supervision does *not* make data safe. Models can memorize identifiers, rare events, or sensitive narratives.
- **Consent and secondary use**: shift notes were not written for ML training; IRBs care about this distinction.
- **Downstream misuse**: representations can encode clinician performance, bias, or sensitive patient attributes even if you never predict them.

If your plan is “pretrain a big model and see what it’s good at,” expect rejection on ethics alone.

To make this defensible:
- Treat ethics as a **design constraint**, not a discussion section: de‑identification limits, auditing for memorization, restricted deployment.
- Define a **single downstream question** that clearly benefits care (e.g., reducing handoff errors), not generic embeddings.

Before going further: what dataset access do you actually have, and is this for method development or a clinical application? That determines whether this is viable or a non‑starter.