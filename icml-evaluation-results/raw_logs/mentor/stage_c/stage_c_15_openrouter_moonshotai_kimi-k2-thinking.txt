<thinking>
User is experienced; topic is sensitive. Key is to be strict about scope (assistive, not legal advice), human-centered metrics, and IRB sequencing. Lead with risks and governance baked in. </thinking>

**Blunt assessment:**  
This is publishable **only if** you (a) constrain claims to *assistive* use, (b) pre-register human-study protocols with IRB rigor, and (c) evaluate harms, not just usefulness. “Accuracy on legal QA” alone will get rejected. The plan below centers human outcomes, ethics, and concrete checkpoints.

---

## 1. Problem framing and goals

**Problem.**  
AI legal assistants are rapidly deployed, yet evidence on **human outcomes** (trust calibration, error propagation, access-to-justice impacts) is thin. Existing evaluations overemphasize answer correctness and ignore how humans actually use—and misuse—these tools.

**Goal.**  
Conduct a **human-centered evaluation** of AI legal assistants that measures **usefulness, trust calibration, error handling, and harm risk**, under ethically approved protocols.

**Scope (locked).**  
- Jurisdiction-limited (e.g., U.S. civil law, small-claims/tenant issues).  
- Assistive drafting, explanation, and retrieval—**not legal advice** or representation.

**Success criteria.**  
Demonstrate measurable differences in human decision quality and risk when using AI assistance vs controls, with transparent ethical safeguards.

---

## 2. Experiments

### Experiment 1: Task performance with and without AI assistance

**Hypothesis.**  
AI assistance improves task completion speed but can reduce correctness on edge cases without proper guardrails.

**Setup.**  
- Participants complete realistic legal tasks (issue spotting, form completion, summarizing statutes).  
- Conditions: no AI, AI with disclaimers, AI with calibrated uncertainty cues.

**Baselines.**  
- No-AI condition  
- Static legal templates/guides

**Evaluation metrics.**  
Task accuracy (expert-graded), time-to-completion, omission/error rates.

**Expected outcomes.**  
Speed gains with AI; accuracy gains only with uncertainty-aware interfaces.

---

### Experiment 2: Trust calibration and overreliance

**Hypothesis.**  
Participants over-trust fluent but incorrect AI outputs unless explicitly prompted to verify.

**Setup.**  
- Inject controlled AI errors (outdated law, jurisdiction mismatch).  
- Measure whether users detect and correct errors.

**Baselines.**  
- Perfect-information condition (no injected errors)

**Evaluation metrics.**  
Error detection rate, correction success, confidence–accuracy gap.

**Expected outcomes.**  
Overreliance without cues; improved calibration with transparency interventions.

---

### Experiment 3: Access-to-justice and equity effects

**Hypothesis.**  
AI assistants benefit users with less legal literacy more—but also expose them to higher harm if errors occur.

**Setup.**  
- Stratify participants by legal familiarity.  
- Compare outcomes and perceived empowerment.

**Baselines.**  
- Human-readable guides alone

**Evaluation metrics.**  
Outcome parity, self-efficacy scores, harm proxies (missed deadlines, wrong filings).

**Expected outcomes.**  
Disparate benefits and risks across user groups.

---

### Experiment 4: Ethical stress tests and misuse scenarios

**Hypothesis.**  
Without safeguards, users will push AI toward unauthorized legal advice.

**Setup.**  
- Observe attempts to elicit advice beyond scope.  
- Evaluate refusal quality and redirection effectiveness.

**Baselines.**  
- Rule-based refusal scripts

**Evaluation metrics.**  
Boundary adherence, user frustration, safe redirection success.

**Expected outcomes.**  
Hybrid policy + UX approaches outperform prompt-only constraints.

---

## 3. Timeline (6 months)

**Month 1:**  
Finalize scope; draft IRB protocol (risk assessment, consent, data handling).

**Month 2:**  
IRB submission and revisions; task and interface prototyping.

**Month 3:**  
Pilot study; refine instruments; pre-register analysis plan.

**Month 4:**  
Main study data collection (Experiments 1–2).

**Month 5:**  
Equity and stress-test studies (Experiments 3–4).

**Month 6:**  
Analysis, ethics appendix, paper drafting, reproducibility package.

**IRB checkpoints:**  
- No recruitment before approval  
- Adverse-event reporting plan active throughout

---

## 4. Resources

**Compute.**  
- Minimal; inference-only use of AI assistants.

**Tools.**  
- Survey/experiment platforms  
- Secure data storage (de-identified)  
- Expert graders (law students or clinicians)

**Datasets.**  
- Public statutes, forms, and case summaries (jurisdiction-scoped)

**Evidence note.**  
There is no gold-standard benchmark for human-centered legal AI evaluation. To strengthen claims, include **expert-graded rubrics** and release anonymized study materials.

---

## 5. Risks and mitigations

| Risk | Impact | Mitigation |
|---|---|---|
| Providing legal advice | Ethical breach | Strict scope + refusals |
| Participant harm | IRB halt | Low-stakes tasks only |
| Overclaiming | Rejection | Limit conclusions |
| Privacy concerns | Noncompliance | De-identification, audits |

---

## 6. Stretch ideas or follow-up directions

- Longitudinal studies of reliance over time  
- Clinician-in-the-loop legal aid models  
- Regulatory guidance for assistive legal AI

---

**Bottom line:**  
If you don’t measure *how humans fail with AI*, you haven’t evaluated a legal assistant.

**Optional next step:**  
Draft the **IRB consent and risk language** now—it will force clarity on what your system is *not* allowed to do.