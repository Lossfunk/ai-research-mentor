<thinking>
User is experienced. Topic is solid but politically and scientifically sensitive: cross‑lab work fails without clear division of labor and agreement on uncertainty definitions. Need to be direct about coordination risks and lock evaluation/UQ early. No web search strictly needed. </thinking>

**Blunt assessment:**  
This collaboration will fail unless you **freeze definitions of uncertainty, metrics, and data splits up front**. Climate downscaling papers often talk past each other because labs mean different things by “skill” and “uncertainty.” The plan below is structured to prevent that failure mode.

---

## 1. Problem framing and goals

**Problem.**  
Large-scale climate model downscaling (statistical and ML-based) shows strong pointwise skill, but **uncertainty quantification (UQ) is inconsistent, incomparable across methods, and rarely validated across institutions**. This limits scientific trust and downstream decision-making.

**Goal.**  
Establish a **cross-lab, apples-to-apples evaluation** of climate downscaling methods with a shared, rigorous UQ framework.

**Core objectives.**
- Compare deterministic and probabilistic downscaling methods under identical conditions.
- Evaluate **calibration, sharpness, and robustness** of uncertainty estimates.
- Produce a shared benchmark + governance protocol other labs can reuse.

**Scope control (critical).**
- One variable (e.g., daily precipitation or temperature).
- One region or small set of climatically distinct regions.
- One base GCM ensemble.

---

## 2. Experiments

### Experiment 1: Deterministic skill vs probabilistic calibration

**Hypothesis.**  
Methods with similar mean skill can differ dramatically in uncertainty quality.

**Setup.**  
- Each lab contributes 1–2 downscaling methods (e.g., bias correction, CNN, diffusion, GAN, GP-based).
- Common training/validation/test splits fixed centrally.
- Probabilistic outputs standardized (e.g., predictive distributions or ensembles).

**Baselines.**  
- Bias-corrected interpolation  
- Quantile mapping  
- Raw GCM output

**Evaluation metrics.**  
RMSE/CRPS, reliability diagrams, rank histograms.

**Expected outcomes.**  
Clear separation between point-skill leaders and well-calibrated UQ methods.

---

### Experiment 2: Out-of-distribution (OOD) and nonstationarity stress test

**Hypothesis.**  
Most downscaling UQ methods are overconfident under climate regime shifts.

**Setup.**  
- Train on historical period; test on later or climatically distinct periods.
- Evaluate extrapolation performance explicitly.

**Baselines.**  
- Deterministic ML with post-hoc uncertainty  
- Ensemble-only uncertainty

**Evaluation metrics.**  
Coverage under shift, sharpness degradation, failure rates.

**Expected outcomes.**  
Evidence that many UQ methods break under nonstationarity.

---

### Experiment 3: Inter-lab reproducibility and sensitivity analysis

**Hypothesis.**  
Reported uncertainty is sensitive to seemingly minor choices (normalization, loss weighting, ensemble size).

**Setup.**  
- Re-run selected methods across labs using shared configs.
- Perturb one design choice at a time.

**Baselines.**  
- Original reported configurations

**Evaluation metrics.**  
Variance in calibration metrics across labs.

**Expected outcomes.**  
Identification of brittle vs robust UQ practices.

---

## 3. Timeline (6 months)

**Month 1:**  
Consortium kickoff; lock variables, region, data, metrics, uncertainty definitions.

**Month 2:**  
Central data preprocessing; baseline implementations distributed.

**Month 3:**  
Lab-specific model training; deterministic + probabilistic outputs.

**Month 4:**  
Centralized evaluation (Exp. 1); initial cross-lab comparison.

**Month 5:**  
OOD and reproducibility experiments (Exp. 2–3).

**Month 6:**  
Synthesis, governance write-up, joint paper drafting.

**Hard checkpoint:**  
If labs cannot agree on UQ representation by end of Month 1, **stop or narrow scope**.

---

## 4. Resources

**Compute.**  
- Distributed across labs; no single lab needs extreme scale.

**Tools.**  
- Shared evaluation codebase (central authority)  
- Standardized data containers  
- Version-controlled experiment configs

**Datasets.**  
- Public GCM ensembles + observational reference datasets.

**Evidence note.**  
There is no universally accepted benchmark for UQ in climate downscaling. This collaboration’s main contribution is *procedural rigor*, not a new model. If challenged, emphasize reproducibility and governance value.

---

## 5. Risks and mitigations

| Risk | Impact | Mitigation |
|---|---|---|
| Metric disagreement | Deadlock | Pre-register metrics |
| Method cherry-picking | Bias | Fixed submissions deadline |
| Uneven compute | Resentment | Normalize by method class |
| Authorship conflict | Delay | Agree order early |

---

## 6. Stretch ideas / follow-up directions

- Decision-relevant uncertainty metrics (impact models)  
- Coupling downscaling UQ with adaptation planning  
- Living benchmark with annual updates

---

**Bottom line:**  
This succeeds only if you treat **coordination and governance as the main experiment**.

**Optional next step:**  
Draft a 2‑page **consortium protocol** (data, metrics, UQ definitions, authorship) and require signed agreement before any training begins.