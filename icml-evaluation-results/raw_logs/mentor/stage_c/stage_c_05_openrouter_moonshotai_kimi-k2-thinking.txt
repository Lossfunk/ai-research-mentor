<thinking>
User is a beginner in coding but wants research. Need to be honest: lack of Python limits training models, but research is still possible. Provide a concrete, credible plan that does not secretly assume coding skills. Lead with assessment and redirect. </thinking>

**Blunt assessment:**  
Not knowing Python blocks *model-building* research, but it does **not** block *AI research*. Many publishable papers rely on **experimental design, analysis, theory, human studies, or qualitative evaluation**. What will fail is pretending you can do systems or training-heavy ML. The plan below is designed to be executable with **no Python** (or minimal copy‑paste use at most).

---

## 1. Problem framing and goals

**Problem.**  
Modern AI systems are deployed widely, but **we poorly understand their behavior, failure modes, and human interaction effects**. These gaps are not compute- or code-limited; they are *thinking-limited*.

**Goal.**  
Produce a publishable AI paper by studying **model behavior, evaluation flaws, or human–AI interaction**, using existing tools and models.

**Concrete framing.**  
Systematically analyze how state-of-the-art AI systems behave under controlled conditions and show that:
- current evaluation is misleading, or  
- humans misuse or misunderstand AI outputs, or  
- models exhibit systematic reasoning or alignment failures.

---

## 2. Experiments

### Experiment 1: Behavioral probing of large language models (no coding)

**Hypothesis.**  
LLMs exhibit inconsistent or brittle reasoning that standard benchmarks fail to detect.

**Setup.**  
- Use ChatGPT / Claude / Gemini manually or via spreadsheets.  
- Design structured prompt variations (rephrasing, ordering, context changes).  
- Record outputs in tables.

**Baselines.**  
- Original prompt format  
- Human answers (if applicable)

**Evaluation metrics.**  
Consistency rate, contradiction frequency, qualitative error taxonomy.

**Expected outcome.**  
Clear, repeatable failure patterns invisible to standard accuracy metrics.

---

### Experiment 2: Human–AI interaction study

**Hypothesis.**  
AI assistance changes human decision-making in harmful or overconfident ways.

**Setup.**  
- Recruit participants (students, online platforms).  
- Give tasks with and without AI assistance.  
- Measure decision quality and confidence.

**Baselines.**  
- Humans without AI  
- AI-only outputs

**Evaluation metrics.**  
Accuracy, overreliance rate, confidence–accuracy mismatch.

**Expected outcome.**  
Evidence that AI harms or helps depending on task structure.

---

### Experiment 3: Evaluation critique / benchmark audit

**Hypothesis.**  
A popular AI benchmark overstates progress due to artifacts or poorly defined success.

**Setup.**  
- Analyze benchmark questions manually.  
- Categorize question types, ambiguity, shortcuts.  
- Compare model performance across categories.

**Baselines.**  
- Reported benchmark scores  
- Simple heuristics

**Evaluation metrics.**  
Performance variance, artifact dependence.

**Expected outcome.**  
Demonstration that reported gains do not reflect real capability improvements.

---

## 3. Timeline (6 months)

**Month 1:**  
Read target papers; choose one model family or benchmark.

**Month 2:**  
Design prompts, tasks, or human-study protocol.

**Month 3:**  
Collect model outputs or human responses.

**Month 4:**  
Analyze results using tables, charts, qualitative coding.

**Month 5:**  
Write paper; focus on clarity and rigor.

**Month 6:**  
Revise, get feedback, submit.

---

## 4. Resources

**Compute.**  
None beyond a laptop.

**Tools.**  
- Spreadsheets (Excel / Google Sheets)  
- Survey tools (Qualtrics, Google Forms)  
- Diagram and visualization tools

**Models / Data.**  
- Public LLM interfaces  
- Public benchmarks (questions only)

---

## 5. Risks and mitigations

| Risk | Impact | Mitigation |
|---|---|---|
| Seen as “non-technical” | Reviewer bias | Anchor claims to concrete evidence |
| Subjectivity | Weak credibility | Predefine categories and criteria |
| Overgeneralization | Rejection | Scope claims tightly |
| API changes | Reproducibility | Log prompts and dates carefully |

---

## 6. Stretch ideas / follow-up directions

- Collaborate with a programmer for replication  
- Formalize findings into theory  
- Design new evaluation benchmarks

---

**Reality check:**  
Some of the **most influential AI papers** are written by people who never trained a model. They asked better questions.

**Optional next step:**  
Pick **one AI system you already use** and try to systematically break it—on paper, not in code.