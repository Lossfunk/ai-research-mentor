<thinking>
Experienced user. Need an ethics/fairness audit plan that preserves comparability. Key risks: label contamination and post-hoc fishing. I should be blunt about limits and uncertainty. 150–300 words.
</thinking>

**Blunt assessment:** Fairness audits on the *same datasets* are defensible only if you treat them as **overlay analyses**, not new tasks. The fastest way to invalidate the original results is to touch labels, splits, or training. Don’t.

---

## How to add audits without contaminating labels
Keep the original dataset immutable. Add **side-channel annotations** stored separately and never used in training or model selection.

Use one of these, in order of defensibility:
- **Metadata-derived groups** already present but unused (e.g., environment attributes, episode difficulty bins).
- **Post-hoc annotations** generated by a frozen external annotator (rule-based or pretrained), applied *after* model training.
- **Weak group proxies** (e.g., stratifying by input statistics) only if you explicitly frame them as exploratory.

Crucially: annotations are attached at evaluation time only, and metrics are computed by joining predictions with the side-channel table.

---

## What to audit (keep it narrow)
Pick **one fairness lens** aligned with the task. For control/decision tasks, audit **performance disparity** (return, success rate) across groups at matched rollout horizons. Do not invent multiple fairness notions; reviewers will see that as fishing.

---

## Uncertainty and reporting
Fairness gaps are noisy. Treat them that way.

- Use **paired, group-conditional estimates** across identical seeds.
- Report **confidence intervals via bootstrap** over episodes, not just point gaps.
- Explicitly state when CIs overlap zero; “no evidence of disparity” is not “fair.”

Include a short limitations paragraph: proxy groups, annotation noise, and lack of causal claims. That honesty increases credibility.

---

## Ethical guardrails reviewers expect
- No retraining or hyperparameter tuning using group metrics.
- No threshold adjustment per group.
- Clear separation: *accuracy results unchanged; fairness is an overlay analysis*.

If you tell me the domain (vision, control, language), I can suggest a single, defensible group definition that won’t get desk-rejected.