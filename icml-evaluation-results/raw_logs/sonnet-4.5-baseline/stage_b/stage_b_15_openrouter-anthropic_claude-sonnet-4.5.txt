## Short Answer: **NOT overdone—still HIGH value and ongoing need. But you need to carefully scope to find your unique contribution. This is important, timely work with clear impact.**

Let me help you assess novelty and scope this responsibly:

---

## Novelty Assessment:

### **What Exists:**

#### **1. Algorithmic Auditing (General)**

**Established:**
- Fairness audits of ML systems
- Bias detection methodologies
- Algorithmic accountability frameworks
- **Status: MATURE FIELD with established methods**

#### **2. AI Hiring/Recruitment Auditing**

**Active:**
- Academic research on hiring algorithm bias
- Some commercial auditing services (e.g., O'Neil Risk Consulting)
- Regulatory attention (NYC Local Law 144, EU AI Act)
- High-profile cases (HireVue, Amazon, etc.)
- **Status: ACTIVE but NOT saturated**

#### **3. Specific Audit Types**

**Existing work:**
- Resume screening bias (race, gender, age)
- Video interview analysis bias
- Personality/psychometric test validity
- Salary prediction fairness
- **Status: Growing literature, but gaps remain**

#### **4. Regulatory/Legal Frameworks**

**Emerging:**
- NYC LL144 (bias audits required)
- EU AI Act (high-risk systems)
- EEOC guidance on AI hiring
- State-level regulations emerging
- **Status: RAPIDLY EVOLVING**

### **What's NOT Well-Covered:**

✅ **Audits of specific platforms** (most research uses proprietary/simulated data)
✅ **Longitudinal audits** (how bias changes over time)
✅ **Intersectional bias** (race × gender × age, etc.)
✅ **Real-world deployment context** (not just lab studies)
✅ **Vendor accountability** (holding companies responsible)
✅ **Audit methodology validation** (are current audits effective?)
✅ **Transparency and interpretability** (understanding WHY bias exists)
✅ **Remediation effectiveness** (do fixes actually work?)

**Novelty Verdict: MODERATE overall, but HIGH for specific angles**

---

## Why This Is Still Valuable (Not Overdone):

### **✅ Ongoing Need:**

#### **1. Rapid Platform Evolution**

**The landscape changes constantly:**
- New platforms emerge (Paradox, Pymetrics, HireVue, etc.)
- Existing platforms update algorithms
- New features added (video analysis, gamification)
- **Previous audits become outdated quickly**

**Your contribution:**
- Current state assessment
- Tracking changes over time
- Identifying new risks

---

#### **2. Regulatory Pressure Increasing**

**New requirements:**
- NYC Local Law 144 (effective 2023)
  - Requires annual bias audits
  - Specifies methodology (somewhat)
  - Penalties for non-compliance

- EU AI Act (2024-2026 implementation)
  - High-risk AI systems (including hiring)
  - Conformity assessments required
  - Transparency obligations

- Other jurisdictions following

**Your contribution:**
- Help companies comply
- Evaluate if regulations are effective
- Inform policy development
- **Practical, timely value**

---

#### **3. Gap Between Research and Practice**

**The problem:**
- Academic papers often use simulated data
- Limited access to real platforms
- Findings may not generalize
- Practitioners need actionable guidance

**Your contribution:**
- Real-world audits
- Practical recommendations
- Bridge research-practice gap

---

#### **4. Methodological Challenges Remain**

**Open questions:**
- What's the "right" way to audit?
- How to handle intersectionality?
- What metrics matter most?
- How to validate audit findings?
- **Room for methodological innovation**

---

#### **5. Stakeholder Diversity**

**Different audiences need different things:**
- **Job seekers:** Want to know if systems are fair
- **Employers:** Need to ensure compliance, avoid liability
- **Regulators:** Need to enforce laws
- **Vendors:** Need to improve products
- **Researchers:** Need data and methods

**Your contribution depends on who you serve**

---

## How to Check Novelty Responsibly:

### **Step 1: Systematic Literature Review**

#### **Search Strategy:**

**Academic databases:**
- Google Scholar, ACM Digital Library, IEEE Xplore
- arXiv (cs.CY, cs.AI, cs.LG)
- SSRN (for legal/policy papers)

**Search terms:**
```
"algorithmic audit" + hiring
"AI recruitment" + bias
"automated hiring" + fairness
"resume screening" + discrimination
"video interview" + bias
"hiring algorithm" + audit
```

**Also check:**
- FAccT (ACM Conference on Fairness, Accountability, and Transparency)
- AIES (AI, Ethics, and Society)
- CHI (Human-Computer Interaction)
- Law reviews (employment discrimination)

---

#### **What to Look For:**

**For each relevant paper, note:**

1. **What platform/system?** (specific vendor or general?)
2. **What bias type?** (race, gender, age, disability, intersectional?)
3. **What methodology?** (correspondence study, scraping, API access?)
4. **What data?** (real vs. synthetic, scale, demographics)
5. **What findings?** (bias detected? magnitude? patterns?)
6. **What limitations?** (gaps you could fill)

**Create a spreadsheet:**
| Paper | Year | Platform | Bias Type | Method | Data | Findings | Gaps |
|-------|------|----------|-----------|--------|------|----------|------|
| ... | ... | ... | ... | ... | ... | ... | ... |

---

#### **Recent Key Papers (Examples to Start With):**

**General auditing:**
- Raji et al. (2020) "Closing the AI Accountability Gap"
- Metaxa et al. (2021) "Auditing Algorithms: Research Methods for Detecting Discrimination"

**Hiring specific:**
- Raghavan et al. (2020) "Mitigating Bias in Algorithmic Hiring"
- Köchling & Wehner (2020) "Discriminated by an algorithm: a systematic review of discrimination and fairness by algorithmic decision-making"
- Ajunwa (2020) "The Paradox of Automation as Anti-Bias Intervention"

**Specific platforms:**
- Various audits of HireVue, Pymetrics, etc. (search by platform name)

**Regulatory:**
- NYC Bias Audit Law analyses
- EU AI Act commentaries

---

### **Step 2: Gray Literature Review**

**Non-academic sources matter here:**

#### **Industry Reports:**
- Vendor white papers (with skepticism)
- Consulting firm reports (Gartner, Forrester)
- Advocacy org reports (ACLU, AI Now, etc.)

#### **News Coverage:**
- Major investigations (e.g., Reuters, ProPublica)
- Case studies of bias incidents
- Company responses

#### **Regulatory Documents:**
- EEOC guidance
- FTC statements
- State attorney general investigations
- Court cases

#### **Company Disclosures:**
- Transparency reports (if any)
- Audit results (NYC LL144 requires publication)
- Terms of service, privacy policies

---

### **Step 3: Identify Your Unique Angle**

**Based on your literature review, find gaps:**

#### **Dimension 1: Platform Specificity**

**Options:**
- **Broad survey:** Audit multiple platforms comparatively
  - Novelty: MODERATE (some exist, but landscape changes)
  - Value: HIGH (helps employers choose, regulators understand market)

- **Deep dive on one platform:** Comprehensive audit of single system
  - Novelty: HIGH if platform not previously audited
  - Value: HIGH for accountability, but limited generalizability

- **Platform type:** Focus on one category (video interviews, resume screeners, etc.)
  - Novelty: MODERATE-HIGH depending on type
  - Value: HIGH for understanding specific technology

---

#### **Dimension 2: Bias Type**

**Options:**
- **Single protected class:** Race, gender, age, disability
  - Novelty: LOW-MODERATE (most studied)
  - Value: Still important, especially if new platform/method

- **Intersectional:** Race × gender, age × disability, etc.
  - Novelty: HIGH (understudied)
  - Value: VERY HIGH (reflects reality better)
  - Challenge: Need larger datasets, more complex analysis

- **Emerging categories:** Neurodiversity, caregiving status, accent/language
  - Novelty: HIGH
  - Value: HIGH (understudied populations)

- **Proxy discrimination:** Zip code, name, school, etc.
  - Novelty: MODERATE
  - Value: HIGH (indirect discrimination matters)

---

#### **Dimension 3: Methodology**

**Options:**

**A. Correspondence/Audit Studies**
- Create fake resumes varying protected characteristics
- Submit to platforms, measure outcomes
- **Novelty: LOW (established method)**
- **Value: HIGH (gold standard, causal inference)**
- **Challenge: Labor-intensive, may violate ToS**

**B. Scraping/Observational**
- Analyze public data from platforms
- Correlate outcomes with demographics
- **Novelty: MODERATE**
- **Value: MODERATE (correlational, not causal)**
- **Challenge: Data access, privacy concerns**

**C. Algorithm Reverse-Engineering**
- Black-box testing to infer decision rules
- Adversarial testing
- **Novelty: MODERATE-HIGH**
- **Value: HIGH (reveals how systems work)**
- **Challenge: Technical difficulty, may violate ToS**

**D. Participatory Auditing**
- Work WITH job seekers who experienced the platform
- Crowdsourced data collection
- **Novelty: HIGH**
- **Value: VERY HIGH (centers affected people)**
- **Challenge: Coordination, data quality**

**E. Vendor Collaboration**
- Work with company to audit their system
- Access to real data, code
- **Novelty: MODERATE**
- **Value: HIGH (most accurate)**
- **Challenge: Rare, potential conflicts of interest**

**F. Regulatory Compliance Audit**
- Evaluate if platforms meet NYC LL144 or EU AI Act
- Check if published audits are valid
- **Novelty: HIGH (regulations are new)**
- **Value: VERY HIGH (timely, practical)**

---

#### **Dimension 4: Temporal Aspect**

**Options:**
- **Snapshot:** Audit at one point in time
  - Novelty: MODERATE
  - Value: MODERATE

- **Longitudinal:** Audit same platform over time
  - Novelty: HIGH (rare)
  - Value: VERY HIGH (track changes, hold accountable)
  - Challenge: Resource-intensive

- **Before/after intervention:** Audit, recommend changes, re-audit
  - Novelty: HIGH
  - Value: VERY HIGH (shows if fixes work)
  - Challenge: Need vendor cooperation

---

#### **Dimension 5: Stakeholder Focus**

**Who is your primary audience?**

**A. Job Seekers**
- Consumer-facing audit results
- Guidance on navigating biased systems
- **Value: HIGH (empowerment)**

**B. Employers**
- Due diligence guidance
- Vendor selection criteria
- **Value: HIGH (risk mitigation)**

**C. Regulators**
- Inform policy
- Enforcement support
- **Value: VERY HIGH (systemic change)**

**D. Vendors**
- Improvement recommendations
- Best practices
- **Value: HIGH if they engage**

**E. Researchers**
- Methodological contributions
- Public datasets (if possible)
- **Value: HIGH (advance field)**

---

### **Step 4: Assess Feasibility and Ethics**

#### **Feasibility Questions:**

**Data access:**
- Can you access the platform? (Public? Subscription? Partnership?)
- Can you collect enough data for statistical power?
- Legal to scrape/test?

**Resources:**
- Time: How long will this take?
- Money: Any costs? (Subscriptions, workers for correspondence study, compute)
- Expertise: Do you need collaborators? (Legal, HR, domain experts)

**Technical:**
- Do you have the skills? (Web scraping, ML, statistics)
- Can you reverse-engineer if needed?

**Risk:**
- Could this violate Terms of Service?
- Could vendors sue?
- Could you face retaliation?

---

#### **Ethical Questions (CRITICAL):**

**1. Privacy**
- Are you using real people's data? (Need consent/IRB)
- Could your audit reveal private information?
- How will you protect participant privacy?

**2. Harm**
- Could your findings harm job seekers? (Stigmatization, vendor retaliation)
- Could you inadvertently help vendors game audits?
- Could you expose vulnerable people?

**3. Consent**
- If using real job seekers, do they consent?
- If creating fake profiles, are you deceiving platforms? (May be justified, but consider)

**4. Responsible Disclosure**
- If you find serious bias, what's your plan?
- Notify vendor first? Go public? Notify regulators?
- Timing and process matter

**5. Conflicts of Interest**
- Are you funded by vendors? Competitors? Advocacy groups?
- How will you maintain independence?
- Disclose all affiliations

---

### **Step 5: Scope Your Project**

**Based on Steps 1-4, define:**

#### **Research Questions (2-3 specific ones):**

**Example RQs:**

❌ **Too broad:** "Are AI hiring platforms biased?"

✅ **Better:**
1. "Do resume screening platforms exhibit racial bias in candidate ranking, and does this vary by job sector?"
2. "How do video interview analysis tools perform across different accents and speech patterns?"
3. "Are bias audits published under NYC Local Law 144 methodologically sound and comparable?"

---

#### **Scope Parameters:**

**Platform(s):**
- Which specific platform(s)?
- Why these? (Market share, prior concerns, access, novelty)

**Bias type(s):**
- Which protected characteristics?
- Intersectional or single-axis?

**Methodology:**
- Which audit method(s)?
- Sample size and power analysis
- Timeline

**Geographic/sectoral focus:**
- Which labor markets? (Tech, healthcare, retail?)
- Which countries/regions? (US, EU, global?)

**Deliverables:**
- Academic paper? (Which venue?)
- Public report? (For whom?)
- Policy brief? (For which policymakers?)
- Tool/dataset? (Open source?)

---

## Potential Novel Angles (Examples):

### **Option 1: Regulatory Compliance Meta-Audit**

**The idea:**
- NYC LL144 requires companies to publish bias audits
- Collect and analyze these published audits
- Evaluate: Are they valid? Comparable? Sufficient?

**Novelty: HIGH (regulations just took effect)**

**Research questions:**
- Do published audits meet legal requirements?
- Are methodologies sound?
- Can we compare across vendors?
- What's hidden in the fine print?

**Value:**
- Inform regulatory enforcement
- Hold vendors accountable
- Improve audit standards

**Feasibility: HIGH (public data)**

---

### **Option 2: Intersectional Bias Deep Dive**

**The idea:**
- Most audits look at single characteristics (race OR gender)
- Audit intersectional bias (race × gender × age)
- Focus on multiply-marginalized groups

**Novelty: HIGH (understudied)**

**Research questions:**
- Are Black women treated differently than Black men or white women?
- Do biases compound or interact?
- Which intersections face greatest discrimination?

**Value:**
- More accurate picture of discrimination
- Centers most vulnerable
- Aligns with legal theory (intersectionality)

**Feasibility: MODERATE (need larger sample sizes)**

---

### **Option 3: Longitudinal Accountability Study**

**The idea:**
- Audit same platform(s) every 6-12 months
- Track changes over time
- Hold vendors accountable for improvements (or lack thereof)

**Novelty: HIGH (rare longitudinal audits)**

**Research questions:**
- Do platforms improve over time?
- Do regulatory interventions work?
- How do vendors respond to public audits?

**Value:**
- Ongoing accountability
- Understand dynamics of bias mitigation
- Inform policy effectiveness

**Feasibility: MODERATE (resource-intensive, multi-year)**

---

### **Option 4: Participatory Audit with Job Seekers**

**The idea:**
- Partner with job seeker organizations
- Crowdsource audit data from real users
- Center lived experiences

**Novel