# **ARTIFACT RELEASE NOTES & REVIEWER RESPONSE**
## **Interpretable Medical Imaging Paper**

---

## **ğŸ“¦ PART 1: ARTIFACT RELEASE NOTES**

### **Version 1.0.0 - Initial Public Release**
**Release Date**: [Date - align with camera-ready deadline]  
**Paper**: "Interpretable Medical Imaging via [Your Method]" ([Venue] 2024)  
**Authors**: [Names after de-anonymization]  
**License**: MIT (Code), CC-BY-NC 4.0 (Models), Restricted (Data - see below)

---

### **ğŸ¯ Overview**

This release accompanies our [Venue] 2024 paper on interpretable medical imaging. We provide:

âœ… **Complete source code** for training and evaluation  
âœ… **Pre-trained models** on de-identified medical datasets  
âœ… **Interpretability visualization tools**  
âœ… **Evaluation protocols** for clinical validation  
âš ï¸ **Limited data release** due to patient privacy regulations (see Data Availability)

**âš ï¸ IMPORTANT DISCLAIMERS**:
- **NOT FOR CLINICAL USE**: This is research software not approved by FDA/CE/regulatory bodies
- **NO DIAGNOSTIC CLAIMS**: Not intended for medical diagnosis or treatment decisions
- **RESEARCH ONLY**: For academic and research purposes only
- **HIPAA/GDPR COMPLIANCE**: Users responsible for compliance when using with patient data

---

### **ğŸ“‚ Repository Structure**

```
interpretable-medical-imaging/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ LICENSE                            # MIT License (code)
â”œâ”€â”€ MODEL_LICENSE                      # CC-BY-NC 4.0 (models)
â”œâ”€â”€ CITATION.bib                       # Citation information
â”œâ”€â”€ ETHICS_STATEMENT.md                # Ethics and compliance information
â”œâ”€â”€ DATA_USAGE_AGREEMENT.md            # Data usage terms
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/                        # Model architectures
â”‚   â”œâ”€â”€ interpretability/              # Interpretability methods
â”‚   â”œâ”€â”€ evaluation/                    # Clinical evaluation metrics
â”‚   â””â”€â”€ utils/                         # Helper functions
â”‚
â”œâ”€â”€ configs/                           # Experiment configurations
â”œâ”€â”€ scripts/                           # Training/evaluation scripts
â”œâ”€â”€ notebooks/                         # Tutorial notebooks
â”œâ”€â”€ tests/                             # Unit tests
â”‚
â”œâ”€â”€ docs/                              # Documentation
â”‚   â”œâ”€â”€ installation.md
â”‚   â”œâ”€â”€ quickstart.md
â”‚   â”œâ”€â”€ clinical_validation.md
â”‚   â”œâ”€â”€ interpretability_guide.md
â”‚   â””â”€â”€ patient_privacy.md
â”‚
â””â”€â”€ examples/                          # Usage examples
    â”œâ”€â”€ train_on_public_data.py
    â”œâ”€â”€ evaluate_interpretability.py
    â””â”€â”€ clinical_evaluation_protocol.py
```

---

### **ğŸš€ Installation**

#### **Requirements**
- Python â‰¥ 3.8
- PyTorch â‰¥ 2.0
- CUDA â‰¥ 11.8 (for GPU support)
- Medical imaging libraries: SimpleITK, MONAI, pydicom

#### **Install from PyPI**
```bash
pip install interpretable-medimg
```

#### **Install from Source**
```bash
git clone https://github.com/your-org/interpretable-medical-imaging.git
cd interpretable-medical-imaging
pip install -e .
```

#### **Using Docker (Recommended for Reproducibility)**
```bash
docker pull your-org/interpretable-medimg:v1.0.0
docker run -it --gpus all -v /path/to/data:/data your-org/interpretable-medimg:v1.0.0
```

---

### **ğŸ”’ Data Availability & Patient Privacy Compliance**

#### **âš ï¸ CRITICAL: Patient Data Protection**

**We CANNOT publicly release the following due to patient privacy regulations:**
- âŒ Raw medical images from our institutional datasets
- âŒ Patient metadata (demographics, clinical history)
- âŒ Any data containing Protected Health Information (PHI)
- âŒ De-identified data from partner hospitals (under DUA restrictions)

**What We CAN Provide:**

âœ… **Public Dataset Access Instructions**
```markdown
We use the following publicly available datasets:

1. **MIMIC-CXR** (Chest X-rays)
   - Source: https://physionet.org/content/mimic-cxr/2.0.0/
   - Access: Requires PhysioNet credentialing
   - License: PhysioNet Credentialed Health Data License
   - Instructions: See docs/data/mimic_cxr_setup.md

2. **ChestX-ray14** (NIH Chest X-rays)
   - Source: https://nihcc.app.box.com/v/ChestXray-NIHCC
   - Access: Public download
   - License: CC0 1.0 Universal
   - Instructions: See docs/data/chestxray14_setup.md

3. **RSNA Pneumonia Detection Challenge**
   - Source: https://www.kaggle.com/c/rsna-pneumonia-detection-challenge
   - Access: Kaggle account required
   - License: Dataset-specific license
   - Instructions: See docs/data/rsna_setup.md

4. **CheXpert** (Stanford Chest X-rays)
   - Source: https://stanfordmlgroup.github.io/competitions/chexpert/
   - Access: Requires registration and data use agreement
   - License: Research use only
   - Instructions: See docs/data/chexpert_setup.md
```

âœ… **Synthetic/Simulated Data**
```markdown
For testing and demonstration, we provide:
- 100 synthetic chest X-ray images (data/synthetic/)
- Simulated pathology annotations
- Example DICOM files with fabricated metadata
- NOT real patient data - for code testing only
```

âœ… **Data Preprocessing Scripts**
```bash
# Download and preprocess public datasets
bash scripts/download_public_datasets.sh

# Preprocess to our format (maintains de-identification)
python scripts/preprocess_mimic_cxr.py --output data/processed/
python scripts/preprocess_chexpert.py --output data/processed/
```

âœ… **Data Split Indices**
```markdown
We provide exact train/val/test split indices for reproducibility:
- data/splits/mimic_cxr_splits.json
- data/splits/chexpert_splits.json
- data/splits/rsna_splits.json

These contain only study IDs (no PHI), allowing exact reproduction
of our experiments on the same public datasets.
```

âœ… **Extracted Features (De-identified)**
```markdown
For researchers without access to raw images, we provide:
- Pre-extracted image features (embeddings) from our model
- No reverse-engineering to original images possible
- Available at: https://zenodo.org/record/XXXXXXX
- License: CC-BY-NC 4.0
- Size: ~2GB compressed
```

#### **Institutional Data Access**

For researchers interested in our institutional datasets:

```markdown
**Institutional Review Board (IRB) Information:**
- Institution: [University/Hospital Name]
- IRB Protocol: #[Redacted for review - will be added in camera-ready]
- IRB Approval Date: [Date]
- Data Use Agreement: Required

**Access Process:**
1. Submit IRB application at your institution
2. Contact corresponding author with IRB approval
3. Sign Data Use Agreement (DUA)
4. Receive de-identified data via secure transfer
5. Comply with HIPAA/GDPR regulations

**Restrictions:**
- Research use only (no commercial use)
- No re-distribution
- No re-identification attempts
- Destruction of data after research completion
- Annual compliance reports required

Contact: [corresponding-author]@[institution].edu
```

---

### **ğŸ¤– Pre-trained Models**

#### **Available Models**

| Model | Training Data | Task | AUROC | License | Download |
|-------|---------------|------|-------|---------|----------|
| InterpretMed-ResNet50 | MIMIC-CXR | 14-class chest X-ray | 0.847 | CC-BY-NC 4.0 | [HF Hub](link) |
| InterpretMed-ViT-B | CheXpert | 5-class chest X-ray | 0.891 | CC-BY-NC 4.0 | [HF Hub](link) |
| InterpretMed-DenseNet121 | ChestX-ray14 | 14-class chest X-ray | 0.823 | CC-BY-NC 4.0 | [HF Hub](link) |

#### **Model Cards**

Each model includes a detailed Model Card (following Mitchell et al., 2019):

```markdown
# Model Card: InterpretMed-ResNet50

## Model Details
- **Developed by**: [Authors/Institution]
- **Model type**: ResNet50 with attention mechanisms
- **Training data**: MIMIC-CXR (227,835 chest X-rays)
- **License**: CC-BY-NC 4.0 (Non-commercial use only)

## Intended Use
- **Primary use**: Research on interpretable medical imaging
- **Intended users**: Medical imaging researchers, AI researchers
- **Out-of-scope**: Clinical diagnosis, treatment decisions, deployment in healthcare

## Training Data
- **Dataset**: MIMIC-CXR v2.0.0
- **Demographics**: 
  - Age: 18-90+ years (median: 62)
  - Sex: 52% male, 48% female
  - Race/Ethnicity: Diverse (see MIMIC-CXR documentation)
- **Data collection**: Beth Israel Deaconess Medical Center (2011-2016)
- **Preprocessing**: Resized to 224Ã—224, normalized

## Evaluation Data
- **Test set**: 5,000 held-out MIMIC-CXR studies
- **External validation**: CheXpert test set (500 studies)

## Performance Metrics
- **Overall AUROC**: 0.847 (95% CI: 0.841-0.853)
- **Per-class AUROC**: See paper Table 2
- **Subgroup performance**: See Fairness Evaluation below

## Fairness Evaluation
We evaluate performance across demographic subgroups:

| Subgroup | AUROC | 95% CI | Î” from Overall |
|----------|-------|--------|----------------|
| Male | 0.851 | 0.843-0.859 | +0.004 |
| Female | 0.843 | 0.835-0.851 | -0.004 |
| Age 18-50 | 0.839 | 0.828-0.850 | -0.008 |
| Age 51-70 | 0.852 | 0.844-0.860 | +0.005 |
| Age 71+ | 0.845 | 0.837-0.853 | -0.002 |

**Fairness Analysis**: Performance differences across subgroups are small 
(<1% AUROC) and not statistically significant (p > 0.05, DeLong test).

## Limitations
- **Training distribution**: Limited to chest X-rays from single institution
- **Generalization**: May not generalize to different imaging equipment, 
  protocols, or patient populations
- **Rare conditions**: Poor performance on rare pathologies (<100 training examples)
- **Image quality**: Assumes standard radiographic quality; may fail on 
  low-quality or unconventional views
- **Temporal drift**: Trained on 2011-2016 data; medical practices may have evolved

## Ethical Considerations
- **Bias risks**: Model trained on data from single institution; may not 
  perform equally across all populations
- **Clinical validation**: NOT clinically validated; requires prospective 
  evaluation before clinical use
- **Interpretability**: Attention maps provided but should be validated by 
  radiologists
- **Overreliance**: Risk of automation bias if clinicians over-rely on model

## Recommendations
- âœ… Use for research and development
- âœ… Validate on your local data before use
- âœ… Involve radiologists in interpretation validation
- âœ… Monitor performance across demographic subgroups
- âŒ Do NOT use for clinical diagnosis without regulatory approval
- âŒ Do NOT use as sole decision-maker
- âŒ Do NOT deploy without prospective clinical validation
```

#### **Model Usage**

```python
from interpretable_medimg import InterpretMedModel
from interpretable_medimg.utils import load_dicom, preprocess_image
import matplotlib.pyplot as plt

# Load pre-trained model
model = InterpretMedModel.from_pretrained("your-org/interpretmed-resnet50")

# Load and preprocess image
image = load_dicom("path/to/chest_xray.dcm")
image = preprocess_image(image)

# Get predictions with interpretability
predictions, attention_map = model.predict_with_interpretation(image)

# Visualize results
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
ax1.imshow(image, cmap='gray')
ax1.set_title("Original Image")
ax2.imshow(image, cmap='gray')
ax2.imshow(attention_map, cmap='jet', alpha=0.5)
ax2.set_title(f"Prediction: {predictions['top_class']} "
              f"(Confidence: {predictions['confidence']:.2f})")
plt.show()

# Get top predictions
for pred in predictions['top_5']:
    print(f"{pred['class']}: {pred['probability']:.3f}")
```

---

### **ğŸ”¬ Reproducibility**

#### **Reproducing Paper Results**

```bash
# Download public datasets (requires credentials)
bash scripts/download_public_datasets.sh

# Preprocess data to our format
python scripts/preprocess_all_datasets.py

# Train models (reproduces Table 1)
bash scripts/reproduce_table1.sh

# Evaluate interpretability (reproduces Figure 3)
bash scripts/reproduce_figure3.sh

# Clinical validation (reproduces Table 3)
python scripts/clinical_validation.py --radiologist-annotations data/radiologist_eval.json

# Expected runtime: ~48 hours on 4Ã— NVIDIA A100 GPUs
# Expected storage: ~500GB for datasets + models
```

#### **Computational Requirements**

```markdown
**Minimum Requirements:**
- GPU: NVIDIA GPU with 16GB+ VRAM (e.g., V100, A100, RTX 4090)
- RAM: 32GB system memory
- Storage: 500GB for datasets and models
- OS: Linux (Ubuntu 20.04+), macOS, Windows with WSL2

**Recommended for Full Reproduction:**
- GPU: 4Ã— NVIDIA A100 (80GB)
- RAM: 128GB system memory
- Storage: 1TB NVMe SSD
- Training time: ~48 hours for all experiments

**Carbon Footprint:**
- Total GPU-hours: ~800 hours
- Estimated CO2: ~320 kg CO2eq (using ML CO2 calculator)
- Cloud cost: ~$2,400 at current pricing
```

---

### **ğŸ“Š Clinical Validation Protocol**

We provide tools for clinical validation by radiologists:

```python
from interpretable_medimg.evaluation import ClinicalValidationProtocol

# Set up validation study
protocol = ClinicalValidationProtocol(
    model=model,
    test_cases=test_dataset,
    num_radiologists=3,
    evaluation_metrics=['accuracy', 'agreement', 'time_to_decision']
)

# Generate evaluation interface
protocol.generate_evaluation_interface(output_dir="clinical_eval/")

# Analyze results
results = protocol.analyze_results("clinical_eval/radiologist_responses.json")
print(f"Inter-rater agreement (Fleiss' Îº): {results['fleiss_kappa']:.3f}")
print(f"Model-radiologist agreement: {results['model_agreement']:.3f}")
```

**Validation Metrics:**
- Diagnostic accuracy vs. ground truth
- Agreement with radiologist consensus
- Time to decision (with vs. without AI assistance)
- Confidence calibration
- False positive/negative analysis
- Interpretability usefulness ratings

---

### **âš–ï¸ Ethics & Compliance**

#### **IRB Approval**

```markdown
**Institutional Review Board (IRB) Status:**
- Institution: [University/Hospital Name]
- Protocol Number: [Will be added in camera-ready