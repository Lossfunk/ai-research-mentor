# **Two-Phase Replication Plan: Hierarchical Transformer**

## **PHASE 1: BASELINE RE-IMPLEMENTATION**

### **1.1 Pre-Implementation Planning**

#### **A. Paper Analysis Checklist**
```python
PAPER_ANALYSIS = {
    'architecture': {
        'components': [
            'Embedding layer specifications',
            'Number of hierarchical levels',
            'Attention mechanism type per level',
            'Pooling/aggregation methods',
            'Position encoding scheme',
            'Normalization layers',
            'Activation functions',
            'Dropout rates and locations',
        ],
        'hyperparameters': [
            'Hidden dimensions per level',
            'Number of attention heads',
            'FFN intermediate size',
            'Layer depths',
        ],
    },
    
    'training': {
        'data': [
            'Exact dataset versions',
            'Train/val/test splits (sizes and seeds)',
            'Preprocessing steps',
            'Tokenization (tokenizer type, vocab size)',
            'Max sequence lengths',
            'Batching strategy',
        ],
        'optimization': [
            'Optimizer (Adam, AdamW, etc.)',
            'Learning rate and schedule',
            'Warmup steps',
            'Weight decay',
            'Gradient clipping',
            'Batch size (effective)',
            'Number of epochs/steps',
            'Mixed precision (fp16/bf16)',
        ],
        'regularization': [
            'Dropout rates',
            'Label smoothing',
            'Data augmentation',
        ],
    },
    
    'evaluation': {
        'protocol': [
            'Metrics (exact formulas)',
            'Evaluation frequency',
            'Early stopping criteria',
            'Number of random seeds',
            'Checkpoint selection (best/last)',
        ],
    },
}
```

#### **B. Implementation Verification Protocol**
```python
class ReplicationVerification:
    """
    Systematic verification of implementation correctness
    """
    
    def __init__(self, paper_results):
        self.paper_results = paper_results
        self.checkpoints = []
        
    def verify_architecture(self, model):
        """
        Verify model architecture matches paper
        """
        checks = {
            'parameter_count': self.check_parameter_count(model),
            'layer_structure': self.check_layer_structure(model),
            'output_shapes': self.check_output_shapes(model),
            'attention_patterns': self.check_attention_patterns(model),
        }
        return checks
    
    def check_parameter_count(self, model):
        """
        Verify total parameters match paper (±1%)
        """
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() 
                              if p.requires_grad)
        
        paper_params = self.paper_results.get('total_parameters')
        
        if paper_params:
            diff_pct = abs(total_params - paper_params) / paper_params * 100
            match = diff_pct < 1.0  # Within 1%
        else:
            match = None
            
        return {
            'total': total_params,
            'trainable': trainable_params,
            'paper_reported': paper_params,
            'match': match,
            'diff_pct': diff_pct if paper_params else None,
        }
    
    def check_layer_structure(self, model):
        """
        Verify layer-by-layer structure
        """
        structure = {}
        for name, module in model.named_modules():
            structure[name] = {
                'type': type(module).__name__,
                'parameters': sum(p.numel() for p in module.parameters()),
            }
        return structure
    
    def check_output_shapes(self, model, dummy_input):
        """
        Verify intermediate and final output shapes
        """
        shapes = {}
        hooks = []
        
        def hook_fn(name):
            def hook(module, input, output):
                if isinstance(output, torch.Tensor):
                    shapes[name] = output.shape
                elif isinstance(output, tuple):
                    shapes[name] = [o.shape for o in output if isinstance(o, torch.Tensor)]
            return hook
        
        # Register hooks
        for name, module in model.named_modules():
            hooks.append(module.register_forward_hook(hook_fn(name)))
        
        # Forward pass
        with torch.no_grad():
            _ = model(dummy_input)
        
        # Remove hooks
        for hook in hooks:
            hook.remove()
            
        return shapes
    
    def verify_gradient_flow(self, model, loss):
        """
        Check for gradient flow issues
        """
        loss.backward()
        
        gradient_stats = {}
        for name, param in model.named_parameters():
            if param.grad is not None:
                gradient_stats[name] = {
                    'mean': param.grad.mean().item(),
                    'std': param.grad.std().item(),
                    'max': param.grad.max().item(),
                    'min': param.grad.min().item(),
                    'has_nan': torch.isnan(param.grad).any().item(),
                    'has_inf': torch.isinf(param.grad).any().item(),
                }
            else:
                gradient_stats[name] = 'NO_GRADIENT'
        
        return gradient_stats
```

---

### **1.2 Staged Implementation**

#### **Stage 1: Core Architecture (Week 1-2)**
```python
class HierarchicalTransformerReplication:
    """
    Systematic re-implementation with verification at each stage
    """
    
    def stage1_basic_components(self):
        """
        Implement and verify basic building blocks
        """
        components = {
            'embedding': self.implement_embedding_layer(),
            'positional_encoding': self.implement_positional_encoding(),
            'attention': self.implement_attention_mechanism(),
            'feedforward': self.implement_feedforward(),
        }
        
        # Unit tests for each component
        for name, component in components.items():
            self.unit_test_component(name, component)
        
        return components
    
    def stage2_hierarchical_structure(self, components):
        """
        Build hierarchical structure from components
        """
        hierarchy = {
            'level_1': self.build_level(components, level=1),
            'level_2': self.build_level(components, level=2),
            'level_3': self.build_level(components, level=3),
            'pooling': self.implement_hierarchical_pooling(),
        }
        
        # Integration tests
        self.test_information_flow(hierarchy)
        
        return hierarchy
    
    def stage3_full_model(self, hierarchy):
        """
        Assemble complete model
        """
        model = self.assemble_model(hierarchy)
        
        # Verification
        self.verify_architecture(model)
        self.verify_forward_pass(model)
        self.verify_backward_pass(model)
        
        return model
```

#### **Stage 2: Training Pipeline (Week 2-3)**
```python
class TrainingPipelineReplication:
    """
    Replicate exact training procedure
    """
    
    def setup_data_pipeline(self):
        """
        Exact data preprocessing and loading
        """
        pipeline = {
            'tokenizer': self.load_exact_tokenizer(),
            'preprocessing': self.implement_preprocessing(),
            'data_splits': self.create_exact_splits(),
            'dataloaders': self.create_dataloaders(),
        }
        
        # Verify data matches paper
        self.verify_data_statistics(pipeline)
        
        return pipeline
    
    def verify_data_statistics(self, pipeline):
        """
        Ensure data matches paper's reported statistics
        """
        stats = {
            'train_size': len(pipeline['data_splits']['train']),
            'val_size': len(pipeline['data_splits']['val']),
            'test_size': len(pipeline['data_splits']['test']),
            'avg_length': self.compute_avg_length(pipeline['data_splits']['train']),
            'vocab_size': len(pipeline['tokenizer']),
        }
        
        # Compare with paper
        paper_stats = self.paper_results['data_statistics']
        for key, value in stats.items():
            paper_value = paper_stats.get(key)
            if paper_value:
                assert abs(value - paper_value) / paper_value < 0.01, \
                    f"{key} mismatch: {value} vs {paper_value}"
        
        return stats
    
    def setup_training_loop(self):
        """
        Exact training configuration
        """
        config = {
            'optimizer': self.create_optimizer(),
            'scheduler': self.create_scheduler(),
            'loss_function': self.create_loss(),
            'gradient_clipping': self.paper_results['gradient_clip_norm'],
            'mixed_precision': self.paper_results['use_amp'],
        }
        
        return config
```

#### **Stage 3: Reproduction Validation (Week 3-4)**
```python
class ReproductionValidation:
    """
    Validate reproduction quality
    """
    
    def run_reproduction_experiments(self, model, data, config):
        """
        Run multiple seeds and compare with paper
        """
        results = []
        
        for seed in [42, 123, 456, 789, 1000]:  # 5 seeds
            print(f"\n=== Running seed {seed} ===")
            
            # Set all random seeds
            self.set_all_seeds(seed)
            
            # Train model
            trained_model, history = self.train_model(
                model=copy.deepcopy(model),
                data=data,
                config=config,
                seed=seed
            )
            
            # Evaluate
            metrics = self.evaluate_model(trained_model, data['test'])
            
            results.append({
                'seed': seed,
                'metrics': metrics,
                'history': history,
            })
        
        # Statistical comparison with paper
        self.compare_with_paper(results)
        
        return results
    
    def compare_with_paper(self, results):
        """
        Statistical comparison with paper's results
        """
        # Extract main metric (e.g., accuracy)
        main_metric = [r['metrics']['accuracy'] for r in results]
        
        paper_mean = self.paper_results['accuracy_mean']
        paper_std = self.paper_results.get('accuracy_std', None)
        
        repro_mean = np.mean(main_metric)
        repro_std = np.std(main_metric)
        
        print("\n=== REPRODUCTION COMPARISON ===")
        print(f"Paper:        {paper_mean:.2f} ± {paper_std:.2f}" if paper_std else f"Paper: {paper_mean:.2f}")
        print(f"Reproduction: {repro_mean:.2f} ± {repro_std:.2f}")
        print(f"Difference:   {abs(repro_mean - paper_mean):.2f}")
        
        # Statistical test
        if paper_std:
            # Two-sample t-test (assuming paper's multiple runs)
            # Null hypothesis: same population mean
            from scipy.stats import ttest_1samp
            t_stat, p_value = ttest_1samp(main_metric, paper_mean)
            print(f"T-test p-value: {p_value:.4f}")
            
            # Equivalence test (TOST)
            equivalence_bound = 1.0  # ±1% considered equivalent
            is_equivalent = abs(repro_mean - paper_mean) < equivalence_bound
            print(f"Equivalent (±{equivalence_bound}%): {is_equivalent}")
        
        # Success criteria
        success = abs(repro_mean - paper_mean) < 2.0  # Within 2%
        
        if success:
            print("\n✓ REPRODUCTION SUCCESSFUL")
        else:
            print("\n✗ REPRODUCTION FAILED - Investigate discrepancies")
            self.debug_discrepancies(results)
        
        return {
            'success': success,
            'repro_mean': repro_mean,
            'repro_std': repro_std,
            'paper_mean': paper_mean,
            'difference': abs(repro_mean - paper_mean),
        }
    
    def debug_discrepancies(self, results):
        """
        Systematic debugging of reproduction failures
        """
        debug_steps = [
            "1. Verify data preprocessing matches exactly",
            "2. Check tokenizer vocabulary and special tokens",
            "3. Verify model architecture parameter-by-parameter",
            "4. Compare learning rate schedules",
            "5. Check for missing regularization",
            "6. Verify evaluation protocol (e.g., dropout during eval)",
            "7. Check for data leakage or contamination",
            "8. Compare gradient computation",
            "9. Verify loss function implementation",
            "10. Check for hardware-specific differences (cuDNN)",
        ]
        
        print("\n=== DEBUGGING CHECKLIST ===")
        for step in debug_steps:
            print(step)
```

---

### **1.3 Replication Documentation**

#### **Replication Report Template**
```python
REPLICATION_REPORT = {
    'metadata': {
        'paper_title': '',
        'paper_authors': '',
        'paper_venue': '',
        'replication_date': '',
        'replicator': '',
        'code_repository': '',
    },
    
    'implementation': {
        'source': 'from_scratch | adapted_from_official | adapted_from_third_party',
        'framework': 'pytorch | tensorflow | jax',
        'framework_version': '',
        'dependencies': [],
        'total_lines_of_code': 0,
        'implementation_time_hours': 0,
    },
    
    'reproduction_results': {
        'target_metric': 'accuracy',
        'paper_reported': {
            'mean': 0.0,
            'std': 0.0,
            'n_seeds': 0,
        },
        'reproduced': {
            'mean': 0.0,
            'std': 0.0,
            'n_seeds': 5,
            'seeds': [42, 123, 456, 789, 1000],
        },
        'difference': 0.0,
        'within_2_percent': False,
        'statistically_equivalent': False,
    },
    
    'deviations_from_paper': [
        # Document any necessary deviations
        {
            'aspect': 'optimizer',
            'paper': 'Adam',
            'implementation': 'AdamW',
            'reason': 'Paper unclear, AdamW is standard',
            'impact': 'minimal',
        },
    ],
    
    'ambiguities_resolved': [
        # Document unclear aspects and how resolved
        {
            'aspect': 'learning_rate_schedule',
            'ambiguity': 'Warmup steps not specified',
            'resolution': 'Used 10% of total steps based on common practice',
            'confidence': 'medium',
        },
    ],
}
```

---

## **PHASE 2: LOW-PARAMETER ALTERNATIVE**

### **2.1 Alternative Design: Grouped Pooling**

#### **A. Architectural Specification**
```python
class GroupedPoolingTransformer(nn.Module):
    """
    Low-parameter alternative using grouped pooling instead of hierarchy
    
    Key Idea: Replace multi-level hierarchical attention with:
    1. Standard transformer encoder
    2. Grouped pooling to aggregate information
    3. Lightweight aggregation head
    
    Parameter Reduction:
    - Eliminate separate encoders per level
    - Share parameters across all positions
    - Simpler pooling mechanism
    """
    
    def __init__(self, config):
        super().__init__()
        
        self.config = config
        