**Web search unavailable.** The following plan draws on established domain knowledge of human-centered AI evaluation, legal AI systems, and research ethics. For publication-grade rigor, you should conduct systematic literature reviews via Google Scholar, arXiv (cs.CL, cs.HC, cs.AI, cs.CY), and legal informatics venues (ICAIL, JURIX, Artificial Intelligence and Law journal) focusing on: (1) LLM evaluation benchmarks for legal reasoning (LegalBench, LexGLUE, LEXTREME, CaseHOLD), (2) human-AI collaboration studies in professional domains, (3) IRB protocols for AI system evaluation with legal professionals, (4) trust calibration and reliance in AI-assisted decision-making, and (5) legal ethics considerations for AI tools.

---

## 1. Problem Framing and Goals

**Problem Statement:**  
AI legal assistants (powered by LLMs and retrieval-augmented generation) promise to democratize legal access and augment attorney productivity, but face critical human-centered challenges:

1. **Over-reliance risks:** Attorneys may uncritically accept AI-generated legal analysis, missing errors (hallucinated case citations, misapplied precedents, jurisdictional mistakes)
2. **Trust calibration:** Users either over-trust (automation bias) or under-trust (dismiss correct outputs), both degrading decision quality
3. **Professional integration:** Unclear how AI tools fit into legal workflows (research, drafting, client communication), ethical obligations (competence, confidentiality), and malpractice liability
4. **Evaluation gaps:** Most legal AI benchmarks measure task accuracy (contract analysis, case outcome prediction) but ignore human factors (comprehension, verification effort, decision confidence, workflow disruption)
5. **Equity concerns:** Tools may perform poorly for underrepresented legal areas (immigration, family law) or non-English jurisdictions, exacerbating access-to-justice gaps

**Core Hypothesis:**  
Human-centered evaluation combining controlled experiments (legal professionals using AI assistants on realistic tasks) with longitudinal field studies (in-practice deployment) can identify critical failure modes (over-reliance, misaligned trust, workflow friction) and design interventions (explanations, uncertainty quantification, verification prompts) that improve decision quality by 15-30% vs. AI-alone or human-alone baselines while maintaining professional standards.

**Primary Goals (6-month horizon):**
- **Controlled user study:** 40-60 legal professionals (attorneys, paralegals, law students) complete 10-15 legal tasks with/without AI assistance, measuring accuracy, efficiency, trust calibration, and cognitive load
- **Field deployment:** 10-15 practitioners use AI assistant in real work for 4-8 weeks, with usage logs, reflective interviews, and client outcome tracking (IRB-approved)
- **Intervention design:** Test 3-5 human-centered features (confidence scores, source attribution, verification checklists, explanation interfaces) via A/B testing
- **Ethics compliance:** IRB approval, informed consent, data protection (attorney-client privilege), debiasing protocols
- **Publication:** CHI/CSCW (human-computer interaction) + FAccT (fairness/accountability) + legal informatics venue (ICAIL)

**Secondary Goals:**
- Benchmark creation: "LegalAssist-Human" dataset with expert annotations, error classifications, trust ratings
- Failure taxonomy: Categorize AI errors by severity (critical vs. minor), detectability (obvious vs. subtle), domain (contract vs. litigation)
- Equity audit: Performance across practice areas, jurisdictions, firm sizes
- Policy recommendations: Guidelines for bar associations on AI competence requirements

---

## 2. Experiments

### **Experiment 1: Controlled Task-Based Evaluation with Legal Professionals (Months 1-3)**

**Hypothesis:** AI assistance improves task completion speed by 30-50% but increases error rate by 10-20% due to over-reliance on incorrect outputs; trust calibration interventions (confidence scores, source citations) reduce errors by 15-25% while maintaining efficiency gains.

**Setup:**
- **Participants:** 40-60 legal professionals
  - **Recruitment:** Bar associations, law firms (small/medium/large), legal aid clinics, law schools
  - **Stratification:** Experience (0-3 years, 3-10 years, 10+ years), practice area (contracts, litigation, regulatory), firm type (BigLaw, solo, nonprofit)
  - **Compensation:** $150-300 for 2-3 hour session (market rate for legal consulting)
- **Tasks (10-15, realistic legal work):**
  1. **Contract review:** Identify unfavorable clauses in 10-page NDA (ground truth from expert panel)
  2. **Case research:** Find relevant precedents for novel fact pattern (compare to expert-curated list)
  3. **Legal memo drafting:** Write 2-page analysis of liability issue (scored by senior attorneys on rubric)
  4. **Statutory interpretation:** Apply ambiguous regulation to hypothetical scenario (compare to model answer)
  5. **Discovery document review:** Classify 50 emails as privileged/responsive/neither (ground truth from litigation dataset)
  6. **Client advice:** Draft response to client inquiry balancing legal risk and business goals (expert scoring)
- **Conditions (within-subjects design):**
  1. **Human-only (baseline):** Complete tasks without AI assistance
  2. **AI-only (comparison):** AI system completes tasks, human reviews output
  3. **Human+AI (standard):** Human uses AI assistant with standard interface (retrieval, generation, no special features)
  4. **Human+AI (calibrated):** AI provides confidence scores, source citations, uncertainty flags
  5. **Human+AI (verification):** AI + checklist prompting critical review ("Did you verify all case citations?")
- **AI system:** GPT-4-based legal assistant with RAG over legal databases (Westlaw, LexisNexis, or open alternatives like Case Law Access Project)
- **Procedure:**
  - Training: 30-min tutorial on AI system
  - Tasks: Randomized order, 10-15 min per task
  - Post-task survey: Confidence, perceived difficulty, trust in AI
  - Post-study interview: 20-30 min semi-structured (workflow integration, concerns, suggestions)
- **IRB considerations:**
  - **Approval:** Submit to university IRB (expedited review likely, minimal risk)
  - **Informed consent:** Explain data collection, anonymization, right to withdraw
  - **Confidentiality:** No real client data; use synthetic/public cases
  - **Deception:** None; participants aware of AI involvement

**Baselines:**
1. Human-only (no AI)
2. AI-only (no human review)
3. Majority voting among 3 attorneys (upper bound for human consensus)

**Evaluation Metrics:**
1. **Decision quality:**
   - **Accuracy:** % correct vs. ground truth (expert consensus)
   - **Error severity:** Critical (could cause malpractice) vs. minor (stylistic)
   - **Omission rate:** % of relevant issues missed
   - **Commission rate:** % of incorrect statements made
2. **Efficiency:**
   - **Time:** Minutes to complete task
   - **Effort:** NASA-TLX cognitive load scale
   - **Cost-effectiveness:** Accuracy per minute (quality-speed trade-off)
3. **Trust calibration:**
   - **Reliance:** % of AI suggestions accepted (should correlate with AI accuracy)
   - **Agreement-performance gap:** |P(accept AI) - P(AI correct)| (lower = better calibration)
   - **Overtrust index:** P(accept AI | AI wrong) (should be low)
   - **Undertrust index:** P(reject AI | AI correct) (should be low)
4. **Subjective:**
   - **Confidence:** Self-reported certainty in final answer (1-7 Likert)
   - **Trust:** "I trust this AI system" (1-7 Likert)
   - **Perceived usefulness:** System Usability Scale (SUS)
   - **Workflow fit:** "This tool fits my work practices" (1-7 Likert)
5. **Qualitative:**
   - **Thematic analysis:** Interview transcripts coded for themes (benefits, concerns, workarounds)
   - **Error attribution:** Do users blame AI, self, or unclear?

**Expected Outcomes:**
- **Human+AI (standard):** +40% speed, -5% accuracy (net positive but over-reliance errors)
- **Human+AI (calibrated):** +35% speed, +2% accuracy (confidence scores reduce over-reliance)
- **Human+AI (verification):** +25% speed, +8% accuracy (checklists slow down but improve quality)
- **Trust calibration:** Standard interface: agreement-performance gap 0.25-0.35; calibrated: 0.10-0.20
- **Experience effects:** Novices (0-3 years) show higher over-reliance (+15-20% error rate) than experts (10+ years, +5-10%)
- **Task variation:** AI most helpful for research (50% speedup), least for client advice (15% speedup, judgment-heavy)
- **Failure modes:** 60-80% of errors involve hallucinated citations, 20-30% misapplied precedents, 10-15% jurisdictional mistakes

**Ablations:**
- Confidence threshold: Show warnings only when AI confidence <70% vs. always show
- Explanation granularity: Sentence-level attribution vs. document-level vs. none
- Verification prompts: Generic checklist vs. task-specific (e.g., "Verify all citations in Westlaw")
- AI capability: GPT-4 vs. GPT-3.5 vs. domain-specific legal LLM (if available)

---

### **Experiment 2: Longitudinal Field Deployment (Months 3-6)**

**Hypothesis:** Real-world deployment reveals adoption barriers (workflow disruption, ethical concerns, client acceptance) and long-term behavioral changes (skill degradation, automation complacency) not captured in lab studies; interventions designed from Exp 1 improve sustained usage and decision quality.

**Setup:**
- **Participants:** 10-15 legal professionals
  - **Recruitment:** Partner with 3-5 law firms/legal aid organizations willing to pilot AI tools
  - **Selection criteria:** Diverse practice areas, willingness to share anonymized usage data, stable client base
  - **Compensation:** Free AI tool access ($500-1000/month value) + $500 participation fee
- **Deployment:** 4-8 weeks of real-world use
  - **Tool:** Best-performing interface from Exp 1 (likely calibrated or verification version)
  - **Integration:** Browser extension, Word/Google Docs plugin, or standalone web app
  - **Usage tracking:** Log queries, AI outputs, user edits, final documents (with consent)
- **Data collection:**
  - **Week 0:** Baseline survey (current workflows, pain points, AI attitudes)
  - **Weeks 1-8:** Continuous usage logging (queries, acceptance rate, time spent)
  - **Weeks 2, 4, 6, 8:** Brief pulse surveys (satisfaction, concerns, incidents)
  - **Week 8:** Exit interview (1 hour, semi-structured)
  - **Optional:** Client outcome tracking (case success, client satisfaction) if ethically/practically feasible
- **IRB considerations:**
  - **Enhanced protections:** Real client data involved (attorney-client privilege)
  - **Data minimization:** Log only non-privileged metadata (query type, time, acceptance rate), not full content
  - **Anonymization:** Strip identifying information (client names, case numbers) before analysis
  - **Informed consent:** Participants + clients (if tracking outcomes)
  - **Data security:** Encrypted storage, access controls, data retention limits (delete after study)
  - **Withdrawal:** Participants can withdraw anytime; data deleted upon request
  - **Monitoring:** Weekly check-ins to identify ethical concerns (e.g., AI suggesting unethical actions)

**Baselines:**
1. Pre-deployment baseline (participants' historical work, if available)
2. Control group (matched attorneys not using AI, if feasible)
3. Industry benchmarks (average case duration, client satisfaction scores)

**Evaluation Metrics:**
1. **Adoption:**
   - **Usage frequency:** Queries per day/week
   - **Retention:** % still using after 4, 6, 8 weeks
   - **Coverage:** % of work tasks where AI used (vs. ignored)
   - **Abandonment triggers:** Why do users stop using AI? (interview themes)
2. **Behavioral changes:**
   - **Reliance trajectory:** Does acceptance rate increase over time (automation complacency)?
   - **Verification effort:** Time spent checking AI outputs (should remain high)
   - **Skill maintenance:** Do users maintain legal research skills or delegate to AI?
3. **Workflow integration:**
   - **Friction points:** Tasks where AI causes delays or confusion (log analysis + interviews)
   - **Workarounds:** How do users adapt tool to fit workflows?
   - **Collaboration:** Do teams share AI outputs? How?
4. **Outcomes (if measurable):**
   - **Productivity:** Cases handled per week (vs. baseline)
   - **Quality:** Client satisfaction, case success rate (causal inference challenging; use matched controls if possible)
   - **Errors:** Self-reported or observed mistakes attributable to AI
5. **Ethics/professional concerns:**
   - **Malpractice risk:** Incidents where AI nearly caused harm
   - **Confidentiality:** Breaches or near-misses
   - **Competence:** Do users feel AI undermines professional judgment?
   - **Client disclosure:** Do attorneys tell clients about AI use? Should they?

**Expected Outcomes:**
- **Adoption:** 70-85% retention at 8 weeks; usage concentrated in research (60% of queries) and drafting (30%)
- **Behavioral drift:** Acceptance rate increases from 60% (week 1) to 75% (week 8), suggesting automation complacency
- **Verification decline:** Time spent checking outputs drops 20-30% over 8 weeks (concerning)
- **Workflow friction:** 40-60% report integration challenges (switching between tools, reformatting outputs)
- **Productivity:** 15-25% more cases handled (self-reported), but quality metrics inconclusive
- **Ethical incidents:** 2-5 near-misses (e.g., AI hallucinated citation almost submitted to court, caught by user)
- **Skill concerns:** 30-50% worry about de-skilling; junior attorneys most concerned

**Ablations:**
- Intervention timing: Calibration features present from day 1 vs. introduced at week 4 (habit formation)
- Training: 30-min tutorial vs. 2-hour workshop vs. ongoing coaching
- Peer support: Solo use vs. team-based deployment (collaborative verification)

---

### **Experiment 3: Trust Calibration Interventions (Months 2-5, parallel with Exp 1-2)**

**Hypothesis:** Explainability features (case law citations, confidence scores, reasoning traces) improve trust calibration by 20-35%, but only if designed for legal professionals' mental models (e.g., Bluebook citation format, jurisdiction-specific precedents).

**Setup:**
- **Intervention types (5-7 to test):**
  1. **Confidence scores:** Numeric (0-100%) or categorical (high/medium/low) confidence per statement
  2. **Source attribution:** Hyperlinks to cited cases, statutes, regulations (with Bluebook formatting)
  3. **Reasoning traces:** Step-by-step explanation of legal reasoning (rule → application → conclusion)
  4. **Uncertainty flags:** Highlight ambiguous/contested legal issues ("Courts are split on this issue")
  5. **Counterfactual explanations:** "If fact X changed, the outcome would be Y"
  6. **Verification checklists:** Task-specific prompts ("Have you verified: [✓] All citations? [✓] Jurisdiction matches?")
  7. **Comparison view:** Show AI output alongside human-written example or alternative AI interpretation
- **Design considerations:**
  - **Legal conventions:** Use Bluebook citations, jurisdiction labels (e.g., "9th Circuit, 2023")
  - **Cognitive load:** Avoid overwhelming users (max 3-4 features simultaneously)
  - **Actionability:** Features must enable verification (clickable citations, not just text)
- **Evaluation:** A/B test interventions in Exp 1 (controlled study) and Exp 2 (field deployment)
- **User research:** 10-15 semi-structured interviews with legal professionals to refine designs

**Baselines:**
1. No explanations (black-box AI outputs)
2. Generic explanations (not tailored to legal domain)
3. Human-written explanations (gold standard, if feasible)

**Evaluation Metrics:**
1. **Trust calibration:** Agreement-performance gap (lower = better)
2. **Verification behavior:**
   - **Citation checking:** % of users who verify ≥1 citation (log